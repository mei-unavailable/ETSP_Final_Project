2025-12-10 22:30:17,078: Using device: cuda:0
2025-12-10 22:30:17,080: Initializing RNN with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-10 22:30:17,778: Using torch.compile
2025-12-10 22:30:18,476: Initialized RNN decoding model
2025-12-10 22:30:18,476: OptimizedModule(
  (_orig_mod): UNetEnhancedModel(
    (unet): NeuralUNet(
      (inc): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (down1): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down2): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down3): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down4): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (up1): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up2): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up3): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up4): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (outc): OutConv(
        (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (decoder): GRUDecoder(
      (day_layer_activation): Softsign()
      (day_weights): ParameterList(
          (0): Parameter containing: [torch.float32 of size 512x512]
          (1): Parameter containing: [torch.float32 of size 512x512]
          (2): Parameter containing: [torch.float32 of size 512x512]
          (3): Parameter containing: [torch.float32 of size 512x512]
          (4): Parameter containing: [torch.float32 of size 512x512]
          (5): Parameter containing: [torch.float32 of size 512x512]
          (6): Parameter containing: [torch.float32 of size 512x512]
          (7): Parameter containing: [torch.float32 of size 512x512]
          (8): Parameter containing: [torch.float32 of size 512x512]
          (9): Parameter containing: [torch.float32 of size 512x512]
          (10): Parameter containing: [torch.float32 of size 512x512]
          (11): Parameter containing: [torch.float32 of size 512x512]
          (12): Parameter containing: [torch.float32 of size 512x512]
          (13): Parameter containing: [torch.float32 of size 512x512]
          (14): Parameter containing: [torch.float32 of size 512x512]
          (15): Parameter containing: [torch.float32 of size 512x512]
          (16): Parameter containing: [torch.float32 of size 512x512]
          (17): Parameter containing: [torch.float32 of size 512x512]
          (18): Parameter containing: [torch.float32 of size 512x512]
          (19): Parameter containing: [torch.float32 of size 512x512]
          (20): Parameter containing: [torch.float32 of size 512x512]
          (21): Parameter containing: [torch.float32 of size 512x512]
          (22): Parameter containing: [torch.float32 of size 512x512]
          (23): Parameter containing: [torch.float32 of size 512x512]
          (24): Parameter containing: [torch.float32 of size 512x512]
          (25): Parameter containing: [torch.float32 of size 512x512]
          (26): Parameter containing: [torch.float32 of size 512x512]
          (27): Parameter containing: [torch.float32 of size 512x512]
          (28): Parameter containing: [torch.float32 of size 512x512]
          (29): Parameter containing: [torch.float32 of size 512x512]
          (30): Parameter containing: [torch.float32 of size 512x512]
          (31): Parameter containing: [torch.float32 of size 512x512]
          (32): Parameter containing: [torch.float32 of size 512x512]
          (33): Parameter containing: [torch.float32 of size 512x512]
          (34): Parameter containing: [torch.float32 of size 512x512]
          (35): Parameter containing: [torch.float32 of size 512x512]
          (36): Parameter containing: [torch.float32 of size 512x512]
          (37): Parameter containing: [torch.float32 of size 512x512]
          (38): Parameter containing: [torch.float32 of size 512x512]
          (39): Parameter containing: [torch.float32 of size 512x512]
          (40): Parameter containing: [torch.float32 of size 512x512]
          (41): Parameter containing: [torch.float32 of size 512x512]
          (42): Parameter containing: [torch.float32 of size 512x512]
          (43): Parameter containing: [torch.float32 of size 512x512]
          (44): Parameter containing: [torch.float32 of size 512x512]
      )
      (day_biases): ParameterList(
          (0): Parameter containing: [torch.float32 of size 1x512]
          (1): Parameter containing: [torch.float32 of size 1x512]
          (2): Parameter containing: [torch.float32 of size 1x512]
          (3): Parameter containing: [torch.float32 of size 1x512]
          (4): Parameter containing: [torch.float32 of size 1x512]
          (5): Parameter containing: [torch.float32 of size 1x512]
          (6): Parameter containing: [torch.float32 of size 1x512]
          (7): Parameter containing: [torch.float32 of size 1x512]
          (8): Parameter containing: [torch.float32 of size 1x512]
          (9): Parameter containing: [torch.float32 of size 1x512]
          (10): Parameter containing: [torch.float32 of size 1x512]
          (11): Parameter containing: [torch.float32 of size 1x512]
          (12): Parameter containing: [torch.float32 of size 1x512]
          (13): Parameter containing: [torch.float32 of size 1x512]
          (14): Parameter containing: [torch.float32 of size 1x512]
          (15): Parameter containing: [torch.float32 of size 1x512]
          (16): Parameter containing: [torch.float32 of size 1x512]
          (17): Parameter containing: [torch.float32 of size 1x512]
          (18): Parameter containing: [torch.float32 of size 1x512]
          (19): Parameter containing: [torch.float32 of size 1x512]
          (20): Parameter containing: [torch.float32 of size 1x512]
          (21): Parameter containing: [torch.float32 of size 1x512]
          (22): Parameter containing: [torch.float32 of size 1x512]
          (23): Parameter containing: [torch.float32 of size 1x512]
          (24): Parameter containing: [torch.float32 of size 1x512]
          (25): Parameter containing: [torch.float32 of size 1x512]
          (26): Parameter containing: [torch.float32 of size 1x512]
          (27): Parameter containing: [torch.float32 of size 1x512]
          (28): Parameter containing: [torch.float32 of size 1x512]
          (29): Parameter containing: [torch.float32 of size 1x512]
          (30): Parameter containing: [torch.float32 of size 1x512]
          (31): Parameter containing: [torch.float32 of size 1x512]
          (32): Parameter containing: [torch.float32 of size 1x512]
          (33): Parameter containing: [torch.float32 of size 1x512]
          (34): Parameter containing: [torch.float32 of size 1x512]
          (35): Parameter containing: [torch.float32 of size 1x512]
          (36): Parameter containing: [torch.float32 of size 1x512]
          (37): Parameter containing: [torch.float32 of size 1x512]
          (38): Parameter containing: [torch.float32 of size 1x512]
          (39): Parameter containing: [torch.float32 of size 1x512]
          (40): Parameter containing: [torch.float32 of size 1x512]
          (41): Parameter containing: [torch.float32 of size 1x512]
          (42): Parameter containing: [torch.float32 of size 1x512]
          (43): Parameter containing: [torch.float32 of size 1x512]
          (44): Parameter containing: [torch.float32 of size 1x512]
      )
      (day_layer_dropout): Dropout(p=0.4, inplace=False)
      (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
      (out): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
2025-12-10 22:30:18,479: Model has 61,577,002 parameters
2025-12-10 22:30:18,479: Model has 11,819,520 day-specific parameters | 19.19% of total parameters
2025-12-10 22:30:27,032: Successfully initialized datasets
2025-12-10 22:30:46,195: Train batch 0: loss: 635.91 grad norm: 255.91 time: 18.599
2025-12-10 22:30:46,196: Running test after training batch: 0
2025-12-10 22:34:37,254: Val batch 0: PER (avg): 1.1562 CTC Loss (avg): 716.0728 time: 231.058
2025-12-10 22:34:37,254: t15.2023.08.11 val PER: 1.0000
2025-12-10 22:34:37,254: t15.2023.08.13 val PER: 1.0135
2025-12-10 22:34:37,254: t15.2023.08.18 val PER: 1.0805
2025-12-10 22:34:37,254: t15.2023.08.20 val PER: 1.0770
2025-12-10 22:34:37,254: t15.2023.08.25 val PER: 1.1069
2025-12-10 22:34:37,254: t15.2023.08.27 val PER: 0.9759
2025-12-10 22:34:37,254: t15.2023.09.01 val PER: 1.1469
2025-12-10 22:34:37,254: t15.2023.09.03 val PER: 1.1010
2025-12-10 22:34:37,254: t15.2023.09.24 val PER: 1.2087
2025-12-10 22:34:37,254: t15.2023.09.29 val PER: 1.1780
2025-12-10 22:34:37,255: t15.2023.10.01 val PER: 1.0231
2025-12-10 22:34:37,255: t15.2023.10.06 val PER: 1.2605
2025-12-10 22:34:37,255: t15.2023.10.08 val PER: 0.9851
2025-12-10 22:34:37,255: t15.2023.10.13 val PER: 1.1482
2025-12-10 22:34:37,255: t15.2023.10.15 val PER: 1.1655
2025-12-10 22:34:37,255: t15.2023.10.20 val PER: 1.2383
2025-12-10 22:34:37,255: t15.2023.10.22 val PER: 1.0913
2025-12-10 22:34:37,255: t15.2023.11.03 val PER: 1.2524
2025-12-10 22:34:37,255: t15.2023.11.04 val PER: 1.3652
2025-12-10 22:34:37,255: t15.2023.11.17 val PER: 1.5381
2025-12-10 22:34:37,255: t15.2023.11.19 val PER: 1.3673
2025-12-10 22:34:37,255: t15.2023.11.26 val PER: 1.2174
2025-12-10 22:34:37,255: t15.2023.12.03 val PER: 1.1828
2025-12-10 22:34:37,255: t15.2023.12.08 val PER: 1.2390
2025-12-10 22:34:37,255: t15.2023.12.10 val PER: 1.2576
2025-12-10 22:34:37,255: t15.2023.12.17 val PER: 1.0468
2025-12-10 22:34:37,255: t15.2023.12.29 val PER: 1.1160
2025-12-10 22:34:37,255: t15.2024.02.25 val PER: 1.2261
2025-12-10 22:34:37,256: t15.2024.03.03 val PER: 1.0000
2025-12-10 22:34:37,256: t15.2024.03.08 val PER: 1.1607
2025-12-10 22:34:37,256: t15.2024.03.15 val PER: 1.1126
2025-12-10 22:34:37,256: t15.2024.03.17 val PER: 1.1283
2025-12-10 22:34:37,256: t15.2024.04.25 val PER: 1.0000
2025-12-10 22:34:37,256: t15.2024.04.28 val PER: 1.0000
2025-12-10 22:34:37,256: t15.2024.05.10 val PER: 1.1456
2025-12-10 22:34:37,256: t15.2024.06.14 val PER: 1.2476
2025-12-10 22:34:37,256: t15.2024.07.19 val PER: 0.9525
2025-12-10 22:34:37,256: t15.2024.07.21 val PER: 1.2221
2025-12-10 22:34:37,256: t15.2024.07.28 val PER: 1.2529
2025-12-10 22:34:37,256: t15.2025.01.10 val PER: 0.9807
2025-12-10 22:34:37,256: t15.2025.01.12 val PER: 1.4296
2025-12-10 22:34:37,256: t15.2025.03.14 val PER: 0.9393
2025-12-10 22:34:37,256: t15.2025.03.16 val PER: 1.2448
2025-12-10 22:34:37,256: t15.2025.03.30 val PER: 1.1253
2025-12-10 22:34:37,256: t15.2025.04.13 val PER: 1.1227
2025-12-10 22:34:37,256: New best test PER inf --> 1.1562
2025-12-10 22:34:37,256: Checkpointing model
2025-12-10 22:34:37,784: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
