2025-12-15 19:58:37,291: Using device: cuda:0
2025-12-15 19:58:37,356: Initialized RNN decoding model
2025-12-15 19:58:37,356: ConformerDecoder(
  (day_activation): Softsign()
  (day_dropout): Dropout(p=0.2, inplace=False)
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (input_proj): Sequential(
    (0): Linear(in_features=7168, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.2, inplace=False)
  )
  (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (conformer): Conformer(
    (conformer_layers): ModuleList(
      (0-3): 4 x ConformerLayer(
        (ffn1): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_dropout): Dropout(p=0.2, inplace=False)
        (conv_module): _ConvolutionModule(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (sequential): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
            (1): GLU(dim=1)
            (2): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): SiLU()
            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
            (6): Dropout(p=0.2, inplace=False)
          )
        )
        (ffn2): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_dropout): Dropout(p=0.1, inplace=False)
  (out): Linear(in_features=256, out_features=41, bias=True)
)
2025-12-15 19:58:37,358: Model has 17,641,001 parameters
2025-12-15 19:58:37,358: Model has 11,819,520 day-specific parameters | 67.00% of total parameters
2025-12-15 19:58:45,695: Successfully initialized datasets
2025-12-15 19:58:47,148: Train batch 0: loss: 678.50 grad norm: 309.14 time: 1.057
2025-12-15 19:58:47,148: Running test after training batch: 0
2025-12-15 19:58:58,718: Val batch 0: PER (avg): 4.5674 CTC Loss (avg): 711.6542 time: 11.570
2025-12-15 19:58:58,719: t15.2023.08.11 val PER: 1.0000
2025-12-15 19:58:58,719: t15.2023.08.13 val PER: 3.6653
2025-12-15 19:58:58,719: t15.2023.08.18 val PER: 4.1802
2025-12-15 19:58:58,719: t15.2023.08.20 val PER: 3.8674
2025-12-15 19:58:58,719: t15.2023.08.25 val PER: 4.0452
2025-12-15 19:58:58,719: t15.2023.08.27 val PER: 3.6141
2025-12-15 19:58:58,719: t15.2023.09.01 val PER: 4.2127
2025-12-15 19:58:58,719: t15.2023.09.03 val PER: 3.9537
2025-12-15 19:58:58,719: t15.2023.09.24 val PER: 4.8823
2025-12-15 19:58:58,719: t15.2023.09.29 val PER: 4.8922
2025-12-15 19:58:58,719: t15.2023.10.01 val PER: 3.7483
2025-12-15 19:58:58,719: t15.2023.10.06 val PER: 4.7309
2025-12-15 19:58:58,719: t15.2023.10.08 val PER: 3.5737
2025-12-15 19:58:58,719: t15.2023.10.13 val PER: 4.4880
2025-12-15 19:58:58,719: t15.2023.10.15 val PER: 4.6948
2025-12-15 19:58:58,720: t15.2023.10.20 val PER: 4.9362
2025-12-15 19:58:58,720: t15.2023.10.22 val PER: 4.7283
2025-12-15 19:58:58,720: t15.2023.11.03 val PER: 5.3602
2025-12-15 19:58:58,720: t15.2023.11.04 val PER: 6.6416
2025-12-15 19:58:58,720: t15.2023.11.17 val PER: 6.2504
2025-12-15 19:58:58,720: t15.2023.11.19 val PER: 5.1697
2025-12-15 19:58:58,720: t15.2023.11.26 val PER: 4.5826
2025-12-15 19:58:58,720: t15.2023.12.03 val PER: 4.3834
2025-12-15 19:58:58,720: t15.2023.12.08 val PER: 4.7636
2025-12-15 19:58:58,720: t15.2023.12.10 val PER: 5.6071
2025-12-15 19:58:58,720: t15.2023.12.17 val PER: 4.2079
2025-12-15 19:58:58,720: t15.2023.12.29 val PER: 4.3892
2025-12-15 19:58:58,720: t15.2024.02.25 val PER: 4.6419
2025-12-15 19:58:58,720: t15.2024.03.03 val PER: 1.0000
2025-12-15 19:58:58,720: t15.2024.03.08 val PER: 4.1110
2025-12-15 19:58:58,720: t15.2024.03.15 val PER: 4.3221
2025-12-15 19:58:58,720: t15.2024.03.17 val PER: 4.7755
2025-12-15 19:58:58,720: t15.2024.04.25 val PER: 1.0000
2025-12-15 19:58:58,720: t15.2024.04.28 val PER: 1.0000
2025-12-15 19:58:58,721: t15.2024.05.10 val PER: 4.4324
2025-12-15 19:58:58,721: t15.2024.06.14 val PER: 5.2082
2025-12-15 19:58:58,721: t15.2024.07.19 val PER: 3.2808
2025-12-15 19:58:58,721: t15.2024.07.21 val PER: 5.3828
2025-12-15 19:58:58,721: t15.2024.07.28 val PER: 5.4272
2025-12-15 19:58:58,721: t15.2025.01.10 val PER: 3.5220
2025-12-15 19:58:58,721: t15.2025.01.12 val PER: 5.7236
2025-12-15 19:58:58,721: t15.2025.03.14 val PER: 3.2899
2025-12-15 19:58:58,721: t15.2025.03.16 val PER: 5.9215
2025-12-15 19:58:58,721: t15.2025.03.30 val PER: 4.1655
2025-12-15 19:58:58,721: t15.2025.04.13 val PER: 5.1498
2025-12-15 19:58:58,721: New best test PER inf --> 4.5674
2025-12-15 19:58:58,721: Checkpointing model
2025-12-15 19:58:58,842: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_195836/checkpoint/best_checkpoint
2025-12-15 19:59:15,383: Train batch 200: loss: 549.35 grad norm: 1065.16 time: 0.064
2025-12-15 19:59:26,733: Train batch 400: loss: 304.64 grad norm: 1427.58 time: 0.066
2025-12-15 19:59:38,904: Train batch 600: loss: 114.62 grad norm: 387.60 time: 0.059
2025-12-15 19:59:53,081: Train batch 800: loss: 97.72 grad norm: 19.02 time: 0.055
2025-12-15 20:00:06,956: Train batch 1000: loss: 103.44 grad norm: 18.61 time: 0.069
2025-12-15 20:00:21,734: Train batch 1200: loss: 59.00 grad norm: 19.01 time: 0.073
2025-12-15 20:00:36,700: Train batch 1400: loss: 87.63 grad norm: 19.98 time: 0.061
2025-12-15 20:00:49,931: Train batch 1600: loss: 78.68 grad norm: 39.85 time: 0.072
2025-12-15 20:01:04,630: Train batch 1800: loss: 61.38 grad norm: 37.31 time: 0.071
2025-12-15 20:01:18,816: Train batch 2000: loss: 72.65 grad norm: 40.00 time: 0.076
2025-12-15 20:01:18,817: Running test after training batch: 2000
2025-12-15 20:01:27,934: Val batch 2000: PER (avg): 0.6821 CTC Loss (avg): 94.8697 time: 9.117
2025-12-15 20:01:27,934: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:01:27,935: t15.2023.08.13 val PER: 0.6507
2025-12-15 20:01:27,935: t15.2023.08.18 val PER: 0.6739
2025-12-15 20:01:27,935: t15.2023.08.20 val PER: 0.6624
2025-12-15 20:01:27,935: t15.2023.08.25 val PER: 0.6581
2025-12-15 20:01:27,935: t15.2023.08.27 val PER: 0.6881
2025-12-15 20:01:27,935: t15.2023.09.01 val PER: 0.6518
2025-12-15 20:01:27,935: t15.2023.09.03 val PER: 0.6639
2025-12-15 20:01:27,935: t15.2023.09.24 val PER: 0.6590
2025-12-15 20:01:27,935: t15.2023.09.29 val PER: 0.6809
2025-12-15 20:01:27,935: t15.2023.10.01 val PER: 0.6882
2025-12-15 20:01:27,935: t15.2023.10.06 val PER: 0.6717
2025-12-15 20:01:27,935: t15.2023.10.08 val PER: 0.6942
2025-12-15 20:01:27,935: t15.2023.10.13 val PER: 0.7261
2025-12-15 20:01:27,935: t15.2023.10.15 val PER: 0.6849
2025-12-15 20:01:27,935: t15.2023.10.20 val PER: 0.6812
2025-12-15 20:01:27,935: t15.2023.10.22 val PER: 0.6670
2025-12-15 20:01:27,935: t15.2023.11.03 val PER: 0.6818
2025-12-15 20:01:27,935: t15.2023.11.04 val PER: 0.6007
2025-12-15 20:01:27,936: t15.2023.11.17 val PER: 0.6345
2025-12-15 20:01:27,936: t15.2023.11.19 val PER: 0.6228
2025-12-15 20:01:27,936: t15.2023.11.26 val PER: 0.7457
2025-12-15 20:01:27,936: t15.2023.12.03 val PER: 0.6796
2025-12-15 20:01:27,936: t15.2023.12.08 val PER: 0.6924
2025-12-15 20:01:27,936: t15.2023.12.10 val PER: 0.6912
2025-12-15 20:01:27,936: t15.2023.12.17 val PER: 0.6778
2025-12-15 20:01:27,936: t15.2023.12.29 val PER: 0.6760
2025-12-15 20:01:27,936: t15.2024.02.25 val PER: 0.6770
2025-12-15 20:01:27,936: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:01:27,936: t15.2024.03.08 val PER: 0.6671
2025-12-15 20:01:27,936: t15.2024.03.15 val PER: 0.7079
2025-12-15 20:01:27,936: t15.2024.03.17 val PER: 0.6743
2025-12-15 20:01:27,936: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:01:27,936: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:01:27,936: t15.2024.05.10 val PER: 0.6850
2025-12-15 20:01:27,936: t15.2024.06.14 val PER: 0.6703
2025-12-15 20:01:27,936: t15.2024.07.19 val PER: 0.7047
2025-12-15 20:01:27,936: t15.2024.07.21 val PER: 0.6752
2025-12-15 20:01:27,937: t15.2024.07.28 val PER: 0.6868
2025-12-15 20:01:27,937: t15.2025.01.10 val PER: 0.7300
2025-12-15 20:01:27,937: t15.2025.01.12 val PER: 0.6551
2025-12-15 20:01:27,937: t15.2025.03.14 val PER: 0.7086
2025-12-15 20:01:27,937: t15.2025.03.16 val PER: 0.6754
2025-12-15 20:01:27,937: t15.2025.03.30 val PER: 0.7138
2025-12-15 20:01:27,937: t15.2025.04.13 val PER: 0.6847
2025-12-15 20:01:27,937: New best test PER 4.5674 --> 0.6821
2025-12-15 20:01:27,937: Checkpointing model
2025-12-15 20:01:28,387: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_195836/checkpoint/best_checkpoint
2025-12-15 20:01:43,067: Train batch 2200: loss: 87.72 grad norm: 47.59 time: 0.085
2025-12-15 20:01:57,219: Train batch 2400: loss: 81.00 grad norm: 55.52 time: 0.082
2025-12-15 20:02:11,324: Train batch 2600: loss: 68.35 grad norm: 42.25 time: 0.067
2025-12-15 20:02:25,989: Train batch 2800: loss: 64.67 grad norm: 52.21 time: 0.067
2025-12-15 20:02:40,135: Train batch 3000: loss: 76.47 grad norm: 64.99 time: 0.057
2025-12-15 20:02:54,197: Train batch 3200: loss: 72.82 grad norm: 51.82 time: 0.083
2025-12-15 20:03:08,125: Train batch 3400: loss: 56.23 grad norm: 49.43 time: 0.056
2025-12-15 20:03:21,533: Train batch 3600: loss: 55.79 grad norm: 50.40 time: 0.042
2025-12-15 20:03:35,154: Train batch 3800: loss: 55.52 grad norm: 55.06 time: 0.048
2025-12-15 20:03:49,596: Train batch 4000: loss: 47.15 grad norm: 54.82 time: 0.070
2025-12-15 20:03:49,596: Running test after training batch: 4000
2025-12-15 20:03:58,643: Val batch 4000: PER (avg): 0.4104 CTC Loss (avg): 53.4293 time: 9.046
2025-12-15 20:03:58,643: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:03:58,643: t15.2023.08.13 val PER: 0.3732
2025-12-15 20:03:58,644: t15.2023.08.18 val PER: 0.3571
2025-12-15 20:03:58,644: t15.2023.08.20 val PER: 0.3368
2025-12-15 20:03:58,644: t15.2023.08.25 val PER: 0.3117
2025-12-15 20:03:58,644: t15.2023.08.27 val PER: 0.4469
2025-12-15 20:03:58,644: t15.2023.09.01 val PER: 0.3433
2025-12-15 20:03:58,644: t15.2023.09.03 val PER: 0.3990
2025-12-15 20:03:58,644: t15.2023.09.24 val PER: 0.3459
2025-12-15 20:03:58,644: t15.2023.09.29 val PER: 0.3829
2025-12-15 20:03:58,644: t15.2023.10.01 val PER: 0.4333
2025-12-15 20:03:58,644: t15.2023.10.06 val PER: 0.3681
2025-12-15 20:03:58,644: t15.2023.10.08 val PER: 0.4276
2025-12-15 20:03:58,644: t15.2023.10.13 val PER: 0.4639
2025-12-15 20:03:58,644: t15.2023.10.15 val PER: 0.4041
2025-12-15 20:03:58,644: t15.2023.10.20 val PER: 0.4295
2025-12-15 20:03:58,644: t15.2023.10.22 val PER: 0.3708
2025-12-15 20:03:58,644: t15.2023.11.03 val PER: 0.4261
2025-12-15 20:03:58,644: t15.2023.11.04 val PER: 0.2423
2025-12-15 20:03:58,644: t15.2023.11.17 val PER: 0.3546
2025-12-15 20:03:58,644: t15.2023.11.19 val PER: 0.2794
2025-12-15 20:03:58,645: t15.2023.11.26 val PER: 0.4457
2025-12-15 20:03:58,645: t15.2023.12.03 val PER: 0.3992
2025-12-15 20:03:58,645: t15.2023.12.08 val PER: 0.4088
2025-12-15 20:03:58,645: t15.2023.12.10 val PER: 0.3982
2025-12-15 20:03:58,645: t15.2023.12.17 val PER: 0.4449
2025-12-15 20:03:58,645: t15.2023.12.29 val PER: 0.4228
2025-12-15 20:03:58,645: t15.2024.02.25 val PER: 0.3497
2025-12-15 20:03:58,645: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:03:58,645: t15.2024.03.08 val PER: 0.4908
2025-12-15 20:03:58,645: t15.2024.03.15 val PER: 0.4478
2025-12-15 20:03:58,645: t15.2024.03.17 val PER: 0.4191
2025-12-15 20:03:58,645: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:03:58,645: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:03:58,645: t15.2024.05.10 val PER: 0.4101
2025-12-15 20:03:58,645: t15.2024.06.14 val PER: 0.3880
2025-12-15 20:03:58,645: t15.2024.07.19 val PER: 0.4931
2025-12-15 20:03:58,645: t15.2024.07.21 val PER: 0.3434
2025-12-15 20:03:58,645: t15.2024.07.28 val PER: 0.3750
2025-12-15 20:03:58,645: t15.2025.01.10 val PER: 0.5496
2025-12-15 20:03:58,646: t15.2025.01.12 val PER: 0.4149
2025-12-15 20:03:58,646: t15.2025.03.14 val PER: 0.5577
2025-12-15 20:03:58,646: t15.2025.03.16 val PER: 0.4529
2025-12-15 20:03:58,646: t15.2025.03.30 val PER: 0.5379
2025-12-15 20:03:58,646: t15.2025.04.13 val PER: 0.4579
2025-12-15 20:03:58,646: New best test PER 0.6821 --> 0.4104
2025-12-15 20:03:58,646: Checkpointing model
2025-12-15 20:03:59,066: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_195836/checkpoint/best_checkpoint
2025-12-15 20:04:12,446: Train batch 4200: loss: 39.18 grad norm: 52.79 time: 0.081
2025-12-15 20:04:27,051: Train batch 4400: loss: 63.17 grad norm: 64.14 time: 0.071
2025-12-15 20:04:41,334: Train batch 4600: loss: 57.32 grad norm: 66.78 time: 0.062
2025-12-15 20:04:55,625: Train batch 4800: loss: 46.36 grad norm: 53.12 time: 0.057
2025-12-15 20:05:10,071: Train batch 5000: loss: 58.87 grad norm: 70.30 time: 0.043
2025-12-15 20:05:24,591: Train batch 5200: loss: 45.34 grad norm: 58.85 time: 0.053
2025-12-15 20:05:38,733: Train batch 5400: loss: 42.79 grad norm: 68.29 time: 0.068
2025-12-15 20:05:53,140: Train batch 5600: loss: 41.05 grad norm: 59.89 time: 0.074
2025-12-15 20:06:07,377: Train batch 5800: loss: 50.98 grad norm: 67.43 time: 0.075
2025-12-15 20:06:22,091: Train batch 6000: loss: 33.27 grad norm: 54.37 time: 0.069
2025-12-15 20:06:22,091: Running test after training batch: 6000
2025-12-15 20:06:31,264: Val batch 6000: PER (avg): 0.2780 CTC Loss (avg): 31.8036 time: 9.172
2025-12-15 20:06:31,264: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:06:31,264: t15.2023.08.13 val PER: 0.2287
2025-12-15 20:06:31,264: t15.2023.08.18 val PER: 0.2540
2025-12-15 20:06:31,264: t15.2023.08.20 val PER: 0.1986
2025-12-15 20:06:31,264: t15.2023.08.25 val PER: 0.2199
2025-12-15 20:06:31,264: t15.2023.08.27 val PER: 0.3248
2025-12-15 20:06:31,264: t15.2023.09.01 val PER: 0.2135
2025-12-15 20:06:31,264: t15.2023.09.03 val PER: 0.2803
2025-12-15 20:06:31,264: t15.2023.09.24 val PER: 0.2184
2025-12-15 20:06:31,264: t15.2023.09.29 val PER: 0.2636
2025-12-15 20:06:31,265: t15.2023.10.01 val PER: 0.2853
2025-12-15 20:06:31,265: t15.2023.10.06 val PER: 0.2411
2025-12-15 20:06:31,265: t15.2023.10.08 val PER: 0.3194
2025-12-15 20:06:31,265: t15.2023.10.13 val PER: 0.3274
2025-12-15 20:06:31,265: t15.2023.10.15 val PER: 0.2650
2025-12-15 20:06:31,265: t15.2023.10.20 val PER: 0.3154
2025-12-15 20:06:31,265: t15.2023.10.22 val PER: 0.2194
2025-12-15 20:06:31,265: t15.2023.11.03 val PER: 0.2904
2025-12-15 20:06:31,265: t15.2023.11.04 val PER: 0.0853
2025-12-15 20:06:31,265: t15.2023.11.17 val PER: 0.1680
2025-12-15 20:06:31,265: t15.2023.11.19 val PER: 0.1697
2025-12-15 20:06:31,265: t15.2023.11.26 val PER: 0.2978
2025-12-15 20:06:31,265: t15.2023.12.03 val PER: 0.2647
2025-12-15 20:06:31,265: t15.2023.12.08 val PER: 0.2656
2025-12-15 20:06:31,265: t15.2023.12.10 val PER: 0.2273
2025-12-15 20:06:31,265: t15.2023.12.17 val PER: 0.3316
2025-12-15 20:06:31,265: t15.2023.12.29 val PER: 0.2642
2025-12-15 20:06:31,265: t15.2024.02.25 val PER: 0.2247
2025-12-15 20:06:31,265: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:06:31,266: t15.2024.03.08 val PER: 0.3457
2025-12-15 20:06:31,266: t15.2024.03.15 val PER: 0.3233
2025-12-15 20:06:31,266: t15.2024.03.17 val PER: 0.3006
2025-12-15 20:06:31,266: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:06:31,266: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:06:31,266: t15.2024.05.10 val PER: 0.2675
2025-12-15 20:06:31,266: t15.2024.06.14 val PER: 0.3107
2025-12-15 20:06:31,266: t15.2024.07.19 val PER: 0.3448
2025-12-15 20:06:31,266: t15.2024.07.21 val PER: 0.2138
2025-12-15 20:06:31,266: t15.2024.07.28 val PER: 0.2566
2025-12-15 20:06:31,266: t15.2025.01.10 val PER: 0.4077
2025-12-15 20:06:31,266: t15.2025.01.12 val PER: 0.2640
2025-12-15 20:06:31,266: t15.2025.03.14 val PER: 0.4290
2025-12-15 20:06:31,266: t15.2025.03.16 val PER: 0.3233
2025-12-15 20:06:31,266: t15.2025.03.30 val PER: 0.4322
2025-12-15 20:06:31,266: t15.2025.04.13 val PER: 0.3452
2025-12-15 20:06:31,266: New best test PER 0.4104 --> 0.2780
2025-12-15 20:06:31,266: Checkpointing model
2025-12-15 20:06:31,751: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_195836/checkpoint/best_checkpoint
2025-12-15 20:06:45,251: Train batch 6200: loss: 38.92 grad norm: 64.45 time: 0.081
2025-12-15 20:06:59,812: Train batch 6400: loss: 35.97 grad norm: 47.60 time: 0.073
2025-12-15 20:07:14,233: Train batch 6600: loss: 40.33 grad norm: 59.06 time: 0.045
2025-12-15 20:07:28,218: Train batch 6800: loss: 42.93 grad norm: 60.85 time: 0.070
2025-12-15 20:07:42,641: Train batch 7000: loss: 33.39 grad norm: 52.11 time: 0.071
