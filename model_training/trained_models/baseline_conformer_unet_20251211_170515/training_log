2025-12-11 17:05:15,671: Using device: cuda:0
2025-12-11 17:05:15,673: Initializing Conformer with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-11 17:05:16,124: Initialized RNN decoding model
2025-12-11 17:05:16,125: UNetEnhancedModel(
  (unet): NeuralUNet(
    (inc): DoubleConv(
      (double_conv): Sequential(
        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (down1): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (outc): OutConv(
      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (decoder): ConformerDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.4, inplace=False)
    (projection): Linear(in_features=7168, out_features=512, bias=True)
    (conformer): Conformer(
      (conformer_layers): ModuleList(
        (0-5): 6 x ConformerLayer(
          (ffn1): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_dropout): Dropout(p=0.4, inplace=False)
          (conv_module): _ConvolutionModule(
            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (sequential): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
              (1): GLU(dim=1)
              (2): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (4): SiLU()
              (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (6): Dropout(p=0.4, inplace=False)
            )
          )
          (ffn2): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (out): Linear(in_features=512, out_features=41, bias=True)
  )
)
2025-12-11 17:05:16,128: Model has 69,136,170 parameters
2025-12-11 17:05:16,129: Model has 11,819,520 day-specific parameters | 17.10% of total parameters
2025-12-11 17:05:24,767: Successfully initialized datasets
2025-12-11 17:05:26,293: Train batch 0: loss: 746.96 grad norm: 1309.27 time: 1.074
2025-12-11 17:05:26,293: Running test after training batch: 0
2025-12-11 17:05:54,525: Val batch 0: PER (avg): 2.9674 CTC Loss (avg): 653.7608 time: 28.232
2025-12-11 17:05:54,525: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:05:54,525: t15.2023.08.13 val PER: 2.5229
2025-12-11 17:05:54,525: t15.2023.08.18 val PER: 2.7133
2025-12-11 17:05:54,526: t15.2023.08.20 val PER: 2.5655
2025-12-11 17:05:54,526: t15.2023.08.25 val PER: 2.8343
2025-12-11 17:05:54,526: t15.2023.08.27 val PER: 2.4775
2025-12-11 17:05:54,526: t15.2023.09.01 val PER: 2.6696
2025-12-11 17:05:54,526: t15.2023.09.03 val PER: 2.7494
2025-12-11 17:05:54,526: t15.2023.09.24 val PER: 3.1917
2025-12-11 17:05:54,526: t15.2023.09.29 val PER: 3.1615
2025-12-11 17:05:54,526: t15.2023.10.01 val PER: 2.4161
2025-12-11 17:05:54,526: t15.2023.10.06 val PER: 3.2476
2025-12-11 17:05:54,526: t15.2023.10.08 val PER: 2.6225
2025-12-11 17:05:54,526: t15.2023.10.13 val PER: 2.5516
2025-12-11 17:05:54,526: t15.2023.10.15 val PER: 2.8833
2025-12-11 17:05:54,526: t15.2023.10.20 val PER: 3.0738
2025-12-11 17:05:54,526: t15.2023.10.22 val PER: 2.4209
2025-12-11 17:05:54,526: t15.2023.11.03 val PER: 3.5421
2025-12-11 17:05:54,526: t15.2023.11.04 val PER: 3.8942
2025-12-11 17:05:54,526: t15.2023.11.17 val PER: 4.4028
2025-12-11 17:05:54,526: t15.2023.11.19 val PER: 3.9820
2025-12-11 17:05:54,526: t15.2023.11.26 val PER: 3.3029
2025-12-11 17:05:54,526: t15.2023.12.03 val PER: 2.9422
2025-12-11 17:05:54,527: t15.2023.12.08 val PER: 3.0293
2025-12-11 17:05:54,527: t15.2023.12.10 val PER: 3.3719
2025-12-11 17:05:54,527: t15.2023.12.17 val PER: 2.6320
2025-12-11 17:05:54,527: t15.2023.12.29 val PER: 3.1434
2025-12-11 17:05:54,527: t15.2024.02.25 val PER: 2.9185
2025-12-11 17:05:54,527: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:05:54,527: t15.2024.03.08 val PER: 3.0114
2025-12-11 17:05:54,527: t15.2024.03.15 val PER: 2.6823
2025-12-11 17:05:54,527: t15.2024.03.17 val PER: 2.8354
2025-12-11 17:05:54,527: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:05:54,527: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:05:54,527: t15.2024.05.10 val PER: 2.9421
2025-12-11 17:05:54,527: t15.2024.06.14 val PER: 3.1562
2025-12-11 17:05:54,527: t15.2024.07.19 val PER: 2.1055
2025-12-11 17:05:54,527: t15.2024.07.21 val PER: 3.4552
2025-12-11 17:05:54,527: t15.2024.07.28 val PER: 3.3875
2025-12-11 17:05:54,527: t15.2025.01.10 val PER: 2.4146
2025-12-11 17:05:54,527: t15.2025.01.12 val PER: 3.6921
2025-12-11 17:05:54,527: t15.2025.03.14 val PER: 1.9970
2025-12-11 17:05:54,528: t15.2025.03.16 val PER: 3.9306
2025-12-11 17:05:54,528: t15.2025.03.30 val PER: 2.6897
2025-12-11 17:05:54,528: t15.2025.04.13 val PER: 3.7061
2025-12-11 17:05:54,528: New best test PER inf --> 2.9674
2025-12-11 17:05:54,528: Checkpointing model
2025-12-11 17:05:54,999: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251211_170515/checkpoint/best_checkpoint
