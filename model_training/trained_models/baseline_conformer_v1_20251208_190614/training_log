2025-12-08 19:06:14,331: Using device: cuda:0
2025-12-08 19:06:14,549: Using torch.compile
2025-12-08 19:06:15,278: Initialized RNN decoding model
2025-12-08 19:06:15,278: OptimizedModule(
  (_orig_mod): ConformerDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (projection): Linear(in_features=7168, out_features=512, bias=True)
    (conformer): Conformer(
      (conformer_layers): ModuleList(
        (0-5): 6 x ConformerLayer(
          (ffn1): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.2, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.2, inplace=False)
            )
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_dropout): Dropout(p=0.2, inplace=False)
          (conv_module): _ConvolutionModule(
            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (sequential): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
              (1): GLU(dim=1)
              (2): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (4): SiLU()
              (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (6): Dropout(p=0.2, inplace=False)
            )
          )
          (ffn2): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.2, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.2, inplace=False)
            )
          )
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (out): Linear(in_features=512, out_features=41, bias=True)
  )
)
2025-12-08 19:06:15,281: Model has 51,874,345 parameters
2025-12-08 19:06:15,281: Model has 11,819,520 day-specific parameters | 22.78% of total parameters
2025-12-08 19:06:24,098: Successfully initialized datasets
2025-12-08 19:07:08,332: Train batch 0: loss: 750.69 grad norm: 1060.01 time: 42.419
2025-12-08 19:07:08,332: Running test after training batch: 0
2025-12-08 19:08:26,347: Val batch 0: PER (avg): 2.7595 CTC Loss (avg): 654.4660 time: 78.015
2025-12-08 19:08:26,347: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:08:26,347: t15.2023.08.13 val PER: 2.5426
2025-12-08 19:08:26,347: t15.2023.08.18 val PER: 2.6186
2025-12-08 19:08:26,347: t15.2023.08.20 val PER: 2.5171
2025-12-08 19:08:26,348: t15.2023.08.25 val PER: 3.0331
2025-12-08 19:08:26,348: t15.2023.08.27 val PER: 2.5723
2025-12-08 19:08:26,348: t15.2023.09.01 val PER: 2.9018
2025-12-08 19:08:26,348: t15.2023.09.03 val PER: 2.8135
2025-12-08 19:08:26,348: t15.2023.09.24 val PER: 2.6602
2025-12-08 19:08:26,348: t15.2023.09.29 val PER: 2.5980
2025-12-08 19:08:26,348: t15.2023.10.01 val PER: 1.9736
2025-12-08 19:08:26,348: t15.2023.10.06 val PER: 2.5856
2025-12-08 19:08:26,348: t15.2023.10.08 val PER: 2.3762
2025-12-08 19:08:26,348: t15.2023.10.13 val PER: 3.0124
2025-12-08 19:08:26,348: t15.2023.10.15 val PER: 2.4614
2025-12-08 19:08:26,348: t15.2023.10.20 val PER: 3.2450
2025-12-08 19:08:26,348: t15.2023.10.22 val PER: 3.1503
2025-12-08 19:08:26,348: t15.2023.11.03 val PER: 3.1384
2025-12-08 19:08:26,348: t15.2023.11.04 val PER: 4.3413
2025-12-08 19:08:26,348: t15.2023.11.17 val PER: 4.7061
2025-12-08 19:08:26,348: t15.2023.11.19 val PER: 3.5150
2025-12-08 19:08:26,348: t15.2023.11.26 val PER: 2.3812
2025-12-08 19:08:26,349: t15.2023.12.03 val PER: 2.8120
2025-12-08 19:08:26,349: t15.2023.12.08 val PER: 2.8589
2025-12-08 19:08:26,349: t15.2023.12.10 val PER: 3.5519
2025-12-08 19:08:26,349: t15.2023.12.17 val PER: 2.5042
2025-12-08 19:08:26,349: t15.2023.12.29 val PER: 2.2183
2025-12-08 19:08:26,349: t15.2024.02.25 val PER: 2.1713
2025-12-08 19:08:26,349: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:08:26,349: t15.2024.03.08 val PER: 2.5718
2025-12-08 19:08:26,349: t15.2024.03.15 val PER: 2.3377
2025-12-08 19:08:26,349: t15.2024.03.17 val PER: 3.0481
2025-12-08 19:08:26,349: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:08:26,349: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:08:26,349: t15.2024.05.10 val PER: 2.4264
2025-12-08 19:08:26,349: t15.2024.06.14 val PER: 3.6719
2025-12-08 19:08:26,349: t15.2024.07.19 val PER: 1.8392
2025-12-08 19:08:26,349: t15.2024.07.21 val PER: 2.9103
2025-12-08 19:08:26,349: t15.2024.07.28 val PER: 3.1831
2025-12-08 19:08:26,349: t15.2025.01.10 val PER: 2.5358
2025-12-08 19:08:26,350: t15.2025.01.12 val PER: 3.1848
2025-12-08 19:08:26,350: t15.2025.03.14 val PER: 2.4201
2025-12-08 19:08:26,350: t15.2025.03.16 val PER: 4.4817
2025-12-08 19:08:26,350: t15.2025.03.30 val PER: 2.0379
2025-12-08 19:08:26,350: t15.2025.04.13 val PER: 3.1997
2025-12-08 19:08:26,350: New best test PER inf --> 2.7595
2025-12-08 19:08:26,350: Checkpointing model
2025-12-08 19:08:26,769: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:10:14,984: Train batch 200: loss: 75.43 grad norm: 51.94 time: 0.140
2025-12-08 19:10:53,233: Train batch 400: loss: 30.96 grad norm: 46.78 time: 0.214
2025-12-08 19:11:28,245: Train batch 600: loss: 24.97 grad norm: 47.82 time: 0.091
2025-12-08 19:11:59,330: Train batch 800: loss: 7.62 grad norm: 28.91 time: 0.232
2025-12-08 19:12:33,595: Train batch 1000: loss: 8.42 grad norm: 32.87 time: 0.151
2025-12-08 19:13:11,381: Train batch 1200: loss: 7.87 grad norm: 32.53 time: 0.230
2025-12-08 19:13:42,862: Train batch 1400: loss: 6.78 grad norm: 26.82 time: 0.141
2025-12-08 19:14:14,231: Train batch 1600: loss: 5.49 grad norm: 22.72 time: 0.153
2025-12-08 19:14:46,975: Train batch 1800: loss: 6.34 grad norm: 21.25 time: 0.194
2025-12-08 19:15:23,100: Train batch 2000: loss: 4.90 grad norm: 20.25 time: 0.259
2025-12-08 19:15:23,101: Running test after training batch: 2000
2025-12-08 19:15:36,924: Val batch 2000: PER (avg): 0.2046 CTC Loss (avg): 27.1614 time: 13.822
2025-12-08 19:15:36,924: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:15:36,924: t15.2023.08.13 val PER: 0.1590
2025-12-08 19:15:36,924: t15.2023.08.18 val PER: 0.1693
2025-12-08 19:15:36,924: t15.2023.08.20 val PER: 0.1390
2025-12-08 19:15:36,924: t15.2023.08.25 val PER: 0.1355
2025-12-08 19:15:36,924: t15.2023.08.27 val PER: 0.2106
2025-12-08 19:15:36,925: t15.2023.09.01 val PER: 0.1242
2025-12-08 19:15:36,925: t15.2023.09.03 val PER: 0.2007
2025-12-08 19:15:36,925: t15.2023.09.24 val PER: 0.1578
2025-12-08 19:15:36,925: t15.2023.09.29 val PER: 0.1787
2025-12-08 19:15:36,925: t15.2023.10.01 val PER: 0.2299
2025-12-08 19:15:36,925: t15.2023.10.06 val PER: 0.1895
2025-12-08 19:15:36,925: t15.2023.10.08 val PER: 0.2395
2025-12-08 19:15:36,925: t15.2023.10.13 val PER: 0.2374
2025-12-08 19:15:36,925: t15.2023.10.15 val PER: 0.1912
2025-12-08 19:15:36,925: t15.2023.10.20 val PER: 0.2785
2025-12-08 19:15:36,925: t15.2023.10.22 val PER: 0.1882
2025-12-08 19:15:36,925: t15.2023.11.03 val PER: 0.2551
2025-12-08 19:15:36,925: t15.2023.11.04 val PER: 0.0648
2025-12-08 19:15:36,925: t15.2023.11.17 val PER: 0.0871
2025-12-08 19:15:36,925: t15.2023.11.19 val PER: 0.1098
2025-12-08 19:15:36,925: t15.2023.11.26 val PER: 0.1942
2025-12-08 19:15:36,926: t15.2023.12.03 val PER: 0.1271
2025-12-08 19:15:36,926: t15.2023.12.08 val PER: 0.1405
2025-12-08 19:15:36,926: t15.2023.12.10 val PER: 0.1511
2025-12-08 19:15:36,926: t15.2023.12.17 val PER: 0.2339
2025-12-08 19:15:36,926: t15.2023.12.29 val PER: 0.2052
2025-12-08 19:15:36,926: t15.2024.02.25 val PER: 0.1334
2025-12-08 19:15:36,926: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:15:36,926: t15.2024.03.08 val PER: 0.3158
2025-12-08 19:15:36,926: t15.2024.03.15 val PER: 0.2889
2025-12-08 19:15:36,926: t15.2024.03.17 val PER: 0.1757
2025-12-08 19:15:36,926: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:15:36,926: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:15:36,926: t15.2024.05.10 val PER: 0.2333
2025-12-08 19:15:36,926: t15.2024.06.14 val PER: 0.2476
2025-12-08 19:15:36,926: t15.2024.07.19 val PER: 0.2676
2025-12-08 19:15:36,926: t15.2024.07.21 val PER: 0.1607
2025-12-08 19:15:36,926: t15.2024.07.28 val PER: 0.1926
2025-12-08 19:15:36,927: t15.2025.01.10 val PER: 0.3678
2025-12-08 19:15:36,927: t15.2025.01.12 val PER: 0.1840
2025-12-08 19:15:36,927: t15.2025.03.14 val PER: 0.3964
2025-12-08 19:15:36,927: t15.2025.03.16 val PER: 0.2408
2025-12-08 19:15:36,927: t15.2025.03.30 val PER: 0.3345
2025-12-08 19:15:36,927: t15.2025.04.13 val PER: 0.2796
2025-12-08 19:15:36,927: New best test PER 2.7595 --> 0.2046
2025-12-08 19:15:36,927: Checkpointing model
2025-12-08 19:15:37,987: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:16:08,465: Train batch 2200: loss: 5.43 grad norm: 19.49 time: 0.173
2025-12-08 19:16:43,315: Train batch 2400: loss: 5.16 grad norm: 21.35 time: 0.238
2025-12-08 19:17:15,198: Train batch 2600: loss: 2.26 grad norm: 13.00 time: 0.087
2025-12-08 19:17:46,953: Train batch 2800: loss: 1.99 grad norm: 11.41 time: 0.111
2025-12-08 19:18:18,646: Train batch 3000: loss: 3.16 grad norm: 14.38 time: 0.121
2025-12-08 19:18:51,300: Train batch 3200: loss: 1.93 grad norm: 11.43 time: 0.072
2025-12-08 19:19:22,949: Train batch 3400: loss: 3.90 grad norm: 18.03 time: 0.102
2025-12-08 19:19:55,062: Train batch 3600: loss: 1.67 grad norm: 10.42 time: 0.123
2025-12-08 19:20:27,072: Train batch 3800: loss: 2.46 grad norm: 12.90 time: 0.158
2025-12-08 19:20:57,976: Train batch 4000: loss: 0.82 grad norm: 7.04 time: 0.107
2025-12-08 19:20:57,976: Running test after training batch: 4000
2025-12-08 19:21:08,751: Val batch 4000: PER (avg): 0.1707 CTC Loss (avg): 28.6822 time: 10.774
2025-12-08 19:21:08,751: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:21:08,751: t15.2023.08.13 val PER: 0.1320
2025-12-08 19:21:08,751: t15.2023.08.18 val PER: 0.1090
2025-12-08 19:21:08,751: t15.2023.08.20 val PER: 0.0953
2025-12-08 19:21:08,751: t15.2023.08.25 val PER: 0.1310
2025-12-08 19:21:08,751: t15.2023.08.27 val PER: 0.1929
2025-12-08 19:21:08,751: t15.2023.09.01 val PER: 0.0901
2025-12-08 19:21:08,751: t15.2023.09.03 val PER: 0.1793
2025-12-08 19:21:08,751: t15.2023.09.24 val PER: 0.1335
2025-12-08 19:21:08,751: t15.2023.09.29 val PER: 0.1538
2025-12-08 19:21:08,751: t15.2023.10.01 val PER: 0.1968
2025-12-08 19:21:08,752: t15.2023.10.06 val PER: 0.1335
2025-12-08 19:21:08,752: t15.2023.10.08 val PER: 0.2314
2025-12-08 19:21:08,752: t15.2023.10.13 val PER: 0.2126
2025-12-08 19:21:08,752: t15.2023.10.15 val PER: 0.1582
2025-12-08 19:21:08,752: t15.2023.10.20 val PER: 0.1879
2025-12-08 19:21:08,752: t15.2023.10.22 val PER: 0.1570
2025-12-08 19:21:08,752: t15.2023.11.03 val PER: 0.2232
2025-12-08 19:21:08,752: t15.2023.11.04 val PER: 0.0683
2025-12-08 19:21:08,752: t15.2023.11.17 val PER: 0.0622
2025-12-08 19:21:08,752: t15.2023.11.19 val PER: 0.0838
2025-12-08 19:21:08,752: t15.2023.11.26 val PER: 0.1623
2025-12-08 19:21:08,752: t15.2023.12.03 val PER: 0.1197
2025-12-08 19:21:08,752: t15.2023.12.08 val PER: 0.1238
2025-12-08 19:21:08,752: t15.2023.12.10 val PER: 0.1248
2025-12-08 19:21:08,752: t15.2023.12.17 val PER: 0.1871
2025-12-08 19:21:08,752: t15.2023.12.29 val PER: 0.1544
2025-12-08 19:21:08,752: t15.2024.02.25 val PER: 0.1166
2025-12-08 19:21:08,752: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:21:08,752: t15.2024.03.08 val PER: 0.2447
2025-12-08 19:21:08,753: t15.2024.03.15 val PER: 0.2295
2025-12-08 19:21:08,753: t15.2024.03.17 val PER: 0.1430
2025-12-08 19:21:08,753: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:21:08,753: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:21:08,753: t15.2024.05.10 val PER: 0.1842
2025-12-08 19:21:08,753: t15.2024.06.14 val PER: 0.2035
2025-12-08 19:21:08,753: t15.2024.07.19 val PER: 0.2096
2025-12-08 19:21:08,753: t15.2024.07.21 val PER: 0.1214
2025-12-08 19:21:08,753: t15.2024.07.28 val PER: 0.1529
2025-12-08 19:21:08,753: t15.2025.01.10 val PER: 0.3567
2025-12-08 19:21:08,753: t15.2025.01.12 val PER: 0.1540
2025-12-08 19:21:08,753: t15.2025.03.14 val PER: 0.3772
2025-12-08 19:21:08,753: t15.2025.03.16 val PER: 0.2173
2025-12-08 19:21:08,753: t15.2025.03.30 val PER: 0.3057
2025-12-08 19:21:08,753: t15.2025.04.13 val PER: 0.2582
2025-12-08 19:21:08,753: New best test PER 0.2046 --> 0.1707
2025-12-08 19:21:08,753: Checkpointing model
2025-12-08 19:21:09,808: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:21:40,916: Train batch 4200: loss: 1.07 grad norm: 7.72 time: 0.129
2025-12-08 19:22:12,393: Train batch 4400: loss: 1.36 grad norm: 10.45 time: 0.132
2025-12-08 19:22:45,161: Train batch 4600: loss: 1.05 grad norm: 12.03 time: 0.147
2025-12-08 19:23:17,408: Train batch 4800: loss: 1.41 grad norm: 9.87 time: 0.118
2025-12-08 19:23:49,357: Train batch 5000: loss: 1.55 grad norm: 12.01 time: 0.104
2025-12-08 19:24:22,127: Train batch 5200: loss: 1.48 grad norm: 11.51 time: 0.149
2025-12-08 19:24:54,933: Train batch 5400: loss: 1.84 grad norm: 13.60 time: 0.107
2025-12-08 19:25:26,587: Train batch 5600: loss: 0.58 grad norm: 5.80 time: 0.086
2025-12-08 19:25:57,800: Train batch 5800: loss: 0.94 grad norm: 7.03 time: 0.131
2025-12-08 19:26:29,476: Train batch 6000: loss: 1.00 grad norm: 8.59 time: 0.122
2025-12-08 19:26:29,477: Running test after training batch: 6000
2025-12-08 19:26:40,558: Val batch 6000: PER (avg): 0.1530 CTC Loss (avg): 28.4704 time: 11.081
2025-12-08 19:26:40,558: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:26:40,558: t15.2023.08.13 val PER: 0.1091
2025-12-08 19:26:40,559: t15.2023.08.18 val PER: 0.0964
2025-12-08 19:26:40,559: t15.2023.08.20 val PER: 0.0834
2025-12-08 19:26:40,559: t15.2023.08.25 val PER: 0.0979
2025-12-08 19:26:40,559: t15.2023.08.27 val PER: 0.1881
2025-12-08 19:26:40,559: t15.2023.09.01 val PER: 0.0657
2025-12-08 19:26:40,559: t15.2023.09.03 val PER: 0.1568
2025-12-08 19:26:40,559: t15.2023.09.24 val PER: 0.1080
2025-12-08 19:26:40,559: t15.2023.09.29 val PER: 0.1404
2025-12-08 19:26:40,559: t15.2023.10.01 val PER: 0.1803
2025-12-08 19:26:40,559: t15.2023.10.06 val PER: 0.1109
2025-12-08 19:26:40,559: t15.2023.10.08 val PER: 0.2152
2025-12-08 19:26:40,559: t15.2023.10.13 val PER: 0.2064
2025-12-08 19:26:40,559: t15.2023.10.15 val PER: 0.1437
2025-12-08 19:26:40,559: t15.2023.10.20 val PER: 0.2248
2025-12-08 19:26:40,559: t15.2023.10.22 val PER: 0.1336
2025-12-08 19:26:40,559: t15.2023.11.03 val PER: 0.2280
2025-12-08 19:26:40,560: t15.2023.11.04 val PER: 0.0751
2025-12-08 19:26:40,560: t15.2023.11.17 val PER: 0.0684
2025-12-08 19:26:40,560: t15.2023.11.19 val PER: 0.0798
2025-12-08 19:26:40,560: t15.2023.11.26 val PER: 0.1159
2025-12-08 19:26:40,560: t15.2023.12.03 val PER: 0.1082
2025-12-08 19:26:40,560: t15.2023.12.08 val PER: 0.0999
2025-12-08 19:26:40,560: t15.2023.12.10 val PER: 0.0933
2025-12-08 19:26:40,560: t15.2023.12.17 val PER: 0.2048
2025-12-08 19:26:40,560: t15.2023.12.29 val PER: 0.1414
2025-12-08 19:26:40,560: t15.2024.02.25 val PER: 0.1138
2025-12-08 19:26:40,560: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:26:40,560: t15.2024.03.08 val PER: 0.2205
2025-12-08 19:26:40,560: t15.2024.03.15 val PER: 0.2158
2025-12-08 19:26:40,560: t15.2024.03.17 val PER: 0.1241
2025-12-08 19:26:40,560: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:26:40,560: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:26:40,561: t15.2024.05.10 val PER: 0.1337
2025-12-08 19:26:40,561: t15.2024.06.14 val PER: 0.1688
2025-12-08 19:26:40,561: t15.2024.07.19 val PER: 0.1925
2025-12-08 19:26:40,561: t15.2024.07.21 val PER: 0.0986
2025-12-08 19:26:40,561: t15.2024.07.28 val PER: 0.1265
2025-12-08 19:26:40,561: t15.2025.01.10 val PER: 0.3196
2025-12-08 19:26:40,561: t15.2025.01.12 val PER: 0.1232
2025-12-08 19:26:40,561: t15.2025.03.14 val PER: 0.3402
2025-12-08 19:26:40,561: t15.2025.03.16 val PER: 0.2147
2025-12-08 19:26:40,561: t15.2025.03.30 val PER: 0.2908
2025-12-08 19:26:40,561: t15.2025.04.13 val PER: 0.2368
2025-12-08 19:26:40,561: New best test PER 0.1707 --> 0.1530
2025-12-08 19:26:40,561: Checkpointing model
2025-12-08 19:26:41,407: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:27:13,392: Train batch 6200: loss: 0.76 grad norm: 7.54 time: 0.109
2025-12-08 19:27:45,049: Train batch 6400: loss: 0.86 grad norm: 8.17 time: 0.090
2025-12-08 19:28:18,376: Train batch 6600: loss: 1.09 grad norm: 8.66 time: 0.108
2025-12-08 19:28:51,181: Train batch 6800: loss: 0.99 grad norm: 7.18 time: 0.082
2025-12-08 19:29:23,714: Train batch 7000: loss: 1.29 grad norm: 8.22 time: 0.078
2025-12-08 19:29:56,391: Train batch 7200: loss: 0.74 grad norm: 7.04 time: 0.116
2025-12-08 19:30:29,681: Train batch 7400: loss: 1.18 grad norm: 9.16 time: 0.098
2025-12-08 19:31:02,789: Train batch 7600: loss: 1.26 grad norm: 10.03 time: 0.106
2025-12-08 19:31:34,620: Train batch 7800: loss: 1.30 grad norm: 9.24 time: 0.117
2025-12-08 19:32:07,225: Train batch 8000: loss: 0.65 grad norm: 6.28 time: 0.115
2025-12-08 19:32:07,226: Running test after training batch: 8000
2025-12-08 19:32:18,095: Val batch 8000: PER (avg): 0.1453 CTC Loss (avg): 28.1237 time: 10.868
2025-12-08 19:32:18,095: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:32:18,095: t15.2023.08.13 val PER: 0.1040
2025-12-08 19:32:18,095: t15.2023.08.18 val PER: 0.1081
2025-12-08 19:32:18,095: t15.2023.08.20 val PER: 0.0770
2025-12-08 19:32:18,095: t15.2023.08.25 val PER: 0.0964
2025-12-08 19:32:18,095: t15.2023.08.27 val PER: 0.1559
2025-12-08 19:32:18,095: t15.2023.09.01 val PER: 0.0690
2025-12-08 19:32:18,095: t15.2023.09.03 val PER: 0.1627
2025-12-08 19:32:18,095: t15.2023.09.24 val PER: 0.1080
2025-12-08 19:32:18,096: t15.2023.09.29 val PER: 0.1378
2025-12-08 19:32:18,096: t15.2023.10.01 val PER: 0.1532
2025-12-08 19:32:18,096: t15.2023.10.06 val PER: 0.1012
2025-12-08 19:32:18,096: t15.2023.10.08 val PER: 0.2084
2025-12-08 19:32:18,096: t15.2023.10.13 val PER: 0.2017
2025-12-08 19:32:18,096: t15.2023.10.15 val PER: 0.1463
2025-12-08 19:32:18,096: t15.2023.10.20 val PER: 0.2416
2025-12-08 19:32:18,096: t15.2023.10.22 val PER: 0.1225
2025-12-08 19:32:18,096: t15.2023.11.03 val PER: 0.1920
2025-12-08 19:32:18,096: t15.2023.11.04 val PER: 0.0512
2025-12-08 19:32:18,096: t15.2023.11.17 val PER: 0.0467
2025-12-08 19:32:18,096: t15.2023.11.19 val PER: 0.0878
2025-12-08 19:32:18,096: t15.2023.11.26 val PER: 0.0928
2025-12-08 19:32:18,096: t15.2023.12.03 val PER: 0.0924
2025-12-08 19:32:18,096: t15.2023.12.08 val PER: 0.0686
2025-12-08 19:32:18,096: t15.2023.12.10 val PER: 0.0657
2025-12-08 19:32:18,096: t15.2023.12.17 val PER: 0.1663
2025-12-08 19:32:18,096: t15.2023.12.29 val PER: 0.1311
2025-12-08 19:32:18,097: t15.2024.02.25 val PER: 0.0871
2025-12-08 19:32:18,097: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:32:18,097: t15.2024.03.08 val PER: 0.2361
2025-12-08 19:32:18,097: t15.2024.03.15 val PER: 0.2008
2025-12-08 19:32:18,097: t15.2024.03.17 val PER: 0.1102
2025-12-08 19:32:18,097: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:32:18,097: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:32:18,097: t15.2024.05.10 val PER: 0.1828
2025-12-08 19:32:18,097: t15.2024.06.14 val PER: 0.1845
2025-12-08 19:32:18,097: t15.2024.07.19 val PER: 0.2063
2025-12-08 19:32:18,097: t15.2024.07.21 val PER: 0.1014
2025-12-08 19:32:18,097: t15.2024.07.28 val PER: 0.1272
2025-12-08 19:32:18,097: t15.2025.01.10 val PER: 0.3209
2025-12-08 19:32:18,097: t15.2025.01.12 val PER: 0.1247
2025-12-08 19:32:18,097: t15.2025.03.14 val PER: 0.3254
2025-12-08 19:32:18,097: t15.2025.03.16 val PER: 0.1937
2025-12-08 19:32:18,097: t15.2025.03.30 val PER: 0.2954
2025-12-08 19:32:18,097: t15.2025.04.13 val PER: 0.2325
2025-12-08 19:32:18,097: New best test PER 0.1530 --> 0.1453
2025-12-08 19:32:18,098: Checkpointing model
2025-12-08 19:32:19,228: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:32:51,596: Train batch 8200: loss: 0.51 grad norm: 5.64 time: 0.134
2025-12-08 19:33:24,551: Train batch 8400: loss: 1.38 grad norm: 8.73 time: 0.148
2025-12-08 19:33:57,352: Train batch 8600: loss: 0.90 grad norm: 7.34 time: 0.139
2025-12-08 19:34:29,825: Train batch 8800: loss: 1.06 grad norm: 7.78 time: 0.081
2025-12-08 19:35:03,558: Train batch 9000: loss: 0.98 grad norm: 8.33 time: 0.138
2025-12-08 19:35:36,763: Train batch 9200: loss: 0.56 grad norm: 4.82 time: 0.143
2025-12-08 19:36:09,672: Train batch 9400: loss: 0.49 grad norm: 6.14 time: 0.145
2025-12-08 19:36:42,018: Train batch 9600: loss: 0.30 grad norm: 4.23 time: 0.152
2025-12-08 19:37:15,122: Train batch 9800: loss: 0.19 grad norm: 3.55 time: 0.136
2025-12-08 19:37:47,266: Train batch 10000: loss: 0.45 grad norm: 9.70 time: 0.107
2025-12-08 19:37:47,267: Running test after training batch: 10000
2025-12-08 19:37:58,100: Val batch 10000: PER (avg): 0.1431 CTC Loss (avg): 28.2900 time: 10.833
2025-12-08 19:37:58,100: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:37:58,100: t15.2023.08.13 val PER: 0.1185
2025-12-08 19:37:58,100: t15.2023.08.18 val PER: 0.1140
2025-12-08 19:37:58,100: t15.2023.08.20 val PER: 0.0675
2025-12-08 19:37:58,100: t15.2023.08.25 val PER: 0.0949
2025-12-08 19:37:58,100: t15.2023.08.27 val PER: 0.1785
2025-12-08 19:37:58,100: t15.2023.09.01 val PER: 0.0641
2025-12-08 19:37:58,101: t15.2023.09.03 val PER: 0.1401
2025-12-08 19:37:58,101: t15.2023.09.24 val PER: 0.1117
2025-12-08 19:37:58,101: t15.2023.09.29 val PER: 0.1423
2025-12-08 19:37:58,101: t15.2023.10.01 val PER: 0.1585
2025-12-08 19:37:58,101: t15.2023.10.06 val PER: 0.1033
2025-12-08 19:37:58,101: t15.2023.10.08 val PER: 0.2070
2025-12-08 19:37:58,101: t15.2023.10.13 val PER: 0.1916
2025-12-08 19:37:58,101: t15.2023.10.15 val PER: 0.1411
2025-12-08 19:37:58,101: t15.2023.10.20 val PER: 0.2248
2025-12-08 19:37:58,101: t15.2023.10.22 val PER: 0.1192
2025-12-08 19:37:58,101: t15.2023.11.03 val PER: 0.2042
2025-12-08 19:37:58,101: t15.2023.11.04 val PER: 0.0341
2025-12-08 19:37:58,101: t15.2023.11.17 val PER: 0.0467
2025-12-08 19:37:58,101: t15.2023.11.19 val PER: 0.0938
2025-12-08 19:37:58,101: t15.2023.11.26 val PER: 0.0775
2025-12-08 19:37:58,101: t15.2023.12.03 val PER: 0.0987
2025-12-08 19:37:58,101: t15.2023.12.08 val PER: 0.0932
2025-12-08 19:37:58,101: t15.2023.12.10 val PER: 0.0631
2025-12-08 19:37:58,101: t15.2023.12.17 val PER: 0.1590
2025-12-08 19:37:58,102: t15.2023.12.29 val PER: 0.1201
2025-12-08 19:37:58,102: t15.2024.02.25 val PER: 0.0801
2025-12-08 19:37:58,102: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:37:58,102: t15.2024.03.08 val PER: 0.2304
2025-12-08 19:37:58,102: t15.2024.03.15 val PER: 0.2158
2025-12-08 19:37:58,102: t15.2024.03.17 val PER: 0.1276
2025-12-08 19:37:58,102: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:37:58,102: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:37:58,102: t15.2024.05.10 val PER: 0.1471
2025-12-08 19:37:58,102: t15.2024.06.14 val PER: 0.1609
2025-12-08 19:37:58,102: t15.2024.07.19 val PER: 0.2024
2025-12-08 19:37:58,102: t15.2024.07.21 val PER: 0.0862
2025-12-08 19:37:58,102: t15.2024.07.28 val PER: 0.1199
2025-12-08 19:37:58,102: t15.2025.01.10 val PER: 0.3003
2025-12-08 19:37:58,102: t15.2025.01.12 val PER: 0.1201
2025-12-08 19:37:58,102: t15.2025.03.14 val PER: 0.3151
2025-12-08 19:37:58,102: t15.2025.03.16 val PER: 0.1976
2025-12-08 19:37:58,102: t15.2025.03.30 val PER: 0.2621
2025-12-08 19:37:58,102: t15.2025.04.13 val PER: 0.2368
2025-12-08 19:37:58,103: New best test PER 0.1453 --> 0.1431
2025-12-08 19:37:58,103: Checkpointing model
2025-12-08 19:37:59,215: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:38:31,692: Train batch 10200: loss: 0.90 grad norm: 7.00 time: 0.121
2025-12-08 19:39:04,725: Train batch 10400: loss: 0.14 grad norm: 2.17 time: 0.103
2025-12-08 19:39:37,590: Train batch 10600: loss: 0.11 grad norm: 1.83 time: 0.130
2025-12-08 19:40:09,195: Train batch 10800: loss: 0.46 grad norm: 5.36 time: 0.103
2025-12-08 19:40:42,130: Train batch 11000: loss: 0.34 grad norm: 3.78 time: 0.153
2025-12-08 19:41:14,985: Train batch 11200: loss: 0.51 grad norm: 5.92 time: 0.140
2025-12-08 19:41:47,822: Train batch 11400: loss: 0.34 grad norm: 6.02 time: 0.109
2025-12-08 19:42:19,709: Train batch 11600: loss: 0.39 grad norm: 5.14 time: 0.154
2025-12-08 19:42:52,214: Train batch 11800: loss: 0.19 grad norm: 2.74 time: 0.129
2025-12-08 19:43:25,459: Train batch 12000: loss: 0.86 grad norm: 7.56 time: 0.088
2025-12-08 19:43:25,460: Running test after training batch: 12000
2025-12-08 19:43:35,900: Val batch 12000: PER (avg): 0.1390 CTC Loss (avg): 29.7474 time: 10.441
2025-12-08 19:43:35,901: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:43:35,901: t15.2023.08.13 val PER: 0.0988
2025-12-08 19:43:35,901: t15.2023.08.18 val PER: 0.0880
2025-12-08 19:43:35,901: t15.2023.08.20 val PER: 0.0659
2025-12-08 19:43:35,901: t15.2023.08.25 val PER: 0.0843
2025-12-08 19:43:35,901: t15.2023.08.27 val PER: 0.1479
2025-12-08 19:43:35,901: t15.2023.09.01 val PER: 0.0674
2025-12-08 19:43:35,901: t15.2023.09.03 val PER: 0.1354
2025-12-08 19:43:35,901: t15.2023.09.24 val PER: 0.1177
2025-12-08 19:43:35,901: t15.2023.09.29 val PER: 0.1308
2025-12-08 19:43:35,901: t15.2023.10.01 val PER: 0.1678
2025-12-08 19:43:35,901: t15.2023.10.06 val PER: 0.0958
2025-12-08 19:43:35,901: t15.2023.10.08 val PER: 0.1908
2025-12-08 19:43:35,901: t15.2023.10.13 val PER: 0.1893
2025-12-08 19:43:35,901: t15.2023.10.15 val PER: 0.1430
2025-12-08 19:43:35,901: t15.2023.10.20 val PER: 0.2047
2025-12-08 19:43:35,901: t15.2023.10.22 val PER: 0.1370
2025-12-08 19:43:35,902: t15.2023.11.03 val PER: 0.2239
2025-12-08 19:43:35,902: t15.2023.11.04 val PER: 0.0375
2025-12-08 19:43:35,902: t15.2023.11.17 val PER: 0.0373
2025-12-08 19:43:35,902: t15.2023.11.19 val PER: 0.1198
2025-12-08 19:43:35,902: t15.2023.11.26 val PER: 0.0732
2025-12-08 19:43:35,902: t15.2023.12.03 val PER: 0.0882
2025-12-08 19:43:35,902: t15.2023.12.08 val PER: 0.0726
2025-12-08 19:43:35,902: t15.2023.12.10 val PER: 0.0880
2025-12-08 19:43:35,902: t15.2023.12.17 val PER: 0.1424
2025-12-08 19:43:35,902: t15.2023.12.29 val PER: 0.1215
2025-12-08 19:43:35,902: t15.2024.02.25 val PER: 0.0632
2025-12-08 19:43:35,902: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:43:35,902: t15.2024.03.08 val PER: 0.2319
2025-12-08 19:43:35,902: t15.2024.03.15 val PER: 0.2095
2025-12-08 19:43:35,902: t15.2024.03.17 val PER: 0.1304
2025-12-08 19:43:35,902: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:43:35,902: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:43:35,902: t15.2024.05.10 val PER: 0.1441
2025-12-08 19:43:35,902: t15.2024.06.14 val PER: 0.1593
2025-12-08 19:43:35,902: t15.2024.07.19 val PER: 0.1846
2025-12-08 19:43:35,903: t15.2024.07.21 val PER: 0.0841
2025-12-08 19:43:35,903: t15.2024.07.28 val PER: 0.1191
2025-12-08 19:43:35,903: t15.2025.01.10 val PER: 0.2934
2025-12-08 19:43:35,903: t15.2025.01.12 val PER: 0.1078
2025-12-08 19:43:35,903: t15.2025.03.14 val PER: 0.3062
2025-12-08 19:43:35,903: t15.2025.03.16 val PER: 0.1793
2025-12-08 19:43:35,903: t15.2025.03.30 val PER: 0.2644
2025-12-08 19:43:35,903: t15.2025.04.13 val PER: 0.2496
2025-12-08 19:43:35,903: New best test PER 0.1431 --> 0.1390
2025-12-08 19:43:35,903: Checkpointing model
2025-12-08 19:43:36,519: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 19:44:09,038: Train batch 12200: loss: 0.29 grad norm: 4.23 time: 0.135
2025-12-08 19:44:40,950: Train batch 12400: loss: 0.17 grad norm: 2.69 time: 0.094
2025-12-08 19:45:13,744: Train batch 12600: loss: 0.31 grad norm: 3.44 time: 0.091
2025-12-08 19:45:46,657: Train batch 12800: loss: 0.38 grad norm: 5.39 time: 0.116
2025-12-08 19:46:17,551: Train batch 13000: loss: 0.20 grad norm: 4.03 time: 0.128
2025-12-08 19:46:49,573: Train batch 13200: loss: 0.13 grad norm: 2.35 time: 0.080
2025-12-08 19:47:20,981: Train batch 13400: loss: 0.25 grad norm: 4.83 time: 0.089
2025-12-08 19:47:53,643: Train batch 13600: loss: 0.14 grad norm: 2.59 time: 0.093
2025-12-08 19:48:24,842: Train batch 13800: loss: 0.35 grad norm: 4.72 time: 0.136
2025-12-08 19:48:55,838: Train batch 14000: loss: 0.72 grad norm: 9.86 time: 0.145
2025-12-08 19:48:55,838: Running test after training batch: 14000
2025-12-08 19:49:06,816: Val batch 14000: PER (avg): 0.1396 CTC Loss (avg): 30.3432 time: 10.977
2025-12-08 19:49:06,816: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:49:06,816: t15.2023.08.13 val PER: 0.0936
2025-12-08 19:49:06,816: t15.2023.08.18 val PER: 0.0930
2025-12-08 19:49:06,816: t15.2023.08.20 val PER: 0.0778
2025-12-08 19:49:06,816: t15.2023.08.25 val PER: 0.1039
2025-12-08 19:49:06,816: t15.2023.08.27 val PER: 0.1704
2025-12-08 19:49:06,816: t15.2023.09.01 val PER: 0.0706
2025-12-08 19:49:06,816: t15.2023.09.03 val PER: 0.1354
2025-12-08 19:49:06,816: t15.2023.09.24 val PER: 0.1068
2025-12-08 19:49:06,817: t15.2023.09.29 val PER: 0.1283
2025-12-08 19:49:06,817: t15.2023.10.01 val PER: 0.1711
2025-12-08 19:49:06,817: t15.2023.10.06 val PER: 0.0947
2025-12-08 19:49:06,817: t15.2023.10.08 val PER: 0.2179
2025-12-08 19:49:06,817: t15.2023.10.13 val PER: 0.2203
2025-12-08 19:49:06,817: t15.2023.10.15 val PER: 0.1417
2025-12-08 19:49:06,817: t15.2023.10.20 val PER: 0.1946
2025-12-08 19:49:06,817: t15.2023.10.22 val PER: 0.1370
2025-12-08 19:49:06,817: t15.2023.11.03 val PER: 0.2103
2025-12-08 19:49:06,817: t15.2023.11.04 val PER: 0.0410
2025-12-08 19:49:06,817: t15.2023.11.17 val PER: 0.0544
2025-12-08 19:49:06,817: t15.2023.11.19 val PER: 0.0659
2025-12-08 19:49:06,817: t15.2023.11.26 val PER: 0.0848
2025-12-08 19:49:06,817: t15.2023.12.03 val PER: 0.0945
2025-12-08 19:49:06,817: t15.2023.12.08 val PER: 0.0686
2025-12-08 19:49:06,817: t15.2023.12.10 val PER: 0.0670
2025-12-08 19:49:06,817: t15.2023.12.17 val PER: 0.1518
2025-12-08 19:49:06,817: t15.2023.12.29 val PER: 0.1201
2025-12-08 19:49:06,818: t15.2024.02.25 val PER: 0.0969
2025-12-08 19:49:06,818: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:49:06,818: t15.2024.03.08 val PER: 0.2105
2025-12-08 19:49:06,818: t15.2024.03.15 val PER: 0.2014
2025-12-08 19:49:06,818: t15.2024.03.17 val PER: 0.1088
2025-12-08 19:49:06,818: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:49:06,818: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:49:06,818: t15.2024.05.10 val PER: 0.1605
2025-12-08 19:49:06,818: t15.2024.06.14 val PER: 0.1672
2025-12-08 19:49:06,818: t15.2024.07.19 val PER: 0.1839
2025-12-08 19:49:06,818: t15.2024.07.21 val PER: 0.0807
2025-12-08 19:49:06,818: t15.2024.07.28 val PER: 0.1162
2025-12-08 19:49:06,818: t15.2025.01.10 val PER: 0.2851
2025-12-08 19:49:06,818: t15.2025.01.12 val PER: 0.1124
2025-12-08 19:49:06,818: t15.2025.03.14 val PER: 0.3462
2025-12-08 19:49:06,818: t15.2025.03.16 val PER: 0.1728
2025-12-08 19:49:06,818: t15.2025.03.30 val PER: 0.2678
2025-12-08 19:49:06,818: t15.2025.04.13 val PER: 0.1969
2025-12-08 19:49:38,183: Train batch 14200: loss: 0.38 grad norm: 4.38 time: 0.095
2025-12-08 19:50:10,073: Train batch 14400: loss: 0.26 grad norm: 3.46 time: 0.121
2025-12-08 19:50:42,614: Train batch 14600: loss: 0.41 grad norm: 4.80 time: 0.163
2025-12-08 19:51:14,490: Train batch 14800: loss: 0.10 grad norm: 3.71 time: 0.110
2025-12-08 19:51:46,391: Train batch 15000: loss: 0.24 grad norm: 3.29 time: 0.181
2025-12-08 19:52:18,329: Train batch 15200: loss: 0.57 grad norm: 5.67 time: 0.101
2025-12-08 19:52:51,879: Train batch 15400: loss: 0.19 grad norm: 3.48 time: 0.087
2025-12-08 19:53:24,826: Train batch 15600: loss: 0.23 grad norm: 3.95 time: 0.115
2025-12-08 19:53:57,384: Train batch 15800: loss: 0.77 grad norm: 6.40 time: 0.091
2025-12-08 19:54:31,264: Train batch 16000: loss: 0.11 grad norm: 3.20 time: 0.118
2025-12-08 19:54:31,265: Running test after training batch: 16000
2025-12-08 19:54:41,776: Val batch 16000: PER (avg): 0.1394 CTC Loss (avg): 31.6419 time: 10.511
2025-12-08 19:54:41,776: t15.2023.08.11 val PER: 1.0000
2025-12-08 19:54:41,776: t15.2023.08.13 val PER: 0.0977
2025-12-08 19:54:41,776: t15.2023.08.18 val PER: 0.0763
2025-12-08 19:54:41,776: t15.2023.08.20 val PER: 0.0604
2025-12-08 19:54:41,776: t15.2023.08.25 val PER: 0.1160
2025-12-08 19:54:41,776: t15.2023.08.27 val PER: 0.1447
2025-12-08 19:54:41,776: t15.2023.09.01 val PER: 0.0633
2025-12-08 19:54:41,776: t15.2023.09.03 val PER: 0.1342
2025-12-08 19:54:41,777: t15.2023.09.24 val PER: 0.1080
2025-12-08 19:54:41,777: t15.2023.09.29 val PER: 0.1327
2025-12-08 19:54:41,777: t15.2023.10.01 val PER: 0.1579
2025-12-08 19:54:41,777: t15.2023.10.06 val PER: 0.1130
2025-12-08 19:54:41,777: t15.2023.10.08 val PER: 0.1854
2025-12-08 19:54:41,777: t15.2023.10.13 val PER: 0.1916
2025-12-08 19:54:41,777: t15.2023.10.15 val PER: 0.1358
2025-12-08 19:54:41,777: t15.2023.10.20 val PER: 0.2215
2025-12-08 19:54:41,777: t15.2023.10.22 val PER: 0.1336
2025-12-08 19:54:41,777: t15.2023.11.03 val PER: 0.2178
2025-12-08 19:54:41,777: t15.2023.11.04 val PER: 0.0580
2025-12-08 19:54:41,777: t15.2023.11.17 val PER: 0.0404
2025-12-08 19:54:41,777: t15.2023.11.19 val PER: 0.1018
2025-12-08 19:54:41,777: t15.2023.11.26 val PER: 0.0797
2025-12-08 19:54:41,777: t15.2023.12.03 val PER: 0.0893
2025-12-08 19:54:41,777: t15.2023.12.08 val PER: 0.0672
2025-12-08 19:54:41,777: t15.2023.12.10 val PER: 0.0604
2025-12-08 19:54:41,777: t15.2023.12.17 val PER: 0.1559
2025-12-08 19:54:41,777: t15.2023.12.29 val PER: 0.1448
2025-12-08 19:54:41,778: t15.2024.02.25 val PER: 0.0857
2025-12-08 19:54:41,778: t15.2024.03.03 val PER: 1.0000
2025-12-08 19:54:41,778: t15.2024.03.08 val PER: 0.2219
2025-12-08 19:54:41,778: t15.2024.03.15 val PER: 0.2133
2025-12-08 19:54:41,778: t15.2024.03.17 val PER: 0.1199
2025-12-08 19:54:41,778: t15.2024.04.25 val PER: 1.0000
2025-12-08 19:54:41,778: t15.2024.04.28 val PER: 1.0000
2025-12-08 19:54:41,778: t15.2024.05.10 val PER: 0.1575
2025-12-08 19:54:41,778: t15.2024.06.14 val PER: 0.1530
2025-12-08 19:54:41,778: t15.2024.07.19 val PER: 0.1978
2025-12-08 19:54:41,778: t15.2024.07.21 val PER: 0.0924
2025-12-08 19:54:41,778: t15.2024.07.28 val PER: 0.1176
2025-12-08 19:54:41,778: t15.2025.01.10 val PER: 0.2906
2025-12-08 19:54:41,778: t15.2025.01.12 val PER: 0.1132
2025-12-08 19:54:41,778: t15.2025.03.14 val PER: 0.3254
2025-12-08 19:54:41,778: t15.2025.03.16 val PER: 0.1754
2025-12-08 19:54:41,778: t15.2025.03.30 val PER: 0.2644
2025-12-08 19:54:41,778: t15.2025.04.13 val PER: 0.2111
2025-12-08 19:55:13,264: Train batch 16200: loss: 0.25 grad norm: 3.08 time: 0.190
2025-12-08 19:55:45,254: Train batch 16400: loss: 0.41 grad norm: 6.14 time: 0.148
2025-12-08 19:56:15,179: Train batch 16600: loss: 0.23 grad norm: 3.15 time: 0.092
2025-12-08 19:56:47,214: Train batch 16800: loss: 0.19 grad norm: 3.81 time: 0.123
2025-12-08 19:57:20,541: Train batch 17000: loss: 0.27 grad norm: 4.20 time: 0.186
2025-12-08 19:57:52,458: Train batch 17200: loss: 0.31 grad norm: 6.36 time: 0.092
2025-12-08 19:58:25,314: Train batch 17400: loss: 0.31 grad norm: 3.56 time: 0.134
2025-12-08 19:58:57,683: Train batch 17600: loss: 0.14 grad norm: 2.78 time: 0.094
2025-12-08 19:59:30,697: Train batch 17800: loss: 0.26 grad norm: 3.50 time: 0.070
2025-12-08 20:00:03,167: Train batch 18000: loss: 0.41 grad norm: 5.80 time: 0.106
2025-12-08 20:00:03,168: Running test after training batch: 18000
2025-12-08 20:00:13,729: Val batch 18000: PER (avg): 0.1315 CTC Loss (avg): 29.8764 time: 10.561
2025-12-08 20:00:13,729: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:00:13,729: t15.2023.08.13 val PER: 0.0769
2025-12-08 20:00:13,730: t15.2023.08.18 val PER: 0.0729
2025-12-08 20:00:13,730: t15.2023.08.20 val PER: 0.0731
2025-12-08 20:00:13,730: t15.2023.08.25 val PER: 0.1024
2025-12-08 20:00:13,730: t15.2023.08.27 val PER: 0.1720
2025-12-08 20:00:13,730: t15.2023.09.01 val PER: 0.0568
2025-12-08 20:00:13,730: t15.2023.09.03 val PER: 0.1473
2025-12-08 20:00:13,730: t15.2023.09.24 val PER: 0.1007
2025-12-08 20:00:13,730: t15.2023.09.29 val PER: 0.1340
2025-12-08 20:00:13,730: t15.2023.10.01 val PER: 0.1486
2025-12-08 20:00:13,730: t15.2023.10.06 val PER: 0.1066
2025-12-08 20:00:13,730: t15.2023.10.08 val PER: 0.1908
2025-12-08 20:00:13,730: t15.2023.10.13 val PER: 0.1808
2025-12-08 20:00:13,730: t15.2023.10.15 val PER: 0.1463
2025-12-08 20:00:13,730: t15.2023.10.20 val PER: 0.2181
2025-12-08 20:00:13,730: t15.2023.10.22 val PER: 0.1281
2025-12-08 20:00:13,730: t15.2023.11.03 val PER: 0.1913
2025-12-08 20:00:13,730: t15.2023.11.04 val PER: 0.0307
2025-12-08 20:00:13,730: t15.2023.11.17 val PER: 0.0404
2025-12-08 20:00:13,730: t15.2023.11.19 val PER: 0.0619
2025-12-08 20:00:13,731: t15.2023.11.26 val PER: 0.0862
2025-12-08 20:00:13,731: t15.2023.12.03 val PER: 0.0798
2025-12-08 20:00:13,731: t15.2023.12.08 val PER: 0.0553
2025-12-08 20:00:13,731: t15.2023.12.10 val PER: 0.0447
2025-12-08 20:00:13,731: t15.2023.12.17 val PER: 0.1455
2025-12-08 20:00:13,731: t15.2023.12.29 val PER: 0.1160
2025-12-08 20:00:13,731: t15.2024.02.25 val PER: 0.0927
2025-12-08 20:00:13,731: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:00:13,731: t15.2024.03.08 val PER: 0.1963
2025-12-08 20:00:13,731: t15.2024.03.15 val PER: 0.1920
2025-12-08 20:00:13,731: t15.2024.03.17 val PER: 0.0969
2025-12-08 20:00:13,731: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:00:13,731: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:00:13,731: t15.2024.05.10 val PER: 0.1441
2025-12-08 20:00:13,731: t15.2024.06.14 val PER: 0.1356
2025-12-08 20:00:13,731: t15.2024.07.19 val PER: 0.1885
2025-12-08 20:00:13,731: t15.2024.07.21 val PER: 0.0979
2025-12-08 20:00:13,731: t15.2024.07.28 val PER: 0.1162
2025-12-08 20:00:13,731: t15.2025.01.10 val PER: 0.2796
2025-12-08 20:00:13,732: t15.2025.01.12 val PER: 0.1008
2025-12-08 20:00:13,732: t15.2025.03.14 val PER: 0.3284
2025-12-08 20:00:13,732: t15.2025.03.16 val PER: 0.1728
2025-12-08 20:00:13,732: t15.2025.03.30 val PER: 0.2276
2025-12-08 20:00:13,732: t15.2025.04.13 val PER: 0.2168
2025-12-08 20:00:13,732: New best test PER 0.1390 --> 0.1315
2025-12-08 20:00:13,732: Checkpointing model
2025-12-08 20:00:14,856: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 20:00:47,181: Train batch 18200: loss: 0.36 grad norm: 5.94 time: 0.129
2025-12-08 20:01:19,604: Train batch 18400: loss: 0.16 grad norm: 2.68 time: 0.105
2025-12-08 20:01:52,275: Train batch 18600: loss: 0.53 grad norm: 7.96 time: 0.148
2025-12-08 20:02:25,106: Train batch 18800: loss: 0.08 grad norm: 1.91 time: 0.123
2025-12-08 20:02:58,052: Train batch 19000: loss: 0.18 grad norm: 2.56 time: 0.133
2025-12-08 20:03:31,720: Train batch 19200: loss: 0.35 grad norm: 4.71 time: 0.125
2025-12-08 20:04:05,100: Train batch 19400: loss: 0.52 grad norm: 4.42 time: 0.120
2025-12-08 20:04:37,392: Train batch 19600: loss: 0.24 grad norm: 3.50 time: 0.086
2025-12-08 20:05:10,116: Train batch 19800: loss: 0.11 grad norm: 2.04 time: 0.095
2025-12-08 20:05:42,436: Train batch 20000: loss: 0.12 grad norm: 3.34 time: 0.093
2025-12-08 20:05:42,436: Running test after training batch: 20000
2025-12-08 20:05:53,027: Val batch 20000: PER (avg): 0.1296 CTC Loss (avg): 28.8828 time: 10.591
2025-12-08 20:05:53,027: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:05:53,027: t15.2023.08.13 val PER: 0.0873
2025-12-08 20:05:53,028: t15.2023.08.18 val PER: 0.0780
2025-12-08 20:05:53,028: t15.2023.08.20 val PER: 0.0604
2025-12-08 20:05:53,028: t15.2023.08.25 val PER: 0.0964
2025-12-08 20:05:53,028: t15.2023.08.27 val PER: 0.1415
2025-12-08 20:05:53,028: t15.2023.09.01 val PER: 0.0511
2025-12-08 20:05:53,028: t15.2023.09.03 val PER: 0.1378
2025-12-08 20:05:53,028: t15.2023.09.24 val PER: 0.1019
2025-12-08 20:05:53,028: t15.2023.09.29 val PER: 0.1289
2025-12-08 20:05:53,028: t15.2023.10.01 val PER: 0.1486
2025-12-08 20:05:53,028: t15.2023.10.06 val PER: 0.1023
2025-12-08 20:05:53,028: t15.2023.10.08 val PER: 0.1867
2025-12-08 20:05:53,028: t15.2023.10.13 val PER: 0.1939
2025-12-08 20:05:53,028: t15.2023.10.15 val PER: 0.1325
2025-12-08 20:05:53,028: t15.2023.10.20 val PER: 0.1946
2025-12-08 20:05:53,028: t15.2023.10.22 val PER: 0.1325
2025-12-08 20:05:53,028: t15.2023.11.03 val PER: 0.1696
2025-12-08 20:05:53,028: t15.2023.11.04 val PER: 0.0512
2025-12-08 20:05:53,028: t15.2023.11.17 val PER: 0.0358
2025-12-08 20:05:53,028: t15.2023.11.19 val PER: 0.0619
2025-12-08 20:05:53,029: t15.2023.11.26 val PER: 0.0804
2025-12-08 20:05:53,029: t15.2023.12.03 val PER: 0.0756
2025-12-08 20:05:53,029: t15.2023.12.08 val PER: 0.0539
2025-12-08 20:05:53,029: t15.2023.12.10 val PER: 0.0355
2025-12-08 20:05:53,029: t15.2023.12.17 val PER: 0.1632
2025-12-08 20:05:53,029: t15.2023.12.29 val PER: 0.1036
2025-12-08 20:05:53,029: t15.2024.02.25 val PER: 0.0758
2025-12-08 20:05:53,029: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:05:53,029: t15.2024.03.08 val PER: 0.2233
2025-12-08 20:05:53,029: t15.2024.03.15 val PER: 0.2076
2025-12-08 20:05:53,029: t15.2024.03.17 val PER: 0.1053
2025-12-08 20:05:53,029: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:05:53,029: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:05:53,029: t15.2024.05.10 val PER: 0.1308
2025-12-08 20:05:53,029: t15.2024.06.14 val PER: 0.1530
2025-12-08 20:05:53,029: t15.2024.07.19 val PER: 0.1892
2025-12-08 20:05:53,029: t15.2024.07.21 val PER: 0.0738
2025-12-08 20:05:53,029: t15.2024.07.28 val PER: 0.1132
2025-12-08 20:05:53,029: t15.2025.01.10 val PER: 0.2782
2025-12-08 20:05:53,030: t15.2025.01.12 val PER: 0.1132
2025-12-08 20:05:53,030: t15.2025.03.14 val PER: 0.3033
2025-12-08 20:05:53,030: t15.2025.03.16 val PER: 0.1688
2025-12-08 20:05:53,030: t15.2025.03.30 val PER: 0.2517
2025-12-08 20:05:53,030: t15.2025.04.13 val PER: 0.2282
2025-12-08 20:05:53,030: New best test PER 0.1315 --> 0.1296
2025-12-08 20:05:53,030: Checkpointing model
2025-12-08 20:05:54,211: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 20:06:26,906: Train batch 20200: loss: 0.20 grad norm: 3.30 time: 0.096
2025-12-08 20:07:00,478: Train batch 20400: loss: 0.11 grad norm: 2.52 time: 0.110
2025-12-08 20:07:33,473: Train batch 20600: loss: 0.16 grad norm: 4.67 time: 0.104
2025-12-08 20:08:06,131: Train batch 20800: loss: 0.33 grad norm: 3.43 time: 0.098
2025-12-08 20:08:43,662: Train batch 21000: loss: 0.23 grad norm: 3.08 time: 0.101
2025-12-08 20:09:11,969: Train batch 21200: loss: 0.25 grad norm: 3.56 time: 0.110
2025-12-08 20:09:40,446: Train batch 21400: loss: 0.11 grad norm: 2.02 time: 0.167
2025-12-08 20:10:11,078: Train batch 21600: loss: 1.33 grad norm: 14.61 time: 0.115
2025-12-08 20:10:40,883: Train batch 21800: loss: 0.08 grad norm: 1.82 time: 0.104
2025-12-08 20:11:14,221: Train batch 22000: loss: 0.21 grad norm: 2.85 time: 0.240
2025-12-08 20:11:14,223: Running test after training batch: 22000
2025-12-08 20:11:30,841: Val batch 22000: PER (avg): 0.1305 CTC Loss (avg): 31.0439 time: 16.618
2025-12-08 20:11:30,842: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:11:30,842: t15.2023.08.13 val PER: 0.0925
2025-12-08 20:11:30,842: t15.2023.08.18 val PER: 0.0704
2025-12-08 20:11:30,842: t15.2023.08.20 val PER: 0.0747
2025-12-08 20:11:30,842: t15.2023.08.25 val PER: 0.0768
2025-12-08 20:11:30,842: t15.2023.08.27 val PER: 0.1383
2025-12-08 20:11:30,842: t15.2023.09.01 val PER: 0.0568
2025-12-08 20:11:30,842: t15.2023.09.03 val PER: 0.1164
2025-12-08 20:11:30,842: t15.2023.09.24 val PER: 0.1104
2025-12-08 20:11:30,842: t15.2023.09.29 val PER: 0.1340
2025-12-08 20:11:30,842: t15.2023.10.01 val PER: 0.1499
2025-12-08 20:11:30,842: t15.2023.10.06 val PER: 0.0872
2025-12-08 20:11:30,842: t15.2023.10.08 val PER: 0.1800
2025-12-08 20:11:30,843: t15.2023.10.13 val PER: 0.2095
2025-12-08 20:11:30,843: t15.2023.10.15 val PER: 0.1384
2025-12-08 20:11:30,843: t15.2023.10.20 val PER: 0.2047
2025-12-08 20:11:30,843: t15.2023.10.22 val PER: 0.1381
2025-12-08 20:11:30,843: t15.2023.11.03 val PER: 0.1906
2025-12-08 20:11:30,843: t15.2023.11.04 val PER: 0.0580
2025-12-08 20:11:30,843: t15.2023.11.17 val PER: 0.0544
2025-12-08 20:11:30,843: t15.2023.11.19 val PER: 0.0599
2025-12-08 20:11:30,843: t15.2023.11.26 val PER: 0.0783
2025-12-08 20:11:30,843: t15.2023.12.03 val PER: 0.0840
2025-12-08 20:11:30,843: t15.2023.12.08 val PER: 0.0626
2025-12-08 20:11:30,843: t15.2023.12.10 val PER: 0.0552
2025-12-08 20:11:30,843: t15.2023.12.17 val PER: 0.1559
2025-12-08 20:11:30,843: t15.2023.12.29 val PER: 0.1160
2025-12-08 20:11:30,843: t15.2024.02.25 val PER: 0.0758
2025-12-08 20:11:30,843: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:11:30,843: t15.2024.03.08 val PER: 0.2091
2025-12-08 20:11:30,844: t15.2024.03.15 val PER: 0.1970
2025-12-08 20:11:30,844: t15.2024.03.17 val PER: 0.1004
2025-12-08 20:11:30,844: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:11:30,844: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:11:30,844: t15.2024.05.10 val PER: 0.1486
2025-12-08 20:11:30,844: t15.2024.06.14 val PER: 0.1451
2025-12-08 20:11:30,844: t15.2024.07.19 val PER: 0.1714
2025-12-08 20:11:30,844: t15.2024.07.21 val PER: 0.0848
2025-12-08 20:11:30,844: t15.2024.07.28 val PER: 0.1066
2025-12-08 20:11:30,844: t15.2025.01.10 val PER: 0.2824
2025-12-08 20:11:30,844: t15.2025.01.12 val PER: 0.1109
2025-12-08 20:11:30,844: t15.2025.03.14 val PER: 0.3180
2025-12-08 20:11:30,844: t15.2025.03.16 val PER: 0.1675
2025-12-08 20:11:30,844: t15.2025.03.30 val PER: 0.2379
2025-12-08 20:11:30,844: t15.2025.04.13 val PER: 0.1954
2025-12-08 20:12:03,026: Train batch 22200: loss: 0.17 grad norm: 3.35 time: 0.126
2025-12-08 20:12:31,870: Train batch 22400: loss: 0.11 grad norm: 2.23 time: 0.118
2025-12-08 20:13:00,478: Train batch 22600: loss: 0.37 grad norm: 4.62 time: 0.153
2025-12-08 20:13:29,126: Train batch 22800: loss: 0.36 grad norm: 3.67 time: 0.125
2025-12-08 20:13:56,737: Train batch 23000: loss: 0.14 grad norm: 2.09 time: 0.113
2025-12-08 20:14:27,681: Train batch 23200: loss: 0.97 grad norm: 6.14 time: 0.119
2025-12-08 20:14:59,995: Train batch 23400: loss: 0.00 grad norm: 0.02 time: 0.135
2025-12-08 20:15:35,967: Train batch 23600: loss: 0.31 grad norm: 5.97 time: 0.125
2025-12-08 20:16:16,359: Train batch 23800: loss: 0.21 grad norm: 3.20 time: 0.096
2025-12-08 20:16:44,705: Train batch 24000: loss: 0.16 grad norm: 5.40 time: 0.152
2025-12-08 20:16:44,705: Running test after training batch: 24000
2025-12-08 20:16:54,786: Val batch 24000: PER (avg): 0.1303 CTC Loss (avg): 31.2816 time: 10.081
2025-12-08 20:16:54,786: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:16:54,786: t15.2023.08.13 val PER: 0.0832
2025-12-08 20:16:54,787: t15.2023.08.18 val PER: 0.0805
2025-12-08 20:16:54,787: t15.2023.08.20 val PER: 0.0675
2025-12-08 20:16:54,787: t15.2023.08.25 val PER: 0.0949
2025-12-08 20:16:54,787: t15.2023.08.27 val PER: 0.1543
2025-12-08 20:16:54,787: t15.2023.09.01 val PER: 0.0528
2025-12-08 20:16:54,787: t15.2023.09.03 val PER: 0.1330
2025-12-08 20:16:54,787: t15.2023.09.24 val PER: 0.1007
2025-12-08 20:16:54,787: t15.2023.09.29 val PER: 0.1404
2025-12-08 20:16:54,787: t15.2023.10.01 val PER: 0.1664
2025-12-08 20:16:54,787: t15.2023.10.06 val PER: 0.0721
2025-12-08 20:16:54,787: t15.2023.10.08 val PER: 0.1935
2025-12-08 20:16:54,787: t15.2023.10.13 val PER: 0.1986
2025-12-08 20:16:54,787: t15.2023.10.15 val PER: 0.1266
2025-12-08 20:16:54,787: t15.2023.10.20 val PER: 0.2148
2025-12-08 20:16:54,787: t15.2023.10.22 val PER: 0.1459
2025-12-08 20:16:54,787: t15.2023.11.03 val PER: 0.2171
2025-12-08 20:16:54,787: t15.2023.11.04 val PER: 0.0410
2025-12-08 20:16:54,788: t15.2023.11.17 val PER: 0.0327
2025-12-08 20:16:54,788: t15.2023.11.19 val PER: 0.0459
2025-12-08 20:16:54,788: t15.2023.11.26 val PER: 0.0768
2025-12-08 20:16:54,788: t15.2023.12.03 val PER: 0.0788
2025-12-08 20:16:54,788: t15.2023.12.08 val PER: 0.0506
2025-12-08 20:16:54,788: t15.2023.12.10 val PER: 0.0565
2025-12-08 20:16:54,788: t15.2023.12.17 val PER: 0.1455
2025-12-08 20:16:54,788: t15.2023.12.29 val PER: 0.1009
2025-12-08 20:16:54,788: t15.2024.02.25 val PER: 0.0829
2025-12-08 20:16:54,788: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:16:54,788: t15.2024.03.08 val PER: 0.1991
2025-12-08 20:16:54,788: t15.2024.03.15 val PER: 0.1864
2025-12-08 20:16:54,788: t15.2024.03.17 val PER: 0.0969
2025-12-08 20:16:54,788: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:16:54,788: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:16:54,788: t15.2024.05.10 val PER: 0.1679
2025-12-08 20:16:54,788: t15.2024.06.14 val PER: 0.1625
2025-12-08 20:16:54,788: t15.2024.07.19 val PER: 0.1819
2025-12-08 20:16:54,788: t15.2024.07.21 val PER: 0.0745
2025-12-08 20:16:54,789: t15.2024.07.28 val PER: 0.1110
2025-12-08 20:16:54,789: t15.2025.01.10 val PER: 0.2769
2025-12-08 20:16:54,789: t15.2025.01.12 val PER: 0.0978
2025-12-08 20:16:54,789: t15.2025.03.14 val PER: 0.3092
2025-12-08 20:16:54,789: t15.2025.03.16 val PER: 0.1649
2025-12-08 20:16:54,789: t15.2025.03.30 val PER: 0.2460
2025-12-08 20:16:54,789: t15.2025.04.13 val PER: 0.2282
2025-12-08 20:17:23,268: Train batch 24200: loss: 0.23 grad norm: 2.74 time: 0.078
2025-12-08 20:17:50,930: Train batch 24400: loss: 0.17 grad norm: 3.37 time: 0.113
2025-12-08 20:18:18,302: Train batch 24600: loss: 0.14 grad norm: 2.88 time: 0.149
2025-12-08 20:18:45,984: Train batch 24800: loss: 0.11 grad norm: 3.01 time: 0.096
2025-12-08 20:19:13,597: Train batch 25000: loss: 0.21 grad norm: 3.63 time: 0.140
2025-12-08 20:19:42,032: Train batch 25200: loss: 0.20 grad norm: 3.24 time: 0.078
2025-12-08 20:20:10,335: Train batch 25400: loss: 0.14 grad norm: 2.30 time: 0.112
2025-12-08 20:20:38,112: Train batch 25600: loss: 0.18 grad norm: 2.13 time: 0.105
2025-12-08 20:21:06,146: Train batch 25800: loss: 0.12 grad norm: 2.89 time: 0.091
2025-12-08 20:21:34,762: Train batch 26000: loss: 0.29 grad norm: 4.89 time: 0.165
2025-12-08 20:21:34,763: Running test after training batch: 26000
2025-12-08 20:21:45,011: Val batch 26000: PER (avg): 0.1327 CTC Loss (avg): 32.7594 time: 10.248
2025-12-08 20:21:45,012: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:21:45,012: t15.2023.08.13 val PER: 0.0852
2025-12-08 20:21:45,012: t15.2023.08.18 val PER: 0.0771
2025-12-08 20:21:45,012: t15.2023.08.20 val PER: 0.0667
2025-12-08 20:21:45,012: t15.2023.08.25 val PER: 0.0843
2025-12-08 20:21:45,012: t15.2023.08.27 val PER: 0.1367
2025-12-08 20:21:45,012: t15.2023.09.01 val PER: 0.0633
2025-12-08 20:21:45,012: t15.2023.09.03 val PER: 0.1188
2025-12-08 20:21:45,012: t15.2023.09.24 val PER: 0.1117
2025-12-08 20:21:45,012: t15.2023.09.29 val PER: 0.1429
2025-12-08 20:21:45,012: t15.2023.10.01 val PER: 0.1645
2025-12-08 20:21:45,012: t15.2023.10.06 val PER: 0.0829
2025-12-08 20:21:45,012: t15.2023.10.08 val PER: 0.1922
2025-12-08 20:21:45,012: t15.2023.10.13 val PER: 0.1784
2025-12-08 20:21:45,012: t15.2023.10.15 val PER: 0.1457
2025-12-08 20:21:45,013: t15.2023.10.20 val PER: 0.2248
2025-12-08 20:21:45,013: t15.2023.10.22 val PER: 0.1303
2025-12-08 20:21:45,013: t15.2023.11.03 val PER: 0.2117
2025-12-08 20:21:45,013: t15.2023.11.04 val PER: 0.0375
2025-12-08 20:21:45,013: t15.2023.11.17 val PER: 0.0467
2025-12-08 20:21:45,013: t15.2023.11.19 val PER: 0.0898
2025-12-08 20:21:45,013: t15.2023.11.26 val PER: 0.0761
2025-12-08 20:21:45,013: t15.2023.12.03 val PER: 0.0819
2025-12-08 20:21:45,013: t15.2023.12.08 val PER: 0.0719
2025-12-08 20:21:45,013: t15.2023.12.10 val PER: 0.0591
2025-12-08 20:21:45,013: t15.2023.12.17 val PER: 0.1341
2025-12-08 20:21:45,013: t15.2023.12.29 val PER: 0.1105
2025-12-08 20:21:45,013: t15.2024.02.25 val PER: 0.0843
2025-12-08 20:21:45,013: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:21:45,013: t15.2024.03.08 val PER: 0.2219
2025-12-08 20:21:45,013: t15.2024.03.15 val PER: 0.1982
2025-12-08 20:21:45,013: t15.2024.03.17 val PER: 0.1032
2025-12-08 20:21:45,013: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:21:45,014: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:21:45,014: t15.2024.05.10 val PER: 0.1441
2025-12-08 20:21:45,014: t15.2024.06.14 val PER: 0.1546
2025-12-08 20:21:45,014: t15.2024.07.19 val PER: 0.1866
2025-12-08 20:21:45,014: t15.2024.07.21 val PER: 0.0731
2025-12-08 20:21:45,014: t15.2024.07.28 val PER: 0.1184
2025-12-08 20:21:45,014: t15.2025.01.10 val PER: 0.2796
2025-12-08 20:21:45,014: t15.2025.01.12 val PER: 0.1062
2025-12-08 20:21:45,014: t15.2025.03.14 val PER: 0.3092
2025-12-08 20:21:45,014: t15.2025.03.16 val PER: 0.1571
2025-12-08 20:21:45,014: t15.2025.03.30 val PER: 0.2563
2025-12-08 20:21:45,014: t15.2025.04.13 val PER: 0.2183
2025-12-08 20:22:12,897: Train batch 26200: loss: 0.10 grad norm: 3.30 time: 0.119
2025-12-08 20:22:41,697: Train batch 26400: loss: 0.13 grad norm: 3.54 time: 0.115
2025-12-08 20:23:11,469: Train batch 26600: loss: 0.53 grad norm: 4.15 time: 0.138
2025-12-08 20:23:39,639: Train batch 26800: loss: 0.28 grad norm: 4.58 time: 0.098
2025-12-08 20:24:08,725: Train batch 27000: loss: 0.09 grad norm: 1.86 time: 0.149
2025-12-08 20:24:37,659: Train batch 27200: loss: 0.10 grad norm: 1.81 time: 0.119
2025-12-08 20:25:07,506: Train batch 27400: loss: 0.09 grad norm: 1.82 time: 0.090
2025-12-08 20:25:35,727: Train batch 27600: loss: 0.05 grad norm: 1.18 time: 0.092
2025-12-08 20:26:03,933: Train batch 27800: loss: 0.09 grad norm: 1.67 time: 0.144
2025-12-08 20:26:32,846: Train batch 28000: loss: 0.33 grad norm: 4.23 time: 0.097
2025-12-08 20:26:32,847: Running test after training batch: 28000
2025-12-08 20:26:43,017: Val batch 28000: PER (avg): 0.1322 CTC Loss (avg): 32.0883 time: 10.170
2025-12-08 20:26:43,018: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:26:43,018: t15.2023.08.13 val PER: 0.0904
2025-12-08 20:26:43,018: t15.2023.08.18 val PER: 0.0863
2025-12-08 20:26:43,018: t15.2023.08.20 val PER: 0.0659
2025-12-08 20:26:43,018: t15.2023.08.25 val PER: 0.0873
2025-12-08 20:26:43,018: t15.2023.08.27 val PER: 0.1624
2025-12-08 20:26:43,018: t15.2023.09.01 val PER: 0.0503
2025-12-08 20:26:43,018: t15.2023.09.03 val PER: 0.1271
2025-12-08 20:26:43,018: t15.2023.09.24 val PER: 0.1044
2025-12-08 20:26:43,018: t15.2023.09.29 val PER: 0.1302
2025-12-08 20:26:43,018: t15.2023.10.01 val PER: 0.1585
2025-12-08 20:26:43,018: t15.2023.10.06 val PER: 0.0936
2025-12-08 20:26:43,018: t15.2023.10.08 val PER: 0.1935
2025-12-08 20:26:43,019: t15.2023.10.13 val PER: 0.1831
2025-12-08 20:26:43,019: t15.2023.10.15 val PER: 0.1305
2025-12-08 20:26:43,019: t15.2023.10.20 val PER: 0.2282
2025-12-08 20:26:43,019: t15.2023.10.22 val PER: 0.1359
2025-12-08 20:26:43,019: t15.2023.11.03 val PER: 0.2178
2025-12-08 20:26:43,019: t15.2023.11.04 val PER: 0.0171
2025-12-08 20:26:43,019: t15.2023.11.17 val PER: 0.0513
2025-12-08 20:26:43,019: t15.2023.11.19 val PER: 0.0579
2025-12-08 20:26:43,019: t15.2023.11.26 val PER: 0.0775
2025-12-08 20:26:43,019: t15.2023.12.03 val PER: 0.0683
2025-12-08 20:26:43,019: t15.2023.12.08 val PER: 0.0726
2025-12-08 20:26:43,019: t15.2023.12.10 val PER: 0.0552
2025-12-08 20:26:43,019: t15.2023.12.17 val PER: 0.1372
2025-12-08 20:26:43,019: t15.2023.12.29 val PER: 0.1091
2025-12-08 20:26:43,019: t15.2024.02.25 val PER: 0.0885
2025-12-08 20:26:43,019: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:26:43,019: t15.2024.03.08 val PER: 0.2290
2025-12-08 20:26:43,019: t15.2024.03.15 val PER: 0.1951
2025-12-08 20:26:43,019: t15.2024.03.17 val PER: 0.1109
2025-12-08 20:26:43,020: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:26:43,020: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:26:43,020: t15.2024.05.10 val PER: 0.1456
2025-12-08 20:26:43,020: t15.2024.06.14 val PER: 0.1577
2025-12-08 20:26:43,020: t15.2024.07.19 val PER: 0.2024
2025-12-08 20:26:43,020: t15.2024.07.21 val PER: 0.0772
2025-12-08 20:26:43,020: t15.2024.07.28 val PER: 0.1088
2025-12-08 20:26:43,020: t15.2025.01.10 val PER: 0.2590
2025-12-08 20:26:43,020: t15.2025.01.12 val PER: 0.0924
2025-12-08 20:26:43,020: t15.2025.03.14 val PER: 0.3180
2025-12-08 20:26:43,020: t15.2025.03.16 val PER: 0.1728
2025-12-08 20:26:43,020: t15.2025.03.30 val PER: 0.2506
2025-12-08 20:26:43,020: t15.2025.04.13 val PER: 0.2197
2025-12-08 20:27:11,419: Train batch 28200: loss: 0.07 grad norm: 2.32 time: 0.134
2025-12-08 20:27:39,498: Train batch 28400: loss: 0.42 grad norm: 3.08 time: 0.147
2025-12-08 20:28:07,869: Train batch 28600: loss: 0.09 grad norm: 1.90 time: 0.125
2025-12-08 20:28:36,116: Train batch 28800: loss: 0.13 grad norm: 2.80 time: 0.134
2025-12-08 20:29:04,909: Train batch 29000: loss: 0.23 grad norm: 4.19 time: 0.133
2025-12-08 20:29:33,012: Train batch 29200: loss: 0.09 grad norm: 1.70 time: 0.109
2025-12-08 20:30:01,791: Train batch 29400: loss: 0.04 grad norm: 1.12 time: 0.137
2025-12-08 20:30:30,379: Train batch 29600: loss: 0.04 grad norm: 1.20 time: 0.153
2025-12-08 20:30:58,635: Train batch 29800: loss: 0.02 grad norm: 0.47 time: 0.110
2025-12-08 20:31:27,439: Train batch 30000: loss: 0.06 grad norm: 1.95 time: 0.101
2025-12-08 20:31:27,440: Running test after training batch: 30000
2025-12-08 20:31:37,888: Val batch 30000: PER (avg): 0.1299 CTC Loss (avg): 33.4665 time: 10.448
2025-12-08 20:31:37,889: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:31:37,889: t15.2023.08.13 val PER: 0.0915
2025-12-08 20:31:37,889: t15.2023.08.18 val PER: 0.0738
2025-12-08 20:31:37,889: t15.2023.08.20 val PER: 0.0675
2025-12-08 20:31:37,889: t15.2023.08.25 val PER: 0.1069
2025-12-08 20:31:37,889: t15.2023.08.27 val PER: 0.1463
2025-12-08 20:31:37,889: t15.2023.09.01 val PER: 0.0487
2025-12-08 20:31:37,889: t15.2023.09.03 val PER: 0.1306
2025-12-08 20:31:37,889: t15.2023.09.24 val PER: 0.0898
2025-12-08 20:31:37,889: t15.2023.09.29 val PER: 0.1340
2025-12-08 20:31:37,889: t15.2023.10.01 val PER: 0.1664
2025-12-08 20:31:37,889: t15.2023.10.06 val PER: 0.0893
2025-12-08 20:31:37,889: t15.2023.10.08 val PER: 0.1935
2025-12-08 20:31:37,889: t15.2023.10.13 val PER: 0.1893
2025-12-08 20:31:37,890: t15.2023.10.15 val PER: 0.1259
2025-12-08 20:31:37,890: t15.2023.10.20 val PER: 0.2148
2025-12-08 20:31:37,890: t15.2023.10.22 val PER: 0.1247
2025-12-08 20:31:37,890: t15.2023.11.03 val PER: 0.1798
2025-12-08 20:31:37,890: t15.2023.11.04 val PER: 0.0171
2025-12-08 20:31:37,890: t15.2023.11.17 val PER: 0.0435
2025-12-08 20:31:37,890: t15.2023.11.19 val PER: 0.0798
2025-12-08 20:31:37,890: t15.2023.11.26 val PER: 0.0768
2025-12-08 20:31:37,890: t15.2023.12.03 val PER: 0.0924
2025-12-08 20:31:37,890: t15.2023.12.08 val PER: 0.0619
2025-12-08 20:31:37,890: t15.2023.12.10 val PER: 0.0512
2025-12-08 20:31:37,890: t15.2023.12.17 val PER: 0.1414
2025-12-08 20:31:37,890: t15.2023.12.29 val PER: 0.1050
2025-12-08 20:31:37,890: t15.2024.02.25 val PER: 0.0815
2025-12-08 20:31:37,890: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:31:37,890: t15.2024.03.08 val PER: 0.2176
2025-12-08 20:31:37,890: t15.2024.03.15 val PER: 0.1939
2025-12-08 20:31:37,890: t15.2024.03.17 val PER: 0.0886
2025-12-08 20:31:37,891: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:31:37,891: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:31:37,891: t15.2024.05.10 val PER: 0.1456
2025-12-08 20:31:37,891: t15.2024.06.14 val PER: 0.1467
2025-12-08 20:31:37,891: t15.2024.07.19 val PER: 0.1951
2025-12-08 20:31:37,891: t15.2024.07.21 val PER: 0.0786
2025-12-08 20:31:37,891: t15.2024.07.28 val PER: 0.1118
2025-12-08 20:31:37,891: t15.2025.01.10 val PER: 0.2865
2025-12-08 20:31:37,891: t15.2025.01.12 val PER: 0.0931
2025-12-08 20:31:37,891: t15.2025.03.14 val PER: 0.3003
2025-12-08 20:31:37,891: t15.2025.03.16 val PER: 0.1558
2025-12-08 20:31:37,891: t15.2025.03.30 val PER: 0.2724
2025-12-08 20:31:37,891: t15.2025.04.13 val PER: 0.2425
2025-12-08 20:32:06,074: Train batch 30200: loss: 0.00 grad norm: 0.07 time: 0.121
2025-12-08 20:32:34,565: Train batch 30400: loss: 0.12 grad norm: 4.12 time: 0.113
2025-12-08 20:33:03,641: Train batch 30600: loss: 0.17 grad norm: 2.77 time: 0.117
2025-12-08 20:33:33,068: Train batch 30800: loss: 0.14 grad norm: 2.27 time: 0.098
2025-12-08 20:34:02,465: Train batch 31000: loss: 0.05 grad norm: 1.43 time: 0.165
2025-12-08 20:34:33,653: Train batch 31200: loss: 0.08 grad norm: 3.02 time: 0.124
2025-12-08 20:35:05,042: Train batch 31400: loss: 0.28 grad norm: 6.15 time: 0.090
2025-12-08 20:35:34,566: Train batch 31600: loss: 0.29 grad norm: 3.56 time: 0.123
2025-12-08 20:36:05,115: Train batch 31800: loss: 0.08 grad norm: 2.03 time: 0.142
2025-12-08 20:36:35,035: Train batch 32000: loss: 0.03 grad norm: 0.86 time: 0.115
2025-12-08 20:36:35,035: Running test after training batch: 32000
2025-12-08 20:36:45,859: Val batch 32000: PER (avg): 0.1251 CTC Loss (avg): 32.6209 time: 10.823
2025-12-08 20:36:45,859: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:36:45,859: t15.2023.08.13 val PER: 0.0873
2025-12-08 20:36:45,859: t15.2023.08.18 val PER: 0.0780
2025-12-08 20:36:45,859: t15.2023.08.20 val PER: 0.0691
2025-12-08 20:36:45,860: t15.2023.08.25 val PER: 0.0979
2025-12-08 20:36:45,860: t15.2023.08.27 val PER: 0.1592
2025-12-08 20:36:45,860: t15.2023.09.01 val PER: 0.0552
2025-12-08 20:36:45,860: t15.2023.09.03 val PER: 0.1140
2025-12-08 20:36:45,860: t15.2023.09.24 val PER: 0.1056
2025-12-08 20:36:45,860: t15.2023.09.29 val PER: 0.1423
2025-12-08 20:36:45,860: t15.2023.10.01 val PER: 0.1671
2025-12-08 20:36:45,860: t15.2023.10.06 val PER: 0.0915
2025-12-08 20:36:45,860: t15.2023.10.08 val PER: 0.1962
2025-12-08 20:36:45,860: t15.2023.10.13 val PER: 0.1831
2025-12-08 20:36:45,860: t15.2023.10.15 val PER: 0.1305
2025-12-08 20:36:45,860: t15.2023.10.20 val PER: 0.2114
2025-12-08 20:36:45,860: t15.2023.10.22 val PER: 0.1169
2025-12-08 20:36:45,860: t15.2023.11.03 val PER: 0.1866
2025-12-08 20:36:45,861: t15.2023.11.04 val PER: 0.0273
2025-12-08 20:36:45,861: t15.2023.11.17 val PER: 0.0513
2025-12-08 20:36:45,861: t15.2023.11.19 val PER: 0.0838
2025-12-08 20:36:45,861: t15.2023.11.26 val PER: 0.0609
2025-12-08 20:36:45,861: t15.2023.12.03 val PER: 0.0819
2025-12-08 20:36:45,861: t15.2023.12.08 val PER: 0.0586
2025-12-08 20:36:45,861: t15.2023.12.10 val PER: 0.0329
2025-12-08 20:36:45,861: t15.2023.12.17 val PER: 0.1299
2025-12-08 20:36:45,861: t15.2023.12.29 val PER: 0.1002
2025-12-08 20:36:45,861: t15.2024.02.25 val PER: 0.0674
2025-12-08 20:36:45,861: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:36:45,861: t15.2024.03.08 val PER: 0.2148
2025-12-08 20:36:45,861: t15.2024.03.15 val PER: 0.1864
2025-12-08 20:36:45,861: t15.2024.03.17 val PER: 0.0900
2025-12-08 20:36:45,861: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:36:45,862: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:36:45,862: t15.2024.05.10 val PER: 0.1129
2025-12-08 20:36:45,862: t15.2024.06.14 val PER: 0.1451
2025-12-08 20:36:45,862: t15.2024.07.19 val PER: 0.1839
2025-12-08 20:36:45,862: t15.2024.07.21 val PER: 0.0703
2025-12-08 20:36:45,862: t15.2024.07.28 val PER: 0.0978
2025-12-08 20:36:45,862: t15.2025.01.10 val PER: 0.2479
2025-12-08 20:36:45,862: t15.2025.01.12 val PER: 0.1047
2025-12-08 20:36:45,862: t15.2025.03.14 val PER: 0.2899
2025-12-08 20:36:45,862: t15.2025.03.16 val PER: 0.1414
2025-12-08 20:36:45,862: t15.2025.03.30 val PER: 0.2437
2025-12-08 20:36:45,862: t15.2025.04.13 val PER: 0.2083
2025-12-08 20:36:45,862: New best test PER 0.1296 --> 0.1251
2025-12-08 20:36:45,862: Checkpointing model
2025-12-08 20:36:46,615: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 20:37:13,663: Train batch 32200: loss: 0.05 grad norm: 1.67 time: 0.105
2025-12-08 20:37:41,662: Train batch 32400: loss: 0.13 grad norm: 2.24 time: 0.143
2025-12-08 20:38:09,313: Train batch 32600: loss: 0.23 grad norm: 4.57 time: 0.126
2025-12-08 20:38:37,653: Train batch 32800: loss: 0.06 grad norm: 1.42 time: 0.084
2025-12-08 20:39:05,424: Train batch 33000: loss: 0.08 grad norm: 1.74 time: 0.091
2025-12-08 20:39:33,557: Train batch 33200: loss: 0.09 grad norm: 1.66 time: 0.074
2025-12-08 20:40:02,755: Train batch 33400: loss: 0.07 grad norm: 2.10 time: 0.151
2025-12-08 20:40:29,896: Train batch 33600: loss: 0.11 grad norm: 2.51 time: 0.110
2025-12-08 20:40:58,041: Train batch 33800: loss: 0.14 grad norm: 1.28 time: 0.151
2025-12-08 20:41:25,899: Train batch 34000: loss: 0.26 grad norm: 3.37 time: 0.106
2025-12-08 20:41:25,900: Running test after training batch: 34000
2025-12-08 20:41:35,950: Val batch 34000: PER (avg): 0.1232 CTC Loss (avg): 31.0265 time: 10.051
2025-12-08 20:41:35,951: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:41:35,951: t15.2023.08.13 val PER: 0.0707
2025-12-08 20:41:35,951: t15.2023.08.18 val PER: 0.0771
2025-12-08 20:41:35,951: t15.2023.08.20 val PER: 0.0675
2025-12-08 20:41:35,951: t15.2023.08.25 val PER: 0.0889
2025-12-08 20:41:35,951: t15.2023.08.27 val PER: 0.1495
2025-12-08 20:41:35,951: t15.2023.09.01 val PER: 0.0495
2025-12-08 20:41:35,951: t15.2023.09.03 val PER: 0.1069
2025-12-08 20:41:35,951: t15.2023.09.24 val PER: 0.0886
2025-12-08 20:41:35,951: t15.2023.09.29 val PER: 0.1295
2025-12-08 20:41:35,951: t15.2023.10.01 val PER: 0.1440
2025-12-08 20:41:35,951: t15.2023.10.06 val PER: 0.0743
2025-12-08 20:41:35,951: t15.2023.10.08 val PER: 0.1827
2025-12-08 20:41:35,952: t15.2023.10.13 val PER: 0.1893
2025-12-08 20:41:35,952: t15.2023.10.15 val PER: 0.1292
2025-12-08 20:41:35,952: t15.2023.10.20 val PER: 0.1946
2025-12-08 20:41:35,952: t15.2023.10.22 val PER: 0.1169
2025-12-08 20:41:35,952: t15.2023.11.03 val PER: 0.1845
2025-12-08 20:41:35,952: t15.2023.11.04 val PER: 0.0375
2025-12-08 20:41:35,952: t15.2023.11.17 val PER: 0.0342
2025-12-08 20:41:35,952: t15.2023.11.19 val PER: 0.0599
2025-12-08 20:41:35,952: t15.2023.11.26 val PER: 0.0732
2025-12-08 20:41:35,952: t15.2023.12.03 val PER: 0.0693
2025-12-08 20:41:35,952: t15.2023.12.08 val PER: 0.0579
2025-12-08 20:41:35,952: t15.2023.12.10 val PER: 0.0407
2025-12-08 20:41:35,952: t15.2023.12.17 val PER: 0.1435
2025-12-08 20:41:35,952: t15.2023.12.29 val PER: 0.0906
2025-12-08 20:41:35,952: t15.2024.02.25 val PER: 0.0702
2025-12-08 20:41:35,952: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:41:35,953: t15.2024.03.08 val PER: 0.1935
2025-12-08 20:41:35,953: t15.2024.03.15 val PER: 0.1920
2025-12-08 20:41:35,953: t15.2024.03.17 val PER: 0.0969
2025-12-08 20:41:35,953: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:41:35,953: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:41:35,953: t15.2024.05.10 val PER: 0.1501
2025-12-08 20:41:35,953: t15.2024.06.14 val PER: 0.1420
2025-12-08 20:41:35,953: t15.2024.07.19 val PER: 0.1819
2025-12-08 20:41:35,953: t15.2024.07.21 val PER: 0.0703
2025-12-08 20:41:35,953: t15.2024.07.28 val PER: 0.1044
2025-12-08 20:41:35,953: t15.2025.01.10 val PER: 0.2466
2025-12-08 20:41:35,953: t15.2025.01.12 val PER: 0.1078
2025-12-08 20:41:35,953: t15.2025.03.14 val PER: 0.3047
2025-12-08 20:41:35,953: t15.2025.03.16 val PER: 0.1545
2025-12-08 20:41:35,953: t15.2025.03.30 val PER: 0.2483
2025-12-08 20:41:35,953: t15.2025.04.13 val PER: 0.2282
2025-12-08 20:41:35,953: New best test PER 0.1251 --> 0.1232
2025-12-08 20:41:35,953: Checkpointing model
2025-12-08 20:41:36,494: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 20:42:04,970: Train batch 34200: loss: 0.04 grad norm: 1.10 time: 0.160
2025-12-08 20:42:33,148: Train batch 34400: loss: 0.06 grad norm: 2.29 time: 0.082
2025-12-08 20:43:01,670: Train batch 34600: loss: 0.03 grad norm: 0.77 time: 0.102
2025-12-08 20:43:29,515: Train batch 34800: loss: 0.07 grad norm: 1.55 time: 0.136
2025-12-08 20:43:57,672: Train batch 35000: loss: 0.08 grad norm: 1.43 time: 0.134
2025-12-08 20:44:27,147: Train batch 35200: loss: 0.22 grad norm: 6.88 time: 0.111
2025-12-08 20:44:58,130: Train batch 35400: loss: 0.01 grad norm: 0.30 time: 0.083
2025-12-08 20:45:27,791: Train batch 35600: loss: 0.02 grad norm: 0.97 time: 0.129
2025-12-08 20:46:00,969: Train batch 35800: loss: 0.07 grad norm: 2.21 time: 0.120
2025-12-08 20:46:34,198: Train batch 36000: loss: 0.02 grad norm: 0.99 time: 0.123
2025-12-08 20:46:34,198: Running test after training batch: 36000
2025-12-08 20:46:44,489: Val batch 36000: PER (avg): 0.1241 CTC Loss (avg): 33.8956 time: 10.290
2025-12-08 20:46:44,489: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:46:44,489: t15.2023.08.13 val PER: 0.0759
2025-12-08 20:46:44,489: t15.2023.08.18 val PER: 0.0763
2025-12-08 20:46:44,489: t15.2023.08.20 val PER: 0.0659
2025-12-08 20:46:44,489: t15.2023.08.25 val PER: 0.0678
2025-12-08 20:46:44,489: t15.2023.08.27 val PER: 0.1367
2025-12-08 20:46:44,489: t15.2023.09.01 val PER: 0.0528
2025-12-08 20:46:44,489: t15.2023.09.03 val PER: 0.1235
2025-12-08 20:46:44,489: t15.2023.09.24 val PER: 0.0971
2025-12-08 20:46:44,489: t15.2023.09.29 val PER: 0.1225
2025-12-08 20:46:44,489: t15.2023.10.01 val PER: 0.1420
2025-12-08 20:46:44,489: t15.2023.10.06 val PER: 0.0775
2025-12-08 20:46:44,489: t15.2023.10.08 val PER: 0.1827
2025-12-08 20:46:44,490: t15.2023.10.13 val PER: 0.2040
2025-12-08 20:46:44,490: t15.2023.10.15 val PER: 0.1173
2025-12-08 20:46:44,490: t15.2023.10.20 val PER: 0.1980
2025-12-08 20:46:44,490: t15.2023.10.22 val PER: 0.1247
2025-12-08 20:46:44,490: t15.2023.11.03 val PER: 0.1940
2025-12-08 20:46:44,490: t15.2023.11.04 val PER: 0.0307
2025-12-08 20:46:44,490: t15.2023.11.17 val PER: 0.0529
2025-12-08 20:46:44,490: t15.2023.11.19 val PER: 0.0898
2025-12-08 20:46:44,490: t15.2023.11.26 val PER: 0.0674
2025-12-08 20:46:44,490: t15.2023.12.03 val PER: 0.0651
2025-12-08 20:46:44,490: t15.2023.12.08 val PER: 0.0499
2025-12-08 20:46:44,490: t15.2023.12.10 val PER: 0.0486
2025-12-08 20:46:44,490: t15.2023.12.17 val PER: 0.1279
2025-12-08 20:46:44,490: t15.2023.12.29 val PER: 0.1002
2025-12-08 20:46:44,490: t15.2024.02.25 val PER: 0.0815
2025-12-08 20:46:44,490: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:46:44,490: t15.2024.03.08 val PER: 0.1664
2025-12-08 20:46:44,490: t15.2024.03.15 val PER: 0.2114
2025-12-08 20:46:44,490: t15.2024.03.17 val PER: 0.0934
2025-12-08 20:46:44,491: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:46:44,491: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:46:44,491: t15.2024.05.10 val PER: 0.1382
2025-12-08 20:46:44,491: t15.2024.06.14 val PER: 0.1372
2025-12-08 20:46:44,491: t15.2024.07.19 val PER: 0.1885
2025-12-08 20:46:44,491: t15.2024.07.21 val PER: 0.0745
2025-12-08 20:46:44,491: t15.2024.07.28 val PER: 0.0949
2025-12-08 20:46:44,491: t15.2025.01.10 val PER: 0.2796
2025-12-08 20:46:44,491: t15.2025.01.12 val PER: 0.1062
2025-12-08 20:46:44,491: t15.2025.03.14 val PER: 0.2870
2025-12-08 20:46:44,491: t15.2025.03.16 val PER: 0.1466
2025-12-08 20:46:44,491: t15.2025.03.30 val PER: 0.2483
2025-12-08 20:46:44,491: t15.2025.04.13 val PER: 0.2282
2025-12-08 20:47:16,144: Train batch 36200: loss: 0.43 grad norm: 5.29 time: 0.145
2025-12-08 20:47:47,694: Train batch 36400: loss: 0.11 grad norm: 2.70 time: 0.101
2025-12-08 20:48:20,349: Train batch 36600: loss: 0.10 grad norm: 2.97 time: 0.148
2025-12-08 20:48:53,793: Train batch 36800: loss: 0.12 grad norm: 3.36 time: 0.135
2025-12-08 20:49:25,506: Train batch 37000: loss: 0.04 grad norm: 1.13 time: 0.207
2025-12-08 20:49:58,761: Train batch 37200: loss: 0.16 grad norm: 2.22 time: 0.140
2025-12-08 20:50:31,791: Train batch 37400: loss: 0.01 grad norm: 0.27 time: 0.123
2025-12-08 20:51:04,717: Train batch 37600: loss: 0.32 grad norm: 3.57 time: 0.117
2025-12-08 20:51:37,212: Train batch 37800: loss: 0.02 grad norm: 0.51 time: 0.093
2025-12-08 20:52:09,867: Train batch 38000: loss: 0.06 grad norm: 1.37 time: 0.092
2025-12-08 20:52:09,867: Running test after training batch: 38000
2025-12-08 20:52:20,177: Val batch 38000: PER (avg): 0.1223 CTC Loss (avg): 34.4314 time: 10.309
2025-12-08 20:52:20,177: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:52:20,177: t15.2023.08.13 val PER: 0.0800
2025-12-08 20:52:20,177: t15.2023.08.18 val PER: 0.0763
2025-12-08 20:52:20,177: t15.2023.08.20 val PER: 0.0747
2025-12-08 20:52:20,177: t15.2023.08.25 val PER: 0.0708
2025-12-08 20:52:20,177: t15.2023.08.27 val PER: 0.1511
2025-12-08 20:52:20,177: t15.2023.09.01 val PER: 0.0552
2025-12-08 20:52:20,177: t15.2023.09.03 val PER: 0.1283
2025-12-08 20:52:20,177: t15.2023.09.24 val PER: 0.0983
2025-12-08 20:52:20,178: t15.2023.09.29 val PER: 0.1257
2025-12-08 20:52:20,178: t15.2023.10.01 val PER: 0.1480
2025-12-08 20:52:20,178: t15.2023.10.06 val PER: 0.0775
2025-12-08 20:52:20,178: t15.2023.10.08 val PER: 0.1786
2025-12-08 20:52:20,178: t15.2023.10.13 val PER: 0.1715
2025-12-08 20:52:20,178: t15.2023.10.15 val PER: 0.1220
2025-12-08 20:52:20,178: t15.2023.10.20 val PER: 0.2248
2025-12-08 20:52:20,178: t15.2023.10.22 val PER: 0.1158
2025-12-08 20:52:20,178: t15.2023.11.03 val PER: 0.1940
2025-12-08 20:52:20,178: t15.2023.11.04 val PER: 0.0478
2025-12-08 20:52:20,178: t15.2023.11.17 val PER: 0.0373
2025-12-08 20:52:20,178: t15.2023.11.19 val PER: 0.0599
2025-12-08 20:52:20,178: t15.2023.11.26 val PER: 0.0652
2025-12-08 20:52:20,178: t15.2023.12.03 val PER: 0.0788
2025-12-08 20:52:20,178: t15.2023.12.08 val PER: 0.0559
2025-12-08 20:52:20,178: t15.2023.12.10 val PER: 0.0578
2025-12-08 20:52:20,179: t15.2023.12.17 val PER: 0.1050
2025-12-08 20:52:20,179: t15.2023.12.29 val PER: 0.0988
2025-12-08 20:52:20,179: t15.2024.02.25 val PER: 0.0772
2025-12-08 20:52:20,179: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:52:20,179: t15.2024.03.08 val PER: 0.2176
2025-12-08 20:52:20,179: t15.2024.03.15 val PER: 0.1901
2025-12-08 20:52:20,179: t15.2024.03.17 val PER: 0.0683
2025-12-08 20:52:20,179: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:52:20,179: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:52:20,179: t15.2024.05.10 val PER: 0.1575
2025-12-08 20:52:20,179: t15.2024.06.14 val PER: 0.1293
2025-12-08 20:52:20,179: t15.2024.07.19 val PER: 0.1707
2025-12-08 20:52:20,179: t15.2024.07.21 val PER: 0.0717
2025-12-08 20:52:20,179: t15.2024.07.28 val PER: 0.1074
2025-12-08 20:52:20,179: t15.2025.01.10 val PER: 0.2893
2025-12-08 20:52:20,179: t15.2025.01.12 val PER: 0.0885
2025-12-08 20:52:20,179: t15.2025.03.14 val PER: 0.2825
2025-12-08 20:52:20,180: t15.2025.03.16 val PER: 0.1440
2025-12-08 20:52:20,180: t15.2025.03.30 val PER: 0.2460
2025-12-08 20:52:20,180: t15.2025.04.13 val PER: 0.2325
2025-12-08 20:52:20,180: New best test PER 0.1232 --> 0.1223
2025-12-08 20:52:20,180: Checkpointing model
2025-12-08 20:52:21,233: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 20:52:53,298: Train batch 38200: loss: 0.08 grad norm: 2.35 time: 0.122
2025-12-08 20:53:25,477: Train batch 38400: loss: 0.09 grad norm: 1.91 time: 0.158
2025-12-08 20:53:58,385: Train batch 38600: loss: 0.07 grad norm: 1.26 time: 0.099
2025-12-08 20:54:32,388: Train batch 38800: loss: 0.03 grad norm: 1.08 time: 0.076
2025-12-08 20:55:05,490: Train batch 39000: loss: 0.03 grad norm: 1.06 time: 0.174
2025-12-08 20:55:40,359: Train batch 39200: loss: 0.25 grad norm: 3.85 time: 0.177
2025-12-08 20:56:09,648: Train batch 39400: loss: 0.13 grad norm: 3.57 time: 0.103
2025-12-08 20:56:37,686: Train batch 39600: loss: 0.20 grad norm: 2.33 time: 0.130
2025-12-08 20:57:06,994: Train batch 39800: loss: 0.01 grad norm: 0.28 time: 0.085
2025-12-08 20:57:38,139: Train batch 40000: loss: 0.07 grad norm: 2.13 time: 0.072
2025-12-08 20:57:38,140: Running test after training batch: 40000
2025-12-08 20:57:48,160: Val batch 40000: PER (avg): 0.1233 CTC Loss (avg): 32.4314 time: 10.020
2025-12-08 20:57:48,161: t15.2023.08.11 val PER: 1.0000
2025-12-08 20:57:48,161: t15.2023.08.13 val PER: 0.0769
2025-12-08 20:57:48,161: t15.2023.08.18 val PER: 0.0763
2025-12-08 20:57:48,161: t15.2023.08.20 val PER: 0.0747
2025-12-08 20:57:48,161: t15.2023.08.25 val PER: 0.0768
2025-12-08 20:57:48,161: t15.2023.08.27 val PER: 0.1576
2025-12-08 20:57:48,161: t15.2023.09.01 val PER: 0.0463
2025-12-08 20:57:48,161: t15.2023.09.03 val PER: 0.1021
2025-12-08 20:57:48,161: t15.2023.09.24 val PER: 0.0934
2025-12-08 20:57:48,161: t15.2023.09.29 val PER: 0.1225
2025-12-08 20:57:48,161: t15.2023.10.01 val PER: 0.1671
2025-12-08 20:57:48,161: t15.2023.10.06 val PER: 0.0797
2025-12-08 20:57:48,161: t15.2023.10.08 val PER: 0.1664
2025-12-08 20:57:48,161: t15.2023.10.13 val PER: 0.1877
2025-12-08 20:57:48,161: t15.2023.10.15 val PER: 0.1154
2025-12-08 20:57:48,161: t15.2023.10.20 val PER: 0.2114
2025-12-08 20:57:48,161: t15.2023.10.22 val PER: 0.1125
2025-12-08 20:57:48,161: t15.2023.11.03 val PER: 0.1967
2025-12-08 20:57:48,162: t15.2023.11.04 val PER: 0.0341
2025-12-08 20:57:48,162: t15.2023.11.17 val PER: 0.0342
2025-12-08 20:57:48,162: t15.2023.11.19 val PER: 0.0579
2025-12-08 20:57:48,162: t15.2023.11.26 val PER: 0.0732
2025-12-08 20:57:48,162: t15.2023.12.03 val PER: 0.0651
2025-12-08 20:57:48,162: t15.2023.12.08 val PER: 0.0526
2025-12-08 20:57:48,162: t15.2023.12.10 val PER: 0.0657
2025-12-08 20:57:48,162: t15.2023.12.17 val PER: 0.1351
2025-12-08 20:57:48,162: t15.2023.12.29 val PER: 0.1023
2025-12-08 20:57:48,162: t15.2024.02.25 val PER: 0.0829
2025-12-08 20:57:48,162: t15.2024.03.03 val PER: 1.0000
2025-12-08 20:57:48,162: t15.2024.03.08 val PER: 0.2105
2025-12-08 20:57:48,162: t15.2024.03.15 val PER: 0.1845
2025-12-08 20:57:48,162: t15.2024.03.17 val PER: 0.0753
2025-12-08 20:57:48,162: t15.2024.04.25 val PER: 1.0000
2025-12-08 20:57:48,162: t15.2024.04.28 val PER: 1.0000
2025-12-08 20:57:48,162: t15.2024.05.10 val PER: 0.1322
2025-12-08 20:57:48,162: t15.2024.06.14 val PER: 0.1278
2025-12-08 20:57:48,162: t15.2024.07.19 val PER: 0.1846
2025-12-08 20:57:48,163: t15.2024.07.21 val PER: 0.0772
2025-12-08 20:57:48,163: t15.2024.07.28 val PER: 0.1140
2025-12-08 20:57:48,163: t15.2025.01.10 val PER: 0.2837
2025-12-08 20:57:48,163: t15.2025.01.12 val PER: 0.1008
2025-12-08 20:57:48,163: t15.2025.03.14 val PER: 0.2914
2025-12-08 20:57:48,163: t15.2025.03.16 val PER: 0.1401
2025-12-08 20:57:48,163: t15.2025.03.30 val PER: 0.2540
2025-12-08 20:57:48,163: t15.2025.04.13 val PER: 0.2026
2025-12-08 20:58:18,971: Train batch 40200: loss: 0.02 grad norm: 0.35 time: 0.137
2025-12-08 20:58:49,815: Train batch 40400: loss: 0.06 grad norm: 2.01 time: 0.123
2025-12-08 20:59:19,826: Train batch 40600: loss: 0.09 grad norm: 2.40 time: 0.151
2025-12-08 20:59:50,924: Train batch 40800: loss: 0.03 grad norm: 1.13 time: 0.139
2025-12-08 21:00:22,721: Train batch 41000: loss: 0.04 grad norm: 1.79 time: 0.112
2025-12-08 21:00:54,829: Train batch 41200: loss: 0.03 grad norm: 0.73 time: 0.111
2025-12-08 21:01:22,807: Train batch 41400: loss: 0.03 grad norm: 1.26 time: 0.097
2025-12-08 21:01:50,192: Train batch 41600: loss: 0.21 grad norm: 4.97 time: 0.098
2025-12-08 21:02:19,457: Train batch 41800: loss: 0.02 grad norm: 0.66 time: 0.088
2025-12-08 21:02:48,627: Train batch 42000: loss: 0.15 grad norm: 3.60 time: 0.108
2025-12-08 21:02:48,628: Running test after training batch: 42000
2025-12-08 21:02:58,914: Val batch 42000: PER (avg): 0.1200 CTC Loss (avg): 33.0988 time: 10.286
2025-12-08 21:02:58,914: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:02:58,914: t15.2023.08.13 val PER: 0.0821
2025-12-08 21:02:58,914: t15.2023.08.18 val PER: 0.0704
2025-12-08 21:02:58,914: t15.2023.08.20 val PER: 0.0572
2025-12-08 21:02:58,914: t15.2023.08.25 val PER: 0.0738
2025-12-08 21:02:58,914: t15.2023.08.27 val PER: 0.1511
2025-12-08 21:02:58,914: t15.2023.09.01 val PER: 0.0503
2025-12-08 21:02:58,914: t15.2023.09.03 val PER: 0.1211
2025-12-08 21:02:58,914: t15.2023.09.24 val PER: 0.0886
2025-12-08 21:02:58,914: t15.2023.09.29 val PER: 0.1225
2025-12-08 21:02:58,914: t15.2023.10.01 val PER: 0.1585
2025-12-08 21:02:58,914: t15.2023.10.06 val PER: 0.0678
2025-12-08 21:02:58,915: t15.2023.10.08 val PER: 0.1908
2025-12-08 21:02:58,915: t15.2023.10.13 val PER: 0.1761
2025-12-08 21:02:58,915: t15.2023.10.15 val PER: 0.1305
2025-12-08 21:02:58,915: t15.2023.10.20 val PER: 0.2081
2025-12-08 21:02:58,915: t15.2023.10.22 val PER: 0.1180
2025-12-08 21:02:58,915: t15.2023.11.03 val PER: 0.1805
2025-12-08 21:02:58,915: t15.2023.11.04 val PER: 0.0341
2025-12-08 21:02:58,915: t15.2023.11.17 val PER: 0.0187
2025-12-08 21:02:58,915: t15.2023.11.19 val PER: 0.0619
2025-12-08 21:02:58,915: t15.2023.11.26 val PER: 0.0565
2025-12-08 21:02:58,915: t15.2023.12.03 val PER: 0.0735
2025-12-08 21:02:58,915: t15.2023.12.08 val PER: 0.0619
2025-12-08 21:02:58,915: t15.2023.12.10 val PER: 0.0460
2025-12-08 21:02:58,915: t15.2023.12.17 val PER: 0.1299
2025-12-08 21:02:58,915: t15.2023.12.29 val PER: 0.0872
2025-12-08 21:02:58,915: t15.2024.02.25 val PER: 0.0646
2025-12-08 21:02:58,915: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:02:58,915: t15.2024.03.08 val PER: 0.1892
2025-12-08 21:02:58,915: t15.2024.03.15 val PER: 0.1895
2025-12-08 21:02:58,916: t15.2024.03.17 val PER: 0.0816
2025-12-08 21:02:58,916: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:02:58,916: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:02:58,916: t15.2024.05.10 val PER: 0.1233
2025-12-08 21:02:58,916: t15.2024.06.14 val PER: 0.1278
2025-12-08 21:02:58,916: t15.2024.07.19 val PER: 0.1760
2025-12-08 21:02:58,916: t15.2024.07.21 val PER: 0.0786
2025-12-08 21:02:58,916: t15.2024.07.28 val PER: 0.1037
2025-12-08 21:02:58,916: t15.2025.01.10 val PER: 0.2824
2025-12-08 21:02:58,916: t15.2025.01.12 val PER: 0.0855
2025-12-08 21:02:58,916: t15.2025.03.14 val PER: 0.2973
2025-12-08 21:02:58,916: t15.2025.03.16 val PER: 0.1453
2025-12-08 21:02:58,916: t15.2025.03.30 val PER: 0.2621
2025-12-08 21:02:58,916: t15.2025.04.13 val PER: 0.1897
2025-12-08 21:02:58,916: New best test PER 0.1223 --> 0.1200
2025-12-08 21:02:58,916: Checkpointing model
2025-12-08 21:03:00,052: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 21:03:29,155: Train batch 42200: loss: 0.02 grad norm: 0.63 time: 0.133
2025-12-08 21:03:57,881: Train batch 42400: loss: 0.08 grad norm: 2.32 time: 0.101
2025-12-08 21:04:27,529: Train batch 42600: loss: 0.10 grad norm: 1.67 time: 0.087
2025-12-08 21:04:56,474: Train batch 42800: loss: 0.04 grad norm: 2.50 time: 0.136
2025-12-08 21:05:26,140: Train batch 43000: loss: 0.03 grad norm: 1.55 time: 0.103
2025-12-08 21:05:56,818: Train batch 43200: loss: 0.02 grad norm: 0.54 time: 0.141
2025-12-08 21:06:27,879: Train batch 43400: loss: 0.02 grad norm: 0.70 time: 0.118
2025-12-08 21:06:59,443: Train batch 43600: loss: 0.16 grad norm: 4.10 time: 0.127
2025-12-08 21:07:31,367: Train batch 43800: loss: 0.06 grad norm: 1.65 time: 0.106
2025-12-08 21:08:02,212: Train batch 44000: loss: 0.21 grad norm: 6.47 time: 0.100
2025-12-08 21:08:02,213: Running test after training batch: 44000
2025-12-08 21:08:12,427: Val batch 44000: PER (avg): 0.1206 CTC Loss (avg): 33.8144 time: 10.214
2025-12-08 21:08:12,427: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:08:12,427: t15.2023.08.13 val PER: 0.0821
2025-12-08 21:08:12,427: t15.2023.08.18 val PER: 0.0662
2025-12-08 21:08:12,427: t15.2023.08.20 val PER: 0.0699
2025-12-08 21:08:12,428: t15.2023.08.25 val PER: 0.0873
2025-12-08 21:08:12,428: t15.2023.08.27 val PER: 0.1431
2025-12-08 21:08:12,428: t15.2023.09.01 val PER: 0.0576
2025-12-08 21:08:12,428: t15.2023.09.03 val PER: 0.1235
2025-12-08 21:08:12,428: t15.2023.09.24 val PER: 0.0934
2025-12-08 21:08:12,428: t15.2023.09.29 val PER: 0.1130
2025-12-08 21:08:12,428: t15.2023.10.01 val PER: 0.1440
2025-12-08 21:08:12,428: t15.2023.10.06 val PER: 0.0775
2025-12-08 21:08:12,428: t15.2023.10.08 val PER: 0.1719
2025-12-08 21:08:12,428: t15.2023.10.13 val PER: 0.1808
2025-12-08 21:08:12,428: t15.2023.10.15 val PER: 0.1187
2025-12-08 21:08:12,428: t15.2023.10.20 val PER: 0.2282
2025-12-08 21:08:12,428: t15.2023.10.22 val PER: 0.1347
2025-12-08 21:08:12,428: t15.2023.11.03 val PER: 0.1764
2025-12-08 21:08:12,428: t15.2023.11.04 val PER: 0.0410
2025-12-08 21:08:12,428: t15.2023.11.17 val PER: 0.0327
2025-12-08 21:08:12,428: t15.2023.11.19 val PER: 0.0599
2025-12-08 21:08:12,428: t15.2023.11.26 val PER: 0.0645
2025-12-08 21:08:12,429: t15.2023.12.03 val PER: 0.0693
2025-12-08 21:08:12,429: t15.2023.12.08 val PER: 0.0586
2025-12-08 21:08:12,429: t15.2023.12.10 val PER: 0.0591
2025-12-08 21:08:12,429: t15.2023.12.17 val PER: 0.1351
2025-12-08 21:08:12,429: t15.2023.12.29 val PER: 0.0933
2025-12-08 21:08:12,429: t15.2024.02.25 val PER: 0.0674
2025-12-08 21:08:12,429: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:08:12,429: t15.2024.03.08 val PER: 0.2048
2025-12-08 21:08:12,429: t15.2024.03.15 val PER: 0.1857
2025-12-08 21:08:12,429: t15.2024.03.17 val PER: 0.0788
2025-12-08 21:08:12,429: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:08:12,429: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:08:12,429: t15.2024.05.10 val PER: 0.1471
2025-12-08 21:08:12,429: t15.2024.06.14 val PER: 0.1372
2025-12-08 21:08:12,429: t15.2024.07.19 val PER: 0.1852
2025-12-08 21:08:12,429: t15.2024.07.21 val PER: 0.0745
2025-12-08 21:08:12,429: t15.2024.07.28 val PER: 0.0956
2025-12-08 21:08:12,429: t15.2025.01.10 val PER: 0.2810
2025-12-08 21:08:12,429: t15.2025.01.12 val PER: 0.0831
2025-12-08 21:08:12,430: t15.2025.03.14 val PER: 0.2811
2025-12-08 21:08:12,430: t15.2025.03.16 val PER: 0.1414
2025-12-08 21:08:12,430: t15.2025.03.30 val PER: 0.2506
2025-12-08 21:08:12,430: t15.2025.04.13 val PER: 0.1997
2025-12-08 21:08:43,460: Train batch 44200: loss: 0.21 grad norm: 1.49 time: 0.098
2025-12-08 21:09:14,356: Train batch 44400: loss: 0.31 grad norm: 2.98 time: 0.108
2025-12-08 21:09:43,659: Train batch 44600: loss: 0.09 grad norm: 1.68 time: 0.093
2025-12-08 21:10:14,239: Train batch 44800: loss: 0.06 grad norm: 2.48 time: 0.124
2025-12-08 21:10:44,922: Train batch 45000: loss: 0.05 grad norm: 1.75 time: 0.094
2025-12-08 21:11:15,716: Train batch 45200: loss: 0.14 grad norm: 1.96 time: 0.084
2025-12-08 21:11:48,096: Train batch 45400: loss: 0.08 grad norm: 2.26 time: 0.094
2025-12-08 21:12:18,048: Train batch 45600: loss: 0.10 grad norm: 2.04 time: 0.092
2025-12-08 21:12:49,739: Train batch 45800: loss: 0.05 grad norm: 3.12 time: 0.124
2025-12-08 21:13:21,292: Train batch 46000: loss: 0.02 grad norm: 1.37 time: 0.089
2025-12-08 21:13:21,292: Running test after training batch: 46000
2025-12-08 21:13:31,663: Val batch 46000: PER (avg): 0.1173 CTC Loss (avg): 33.4937 time: 10.371
2025-12-08 21:13:31,663: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:13:31,663: t15.2023.08.13 val PER: 0.0821
2025-12-08 21:13:31,663: t15.2023.08.18 val PER: 0.0805
2025-12-08 21:13:31,663: t15.2023.08.20 val PER: 0.0612
2025-12-08 21:13:31,663: t15.2023.08.25 val PER: 0.0708
2025-12-08 21:13:31,663: t15.2023.08.27 val PER: 0.1190
2025-12-08 21:13:31,663: t15.2023.09.01 val PER: 0.0552
2025-12-08 21:13:31,663: t15.2023.09.03 val PER: 0.1081
2025-12-08 21:13:31,663: t15.2023.09.24 val PER: 0.0959
2025-12-08 21:13:31,663: t15.2023.09.29 val PER: 0.1206
2025-12-08 21:13:31,664: t15.2023.10.01 val PER: 0.1519
2025-12-08 21:13:31,664: t15.2023.10.06 val PER: 0.0700
2025-12-08 21:13:31,664: t15.2023.10.08 val PER: 0.1678
2025-12-08 21:13:31,664: t15.2023.10.13 val PER: 0.1761
2025-12-08 21:13:31,664: t15.2023.10.15 val PER: 0.1127
2025-12-08 21:13:31,664: t15.2023.10.20 val PER: 0.2148
2025-12-08 21:13:31,664: t15.2023.10.22 val PER: 0.1292
2025-12-08 21:13:31,664: t15.2023.11.03 val PER: 0.1784
2025-12-08 21:13:31,664: t15.2023.11.04 val PER: 0.0410
2025-12-08 21:13:31,664: t15.2023.11.17 val PER: 0.0280
2025-12-08 21:13:31,664: t15.2023.11.19 val PER: 0.0379
2025-12-08 21:13:31,664: t15.2023.11.26 val PER: 0.0681
2025-12-08 21:13:31,664: t15.2023.12.03 val PER: 0.0609
2025-12-08 21:13:31,664: t15.2023.12.08 val PER: 0.0526
2025-12-08 21:13:31,664: t15.2023.12.10 val PER: 0.0394
2025-12-08 21:13:31,664: t15.2023.12.17 val PER: 0.1310
2025-12-08 21:13:31,664: t15.2023.12.29 val PER: 0.0981
2025-12-08 21:13:31,664: t15.2024.02.25 val PER: 0.0716
2025-12-08 21:13:31,665: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:13:31,665: t15.2024.03.08 val PER: 0.1963
2025-12-08 21:13:31,665: t15.2024.03.15 val PER: 0.1701
2025-12-08 21:13:31,665: t15.2024.03.17 val PER: 0.0704
2025-12-08 21:13:31,665: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:13:31,665: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:13:31,665: t15.2024.05.10 val PER: 0.1367
2025-12-08 21:13:31,665: t15.2024.06.14 val PER: 0.1246
2025-12-08 21:13:31,665: t15.2024.07.19 val PER: 0.1747
2025-12-08 21:13:31,665: t15.2024.07.21 val PER: 0.0621
2025-12-08 21:13:31,665: t15.2024.07.28 val PER: 0.1022
2025-12-08 21:13:31,665: t15.2025.01.10 val PER: 0.2769
2025-12-08 21:13:31,665: t15.2025.01.12 val PER: 0.0901
2025-12-08 21:13:31,665: t15.2025.03.14 val PER: 0.2796
2025-12-08 21:13:31,665: t15.2025.03.16 val PER: 0.1623
2025-12-08 21:13:31,665: t15.2025.03.30 val PER: 0.2517
2025-12-08 21:13:31,665: t15.2025.04.13 val PER: 0.1983
2025-12-08 21:13:31,665: New best test PER 0.1200 --> 0.1173
2025-12-08 21:13:31,665: Checkpointing model
2025-12-08 21:13:32,760: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 21:14:02,292: Train batch 46200: loss: 0.03 grad norm: 2.63 time: 0.087
2025-12-08 21:14:32,304: Train batch 46400: loss: 0.02 grad norm: 1.09 time: 0.106
2025-12-08 21:15:03,836: Train batch 46600: loss: 0.16 grad norm: 2.91 time: 0.072
2025-12-08 21:15:37,622: Train batch 46800: loss: 0.17 grad norm: 4.46 time: 0.158
2025-12-08 21:16:10,673: Train batch 47000: loss: 0.09 grad norm: 1.05 time: 0.091
2025-12-08 21:16:42,847: Train batch 47200: loss: 0.11 grad norm: 1.93 time: 0.108
2025-12-08 21:17:16,161: Train batch 47400: loss: 0.00 grad norm: 0.14 time: 0.157
2025-12-08 21:17:50,090: Train batch 47600: loss: 0.09 grad norm: 4.00 time: 0.074
2025-12-08 21:18:19,582: Train batch 47800: loss: 0.01 grad norm: 0.43 time: 0.122
2025-12-08 21:18:49,484: Train batch 48000: loss: 0.08 grad norm: 2.58 time: 0.126
2025-12-08 21:18:49,485: Running test after training batch: 48000
2025-12-08 21:18:59,575: Val batch 48000: PER (avg): 0.1191 CTC Loss (avg): 35.1078 time: 10.090
2025-12-08 21:18:59,575: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:18:59,576: t15.2023.08.13 val PER: 0.0748
2025-12-08 21:18:59,576: t15.2023.08.18 val PER: 0.0738
2025-12-08 21:18:59,576: t15.2023.08.20 val PER: 0.0604
2025-12-08 21:18:59,576: t15.2023.08.25 val PER: 0.0768
2025-12-08 21:18:59,576: t15.2023.08.27 val PER: 0.1254
2025-12-08 21:18:59,576: t15.2023.09.01 val PER: 0.0519
2025-12-08 21:18:59,576: t15.2023.09.03 val PER: 0.1247
2025-12-08 21:18:59,576: t15.2023.09.24 val PER: 0.0934
2025-12-08 21:18:59,576: t15.2023.09.29 val PER: 0.1200
2025-12-08 21:18:59,576: t15.2023.10.01 val PER: 0.1532
2025-12-08 21:18:59,576: t15.2023.10.06 val PER: 0.0818
2025-12-08 21:18:59,576: t15.2023.10.08 val PER: 0.1881
2025-12-08 21:18:59,576: t15.2023.10.13 val PER: 0.1746
2025-12-08 21:18:59,576: t15.2023.10.15 val PER: 0.1140
2025-12-08 21:18:59,576: t15.2023.10.20 val PER: 0.1846
2025-12-08 21:18:59,576: t15.2023.10.22 val PER: 0.1203
2025-12-08 21:18:59,576: t15.2023.11.03 val PER: 0.1805
2025-12-08 21:18:59,576: t15.2023.11.04 val PER: 0.0546
2025-12-08 21:18:59,577: t15.2023.11.17 val PER: 0.0358
2025-12-08 21:18:59,577: t15.2023.11.19 val PER: 0.0519
2025-12-08 21:18:59,577: t15.2023.11.26 val PER: 0.0623
2025-12-08 21:18:59,577: t15.2023.12.03 val PER: 0.0704
2025-12-08 21:18:59,577: t15.2023.12.08 val PER: 0.0619
2025-12-08 21:18:59,577: t15.2023.12.10 val PER: 0.0473
2025-12-08 21:18:59,577: t15.2023.12.17 val PER: 0.1351
2025-12-08 21:18:59,577: t15.2023.12.29 val PER: 0.1071
2025-12-08 21:18:59,577: t15.2024.02.25 val PER: 0.0829
2025-12-08 21:18:59,577: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:18:59,577: t15.2024.03.08 val PER: 0.1849
2025-12-08 21:18:59,577: t15.2024.03.15 val PER: 0.1851
2025-12-08 21:18:59,577: t15.2024.03.17 val PER: 0.0732
2025-12-08 21:18:59,577: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:18:59,577: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:18:59,577: t15.2024.05.10 val PER: 0.1397
2025-12-08 21:18:59,577: t15.2024.06.14 val PER: 0.1246
2025-12-08 21:18:59,577: t15.2024.07.19 val PER: 0.1773
2025-12-08 21:18:59,577: t15.2024.07.21 val PER: 0.0710
2025-12-08 21:18:59,578: t15.2024.07.28 val PER: 0.1007
2025-12-08 21:18:59,578: t15.2025.01.10 val PER: 0.2521
2025-12-08 21:18:59,578: t15.2025.01.12 val PER: 0.0885
2025-12-08 21:18:59,578: t15.2025.03.14 val PER: 0.2959
2025-12-08 21:18:59,578: t15.2025.03.16 val PER: 0.1545
2025-12-08 21:18:59,578: t15.2025.03.30 val PER: 0.2230
2025-12-08 21:18:59,578: t15.2025.04.13 val PER: 0.1997
2025-12-08 21:19:30,145: Train batch 48200: loss: 0.31 grad norm: 4.70 time: 0.090
2025-12-08 21:20:02,503: Train batch 48400: loss: 0.04 grad norm: 1.40 time: 0.122
2025-12-08 21:20:32,998: Train batch 48600: loss: 0.14 grad norm: 1.92 time: 0.153
2025-12-08 21:21:03,471: Train batch 48800: loss: 0.11 grad norm: 2.30 time: 0.125
2025-12-08 21:21:31,134: Train batch 49000: loss: 0.11 grad norm: 2.43 time: 0.123
2025-12-08 21:22:01,719: Train batch 49200: loss: 0.05 grad norm: 1.90 time: 0.108
2025-12-08 21:22:31,617: Train batch 49400: loss: 0.01 grad norm: 0.24 time: 0.121
2025-12-08 21:23:00,784: Train batch 49600: loss: 0.04 grad norm: 3.37 time: 0.130
2025-12-08 21:23:29,596: Train batch 49800: loss: 0.10 grad norm: 2.76 time: 0.105
2025-12-08 21:23:58,981: Train batch 50000: loss: 0.01 grad norm: 0.17 time: 0.099
2025-12-08 21:23:58,981: Running test after training batch: 50000
2025-12-08 21:24:09,054: Val batch 50000: PER (avg): 0.1176 CTC Loss (avg): 33.0626 time: 10.072
2025-12-08 21:24:09,054: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:24:09,054: t15.2023.08.13 val PER: 0.0800
2025-12-08 21:24:09,054: t15.2023.08.18 val PER: 0.0721
2025-12-08 21:24:09,054: t15.2023.08.20 val PER: 0.0635
2025-12-08 21:24:09,054: t15.2023.08.25 val PER: 0.0678
2025-12-08 21:24:09,054: t15.2023.08.27 val PER: 0.1254
2025-12-08 21:24:09,054: t15.2023.09.01 val PER: 0.0528
2025-12-08 21:24:09,054: t15.2023.09.03 val PER: 0.1128
2025-12-08 21:24:09,055: t15.2023.09.24 val PER: 0.1007
2025-12-08 21:24:09,055: t15.2023.09.29 val PER: 0.1225
2025-12-08 21:24:09,055: t15.2023.10.01 val PER: 0.1526
2025-12-08 21:24:09,055: t15.2023.10.06 val PER: 0.0850
2025-12-08 21:24:09,055: t15.2023.10.08 val PER: 0.1678
2025-12-08 21:24:09,055: t15.2023.10.13 val PER: 0.1753
2025-12-08 21:24:09,055: t15.2023.10.15 val PER: 0.1173
2025-12-08 21:24:09,055: t15.2023.10.20 val PER: 0.2013
2025-12-08 21:24:09,055: t15.2023.10.22 val PER: 0.1269
2025-12-08 21:24:09,055: t15.2023.11.03 val PER: 0.1662
2025-12-08 21:24:09,055: t15.2023.11.04 val PER: 0.0307
2025-12-08 21:24:09,055: t15.2023.11.17 val PER: 0.0358
2025-12-08 21:24:09,055: t15.2023.11.19 val PER: 0.0898
2025-12-08 21:24:09,055: t15.2023.11.26 val PER: 0.0630
2025-12-08 21:24:09,055: t15.2023.12.03 val PER: 0.0672
2025-12-08 21:24:09,055: t15.2023.12.08 val PER: 0.0533
2025-12-08 21:24:09,055: t15.2023.12.10 val PER: 0.0696
2025-12-08 21:24:09,055: t15.2023.12.17 val PER: 0.1289
2025-12-08 21:24:09,055: t15.2023.12.29 val PER: 0.0865
2025-12-08 21:24:09,056: t15.2024.02.25 val PER: 0.0758
2025-12-08 21:24:09,056: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:24:09,056: t15.2024.03.08 val PER: 0.1935
2025-12-08 21:24:09,056: t15.2024.03.15 val PER: 0.1945
2025-12-08 21:24:09,056: t15.2024.03.17 val PER: 0.0683
2025-12-08 21:24:09,056: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:24:09,056: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:24:09,056: t15.2024.05.10 val PER: 0.1412
2025-12-08 21:24:09,056: t15.2024.06.14 val PER: 0.1372
2025-12-08 21:24:09,056: t15.2024.07.19 val PER: 0.1655
2025-12-08 21:24:09,056: t15.2024.07.21 val PER: 0.0738
2025-12-08 21:24:09,056: t15.2024.07.28 val PER: 0.1029
2025-12-08 21:24:09,056: t15.2025.01.10 val PER: 0.2631
2025-12-08 21:24:09,056: t15.2025.01.12 val PER: 0.0708
2025-12-08 21:24:09,056: t15.2025.03.14 val PER: 0.2485
2025-12-08 21:24:09,056: t15.2025.03.16 val PER: 0.1440
2025-12-08 21:24:09,056: t15.2025.03.30 val PER: 0.2552
2025-12-08 21:24:09,056: t15.2025.04.13 val PER: 0.1926
2025-12-08 21:24:38,178: Train batch 50200: loss: 0.09 grad norm: 3.79 time: 0.146
2025-12-08 21:25:06,934: Train batch 50400: loss: 0.06 grad norm: 1.99 time: 0.113
2025-12-08 21:25:35,145: Train batch 50600: loss: 0.01 grad norm: 0.11 time: 0.102
2025-12-08 21:26:03,368: Train batch 50800: loss: 0.02 grad norm: 0.80 time: 0.099
2025-12-08 21:26:31,849: Train batch 51000: loss: 0.02 grad norm: 1.08 time: 0.113
2025-12-08 21:27:01,146: Train batch 51200: loss: 0.09 grad norm: 1.87 time: 0.096
2025-12-08 21:27:30,302: Train batch 51400: loss: 0.08 grad norm: 2.57 time: 0.099
2025-12-08 21:27:59,922: Train batch 51600: loss: 0.06 grad norm: 1.57 time: 0.121
2025-12-08 21:28:30,162: Train batch 51800: loss: 0.18 grad norm: 5.21 time: 0.119
2025-12-08 21:28:58,323: Train batch 52000: loss: 0.06 grad norm: 1.80 time: 0.116
2025-12-08 21:28:58,324: Running test after training batch: 52000
2025-12-08 21:29:08,574: Val batch 52000: PER (avg): 0.1202 CTC Loss (avg): 36.0881 time: 10.250
2025-12-08 21:29:08,574: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:29:08,574: t15.2023.08.13 val PER: 0.0780
2025-12-08 21:29:08,574: t15.2023.08.18 val PER: 0.0712
2025-12-08 21:29:08,574: t15.2023.08.20 val PER: 0.0691
2025-12-08 21:29:08,574: t15.2023.08.25 val PER: 0.0873
2025-12-08 21:29:08,574: t15.2023.08.27 val PER: 0.1399
2025-12-08 21:29:08,574: t15.2023.09.01 val PER: 0.0560
2025-12-08 21:29:08,574: t15.2023.09.03 val PER: 0.1247
2025-12-08 21:29:08,574: t15.2023.09.24 val PER: 0.0874
2025-12-08 21:29:08,574: t15.2023.09.29 val PER: 0.1257
2025-12-08 21:29:08,574: t15.2023.10.01 val PER: 0.1334
2025-12-08 21:29:08,574: t15.2023.10.06 val PER: 0.0861
2025-12-08 21:29:08,575: t15.2023.10.08 val PER: 0.1732
2025-12-08 21:29:08,575: t15.2023.10.13 val PER: 0.1715
2025-12-08 21:29:08,575: t15.2023.10.15 val PER: 0.1272
2025-12-08 21:29:08,575: t15.2023.10.20 val PER: 0.2114
2025-12-08 21:29:08,575: t15.2023.10.22 val PER: 0.1158
2025-12-08 21:29:08,575: t15.2023.11.03 val PER: 0.1703
2025-12-08 21:29:08,575: t15.2023.11.04 val PER: 0.0375
2025-12-08 21:29:08,575: t15.2023.11.17 val PER: 0.0451
2025-12-08 21:29:08,575: t15.2023.11.19 val PER: 0.0699
2025-12-08 21:29:08,575: t15.2023.11.26 val PER: 0.0710
2025-12-08 21:29:08,575: t15.2023.12.03 val PER: 0.0567
2025-12-08 21:29:08,575: t15.2023.12.08 val PER: 0.0546
2025-12-08 21:29:08,575: t15.2023.12.10 val PER: 0.0473
2025-12-08 21:29:08,575: t15.2023.12.17 val PER: 0.1445
2025-12-08 21:29:08,575: t15.2023.12.29 val PER: 0.0968
2025-12-08 21:29:08,575: t15.2024.02.25 val PER: 0.0843
2025-12-08 21:29:08,575: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:29:08,575: t15.2024.03.08 val PER: 0.1863
2025-12-08 21:29:08,575: t15.2024.03.15 val PER: 0.1832
2025-12-08 21:29:08,576: t15.2024.03.17 val PER: 0.0816
2025-12-08 21:29:08,576: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:29:08,576: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:29:08,576: t15.2024.05.10 val PER: 0.1456
2025-12-08 21:29:08,576: t15.2024.06.14 val PER: 0.1451
2025-12-08 21:29:08,576: t15.2024.07.19 val PER: 0.1800
2025-12-08 21:29:08,576: t15.2024.07.21 val PER: 0.0710
2025-12-08 21:29:08,576: t15.2024.07.28 val PER: 0.1066
2025-12-08 21:29:08,576: t15.2025.01.10 val PER: 0.2645
2025-12-08 21:29:08,576: t15.2025.01.12 val PER: 0.0785
2025-12-08 21:29:08,576: t15.2025.03.14 val PER: 0.2959
2025-12-08 21:29:08,576: t15.2025.03.16 val PER: 0.1479
2025-12-08 21:29:08,576: t15.2025.03.30 val PER: 0.2471
2025-12-08 21:29:08,576: t15.2025.04.13 val PER: 0.1997
2025-12-08 21:29:38,406: Train batch 52200: loss: 0.02 grad norm: 0.56 time: 0.126
2025-12-08 21:30:09,033: Train batch 52400: loss: 0.01 grad norm: 0.39 time: 0.104
2025-12-08 21:30:40,815: Train batch 52600: loss: 0.07 grad norm: 2.34 time: 0.138
2025-12-08 21:31:12,197: Train batch 52800: loss: 0.12 grad norm: 2.22 time: 0.181
2025-12-08 21:31:45,213: Train batch 53000: loss: 0.01 grad norm: 0.52 time: 0.135
2025-12-08 21:32:17,816: Train batch 53200: loss: 0.07 grad norm: 3.78 time: 0.139
2025-12-08 21:32:48,269: Train batch 53400: loss: 0.00 grad norm: 0.19 time: 0.123
2025-12-08 21:33:19,801: Train batch 53600: loss: 0.05 grad norm: 2.25 time: 0.157
2025-12-08 21:33:50,151: Train batch 53800: loss: 0.00 grad norm: 0.26 time: 0.153
2025-12-08 21:34:21,353: Train batch 54000: loss: 0.08 grad norm: 4.01 time: 0.093
2025-12-08 21:34:21,353: Running test after training batch: 54000
2025-12-08 21:34:34,718: Val batch 54000: PER (avg): 0.1205 CTC Loss (avg): 34.6659 time: 13.365
2025-12-08 21:34:34,719: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:34:34,719: t15.2023.08.13 val PER: 0.0832
2025-12-08 21:34:34,719: t15.2023.08.18 val PER: 0.0780
2025-12-08 21:34:34,719: t15.2023.08.20 val PER: 0.0667
2025-12-08 21:34:34,719: t15.2023.08.25 val PER: 0.0753
2025-12-08 21:34:34,719: t15.2023.08.27 val PER: 0.1399
2025-12-08 21:34:34,719: t15.2023.09.01 val PER: 0.0446
2025-12-08 21:34:34,719: t15.2023.09.03 val PER: 0.1318
2025-12-08 21:34:34,719: t15.2023.09.24 val PER: 0.0886
2025-12-08 21:34:34,719: t15.2023.09.29 val PER: 0.1213
2025-12-08 21:34:34,719: t15.2023.10.01 val PER: 0.1413
2025-12-08 21:34:34,719: t15.2023.10.06 val PER: 0.0818
2025-12-08 21:34:34,719: t15.2023.10.08 val PER: 0.1800
2025-12-08 21:34:34,719: t15.2023.10.13 val PER: 0.1668
2025-12-08 21:34:34,720: t15.2023.10.15 val PER: 0.1272
2025-12-08 21:34:34,720: t15.2023.10.20 val PER: 0.2248
2025-12-08 21:34:34,720: t15.2023.10.22 val PER: 0.1225
2025-12-08 21:34:34,720: t15.2023.11.03 val PER: 0.1710
2025-12-08 21:34:34,720: t15.2023.11.04 val PER: 0.0512
2025-12-08 21:34:34,720: t15.2023.11.17 val PER: 0.0420
2025-12-08 21:34:34,720: t15.2023.11.19 val PER: 0.0659
2025-12-08 21:34:34,720: t15.2023.11.26 val PER: 0.0601
2025-12-08 21:34:34,720: t15.2023.12.03 val PER: 0.0651
2025-12-08 21:34:34,720: t15.2023.12.08 val PER: 0.0479
2025-12-08 21:34:34,720: t15.2023.12.10 val PER: 0.0526
2025-12-08 21:34:34,720: t15.2023.12.17 val PER: 0.1351
2025-12-08 21:34:34,720: t15.2023.12.29 val PER: 0.0988
2025-12-08 21:34:34,720: t15.2024.02.25 val PER: 0.0787
2025-12-08 21:34:34,720: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:34:34,720: t15.2024.03.08 val PER: 0.1878
2025-12-08 21:34:34,721: t15.2024.03.15 val PER: 0.1914
2025-12-08 21:34:34,721: t15.2024.03.17 val PER: 0.0809
2025-12-08 21:34:34,721: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:34:34,721: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:34:34,721: t15.2024.05.10 val PER: 0.1337
2025-12-08 21:34:34,721: t15.2024.06.14 val PER: 0.1246
2025-12-08 21:34:34,721: t15.2024.07.19 val PER: 0.1661
2025-12-08 21:34:34,721: t15.2024.07.21 val PER: 0.0841
2025-12-08 21:34:34,721: t15.2024.07.28 val PER: 0.1125
2025-12-08 21:34:34,721: t15.2025.01.10 val PER: 0.2741
2025-12-08 21:34:34,721: t15.2025.01.12 val PER: 0.0847
2025-12-08 21:34:34,721: t15.2025.03.14 val PER: 0.3121
2025-12-08 21:34:34,721: t15.2025.03.16 val PER: 0.1440
2025-12-08 21:34:34,721: t15.2025.03.30 val PER: 0.2506
2025-12-08 21:34:34,721: t15.2025.04.13 val PER: 0.2054
2025-12-08 21:35:05,726: Train batch 54200: loss: 0.06 grad norm: 1.50 time: 0.105
2025-12-08 21:35:33,129: Train batch 54400: loss: 0.01 grad norm: 0.28 time: 0.108
2025-12-08 21:36:00,900: Train batch 54600: loss: 0.00 grad norm: 0.09 time: 0.162
2025-12-08 21:36:28,605: Train batch 54800: loss: 0.10 grad norm: 2.43 time: 0.146
2025-12-08 21:36:56,078: Train batch 55000: loss: 0.00 grad norm: 0.17 time: 0.130
2025-12-08 21:37:24,368: Train batch 55200: loss: 0.04 grad norm: 1.15 time: 0.077
2025-12-08 21:37:54,535: Train batch 55400: loss: 0.00 grad norm: 0.12 time: 0.173
2025-12-08 21:38:24,438: Train batch 55600: loss: 0.03 grad norm: 1.07 time: 0.137
2025-12-08 21:38:54,500: Train batch 55800: loss: 0.07 grad norm: 3.84 time: 0.128
2025-12-08 21:39:24,889: Train batch 56000: loss: 0.03 grad norm: 1.35 time: 0.084
2025-12-08 21:39:24,889: Running test after training batch: 56000
2025-12-08 21:39:34,859: Val batch 56000: PER (avg): 0.1207 CTC Loss (avg): 37.0922 time: 9.970
2025-12-08 21:39:34,860: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:39:34,860: t15.2023.08.13 val PER: 0.0790
2025-12-08 21:39:34,860: t15.2023.08.18 val PER: 0.0687
2025-12-08 21:39:34,860: t15.2023.08.20 val PER: 0.0604
2025-12-08 21:39:34,860: t15.2023.08.25 val PER: 0.0964
2025-12-08 21:39:34,860: t15.2023.08.27 val PER: 0.1479
2025-12-08 21:39:34,860: t15.2023.09.01 val PER: 0.0446
2025-12-08 21:39:34,860: t15.2023.09.03 val PER: 0.1235
2025-12-08 21:39:34,860: t15.2023.09.24 val PER: 0.1044
2025-12-08 21:39:34,860: t15.2023.09.29 val PER: 0.1168
2025-12-08 21:39:34,860: t15.2023.10.01 val PER: 0.1446
2025-12-08 21:39:34,860: t15.2023.10.06 val PER: 0.0753
2025-12-08 21:39:34,860: t15.2023.10.08 val PER: 0.1813
2025-12-08 21:39:34,860: t15.2023.10.13 val PER: 0.1652
2025-12-08 21:39:34,860: t15.2023.10.15 val PER: 0.1246
2025-12-08 21:39:34,860: t15.2023.10.20 val PER: 0.2081
2025-12-08 21:39:34,860: t15.2023.10.22 val PER: 0.1269
2025-12-08 21:39:34,861: t15.2023.11.03 val PER: 0.1886
2025-12-08 21:39:34,861: t15.2023.11.04 val PER: 0.0444
2025-12-08 21:39:34,861: t15.2023.11.17 val PER: 0.0327
2025-12-08 21:39:34,861: t15.2023.11.19 val PER: 0.0599
2025-12-08 21:39:34,861: t15.2023.11.26 val PER: 0.0536
2025-12-08 21:39:34,861: t15.2023.12.03 val PER: 0.0651
2025-12-08 21:39:34,861: t15.2023.12.08 val PER: 0.0439
2025-12-08 21:39:34,861: t15.2023.12.10 val PER: 0.0460
2025-12-08 21:39:34,861: t15.2023.12.17 val PER: 0.1258
2025-12-08 21:39:34,861: t15.2023.12.29 val PER: 0.1201
2025-12-08 21:39:34,861: t15.2024.02.25 val PER: 0.0702
2025-12-08 21:39:34,861: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:39:34,861: t15.2024.03.08 val PER: 0.1892
2025-12-08 21:39:34,861: t15.2024.03.15 val PER: 0.2089
2025-12-08 21:39:34,861: t15.2024.03.17 val PER: 0.0865
2025-12-08 21:39:34,861: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:39:34,861: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:39:34,861: t15.2024.05.10 val PER: 0.1605
2025-12-08 21:39:34,861: t15.2024.06.14 val PER: 0.1262
2025-12-08 21:39:34,862: t15.2024.07.19 val PER: 0.1721
2025-12-08 21:39:34,862: t15.2024.07.21 val PER: 0.0752
2025-12-08 21:39:34,862: t15.2024.07.28 val PER: 0.1037
2025-12-08 21:39:34,862: t15.2025.01.10 val PER: 0.2713
2025-12-08 21:39:34,862: t15.2025.01.12 val PER: 0.0847
2025-12-08 21:39:34,862: t15.2025.03.14 val PER: 0.2825
2025-12-08 21:39:34,862: t15.2025.03.16 val PER: 0.1401
2025-12-08 21:39:34,862: t15.2025.03.30 val PER: 0.2471
2025-12-08 21:39:34,862: t15.2025.04.13 val PER: 0.1997
2025-12-08 21:40:06,032: Train batch 56200: loss: 0.01 grad norm: 0.54 time: 0.136
2025-12-08 21:40:35,451: Train batch 56400: loss: 0.01 grad norm: 0.55 time: 0.088
2025-12-08 21:41:04,402: Train batch 56600: loss: 0.01 grad norm: 0.85 time: 0.105
2025-12-08 21:41:35,809: Train batch 56800: loss: 0.00 grad norm: 0.02 time: 0.144
2025-12-08 21:42:08,087: Train batch 57000: loss: 0.01 grad norm: 0.58 time: 0.096
2025-12-08 21:42:41,514: Train batch 57200: loss: 0.06 grad norm: 1.07 time: 0.127
2025-12-08 21:43:14,971: Train batch 57400: loss: 0.00 grad norm: 0.19 time: 0.088
2025-12-08 21:43:48,679: Train batch 57600: loss: 0.14 grad norm: 2.40 time: 0.124
2025-12-08 21:44:21,663: Train batch 57800: loss: 0.08 grad norm: 3.23 time: 0.121
2025-12-08 21:44:52,643: Train batch 58000: loss: 0.02 grad norm: 1.05 time: 0.144
2025-12-08 21:44:52,644: Running test after training batch: 58000
2025-12-08 21:45:02,557: Val batch 58000: PER (avg): 0.1195 CTC Loss (avg): 39.8611 time: 9.913
2025-12-08 21:45:02,558: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:45:02,558: t15.2023.08.13 val PER: 0.0800
2025-12-08 21:45:02,558: t15.2023.08.18 val PER: 0.0729
2025-12-08 21:45:02,558: t15.2023.08.20 val PER: 0.0612
2025-12-08 21:45:02,558: t15.2023.08.25 val PER: 0.1024
2025-12-08 21:45:02,558: t15.2023.08.27 val PER: 0.1334
2025-12-08 21:45:02,558: t15.2023.09.01 val PER: 0.0430
2025-12-08 21:45:02,558: t15.2023.09.03 val PER: 0.1259
2025-12-08 21:45:02,558: t15.2023.09.24 val PER: 0.1032
2025-12-08 21:45:02,558: t15.2023.09.29 val PER: 0.1264
2025-12-08 21:45:02,558: t15.2023.10.01 val PER: 0.1466
2025-12-08 21:45:02,558: t15.2023.10.06 val PER: 0.0850
2025-12-08 21:45:02,558: t15.2023.10.08 val PER: 0.1786
2025-12-08 21:45:02,558: t15.2023.10.13 val PER: 0.1831
2025-12-08 21:45:02,559: t15.2023.10.15 val PER: 0.1252
2025-12-08 21:45:02,559: t15.2023.10.20 val PER: 0.1980
2025-12-08 21:45:02,559: t15.2023.10.22 val PER: 0.1314
2025-12-08 21:45:02,559: t15.2023.11.03 val PER: 0.1967
2025-12-08 21:45:02,559: t15.2023.11.04 val PER: 0.0375
2025-12-08 21:45:02,559: t15.2023.11.17 val PER: 0.0311
2025-12-08 21:45:02,559: t15.2023.11.19 val PER: 0.0559
2025-12-08 21:45:02,559: t15.2023.11.26 val PER: 0.0565
2025-12-08 21:45:02,559: t15.2023.12.03 val PER: 0.0714
2025-12-08 21:45:02,559: t15.2023.12.08 val PER: 0.0419
2025-12-08 21:45:02,559: t15.2023.12.10 val PER: 0.0473
2025-12-08 21:45:02,559: t15.2023.12.17 val PER: 0.1320
2025-12-08 21:45:02,559: t15.2023.12.29 val PER: 0.1050
2025-12-08 21:45:02,559: t15.2024.02.25 val PER: 0.0688
2025-12-08 21:45:02,559: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:45:02,559: t15.2024.03.08 val PER: 0.1664
2025-12-08 21:45:02,559: t15.2024.03.15 val PER: 0.1964
2025-12-08 21:45:02,559: t15.2024.03.17 val PER: 0.0753
2025-12-08 21:45:02,559: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:45:02,560: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:45:02,560: t15.2024.05.10 val PER: 0.1575
2025-12-08 21:45:02,560: t15.2024.06.14 val PER: 0.1325
2025-12-08 21:45:02,560: t15.2024.07.19 val PER: 0.1694
2025-12-08 21:45:02,560: t15.2024.07.21 val PER: 0.0690
2025-12-08 21:45:02,560: t15.2024.07.28 val PER: 0.0890
2025-12-08 21:45:02,560: t15.2025.01.10 val PER: 0.2521
2025-12-08 21:45:02,560: t15.2025.01.12 val PER: 0.0808
2025-12-08 21:45:02,560: t15.2025.03.14 val PER: 0.2811
2025-12-08 21:45:02,560: t15.2025.03.16 val PER: 0.1545
2025-12-08 21:45:02,560: t15.2025.03.30 val PER: 0.2471
2025-12-08 21:45:02,560: t15.2025.04.13 val PER: 0.1926
2025-12-08 21:45:32,649: Train batch 58200: loss: 0.01 grad norm: 0.28 time: 0.100
2025-12-08 21:46:03,849: Train batch 58400: loss: 0.01 grad norm: 0.42 time: 0.105
2025-12-08 21:46:36,299: Train batch 58600: loss: 0.04 grad norm: 1.35 time: 0.110
2025-12-08 21:47:06,176: Train batch 58800: loss: 0.00 grad norm: 0.22 time: 0.124
2025-12-08 21:47:35,997: Train batch 59000: loss: 0.01 grad norm: 0.79 time: 0.097
2025-12-08 21:48:07,776: Train batch 59200: loss: 0.00 grad norm: 0.11 time: 0.124
2025-12-08 21:48:39,105: Train batch 59400: loss: 0.24 grad norm: 3.95 time: 0.104
2025-12-08 21:49:10,738: Train batch 59600: loss: 0.10 grad norm: 3.88 time: 0.139
2025-12-08 21:49:41,957: Train batch 59800: loss: 0.01 grad norm: 0.51 time: 0.120
2025-12-08 21:50:13,274: Train batch 60000: loss: 0.01 grad norm: 0.33 time: 0.126
2025-12-08 21:50:13,275: Running test after training batch: 60000
2025-12-08 21:50:23,466: Val batch 60000: PER (avg): 0.1142 CTC Loss (avg): 36.3222 time: 10.191
2025-12-08 21:50:23,466: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:50:23,466: t15.2023.08.13 val PER: 0.0717
2025-12-08 21:50:23,466: t15.2023.08.18 val PER: 0.0679
2025-12-08 21:50:23,466: t15.2023.08.20 val PER: 0.0643
2025-12-08 21:50:23,466: t15.2023.08.25 val PER: 0.0858
2025-12-08 21:50:23,466: t15.2023.08.27 val PER: 0.1318
2025-12-08 21:50:23,466: t15.2023.09.01 val PER: 0.0398
2025-12-08 21:50:23,466: t15.2023.09.03 val PER: 0.1164
2025-12-08 21:50:23,467: t15.2023.09.24 val PER: 0.0983
2025-12-08 21:50:23,467: t15.2023.09.29 val PER: 0.1136
2025-12-08 21:50:23,467: t15.2023.10.01 val PER: 0.1288
2025-12-08 21:50:23,467: t15.2023.10.06 val PER: 0.0678
2025-12-08 21:50:23,467: t15.2023.10.08 val PER: 0.1827
2025-12-08 21:50:23,467: t15.2023.10.13 val PER: 0.1769
2025-12-08 21:50:23,467: t15.2023.10.15 val PER: 0.1167
2025-12-08 21:50:23,467: t15.2023.10.20 val PER: 0.2013
2025-12-08 21:50:23,467: t15.2023.10.22 val PER: 0.1024
2025-12-08 21:50:23,467: t15.2023.11.03 val PER: 0.1845
2025-12-08 21:50:23,467: t15.2023.11.04 val PER: 0.0478
2025-12-08 21:50:23,467: t15.2023.11.17 val PER: 0.0280
2025-12-08 21:50:23,467: t15.2023.11.19 val PER: 0.0758
2025-12-08 21:50:23,467: t15.2023.11.26 val PER: 0.0536
2025-12-08 21:50:23,467: t15.2023.12.03 val PER: 0.0620
2025-12-08 21:50:23,467: t15.2023.12.08 val PER: 0.0433
2025-12-08 21:50:23,468: t15.2023.12.10 val PER: 0.0526
2025-12-08 21:50:23,468: t15.2023.12.17 val PER: 0.1310
2025-12-08 21:50:23,468: t15.2023.12.29 val PER: 0.0927
2025-12-08 21:50:23,468: t15.2024.02.25 val PER: 0.0660
2025-12-08 21:50:23,468: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:50:23,468: t15.2024.03.08 val PER: 0.1821
2025-12-08 21:50:23,468: t15.2024.03.15 val PER: 0.1889
2025-12-08 21:50:23,468: t15.2024.03.17 val PER: 0.0809
2025-12-08 21:50:23,468: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:50:23,468: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:50:23,468: t15.2024.05.10 val PER: 0.1322
2025-12-08 21:50:23,468: t15.2024.06.14 val PER: 0.1388
2025-12-08 21:50:23,468: t15.2024.07.19 val PER: 0.1536
2025-12-08 21:50:23,468: t15.2024.07.21 val PER: 0.0752
2025-12-08 21:50:23,468: t15.2024.07.28 val PER: 0.0890
2025-12-08 21:50:23,468: t15.2025.01.10 val PER: 0.2521
2025-12-08 21:50:23,468: t15.2025.01.12 val PER: 0.0816
2025-12-08 21:50:23,469: t15.2025.03.14 val PER: 0.2899
2025-12-08 21:50:23,469: t15.2025.03.16 val PER: 0.1309
2025-12-08 21:50:23,469: t15.2025.03.30 val PER: 0.2437
2025-12-08 21:50:23,469: t15.2025.04.13 val PER: 0.1869
2025-12-08 21:50:23,469: New best test PER 0.1173 --> 0.1142
2025-12-08 21:50:23,469: Checkpointing model
2025-12-08 21:50:24,604: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 21:50:55,947: Train batch 60200: loss: 0.03 grad norm: 1.31 time: 0.147
2025-12-08 21:51:27,565: Train batch 60400: loss: 0.00 grad norm: 0.16 time: 0.145
2025-12-08 21:51:58,957: Train batch 60600: loss: 0.01 grad norm: 0.48 time: 0.131
2025-12-08 21:52:30,367: Train batch 60800: loss: 0.01 grad norm: 0.32 time: 0.134
2025-12-08 21:53:01,886: Train batch 61000: loss: 0.01 grad norm: 0.53 time: 0.095
2025-12-08 21:53:33,270: Train batch 61200: loss: 0.01 grad norm: 0.30 time: 0.109
2025-12-08 21:54:04,680: Train batch 61400: loss: 0.01 grad norm: 0.87 time: 0.145
2025-12-08 21:54:35,589: Train batch 61600: loss: 0.00 grad norm: 0.38 time: 0.102
2025-12-08 21:55:07,889: Train batch 61800: loss: 0.03 grad norm: 1.12 time: 0.083
2025-12-08 21:55:39,483: Train batch 62000: loss: 0.00 grad norm: 0.05 time: 0.110
2025-12-08 21:55:39,483: Running test after training batch: 62000
2025-12-08 21:55:49,403: Val batch 62000: PER (avg): 0.1128 CTC Loss (avg): 37.9468 time: 9.920
2025-12-08 21:55:49,404: t15.2023.08.11 val PER: 1.0000
2025-12-08 21:55:49,404: t15.2023.08.13 val PER: 0.0728
2025-12-08 21:55:49,404: t15.2023.08.18 val PER: 0.0662
2025-12-08 21:55:49,404: t15.2023.08.20 val PER: 0.0516
2025-12-08 21:55:49,404: t15.2023.08.25 val PER: 0.0889
2025-12-08 21:55:49,404: t15.2023.08.27 val PER: 0.1367
2025-12-08 21:55:49,404: t15.2023.09.01 val PER: 0.0438
2025-12-08 21:55:49,404: t15.2023.09.03 val PER: 0.1164
2025-12-08 21:55:49,404: t15.2023.09.24 val PER: 0.0837
2025-12-08 21:55:49,404: t15.2023.09.29 val PER: 0.1238
2025-12-08 21:55:49,404: t15.2023.10.01 val PER: 0.1433
2025-12-08 21:55:49,404: t15.2023.10.06 val PER: 0.0678
2025-12-08 21:55:49,404: t15.2023.10.08 val PER: 0.1773
2025-12-08 21:55:49,404: t15.2023.10.13 val PER: 0.1598
2025-12-08 21:55:49,404: t15.2023.10.15 val PER: 0.1252
2025-12-08 21:55:49,404: t15.2023.10.20 val PER: 0.2047
2025-12-08 21:55:49,404: t15.2023.10.22 val PER: 0.1158
2025-12-08 21:55:49,405: t15.2023.11.03 val PER: 0.1845
2025-12-08 21:55:49,405: t15.2023.11.04 val PER: 0.0512
2025-12-08 21:55:49,405: t15.2023.11.17 val PER: 0.0420
2025-12-08 21:55:49,405: t15.2023.11.19 val PER: 0.0559
2025-12-08 21:55:49,405: t15.2023.11.26 val PER: 0.0471
2025-12-08 21:55:49,405: t15.2023.12.03 val PER: 0.0683
2025-12-08 21:55:49,405: t15.2023.12.08 val PER: 0.0286
2025-12-08 21:55:49,405: t15.2023.12.10 val PER: 0.0355
2025-12-08 21:55:49,405: t15.2023.12.17 val PER: 0.1247
2025-12-08 21:55:49,405: t15.2023.12.29 val PER: 0.0954
2025-12-08 21:55:49,405: t15.2024.02.25 val PER: 0.0688
2025-12-08 21:55:49,405: t15.2024.03.03 val PER: 1.0000
2025-12-08 21:55:49,405: t15.2024.03.08 val PER: 0.1863
2025-12-08 21:55:49,405: t15.2024.03.15 val PER: 0.1820
2025-12-08 21:55:49,405: t15.2024.03.17 val PER: 0.0746
2025-12-08 21:55:49,405: t15.2024.04.25 val PER: 1.0000
2025-12-08 21:55:49,405: t15.2024.04.28 val PER: 1.0000
2025-12-08 21:55:49,405: t15.2024.05.10 val PER: 0.1501
2025-12-08 21:55:49,405: t15.2024.06.14 val PER: 0.1420
2025-12-08 21:55:49,406: t15.2024.07.19 val PER: 0.1602
2025-12-08 21:55:49,406: t15.2024.07.21 val PER: 0.0683
2025-12-08 21:55:49,406: t15.2024.07.28 val PER: 0.0868
2025-12-08 21:55:49,406: t15.2025.01.10 val PER: 0.2617
2025-12-08 21:55:49,406: t15.2025.01.12 val PER: 0.0801
2025-12-08 21:55:49,406: t15.2025.03.14 val PER: 0.2618
2025-12-08 21:55:49,406: t15.2025.03.16 val PER: 0.1126
2025-12-08 21:55:49,406: t15.2025.03.30 val PER: 0.2241
2025-12-08 21:55:49,406: t15.2025.04.13 val PER: 0.2026
2025-12-08 21:55:49,406: New best test PER 0.1142 --> 0.1128
2025-12-08 21:55:49,406: Checkpointing model
2025-12-08 21:55:50,437: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 21:56:22,216: Train batch 62200: loss: 0.01 grad norm: 0.58 time: 0.131
2025-12-08 21:56:55,215: Train batch 62400: loss: 0.02 grad norm: 1.07 time: 0.120
2025-12-08 21:57:28,190: Train batch 62600: loss: 0.00 grad norm: 0.14 time: 0.124
2025-12-08 21:58:00,829: Train batch 62800: loss: 0.01 grad norm: 0.33 time: 0.131
2025-12-08 21:58:32,975: Train batch 63000: loss: 0.00 grad norm: 0.19 time: 0.152
2025-12-08 21:59:03,980: Train batch 63200: loss: 0.06 grad norm: 2.21 time: 0.153
2025-12-08 21:59:34,887: Train batch 63400: loss: 0.02 grad norm: 0.86 time: 0.125
2025-12-08 22:00:07,716: Train batch 63600: loss: 0.02 grad norm: 1.60 time: 0.127
2025-12-08 22:00:40,290: Train batch 63800: loss: 0.00 grad norm: 0.06 time: 0.117
2025-12-08 22:01:12,418: Train batch 64000: loss: 0.01 grad norm: 0.70 time: 0.125
2025-12-08 22:01:12,419: Running test after training batch: 64000
2025-12-08 22:01:22,312: Val batch 64000: PER (avg): 0.1134 CTC Loss (avg): 38.5216 time: 9.893
2025-12-08 22:01:22,312: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:01:22,313: t15.2023.08.13 val PER: 0.0738
2025-12-08 22:01:22,313: t15.2023.08.18 val PER: 0.0696
2025-12-08 22:01:22,313: t15.2023.08.20 val PER: 0.0508
2025-12-08 22:01:22,313: t15.2023.08.25 val PER: 0.1024
2025-12-08 22:01:22,313: t15.2023.08.27 val PER: 0.1270
2025-12-08 22:01:22,313: t15.2023.09.01 val PER: 0.0528
2025-12-08 22:01:22,313: t15.2023.09.03 val PER: 0.1116
2025-12-08 22:01:22,313: t15.2023.09.24 val PER: 0.0825
2025-12-08 22:01:22,313: t15.2023.09.29 val PER: 0.1251
2025-12-08 22:01:22,313: t15.2023.10.01 val PER: 0.1387
2025-12-08 22:01:22,313: t15.2023.10.06 val PER: 0.0872
2025-12-08 22:01:22,313: t15.2023.10.08 val PER: 0.1827
2025-12-08 22:01:22,313: t15.2023.10.13 val PER: 0.1738
2025-12-08 22:01:22,313: t15.2023.10.15 val PER: 0.1173
2025-12-08 22:01:22,313: t15.2023.10.20 val PER: 0.2114
2025-12-08 22:01:22,313: t15.2023.10.22 val PER: 0.1147
2025-12-08 22:01:22,313: t15.2023.11.03 val PER: 0.1811
2025-12-08 22:01:22,313: t15.2023.11.04 val PER: 0.0512
2025-12-08 22:01:22,314: t15.2023.11.17 val PER: 0.0389
2025-12-08 22:01:22,314: t15.2023.11.19 val PER: 0.0599
2025-12-08 22:01:22,314: t15.2023.11.26 val PER: 0.0442
2025-12-08 22:01:22,314: t15.2023.12.03 val PER: 0.0452
2025-12-08 22:01:22,314: t15.2023.12.08 val PER: 0.0373
2025-12-08 22:01:22,314: t15.2023.12.10 val PER: 0.0420
2025-12-08 22:01:22,314: t15.2023.12.17 val PER: 0.1331
2025-12-08 22:01:22,314: t15.2023.12.29 val PER: 0.1002
2025-12-08 22:01:22,314: t15.2024.02.25 val PER: 0.0688
2025-12-08 22:01:22,314: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:01:22,314: t15.2024.03.08 val PER: 0.1849
2025-12-08 22:01:22,314: t15.2024.03.15 val PER: 0.1795
2025-12-08 22:01:22,314: t15.2024.03.17 val PER: 0.0746
2025-12-08 22:01:22,314: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:01:22,314: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:01:22,314: t15.2024.05.10 val PER: 0.1367
2025-12-08 22:01:22,314: t15.2024.06.14 val PER: 0.1057
2025-12-08 22:01:22,314: t15.2024.07.19 val PER: 0.1668
2025-12-08 22:01:22,314: t15.2024.07.21 val PER: 0.0655
2025-12-08 22:01:22,315: t15.2024.07.28 val PER: 0.0824
2025-12-08 22:01:22,315: t15.2025.01.10 val PER: 0.2479
2025-12-08 22:01:22,315: t15.2025.01.12 val PER: 0.0785
2025-12-08 22:01:22,315: t15.2025.03.14 val PER: 0.2885
2025-12-08 22:01:22,315: t15.2025.03.16 val PER: 0.1257
2025-12-08 22:01:22,315: t15.2025.03.30 val PER: 0.2356
2025-12-08 22:01:22,315: t15.2025.04.13 val PER: 0.1983
2025-12-08 22:01:55,045: Train batch 64200: loss: 0.03 grad norm: 1.23 time: 0.134
2025-12-08 22:02:27,326: Train batch 64400: loss: 0.00 grad norm: 0.03 time: 0.127
2025-12-08 22:02:57,689: Train batch 64600: loss: 0.00 grad norm: 0.38 time: 0.114
2025-12-08 22:03:26,080: Train batch 64800: loss: 0.02 grad norm: 0.86 time: 0.108
2025-12-08 22:03:54,952: Train batch 65000: loss: 0.02 grad norm: 1.50 time: 0.147
2025-12-08 22:04:23,572: Train batch 65200: loss: 0.05 grad norm: 1.57 time: 0.080
2025-12-08 22:04:54,091: Train batch 65400: loss: 0.01 grad norm: 0.59 time: 0.085
2025-12-08 22:05:25,155: Train batch 65600: loss: 0.00 grad norm: 0.24 time: 0.089
2025-12-08 22:05:56,412: Train batch 65800: loss: 0.00 grad norm: 0.05 time: 0.127
2025-12-08 22:06:27,785: Train batch 66000: loss: 0.03 grad norm: 1.14 time: 0.097
2025-12-08 22:06:27,785: Running test after training batch: 66000
2025-12-08 22:06:38,352: Val batch 66000: PER (avg): 0.1110 CTC Loss (avg): 36.7585 time: 10.567
2025-12-08 22:06:38,352: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:06:38,352: t15.2023.08.13 val PER: 0.0790
2025-12-08 22:06:38,352: t15.2023.08.18 val PER: 0.0654
2025-12-08 22:06:38,352: t15.2023.08.20 val PER: 0.0596
2025-12-08 22:06:38,352: t15.2023.08.25 val PER: 0.0723
2025-12-08 22:06:38,352: t15.2023.08.27 val PER: 0.1286
2025-12-08 22:06:38,352: t15.2023.09.01 val PER: 0.0463
2025-12-08 22:06:38,352: t15.2023.09.03 val PER: 0.1247
2025-12-08 22:06:38,352: t15.2023.09.24 val PER: 0.0862
2025-12-08 22:06:38,352: t15.2023.09.29 val PER: 0.1213
2025-12-08 22:06:38,353: t15.2023.10.01 val PER: 0.1367
2025-12-08 22:06:38,353: t15.2023.10.06 val PER: 0.0667
2025-12-08 22:06:38,353: t15.2023.10.08 val PER: 0.1719
2025-12-08 22:06:38,353: t15.2023.10.13 val PER: 0.1590
2025-12-08 22:06:38,353: t15.2023.10.15 val PER: 0.1193
2025-12-08 22:06:38,353: t15.2023.10.20 val PER: 0.1611
2025-12-08 22:06:38,353: t15.2023.10.22 val PER: 0.1203
2025-12-08 22:06:38,353: t15.2023.11.03 val PER: 0.1920
2025-12-08 22:06:38,353: t15.2023.11.04 val PER: 0.0546
2025-12-08 22:06:38,353: t15.2023.11.17 val PER: 0.0311
2025-12-08 22:06:38,353: t15.2023.11.19 val PER: 0.0479
2025-12-08 22:06:38,353: t15.2023.11.26 val PER: 0.0580
2025-12-08 22:06:38,353: t15.2023.12.03 val PER: 0.0557
2025-12-08 22:06:38,353: t15.2023.12.08 val PER: 0.0366
2025-12-08 22:06:38,353: t15.2023.12.10 val PER: 0.0315
2025-12-08 22:06:38,353: t15.2023.12.17 val PER: 0.1310
2025-12-08 22:06:38,353: t15.2023.12.29 val PER: 0.1016
2025-12-08 22:06:38,353: t15.2024.02.25 val PER: 0.0730
2025-12-08 22:06:38,353: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:06:38,354: t15.2024.03.08 val PER: 0.1693
2025-12-08 22:06:38,354: t15.2024.03.15 val PER: 0.1776
2025-12-08 22:06:38,354: t15.2024.03.17 val PER: 0.0718
2025-12-08 22:06:38,354: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:06:38,354: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:06:38,354: t15.2024.05.10 val PER: 0.1352
2025-12-08 22:06:38,354: t15.2024.06.14 val PER: 0.1136
2025-12-08 22:06:38,354: t15.2024.07.19 val PER: 0.1569
2025-12-08 22:06:38,354: t15.2024.07.21 val PER: 0.0600
2025-12-08 22:06:38,354: t15.2024.07.28 val PER: 0.0875
2025-12-08 22:06:38,354: t15.2025.01.10 val PER: 0.2452
2025-12-08 22:06:38,354: t15.2025.01.12 val PER: 0.0816
2025-12-08 22:06:38,354: t15.2025.03.14 val PER: 0.2589
2025-12-08 22:06:38,354: t15.2025.03.16 val PER: 0.1230
2025-12-08 22:06:38,354: t15.2025.03.30 val PER: 0.2253
2025-12-08 22:06:38,354: t15.2025.04.13 val PER: 0.1912
2025-12-08 22:06:38,354: New best test PER 0.1128 --> 0.1110
2025-12-08 22:06:38,354: Checkpointing model
2025-12-08 22:06:39,558: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 22:07:13,253: Train batch 66200: loss: 0.07 grad norm: 2.42 time: 0.119
2025-12-08 22:07:46,514: Train batch 66400: loss: 0.06 grad norm: 3.67 time: 0.133
2025-12-08 22:08:19,155: Train batch 66600: loss: 0.02 grad norm: 0.83 time: 0.148
2025-12-08 22:08:47,607: Train batch 66800: loss: 0.01 grad norm: 0.62 time: 0.096
2025-12-08 22:09:16,411: Train batch 67000: loss: 0.00 grad norm: 0.04 time: 0.129
2025-12-08 22:09:45,295: Train batch 67200: loss: 0.00 grad norm: 0.16 time: 0.125
2025-12-08 22:10:14,778: Train batch 67400: loss: 0.00 grad norm: 0.04 time: 0.096
2025-12-08 22:10:43,726: Train batch 67600: loss: 0.02 grad norm: 1.23 time: 0.091
2025-12-08 22:11:13,303: Train batch 67800: loss: 0.02 grad norm: 0.97 time: 0.165
2025-12-08 22:11:43,046: Train batch 68000: loss: 0.01 grad norm: 0.45 time: 0.095
2025-12-08 22:11:43,046: Running test after training batch: 68000
2025-12-08 22:11:53,149: Val batch 68000: PER (avg): 0.1114 CTC Loss (avg): 38.2928 time: 10.102
2025-12-08 22:11:53,149: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:11:53,149: t15.2023.08.13 val PER: 0.0863
2025-12-08 22:11:53,149: t15.2023.08.18 val PER: 0.0687
2025-12-08 22:11:53,149: t15.2023.08.20 val PER: 0.0604
2025-12-08 22:11:53,150: t15.2023.08.25 val PER: 0.0873
2025-12-08 22:11:53,150: t15.2023.08.27 val PER: 0.1399
2025-12-08 22:11:53,150: t15.2023.09.01 val PER: 0.0463
2025-12-08 22:11:53,150: t15.2023.09.03 val PER: 0.1176
2025-12-08 22:11:53,150: t15.2023.09.24 val PER: 0.0874
2025-12-08 22:11:53,150: t15.2023.09.29 val PER: 0.1213
2025-12-08 22:11:53,150: t15.2023.10.01 val PER: 0.1367
2025-12-08 22:11:53,150: t15.2023.10.06 val PER: 0.0678
2025-12-08 22:11:53,150: t15.2023.10.08 val PER: 0.1773
2025-12-08 22:11:53,150: t15.2023.10.13 val PER: 0.1652
2025-12-08 22:11:53,150: t15.2023.10.15 val PER: 0.1154
2025-12-08 22:11:53,150: t15.2023.10.20 val PER: 0.1879
2025-12-08 22:11:53,150: t15.2023.10.22 val PER: 0.0991
2025-12-08 22:11:53,150: t15.2023.11.03 val PER: 0.1737
2025-12-08 22:11:53,150: t15.2023.11.04 val PER: 0.0307
2025-12-08 22:11:53,150: t15.2023.11.17 val PER: 0.0187
2025-12-08 22:11:53,151: t15.2023.11.19 val PER: 0.0659
2025-12-08 22:11:53,151: t15.2023.11.26 val PER: 0.0464
2025-12-08 22:11:53,151: t15.2023.12.03 val PER: 0.0504
2025-12-08 22:11:53,151: t15.2023.12.08 val PER: 0.0439
2025-12-08 22:11:53,151: t15.2023.12.10 val PER: 0.0355
2025-12-08 22:11:53,151: t15.2023.12.17 val PER: 0.1227
2025-12-08 22:11:53,151: t15.2023.12.29 val PER: 0.1043
2025-12-08 22:11:53,151: t15.2024.02.25 val PER: 0.0590
2025-12-08 22:11:53,151: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:11:53,151: t15.2024.03.08 val PER: 0.1693
2025-12-08 22:11:53,151: t15.2024.03.15 val PER: 0.1832
2025-12-08 22:11:53,151: t15.2024.03.17 val PER: 0.0725
2025-12-08 22:11:53,151: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:11:53,151: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:11:53,151: t15.2024.05.10 val PER: 0.1397
2025-12-08 22:11:53,151: t15.2024.06.14 val PER: 0.1356
2025-12-08 22:11:53,151: t15.2024.07.19 val PER: 0.1641
2025-12-08 22:11:53,152: t15.2024.07.21 val PER: 0.0579
2025-12-08 22:11:53,152: t15.2024.07.28 val PER: 0.0971
2025-12-08 22:11:53,152: t15.2025.01.10 val PER: 0.2562
2025-12-08 22:11:53,152: t15.2025.01.12 val PER: 0.0693
2025-12-08 22:11:53,152: t15.2025.03.14 val PER: 0.2678
2025-12-08 22:11:53,152: t15.2025.03.16 val PER: 0.1152
2025-12-08 22:11:53,152: t15.2025.03.30 val PER: 0.2310
2025-12-08 22:11:53,152: t15.2025.04.13 val PER: 0.1954
2025-12-08 22:12:21,896: Train batch 68200: loss: 0.00 grad norm: 0.21 time: 0.124
2025-12-08 22:12:51,639: Train batch 68400: loss: 0.02 grad norm: 0.94 time: 0.140
2025-12-08 22:13:20,938: Train batch 68600: loss: 0.00 grad norm: 0.01 time: 0.105
2025-12-08 22:13:50,572: Train batch 68800: loss: 0.02 grad norm: 0.69 time: 0.129
2025-12-08 22:14:19,955: Train batch 69000: loss: 0.00 grad norm: 0.07 time: 0.119
2025-12-08 22:14:49,364: Train batch 69200: loss: 0.00 grad norm: 0.16 time: 0.080
2025-12-08 22:15:18,847: Train batch 69400: loss: 0.01 grad norm: 0.55 time: 0.122
2025-12-08 22:15:48,235: Train batch 69600: loss: 0.00 grad norm: 0.03 time: 0.122
2025-12-08 22:16:17,747: Train batch 69800: loss: 0.01 grad norm: 0.35 time: 0.134
2025-12-08 22:16:47,708: Train batch 70000: loss: 0.01 grad norm: 1.07 time: 0.108
2025-12-08 22:16:47,708: Running test after training batch: 70000
2025-12-08 22:16:57,888: Val batch 70000: PER (avg): 0.1108 CTC Loss (avg): 38.3074 time: 10.179
2025-12-08 22:16:57,888: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:16:57,888: t15.2023.08.13 val PER: 0.0800
2025-12-08 22:16:57,888: t15.2023.08.18 val PER: 0.0754
2025-12-08 22:16:57,888: t15.2023.08.20 val PER: 0.0524
2025-12-08 22:16:57,889: t15.2023.08.25 val PER: 0.0979
2025-12-08 22:16:57,889: t15.2023.08.27 val PER: 0.1367
2025-12-08 22:16:57,889: t15.2023.09.01 val PER: 0.0438
2025-12-08 22:16:57,889: t15.2023.09.03 val PER: 0.1211
2025-12-08 22:16:57,889: t15.2023.09.24 val PER: 0.0825
2025-12-08 22:16:57,889: t15.2023.09.29 val PER: 0.1232
2025-12-08 22:16:57,889: t15.2023.10.01 val PER: 0.1394
2025-12-08 22:16:57,889: t15.2023.10.06 val PER: 0.0646
2025-12-08 22:16:57,889: t15.2023.10.08 val PER: 0.1800
2025-12-08 22:16:57,889: t15.2023.10.13 val PER: 0.1660
2025-12-08 22:16:57,889: t15.2023.10.15 val PER: 0.1193
2025-12-08 22:16:57,889: t15.2023.10.20 val PER: 0.1779
2025-12-08 22:16:57,889: t15.2023.10.22 val PER: 0.0991
2025-12-08 22:16:57,889: t15.2023.11.03 val PER: 0.1825
2025-12-08 22:16:57,889: t15.2023.11.04 val PER: 0.0239
2025-12-08 22:16:57,889: t15.2023.11.17 val PER: 0.0327
2025-12-08 22:16:57,889: t15.2023.11.19 val PER: 0.0719
2025-12-08 22:16:57,889: t15.2023.11.26 val PER: 0.0500
2025-12-08 22:16:57,889: t15.2023.12.03 val PER: 0.0620
2025-12-08 22:16:57,890: t15.2023.12.08 val PER: 0.0519
2025-12-08 22:16:57,890: t15.2023.12.10 val PER: 0.0368
2025-12-08 22:16:57,890: t15.2023.12.17 val PER: 0.1216
2025-12-08 22:16:57,890: t15.2023.12.29 val PER: 0.0803
2025-12-08 22:16:57,890: t15.2024.02.25 val PER: 0.0520
2025-12-08 22:16:57,890: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:16:57,890: t15.2024.03.08 val PER: 0.1792
2025-12-08 22:16:57,890: t15.2024.03.15 val PER: 0.1689
2025-12-08 22:16:57,890: t15.2024.03.17 val PER: 0.0662
2025-12-08 22:16:57,890: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:16:57,890: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:16:57,890: t15.2024.05.10 val PER: 0.1367
2025-12-08 22:16:57,890: t15.2024.06.14 val PER: 0.1293
2025-12-08 22:16:57,890: t15.2024.07.19 val PER: 0.1543
2025-12-08 22:16:57,890: t15.2024.07.21 val PER: 0.0669
2025-12-08 22:16:57,890: t15.2024.07.28 val PER: 0.0706
2025-12-08 22:16:57,890: t15.2025.01.10 val PER: 0.2521
2025-12-08 22:16:57,890: t15.2025.01.12 val PER: 0.0731
2025-12-08 22:16:57,890: t15.2025.03.14 val PER: 0.2929
2025-12-08 22:16:57,891: t15.2025.03.16 val PER: 0.1191
2025-12-08 22:16:57,891: t15.2025.03.30 val PER: 0.2437
2025-12-08 22:16:57,891: t15.2025.04.13 val PER: 0.1969
2025-12-08 22:16:57,891: New best test PER 0.1110 --> 0.1108
2025-12-08 22:16:57,891: Checkpointing model
2025-12-08 22:16:59,039: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 22:17:31,465: Train batch 70200: loss: 0.00 grad norm: 0.04 time: 0.113
2025-12-08 22:18:02,145: Train batch 70400: loss: 0.00 grad norm: 0.17 time: 0.153
2025-12-08 22:18:33,206: Train batch 70600: loss: 0.00 grad norm: 0.09 time: 0.087
2025-12-08 22:19:03,509: Train batch 70800: loss: 0.04 grad norm: 2.02 time: 0.110
2025-12-08 22:19:33,636: Train batch 71000: loss: 0.00 grad norm: 0.03 time: 0.106
2025-12-08 22:20:04,741: Train batch 71200: loss: 0.03 grad norm: 1.46 time: 0.102
2025-12-08 22:20:35,543: Train batch 71400: loss: 0.01 grad norm: 0.36 time: 0.099
2025-12-08 22:21:05,868: Train batch 71600: loss: 0.01 grad norm: 0.74 time: 0.121
2025-12-08 22:21:36,652: Train batch 71800: loss: 0.01 grad norm: 0.88 time: 0.158
2025-12-08 22:22:07,958: Train batch 72000: loss: 0.00 grad norm: 0.28 time: 0.093
2025-12-08 22:22:07,958: Running test after training batch: 72000
2025-12-08 22:22:18,234: Val batch 72000: PER (avg): 0.1121 CTC Loss (avg): 40.3305 time: 10.276
2025-12-08 22:22:18,234: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:22:18,234: t15.2023.08.13 val PER: 0.0894
2025-12-08 22:22:18,234: t15.2023.08.18 val PER: 0.0712
2025-12-08 22:22:18,235: t15.2023.08.20 val PER: 0.0469
2025-12-08 22:22:18,235: t15.2023.08.25 val PER: 0.0994
2025-12-08 22:22:18,235: t15.2023.08.27 val PER: 0.1254
2025-12-08 22:22:18,235: t15.2023.09.01 val PER: 0.0503
2025-12-08 22:22:18,235: t15.2023.09.03 val PER: 0.1247
2025-12-08 22:22:18,235: t15.2023.09.24 val PER: 0.0959
2025-12-08 22:22:18,235: t15.2023.09.29 val PER: 0.1117
2025-12-08 22:22:18,235: t15.2023.10.01 val PER: 0.1334
2025-12-08 22:22:18,235: t15.2023.10.06 val PER: 0.0764
2025-12-08 22:22:18,235: t15.2023.10.08 val PER: 0.1759
2025-12-08 22:22:18,235: t15.2023.10.13 val PER: 0.1652
2025-12-08 22:22:18,235: t15.2023.10.15 val PER: 0.1160
2025-12-08 22:22:18,235: t15.2023.10.20 val PER: 0.2114
2025-12-08 22:22:18,235: t15.2023.10.22 val PER: 0.1147
2025-12-08 22:22:18,235: t15.2023.11.03 val PER: 0.1730
2025-12-08 22:22:18,235: t15.2023.11.04 val PER: 0.0171
2025-12-08 22:22:18,235: t15.2023.11.17 val PER: 0.0295
2025-12-08 22:22:18,235: t15.2023.11.19 val PER: 0.0559
2025-12-08 22:22:18,235: t15.2023.11.26 val PER: 0.0536
2025-12-08 22:22:18,236: t15.2023.12.03 val PER: 0.0609
2025-12-08 22:22:18,236: t15.2023.12.08 val PER: 0.0479
2025-12-08 22:22:18,236: t15.2023.12.10 val PER: 0.0263
2025-12-08 22:22:18,236: t15.2023.12.17 val PER: 0.1164
2025-12-08 22:22:18,236: t15.2023.12.29 val PER: 0.1078
2025-12-08 22:22:18,236: t15.2024.02.25 val PER: 0.0492
2025-12-08 22:22:18,236: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:22:18,236: t15.2024.03.08 val PER: 0.1821
2025-12-08 22:22:18,236: t15.2024.03.15 val PER: 0.1870
2025-12-08 22:22:18,236: t15.2024.03.17 val PER: 0.0718
2025-12-08 22:22:18,236: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:22:18,236: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:22:18,236: t15.2024.05.10 val PER: 0.1293
2025-12-08 22:22:18,236: t15.2024.06.14 val PER: 0.1215
2025-12-08 22:22:18,236: t15.2024.07.19 val PER: 0.1661
2025-12-08 22:22:18,236: t15.2024.07.21 val PER: 0.0614
2025-12-08 22:22:18,236: t15.2024.07.28 val PER: 0.0765
2025-12-08 22:22:18,236: t15.2025.01.10 val PER: 0.2700
2025-12-08 22:22:18,236: t15.2025.01.12 val PER: 0.0754
2025-12-08 22:22:18,237: t15.2025.03.14 val PER: 0.2722
2025-12-08 22:22:18,237: t15.2025.03.16 val PER: 0.1217
2025-12-08 22:22:18,237: t15.2025.03.30 val PER: 0.2425
2025-12-08 22:22:18,237: t15.2025.04.13 val PER: 0.1869
2025-12-08 22:22:50,222: Train batch 72200: loss: 0.00 grad norm: 0.30 time: 0.163
2025-12-08 22:23:21,856: Train batch 72400: loss: 0.00 grad norm: 0.03 time: 0.118
2025-12-08 22:23:53,681: Train batch 72600: loss: 0.13 grad norm: 5.54 time: 0.127
2025-12-08 22:24:25,082: Train batch 72800: loss: 0.01 grad norm: 0.43 time: 0.090
2025-12-08 22:24:56,708: Train batch 73000: loss: 0.00 grad norm: 0.22 time: 0.117
2025-12-08 22:25:27,690: Train batch 73200: loss: 0.10 grad norm: 1.88 time: 0.096
2025-12-08 22:25:59,967: Train batch 73400: loss: 0.05 grad norm: 1.73 time: 0.143
2025-12-08 22:26:31,868: Train batch 73600: loss: 0.00 grad norm: 0.06 time: 0.103
2025-12-08 22:27:02,860: Train batch 73800: loss: 0.00 grad norm: 0.04 time: 0.088
2025-12-08 22:27:34,660: Train batch 74000: loss: 0.00 grad norm: 0.13 time: 0.112
2025-12-08 22:27:34,661: Running test after training batch: 74000
2025-12-08 22:27:44,862: Val batch 74000: PER (avg): 0.1094 CTC Loss (avg): 38.5808 time: 10.201
2025-12-08 22:27:44,862: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:27:44,862: t15.2023.08.13 val PER: 0.0863
2025-12-08 22:27:44,862: t15.2023.08.18 val PER: 0.0738
2025-12-08 22:27:44,863: t15.2023.08.20 val PER: 0.0580
2025-12-08 22:27:44,863: t15.2023.08.25 val PER: 0.0979
2025-12-08 22:27:44,863: t15.2023.08.27 val PER: 0.1383
2025-12-08 22:27:44,863: t15.2023.09.01 val PER: 0.0479
2025-12-08 22:27:44,863: t15.2023.09.03 val PER: 0.1081
2025-12-08 22:27:44,863: t15.2023.09.24 val PER: 0.0825
2025-12-08 22:27:44,863: t15.2023.09.29 val PER: 0.1219
2025-12-08 22:27:44,863: t15.2023.10.01 val PER: 0.1308
2025-12-08 22:27:44,863: t15.2023.10.06 val PER: 0.0818
2025-12-08 22:27:44,863: t15.2023.10.08 val PER: 0.1746
2025-12-08 22:27:44,863: t15.2023.10.13 val PER: 0.1497
2025-12-08 22:27:44,863: t15.2023.10.15 val PER: 0.1160
2025-12-08 22:27:44,863: t15.2023.10.20 val PER: 0.1812
2025-12-08 22:27:44,863: t15.2023.10.22 val PER: 0.1102
2025-12-08 22:27:44,863: t15.2023.11.03 val PER: 0.1730
2025-12-08 22:27:44,863: t15.2023.11.04 val PER: 0.0239
2025-12-08 22:27:44,863: t15.2023.11.17 val PER: 0.0280
2025-12-08 22:27:44,863: t15.2023.11.19 val PER: 0.0659
2025-12-08 22:27:44,864: t15.2023.11.26 val PER: 0.0449
2025-12-08 22:27:44,864: t15.2023.12.03 val PER: 0.0420
2025-12-08 22:27:44,864: t15.2023.12.08 val PER: 0.0386
2025-12-08 22:27:44,864: t15.2023.12.10 val PER: 0.0237
2025-12-08 22:27:44,864: t15.2023.12.17 val PER: 0.1164
2025-12-08 22:27:44,864: t15.2023.12.29 val PER: 0.0858
2025-12-08 22:27:44,864: t15.2024.02.25 val PER: 0.0646
2025-12-08 22:27:44,864: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:27:44,864: t15.2024.03.08 val PER: 0.1693
2025-12-08 22:27:44,864: t15.2024.03.15 val PER: 0.1676
2025-12-08 22:27:44,864: t15.2024.03.17 val PER: 0.0802
2025-12-08 22:27:44,864: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:27:44,864: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:27:44,864: t15.2024.05.10 val PER: 0.1293
2025-12-08 22:27:44,864: t15.2024.06.14 val PER: 0.1151
2025-12-08 22:27:44,864: t15.2024.07.19 val PER: 0.1549
2025-12-08 22:27:44,864: t15.2024.07.21 val PER: 0.0648
2025-12-08 22:27:44,864: t15.2024.07.28 val PER: 0.0926
2025-12-08 22:27:44,864: t15.2025.01.10 val PER: 0.2658
2025-12-08 22:27:44,865: t15.2025.01.12 val PER: 0.0793
2025-12-08 22:27:44,865: t15.2025.03.14 val PER: 0.2692
2025-12-08 22:27:44,865: t15.2025.03.16 val PER: 0.1165
2025-12-08 22:27:44,865: t15.2025.03.30 val PER: 0.2437
2025-12-08 22:27:44,865: t15.2025.04.13 val PER: 0.1869
2025-12-08 22:27:44,865: New best test PER 0.1108 --> 0.1094
2025-12-08 22:27:44,865: Checkpointing model
2025-12-08 22:27:45,943: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 22:28:17,244: Train batch 74200: loss: 0.08 grad norm: 1.88 time: 0.169
2025-12-08 22:28:48,207: Train batch 74400: loss: 0.00 grad norm: 0.03 time: 0.132
2025-12-08 22:29:18,997: Train batch 74600: loss: 0.00 grad norm: 0.12 time: 0.111
2025-12-08 22:29:50,185: Train batch 74800: loss: 0.00 grad norm: 0.04 time: 0.141
2025-12-08 22:30:21,438: Train batch 75000: loss: 0.01 grad norm: 1.08 time: 0.116
2025-12-08 22:30:53,041: Train batch 75200: loss: 0.00 grad norm: 0.04 time: 0.117
2025-12-08 22:31:24,314: Train batch 75400: loss: 0.02 grad norm: 2.18 time: 0.102
2025-12-08 22:31:55,292: Train batch 75600: loss: 0.00 grad norm: 0.02 time: 0.082
2025-12-08 22:32:26,924: Train batch 75800: loss: 0.01 grad norm: 1.05 time: 0.126
2025-12-08 22:32:57,272: Train batch 76000: loss: 0.15 grad norm: 1.78 time: 0.095
2025-12-08 22:32:57,273: Running test after training batch: 76000
2025-12-08 22:33:07,420: Val batch 76000: PER (avg): 0.1088 CTC Loss (avg): 39.9903 time: 10.148
2025-12-08 22:33:07,421: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:33:07,421: t15.2023.08.13 val PER: 0.0738
2025-12-08 22:33:07,421: t15.2023.08.18 val PER: 0.0696
2025-12-08 22:33:07,421: t15.2023.08.20 val PER: 0.0556
2025-12-08 22:33:07,421: t15.2023.08.25 val PER: 0.0858
2025-12-08 22:33:07,421: t15.2023.08.27 val PER: 0.1206
2025-12-08 22:33:07,421: t15.2023.09.01 val PER: 0.0455
2025-12-08 22:33:07,421: t15.2023.09.03 val PER: 0.1176
2025-12-08 22:33:07,421: t15.2023.09.24 val PER: 0.0983
2025-12-08 22:33:07,421: t15.2023.09.29 val PER: 0.1238
2025-12-08 22:33:07,421: t15.2023.10.01 val PER: 0.1281
2025-12-08 22:33:07,421: t15.2023.10.06 val PER: 0.0700
2025-12-08 22:33:07,421: t15.2023.10.08 val PER: 0.1746
2025-12-08 22:33:07,422: t15.2023.10.13 val PER: 0.1598
2025-12-08 22:33:07,422: t15.2023.10.15 val PER: 0.1252
2025-12-08 22:33:07,422: t15.2023.10.20 val PER: 0.1611
2025-12-08 22:33:07,422: t15.2023.10.22 val PER: 0.1125
2025-12-08 22:33:07,422: t15.2023.11.03 val PER: 0.1703
2025-12-08 22:33:07,422: t15.2023.11.04 val PER: 0.0307
2025-12-08 22:33:07,422: t15.2023.11.17 val PER: 0.0249
2025-12-08 22:33:07,422: t15.2023.11.19 val PER: 0.0499
2025-12-08 22:33:07,422: t15.2023.11.26 val PER: 0.0377
2025-12-08 22:33:07,422: t15.2023.12.03 val PER: 0.0431
2025-12-08 22:33:07,422: t15.2023.12.08 val PER: 0.0393
2025-12-08 22:33:07,422: t15.2023.12.10 val PER: 0.0276
2025-12-08 22:33:07,422: t15.2023.12.17 val PER: 0.1164
2025-12-08 22:33:07,422: t15.2023.12.29 val PER: 0.0803
2025-12-08 22:33:07,422: t15.2024.02.25 val PER: 0.0548
2025-12-08 22:33:07,422: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:33:07,422: t15.2024.03.08 val PER: 0.1764
2025-12-08 22:33:07,422: t15.2024.03.15 val PER: 0.1826
2025-12-08 22:33:07,423: t15.2024.03.17 val PER: 0.0704
2025-12-08 22:33:07,423: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:33:07,423: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:33:07,423: t15.2024.05.10 val PER: 0.1293
2025-12-08 22:33:07,423: t15.2024.06.14 val PER: 0.1246
2025-12-08 22:33:07,423: t15.2024.07.19 val PER: 0.1556
2025-12-08 22:33:07,423: t15.2024.07.21 val PER: 0.0586
2025-12-08 22:33:07,423: t15.2024.07.28 val PER: 0.0831
2025-12-08 22:33:07,423: t15.2025.01.10 val PER: 0.2713
2025-12-08 22:33:07,423: t15.2025.01.12 val PER: 0.0662
2025-12-08 22:33:07,423: t15.2025.03.14 val PER: 0.3033
2025-12-08 22:33:07,423: t15.2025.03.16 val PER: 0.1230
2025-12-08 22:33:07,423: t15.2025.03.30 val PER: 0.2529
2025-12-08 22:33:07,423: t15.2025.04.13 val PER: 0.1755
2025-12-08 22:33:07,423: New best test PER 0.1094 --> 0.1088
2025-12-08 22:33:07,423: Checkpointing model
2025-12-08 22:33:08,467: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 22:33:40,781: Train batch 76200: loss: 0.00 grad norm: 0.02 time: 0.092
2025-12-08 22:34:12,321: Train batch 76400: loss: 0.00 grad norm: 0.01 time: 0.119
2025-12-08 22:34:43,465: Train batch 76600: loss: 0.00 grad norm: 0.02 time: 0.090
2025-12-08 22:35:14,584: Train batch 76800: loss: 0.01 grad norm: 0.49 time: 0.103
2025-12-08 22:35:46,056: Train batch 77000: loss: 0.13 grad norm: 5.16 time: 0.104
2025-12-08 22:36:17,621: Train batch 77200: loss: 0.01 grad norm: 1.00 time: 0.155
2025-12-08 22:36:49,959: Train batch 77400: loss: 0.00 grad norm: 0.11 time: 0.141
2025-12-08 22:37:22,832: Train batch 77600: loss: 0.00 grad norm: 0.04 time: 0.128
2025-12-08 22:37:55,541: Train batch 77800: loss: 0.00 grad norm: 0.07 time: 0.166
2025-12-08 22:38:27,264: Train batch 78000: loss: 0.00 grad norm: 0.07 time: 0.087
2025-12-08 22:38:27,265: Running test after training batch: 78000
2025-12-08 22:38:37,411: Val batch 78000: PER (avg): 0.1066 CTC Loss (avg): 38.7696 time: 10.146
2025-12-08 22:38:37,411: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:38:37,412: t15.2023.08.13 val PER: 0.0780
2025-12-08 22:38:37,412: t15.2023.08.18 val PER: 0.0679
2025-12-08 22:38:37,412: t15.2023.08.20 val PER: 0.0564
2025-12-08 22:38:37,412: t15.2023.08.25 val PER: 0.0828
2025-12-08 22:38:37,412: t15.2023.08.27 val PER: 0.1399
2025-12-08 22:38:37,412: t15.2023.09.01 val PER: 0.0430
2025-12-08 22:38:37,412: t15.2023.09.03 val PER: 0.1223
2025-12-08 22:38:37,412: t15.2023.09.24 val PER: 0.0934
2025-12-08 22:38:37,412: t15.2023.09.29 val PER: 0.1155
2025-12-08 22:38:37,412: t15.2023.10.01 val PER: 0.1314
2025-12-08 22:38:37,412: t15.2023.10.06 val PER: 0.0635
2025-12-08 22:38:37,412: t15.2023.10.08 val PER: 0.1773
2025-12-08 22:38:37,412: t15.2023.10.13 val PER: 0.1583
2025-12-08 22:38:37,412: t15.2023.10.15 val PER: 0.1180
2025-12-08 22:38:37,412: t15.2023.10.20 val PER: 0.1711
2025-12-08 22:38:37,412: t15.2023.10.22 val PER: 0.1158
2025-12-08 22:38:37,412: t15.2023.11.03 val PER: 0.1710
2025-12-08 22:38:37,412: t15.2023.11.04 val PER: 0.0307
2025-12-08 22:38:37,413: t15.2023.11.17 val PER: 0.0373
2025-12-08 22:38:37,413: t15.2023.11.19 val PER: 0.0639
2025-12-08 22:38:37,413: t15.2023.11.26 val PER: 0.0435
2025-12-08 22:38:37,413: t15.2023.12.03 val PER: 0.0494
2025-12-08 22:38:37,413: t15.2023.12.08 val PER: 0.0413
2025-12-08 22:38:37,413: t15.2023.12.10 val PER: 0.0184
2025-12-08 22:38:37,413: t15.2023.12.17 val PER: 0.0998
2025-12-08 22:38:37,413: t15.2023.12.29 val PER: 0.0776
2025-12-08 22:38:37,413: t15.2024.02.25 val PER: 0.0590
2025-12-08 22:38:37,413: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:38:37,413: t15.2024.03.08 val PER: 0.1650
2025-12-08 22:38:37,413: t15.2024.03.15 val PER: 0.1670
2025-12-08 22:38:37,413: t15.2024.03.17 val PER: 0.0732
2025-12-08 22:38:37,413: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:38:37,413: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:38:37,413: t15.2024.05.10 val PER: 0.1337
2025-12-08 22:38:37,413: t15.2024.06.14 val PER: 0.1230
2025-12-08 22:38:37,413: t15.2024.07.19 val PER: 0.1503
2025-12-08 22:38:37,413: t15.2024.07.21 val PER: 0.0600
2025-12-08 22:38:37,414: t15.2024.07.28 val PER: 0.0750
2025-12-08 22:38:37,414: t15.2025.01.10 val PER: 0.2562
2025-12-08 22:38:37,414: t15.2025.01.12 val PER: 0.0670
2025-12-08 22:38:37,414: t15.2025.03.14 val PER: 0.2825
2025-12-08 22:38:37,414: t15.2025.03.16 val PER: 0.1230
2025-12-08 22:38:37,414: t15.2025.03.30 val PER: 0.2368
2025-12-08 22:38:37,414: t15.2025.04.13 val PER: 0.1641
2025-12-08 22:38:37,414: New best test PER 0.1088 --> 0.1066
2025-12-08 22:38:37,414: Checkpointing model
2025-12-08 22:38:38,515: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 22:39:09,998: Train batch 78200: loss: 0.00 grad norm: 0.07 time: 0.094
2025-12-08 22:39:43,279: Train batch 78400: loss: 0.00 grad norm: 0.01 time: 0.100
2025-12-08 22:40:15,835: Train batch 78600: loss: 0.00 grad norm: 0.22 time: 0.092
2025-12-08 22:40:48,699: Train batch 78800: loss: 0.04 grad norm: 2.20 time: 0.083
2025-12-08 22:41:19,692: Train batch 79000: loss: 0.00 grad norm: 0.04 time: 0.074
2025-12-08 22:41:50,530: Train batch 79200: loss: 0.00 grad norm: 0.00 time: 0.123
2025-12-08 22:42:22,198: Train batch 79400: loss: 0.00 grad norm: 0.02 time: 0.144
2025-12-08 22:42:53,662: Train batch 79600: loss: 0.00 grad norm: 0.04 time: 0.125
2025-12-08 22:43:25,514: Train batch 79800: loss: 0.01 grad norm: 0.71 time: 0.134
2025-12-08 22:43:58,258: Train batch 80000: loss: 0.00 grad norm: 0.22 time: 0.096
2025-12-08 22:43:58,259: Running test after training batch: 80000
2025-12-08 22:44:08,182: Val batch 80000: PER (avg): 0.1068 CTC Loss (avg): 41.4341 time: 9.923
2025-12-08 22:44:08,182: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:44:08,183: t15.2023.08.13 val PER: 0.0634
2025-12-08 22:44:08,183: t15.2023.08.18 val PER: 0.0754
2025-12-08 22:44:08,183: t15.2023.08.20 val PER: 0.0564
2025-12-08 22:44:08,183: t15.2023.08.25 val PER: 0.0949
2025-12-08 22:44:08,183: t15.2023.08.27 val PER: 0.1334
2025-12-08 22:44:08,183: t15.2023.09.01 val PER: 0.0341
2025-12-08 22:44:08,183: t15.2023.09.03 val PER: 0.1105
2025-12-08 22:44:08,183: t15.2023.09.24 val PER: 0.0898
2025-12-08 22:44:08,183: t15.2023.09.29 val PER: 0.1155
2025-12-08 22:44:08,183: t15.2023.10.01 val PER: 0.1446
2025-12-08 22:44:08,183: t15.2023.10.06 val PER: 0.0797
2025-12-08 22:44:08,183: t15.2023.10.08 val PER: 0.1651
2025-12-08 22:44:08,183: t15.2023.10.13 val PER: 0.1544
2025-12-08 22:44:08,183: t15.2023.10.15 val PER: 0.1081
2025-12-08 22:44:08,183: t15.2023.10.20 val PER: 0.1510
2025-12-08 22:44:08,183: t15.2023.10.22 val PER: 0.1180
2025-12-08 22:44:08,183: t15.2023.11.03 val PER: 0.1737
2025-12-08 22:44:08,184: t15.2023.11.04 val PER: 0.0307
2025-12-08 22:44:08,184: t15.2023.11.17 val PER: 0.0295
2025-12-08 22:44:08,184: t15.2023.11.19 val PER: 0.0679
2025-12-08 22:44:08,184: t15.2023.11.26 val PER: 0.0420
2025-12-08 22:44:08,184: t15.2023.12.03 val PER: 0.0473
2025-12-08 22:44:08,184: t15.2023.12.08 val PER: 0.0386
2025-12-08 22:44:08,184: t15.2023.12.10 val PER: 0.0263
2025-12-08 22:44:08,184: t15.2023.12.17 val PER: 0.1091
2025-12-08 22:44:08,184: t15.2023.12.29 val PER: 0.0824
2025-12-08 22:44:08,184: t15.2024.02.25 val PER: 0.0590
2025-12-08 22:44:08,184: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:44:08,184: t15.2024.03.08 val PER: 0.1693
2025-12-08 22:44:08,184: t15.2024.03.15 val PER: 0.1795
2025-12-08 22:44:08,184: t15.2024.03.17 val PER: 0.0683
2025-12-08 22:44:08,184: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:44:08,184: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:44:08,184: t15.2024.05.10 val PER: 0.1129
2025-12-08 22:44:08,184: t15.2024.06.14 val PER: 0.1167
2025-12-08 22:44:08,185: t15.2024.07.19 val PER: 0.1529
2025-12-08 22:44:08,185: t15.2024.07.21 val PER: 0.0641
2025-12-08 22:44:08,185: t15.2024.07.28 val PER: 0.0735
2025-12-08 22:44:08,185: t15.2025.01.10 val PER: 0.2548
2025-12-08 22:44:08,185: t15.2025.01.12 val PER: 0.0731
2025-12-08 22:44:08,185: t15.2025.03.14 val PER: 0.2692
2025-12-08 22:44:08,185: t15.2025.03.16 val PER: 0.1217
2025-12-08 22:44:08,185: t15.2025.03.30 val PER: 0.2517
2025-12-08 22:44:08,185: t15.2025.04.13 val PER: 0.1612
2025-12-08 22:44:41,077: Train batch 80200: loss: 0.05 grad norm: 2.32 time: 0.153
2025-12-08 22:45:12,536: Train batch 80400: loss: 0.00 grad norm: 0.01 time: 0.120
2025-12-08 22:45:44,210: Train batch 80600: loss: 0.00 grad norm: 0.09 time: 0.094
2025-12-08 22:46:17,406: Train batch 80800: loss: 0.01 grad norm: 0.73 time: 0.141
2025-12-08 22:46:49,021: Train batch 81000: loss: 0.00 grad norm: 0.16 time: 0.093
2025-12-08 22:47:20,674: Train batch 81200: loss: 0.00 grad norm: 0.04 time: 0.136
2025-12-08 22:47:51,668: Train batch 81400: loss: 0.00 grad norm: 0.23 time: 0.161
2025-12-08 22:48:22,671: Train batch 81600: loss: 0.00 grad norm: 0.04 time: 0.101
2025-12-08 22:48:54,010: Train batch 81800: loss: 0.00 grad norm: 0.17 time: 0.099
2025-12-08 22:49:27,210: Train batch 82000: loss: 0.00 grad norm: 0.02 time: 0.123
2025-12-08 22:49:27,210: Running test after training batch: 82000
2025-12-08 22:49:37,411: Val batch 82000: PER (avg): 0.1084 CTC Loss (avg): 42.3957 time: 10.200
2025-12-08 22:49:37,411: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:49:37,411: t15.2023.08.13 val PER: 0.0676
2025-12-08 22:49:37,411: t15.2023.08.18 val PER: 0.0696
2025-12-08 22:49:37,411: t15.2023.08.20 val PER: 0.0500
2025-12-08 22:49:37,411: t15.2023.08.25 val PER: 0.0994
2025-12-08 22:49:37,411: t15.2023.08.27 val PER: 0.1367
2025-12-08 22:49:37,411: t15.2023.09.01 val PER: 0.0446
2025-12-08 22:49:37,411: t15.2023.09.03 val PER: 0.1164
2025-12-08 22:49:37,411: t15.2023.09.24 val PER: 0.0874
2025-12-08 22:49:37,411: t15.2023.09.29 val PER: 0.1283
2025-12-08 22:49:37,411: t15.2023.10.01 val PER: 0.1367
2025-12-08 22:49:37,412: t15.2023.10.06 val PER: 0.0700
2025-12-08 22:49:37,412: t15.2023.10.08 val PER: 0.1719
2025-12-08 22:49:37,412: t15.2023.10.13 val PER: 0.1598
2025-12-08 22:49:37,412: t15.2023.10.15 val PER: 0.1180
2025-12-08 22:49:37,412: t15.2023.10.20 val PER: 0.1913
2025-12-08 22:49:37,412: t15.2023.10.22 val PER: 0.1192
2025-12-08 22:49:37,412: t15.2023.11.03 val PER: 0.1866
2025-12-08 22:49:37,412: t15.2023.11.04 val PER: 0.0307
2025-12-08 22:49:37,412: t15.2023.11.17 val PER: 0.0218
2025-12-08 22:49:37,412: t15.2023.11.19 val PER: 0.0619
2025-12-08 22:49:37,412: t15.2023.11.26 val PER: 0.0529
2025-12-08 22:49:37,412: t15.2023.12.03 val PER: 0.0483
2025-12-08 22:49:37,412: t15.2023.12.08 val PER: 0.0366
2025-12-08 22:49:37,412: t15.2023.12.10 val PER: 0.0329
2025-12-08 22:49:37,412: t15.2023.12.17 val PER: 0.1060
2025-12-08 22:49:37,412: t15.2023.12.29 val PER: 0.0817
2025-12-08 22:49:37,412: t15.2024.02.25 val PER: 0.0534
2025-12-08 22:49:37,413: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:49:37,413: t15.2024.03.08 val PER: 0.1735
2025-12-08 22:49:37,413: t15.2024.03.15 val PER: 0.1770
2025-12-08 22:49:37,413: t15.2024.03.17 val PER: 0.0662
2025-12-08 22:49:37,413: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:49:37,413: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:49:37,413: t15.2024.05.10 val PER: 0.1129
2025-12-08 22:49:37,413: t15.2024.06.14 val PER: 0.1151
2025-12-08 22:49:37,413: t15.2024.07.19 val PER: 0.1635
2025-12-08 22:49:37,413: t15.2024.07.21 val PER: 0.0621
2025-12-08 22:49:37,413: t15.2024.07.28 val PER: 0.0721
2025-12-08 22:49:37,413: t15.2025.01.10 val PER: 0.2410
2025-12-08 22:49:37,413: t15.2025.01.12 val PER: 0.0716
2025-12-08 22:49:37,413: t15.2025.03.14 val PER: 0.2811
2025-12-08 22:49:37,413: t15.2025.03.16 val PER: 0.1217
2025-12-08 22:49:37,413: t15.2025.03.30 val PER: 0.2368
2025-12-08 22:49:37,414: t15.2025.04.13 val PER: 0.1726
2025-12-08 22:50:09,718: Train batch 82200: loss: 0.00 grad norm: 0.01 time: 0.140
2025-12-08 22:50:43,737: Train batch 82400: loss: 0.00 grad norm: 0.06 time: 0.089
2025-12-08 22:51:15,315: Train batch 82600: loss: 0.00 grad norm: 0.54 time: 0.103
2025-12-08 22:51:48,278: Train batch 82800: loss: 0.02 grad norm: 1.61 time: 0.141
2025-12-08 22:52:18,699: Train batch 83000: loss: 0.00 grad norm: 0.00 time: 0.094
2025-12-08 22:52:49,913: Train batch 83200: loss: 0.00 grad norm: 0.01 time: 0.147
2025-12-08 22:53:22,100: Train batch 83400: loss: 0.00 grad norm: 0.01 time: 0.094
2025-12-08 22:53:53,449: Train batch 83600: loss: 0.00 grad norm: 0.00 time: 0.122
2025-12-08 22:54:25,245: Train batch 83800: loss: 0.00 grad norm: 0.13 time: 0.146
2025-12-08 22:54:54,035: Train batch 84000: loss: 0.00 grad norm: 0.00 time: 0.135
2025-12-08 22:54:54,035: Running test after training batch: 84000
2025-12-08 22:55:06,093: Val batch 84000: PER (avg): 0.1077 CTC Loss (avg): 43.0539 time: 12.058
2025-12-08 22:55:06,094: t15.2023.08.11 val PER: 1.0000
2025-12-08 22:55:06,094: t15.2023.08.13 val PER: 0.0676
2025-12-08 22:55:06,094: t15.2023.08.18 val PER: 0.0704
2025-12-08 22:55:06,094: t15.2023.08.20 val PER: 0.0445
2025-12-08 22:55:06,094: t15.2023.08.25 val PER: 0.0994
2025-12-08 22:55:06,094: t15.2023.08.27 val PER: 0.1206
2025-12-08 22:55:06,094: t15.2023.09.01 val PER: 0.0373
2025-12-08 22:55:06,094: t15.2023.09.03 val PER: 0.1116
2025-12-08 22:55:06,094: t15.2023.09.24 val PER: 0.0862
2025-12-08 22:55:06,094: t15.2023.09.29 val PER: 0.1232
2025-12-08 22:55:06,094: t15.2023.10.01 val PER: 0.1341
2025-12-08 22:55:06,095: t15.2023.10.06 val PER: 0.0700
2025-12-08 22:55:06,095: t15.2023.10.08 val PER: 0.1705
2025-12-08 22:55:06,095: t15.2023.10.13 val PER: 0.1629
2025-12-08 22:55:06,095: t15.2023.10.15 val PER: 0.1107
2025-12-08 22:55:06,095: t15.2023.10.20 val PER: 0.1879
2025-12-08 22:55:06,095: t15.2023.10.22 val PER: 0.1125
2025-12-08 22:55:06,095: t15.2023.11.03 val PER: 0.1716
2025-12-08 22:55:06,095: t15.2023.11.04 val PER: 0.0273
2025-12-08 22:55:06,095: t15.2023.11.17 val PER: 0.0187
2025-12-08 22:55:06,095: t15.2023.11.19 val PER: 0.0739
2025-12-08 22:55:06,095: t15.2023.11.26 val PER: 0.0500
2025-12-08 22:55:06,095: t15.2023.12.03 val PER: 0.0452
2025-12-08 22:55:06,095: t15.2023.12.08 val PER: 0.0366
2025-12-08 22:55:06,095: t15.2023.12.10 val PER: 0.0355
2025-12-08 22:55:06,095: t15.2023.12.17 val PER: 0.1102
2025-12-08 22:55:06,095: t15.2023.12.29 val PER: 0.0844
2025-12-08 22:55:06,096: t15.2024.02.25 val PER: 0.0576
2025-12-08 22:55:06,096: t15.2024.03.03 val PER: 1.0000
2025-12-08 22:55:06,096: t15.2024.03.08 val PER: 0.1764
2025-12-08 22:55:06,096: t15.2024.03.15 val PER: 0.1826
2025-12-08 22:55:06,096: t15.2024.03.17 val PER: 0.0697
2025-12-08 22:55:06,096: t15.2024.04.25 val PER: 1.0000
2025-12-08 22:55:06,096: t15.2024.04.28 val PER: 1.0000
2025-12-08 22:55:06,096: t15.2024.05.10 val PER: 0.1263
2025-12-08 22:55:06,096: t15.2024.06.14 val PER: 0.1183
2025-12-08 22:55:06,096: t15.2024.07.19 val PER: 0.1622
2025-12-08 22:55:06,096: t15.2024.07.21 val PER: 0.0683
2025-12-08 22:55:06,096: t15.2024.07.28 val PER: 0.0721
2025-12-08 22:55:06,096: t15.2025.01.10 val PER: 0.2658
2025-12-08 22:55:06,096: t15.2025.01.12 val PER: 0.0685
2025-12-08 22:55:06,096: t15.2025.03.14 val PER: 0.2781
2025-12-08 22:55:06,096: t15.2025.03.16 val PER: 0.1270
2025-12-08 22:55:06,097: t15.2025.03.30 val PER: 0.2299
2025-12-08 22:55:06,097: t15.2025.04.13 val PER: 0.1698
2025-12-08 22:55:36,370: Train batch 84200: loss: 0.00 grad norm: 0.04 time: 0.134
2025-12-08 22:56:09,280: Train batch 84400: loss: 0.00 grad norm: 0.41 time: 0.121
2025-12-08 22:56:41,776: Train batch 84600: loss: 0.00 grad norm: 0.03 time: 0.093
2025-12-08 22:57:15,629: Train batch 84800: loss: 0.00 grad norm: 0.02 time: 0.170
2025-12-08 22:57:47,506: Train batch 85000: loss: 0.00 grad norm: 0.01 time: 0.146
2025-12-08 22:58:18,205: Train batch 85200: loss: 0.00 grad norm: 0.08 time: 0.127
2025-12-08 22:58:47,715: Train batch 85400: loss: 0.00 grad norm: 0.03 time: 0.106
2025-12-08 22:59:19,397: Train batch 85600: loss: 0.00 grad norm: 0.03 time: 0.144
2025-12-08 22:59:50,586: Train batch 85800: loss: 0.00 grad norm: 0.00 time: 0.083
2025-12-08 23:00:21,293: Train batch 86000: loss: 0.00 grad norm: 0.02 time: 0.135
2025-12-08 23:00:21,293: Running test after training batch: 86000
2025-12-08 23:00:32,226: Val batch 86000: PER (avg): 0.1046 CTC Loss (avg): 41.4128 time: 10.933
2025-12-08 23:00:32,226: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:00:32,226: t15.2023.08.13 val PER: 0.0717
2025-12-08 23:00:32,226: t15.2023.08.18 val PER: 0.0712
2025-12-08 23:00:32,226: t15.2023.08.20 val PER: 0.0492
2025-12-08 23:00:32,226: t15.2023.08.25 val PER: 0.1024
2025-12-08 23:00:32,226: t15.2023.08.27 val PER: 0.1334
2025-12-08 23:00:32,226: t15.2023.09.01 val PER: 0.0381
2025-12-08 23:00:32,226: t15.2023.09.03 val PER: 0.1116
2025-12-08 23:00:32,226: t15.2023.09.24 val PER: 0.0850
2025-12-08 23:00:32,227: t15.2023.09.29 val PER: 0.1238
2025-12-08 23:00:32,227: t15.2023.10.01 val PER: 0.1229
2025-12-08 23:00:32,227: t15.2023.10.06 val PER: 0.0689
2025-12-08 23:00:32,227: t15.2023.10.08 val PER: 0.1651
2025-12-08 23:00:32,227: t15.2023.10.13 val PER: 0.1606
2025-12-08 23:00:32,227: t15.2023.10.15 val PER: 0.1028
2025-12-08 23:00:32,227: t15.2023.10.20 val PER: 0.1711
2025-12-08 23:00:32,227: t15.2023.10.22 val PER: 0.1125
2025-12-08 23:00:32,227: t15.2023.11.03 val PER: 0.1757
2025-12-08 23:00:32,227: t15.2023.11.04 val PER: 0.0239
2025-12-08 23:00:32,227: t15.2023.11.17 val PER: 0.0249
2025-12-08 23:00:32,227: t15.2023.11.19 val PER: 0.0459
2025-12-08 23:00:32,227: t15.2023.11.26 val PER: 0.0428
2025-12-08 23:00:32,227: t15.2023.12.03 val PER: 0.0441
2025-12-08 23:00:32,227: t15.2023.12.08 val PER: 0.0446
2025-12-08 23:00:32,227: t15.2023.12.10 val PER: 0.0368
2025-12-08 23:00:32,227: t15.2023.12.17 val PER: 0.1071
2025-12-08 23:00:32,227: t15.2023.12.29 val PER: 0.0879
2025-12-08 23:00:32,228: t15.2024.02.25 val PER: 0.0548
2025-12-08 23:00:32,228: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:00:32,228: t15.2024.03.08 val PER: 0.1650
2025-12-08 23:00:32,228: t15.2024.03.15 val PER: 0.1614
2025-12-08 23:00:32,228: t15.2024.03.17 val PER: 0.0711
2025-12-08 23:00:32,228: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:00:32,228: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:00:32,228: t15.2024.05.10 val PER: 0.1114
2025-12-08 23:00:32,228: t15.2024.06.14 val PER: 0.1183
2025-12-08 23:00:32,228: t15.2024.07.19 val PER: 0.1635
2025-12-08 23:00:32,228: t15.2024.07.21 val PER: 0.0697
2025-12-08 23:00:32,228: t15.2024.07.28 val PER: 0.0647
2025-12-08 23:00:32,228: t15.2025.01.10 val PER: 0.2342
2025-12-08 23:00:32,228: t15.2025.01.12 val PER: 0.0677
2025-12-08 23:00:32,228: t15.2025.03.14 val PER: 0.2855
2025-12-08 23:00:32,228: t15.2025.03.16 val PER: 0.0995
2025-12-08 23:00:32,228: t15.2025.03.30 val PER: 0.2230
2025-12-08 23:00:32,228: t15.2025.04.13 val PER: 0.1641
2025-12-08 23:00:32,228: New best test PER 0.1066 --> 0.1046
2025-12-08 23:00:32,228: Checkpointing model
2025-12-08 23:00:33,274: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:01:03,357: Train batch 86200: loss: 0.00 grad norm: 0.01 time: 0.218
2025-12-08 23:01:33,082: Train batch 86400: loss: 0.01 grad norm: 1.09 time: 0.117
2025-12-08 23:02:03,320: Train batch 86600: loss: 0.00 grad norm: 0.00 time: 0.122
2025-12-08 23:02:33,519: Train batch 86800: loss: 0.01 grad norm: 0.67 time: 0.119
2025-12-08 23:03:03,889: Train batch 87000: loss: 0.00 grad norm: 0.01 time: 0.096
2025-12-08 23:03:35,906: Train batch 87200: loss: 0.00 grad norm: 0.01 time: 0.147
2025-12-08 23:04:08,750: Train batch 87400: loss: 0.00 grad norm: 0.31 time: 0.146
2025-12-08 23:04:40,803: Train batch 87600: loss: 0.00 grad norm: 0.01 time: 0.087
2025-12-08 23:05:14,320: Train batch 87800: loss: 0.00 grad norm: 0.03 time: 0.106
2025-12-08 23:05:46,910: Train batch 88000: loss: 0.00 grad norm: 0.00 time: 0.102
2025-12-08 23:05:46,910: Running test after training batch: 88000
2025-12-08 23:05:57,401: Val batch 88000: PER (avg): 0.1064 CTC Loss (avg): 44.6525 time: 10.491
2025-12-08 23:05:57,401: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:05:57,401: t15.2023.08.13 val PER: 0.0748
2025-12-08 23:05:57,401: t15.2023.08.18 val PER: 0.0746
2025-12-08 23:05:57,401: t15.2023.08.20 val PER: 0.0492
2025-12-08 23:05:57,402: t15.2023.08.25 val PER: 0.0919
2025-12-08 23:05:57,402: t15.2023.08.27 val PER: 0.1270
2025-12-08 23:05:57,402: t15.2023.09.01 val PER: 0.0398
2025-12-08 23:05:57,402: t15.2023.09.03 val PER: 0.1021
2025-12-08 23:05:57,402: t15.2023.09.24 val PER: 0.0886
2025-12-08 23:05:57,402: t15.2023.09.29 val PER: 0.1244
2025-12-08 23:05:57,402: t15.2023.10.01 val PER: 0.1347
2025-12-08 23:05:57,402: t15.2023.10.06 val PER: 0.0743
2025-12-08 23:05:57,402: t15.2023.10.08 val PER: 0.1746
2025-12-08 23:05:57,402: t15.2023.10.13 val PER: 0.1629
2025-12-08 23:05:57,402: t15.2023.10.15 val PER: 0.1101
2025-12-08 23:05:57,402: t15.2023.10.20 val PER: 0.1846
2025-12-08 23:05:57,402: t15.2023.10.22 val PER: 0.1192
2025-12-08 23:05:57,402: t15.2023.11.03 val PER: 0.1750
2025-12-08 23:05:57,402: t15.2023.11.04 val PER: 0.0205
2025-12-08 23:05:57,402: t15.2023.11.17 val PER: 0.0311
2025-12-08 23:05:57,403: t15.2023.11.19 val PER: 0.0679
2025-12-08 23:05:57,403: t15.2023.11.26 val PER: 0.0486
2025-12-08 23:05:57,403: t15.2023.12.03 val PER: 0.0431
2025-12-08 23:05:57,403: t15.2023.12.08 val PER: 0.0393
2025-12-08 23:05:57,403: t15.2023.12.10 val PER: 0.0302
2025-12-08 23:05:57,403: t15.2023.12.17 val PER: 0.1164
2025-12-08 23:05:57,403: t15.2023.12.29 val PER: 0.0830
2025-12-08 23:05:57,403: t15.2024.02.25 val PER: 0.0520
2025-12-08 23:05:57,403: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:05:57,403: t15.2024.03.08 val PER: 0.1593
2025-12-08 23:05:57,403: t15.2024.03.15 val PER: 0.1720
2025-12-08 23:05:57,403: t15.2024.03.17 val PER: 0.0642
2025-12-08 23:05:57,403: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:05:57,403: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:05:57,403: t15.2024.05.10 val PER: 0.1159
2025-12-08 23:05:57,403: t15.2024.06.14 val PER: 0.1215
2025-12-08 23:05:57,404: t15.2024.07.19 val PER: 0.1549
2025-12-08 23:05:57,404: t15.2024.07.21 val PER: 0.0724
2025-12-08 23:05:57,404: t15.2024.07.28 val PER: 0.0640
2025-12-08 23:05:57,404: t15.2025.01.10 val PER: 0.2479
2025-12-08 23:05:57,404: t15.2025.01.12 val PER: 0.0770
2025-12-08 23:05:57,404: t15.2025.03.14 val PER: 0.2692
2025-12-08 23:05:57,404: t15.2025.03.16 val PER: 0.1047
2025-12-08 23:05:57,404: t15.2025.03.30 val PER: 0.2264
2025-12-08 23:05:57,404: t15.2025.04.13 val PER: 0.1683
2025-12-08 23:06:29,312: Train batch 88200: loss: 0.00 grad norm: 0.00 time: 0.077
2025-12-08 23:07:02,563: Train batch 88400: loss: 0.00 grad norm: 0.01 time: 0.122
2025-12-08 23:07:35,504: Train batch 88600: loss: 0.00 grad norm: 0.02 time: 0.191
2025-12-08 23:08:08,090: Train batch 88800: loss: 0.00 grad norm: 0.21 time: 0.097
2025-12-08 23:08:41,432: Train batch 89000: loss: 0.01 grad norm: 0.51 time: 0.111
2025-12-08 23:09:14,612: Train batch 89200: loss: 0.00 grad norm: 0.01 time: 0.128
2025-12-08 23:09:47,296: Train batch 89400: loss: 0.00 grad norm: 0.37 time: 0.109
2025-12-08 23:10:19,721: Train batch 89600: loss: 0.00 grad norm: 0.02 time: 0.100
2025-12-08 23:10:52,864: Train batch 89800: loss: 0.00 grad norm: 0.41 time: 0.120
2025-12-08 23:11:26,252: Train batch 90000: loss: 0.00 grad norm: 0.02 time: 0.131
2025-12-08 23:11:26,252: Running test after training batch: 90000
2025-12-08 23:11:36,673: Val batch 90000: PER (avg): 0.1041 CTC Loss (avg): 44.5603 time: 10.420
2025-12-08 23:11:36,673: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:11:36,673: t15.2023.08.13 val PER: 0.0696
2025-12-08 23:11:36,673: t15.2023.08.18 val PER: 0.0712
2025-12-08 23:11:36,673: t15.2023.08.20 val PER: 0.0548
2025-12-08 23:11:36,673: t15.2023.08.25 val PER: 0.0889
2025-12-08 23:11:36,673: t15.2023.08.27 val PER: 0.1302
2025-12-08 23:11:36,674: t15.2023.09.01 val PER: 0.0365
2025-12-08 23:11:36,674: t15.2023.09.03 val PER: 0.1069
2025-12-08 23:11:36,674: t15.2023.09.24 val PER: 0.0874
2025-12-08 23:11:36,674: t15.2023.09.29 val PER: 0.1206
2025-12-08 23:11:36,674: t15.2023.10.01 val PER: 0.1229
2025-12-08 23:11:36,674: t15.2023.10.06 val PER: 0.0753
2025-12-08 23:11:36,674: t15.2023.10.08 val PER: 0.1705
2025-12-08 23:11:36,674: t15.2023.10.13 val PER: 0.1544
2025-12-08 23:11:36,674: t15.2023.10.15 val PER: 0.1140
2025-12-08 23:11:36,674: t15.2023.10.20 val PER: 0.1711
2025-12-08 23:11:36,674: t15.2023.10.22 val PER: 0.1125
2025-12-08 23:11:36,674: t15.2023.11.03 val PER: 0.1676
2025-12-08 23:11:36,674: t15.2023.11.04 val PER: 0.0307
2025-12-08 23:11:36,674: t15.2023.11.17 val PER: 0.0171
2025-12-08 23:11:36,674: t15.2023.11.19 val PER: 0.0499
2025-12-08 23:11:36,674: t15.2023.11.26 val PER: 0.0486
2025-12-08 23:11:36,674: t15.2023.12.03 val PER: 0.0389
2025-12-08 23:11:36,674: t15.2023.12.08 val PER: 0.0360
2025-12-08 23:11:36,675: t15.2023.12.10 val PER: 0.0355
2025-12-08 23:11:36,675: t15.2023.12.17 val PER: 0.1164
2025-12-08 23:11:36,675: t15.2023.12.29 val PER: 0.0844
2025-12-08 23:11:36,675: t15.2024.02.25 val PER: 0.0590
2025-12-08 23:11:36,675: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:11:36,675: t15.2024.03.08 val PER: 0.1579
2025-12-08 23:11:36,675: t15.2024.03.15 val PER: 0.1701
2025-12-08 23:11:36,675: t15.2024.03.17 val PER: 0.0662
2025-12-08 23:11:36,675: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:11:36,675: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:11:36,675: t15.2024.05.10 val PER: 0.1070
2025-12-08 23:11:36,675: t15.2024.06.14 val PER: 0.1120
2025-12-08 23:11:36,675: t15.2024.07.19 val PER: 0.1516
2025-12-08 23:11:36,675: t15.2024.07.21 val PER: 0.0710
2025-12-08 23:11:36,675: t15.2024.07.28 val PER: 0.0596
2025-12-08 23:11:36,675: t15.2025.01.10 val PER: 0.2466
2025-12-08 23:11:36,675: t15.2025.01.12 val PER: 0.0762
2025-12-08 23:11:36,675: t15.2025.03.14 val PER: 0.2604
2025-12-08 23:11:36,675: t15.2025.03.16 val PER: 0.1047
2025-12-08 23:11:36,676: t15.2025.03.30 val PER: 0.2241
2025-12-08 23:11:36,676: t15.2025.04.13 val PER: 0.1797
2025-12-08 23:11:36,676: New best test PER 0.1046 --> 0.1041
2025-12-08 23:11:36,676: Checkpointing model
2025-12-08 23:11:37,858: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:12:10,174: Train batch 90200: loss: 0.00 grad norm: 0.00 time: 0.118
2025-12-08 23:12:42,719: Train batch 90400: loss: 0.00 grad norm: 0.01 time: 0.117
2025-12-08 23:13:15,373: Train batch 90600: loss: 0.00 grad norm: 0.00 time: 0.076
2025-12-08 23:13:48,273: Train batch 90800: loss: 0.00 grad norm: 0.02 time: 0.141
2025-12-08 23:14:21,664: Train batch 91000: loss: 0.00 grad norm: 0.00 time: 0.110
2025-12-08 23:14:54,173: Train batch 91200: loss: 0.01 grad norm: 1.18 time: 0.149
2025-12-08 23:15:27,286: Train batch 91400: loss: 0.00 grad norm: 0.02 time: 0.133
2025-12-08 23:16:00,457: Train batch 91600: loss: 0.00 grad norm: 0.00 time: 0.132
2025-12-08 23:16:33,868: Train batch 91800: loss: 0.00 grad norm: 0.01 time: 0.115
2025-12-08 23:17:07,702: Train batch 92000: loss: 0.00 grad norm: 0.00 time: 0.096
2025-12-08 23:17:07,702: Running test after training batch: 92000
2025-12-08 23:17:18,198: Val batch 92000: PER (avg): 0.1043 CTC Loss (avg): 44.7991 time: 10.496
2025-12-08 23:17:18,198: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:17:18,198: t15.2023.08.13 val PER: 0.0780
2025-12-08 23:17:18,198: t15.2023.08.18 val PER: 0.0721
2025-12-08 23:17:18,198: t15.2023.08.20 val PER: 0.0548
2025-12-08 23:17:18,198: t15.2023.08.25 val PER: 0.0949
2025-12-08 23:17:18,198: t15.2023.08.27 val PER: 0.1334
2025-12-08 23:17:18,199: t15.2023.09.01 val PER: 0.0414
2025-12-08 23:17:18,199: t15.2023.09.03 val PER: 0.1069
2025-12-08 23:17:18,199: t15.2023.09.24 val PER: 0.0850
2025-12-08 23:17:18,199: t15.2023.09.29 val PER: 0.1174
2025-12-08 23:17:18,199: t15.2023.10.01 val PER: 0.1314
2025-12-08 23:17:18,199: t15.2023.10.06 val PER: 0.0710
2025-12-08 23:17:18,199: t15.2023.10.08 val PER: 0.1678
2025-12-08 23:17:18,199: t15.2023.10.13 val PER: 0.1528
2025-12-08 23:17:18,199: t15.2023.10.15 val PER: 0.1121
2025-12-08 23:17:18,199: t15.2023.10.20 val PER: 0.1946
2025-12-08 23:17:18,199: t15.2023.10.22 val PER: 0.1225
2025-12-08 23:17:18,199: t15.2023.11.03 val PER: 0.1682
2025-12-08 23:17:18,199: t15.2023.11.04 val PER: 0.0273
2025-12-08 23:17:18,199: t15.2023.11.17 val PER: 0.0156
2025-12-08 23:17:18,199: t15.2023.11.19 val PER: 0.0479
2025-12-08 23:17:18,199: t15.2023.11.26 val PER: 0.0471
2025-12-08 23:17:18,199: t15.2023.12.03 val PER: 0.0441
2025-12-08 23:17:18,199: t15.2023.12.08 val PER: 0.0353
2025-12-08 23:17:18,200: t15.2023.12.10 val PER: 0.0368
2025-12-08 23:17:18,200: t15.2023.12.17 val PER: 0.1175
2025-12-08 23:17:18,200: t15.2023.12.29 val PER: 0.0940
2025-12-08 23:17:18,200: t15.2024.02.25 val PER: 0.0618
2025-12-08 23:17:18,200: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:17:18,200: t15.2024.03.08 val PER: 0.1565
2025-12-08 23:17:18,200: t15.2024.03.15 val PER: 0.1695
2025-12-08 23:17:18,200: t15.2024.03.17 val PER: 0.0642
2025-12-08 23:17:18,200: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:17:18,200: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:17:18,200: t15.2024.05.10 val PER: 0.0981
2025-12-08 23:17:18,200: t15.2024.06.14 val PER: 0.1246
2025-12-08 23:17:18,200: t15.2024.07.19 val PER: 0.1523
2025-12-08 23:17:18,200: t15.2024.07.21 val PER: 0.0621
2025-12-08 23:17:18,200: t15.2024.07.28 val PER: 0.0625
2025-12-08 23:17:18,200: t15.2025.01.10 val PER: 0.2410
2025-12-08 23:17:18,200: t15.2025.01.12 val PER: 0.0662
2025-12-08 23:17:18,200: t15.2025.03.14 val PER: 0.2485
2025-12-08 23:17:18,201: t15.2025.03.16 val PER: 0.1034
2025-12-08 23:17:18,201: t15.2025.03.30 val PER: 0.2184
2025-12-08 23:17:18,201: t15.2025.04.13 val PER: 0.1840
2025-12-08 23:17:50,525: Train batch 92200: loss: 0.02 grad norm: 1.49 time: 0.114
2025-12-08 23:18:25,027: Train batch 92400: loss: 0.00 grad norm: 0.00 time: 0.163
2025-12-08 23:18:57,795: Train batch 92600: loss: 0.02 grad norm: 1.99 time: 0.104
2025-12-08 23:19:31,049: Train batch 92800: loss: 0.00 grad norm: 0.03 time: 0.085
2025-12-08 23:20:03,094: Train batch 93000: loss: 0.00 grad norm: 0.00 time: 0.141
2025-12-08 23:20:36,103: Train batch 93200: loss: 0.00 grad norm: 0.00 time: 0.166
2025-12-08 23:21:08,074: Train batch 93400: loss: 0.00 grad norm: 0.00 time: 0.124
2025-12-08 23:21:40,069: Train batch 93600: loss: 0.00 grad norm: 0.00 time: 0.105
2025-12-08 23:22:12,597: Train batch 93800: loss: 0.00 grad norm: 0.01 time: 0.111
2025-12-08 23:22:45,786: Train batch 94000: loss: 0.00 grad norm: 0.00 time: 0.120
2025-12-08 23:22:45,787: Running test after training batch: 94000
2025-12-08 23:22:56,251: Val batch 94000: PER (avg): 0.1053 CTC Loss (avg): 46.1498 time: 10.464
2025-12-08 23:22:56,251: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:22:56,251: t15.2023.08.13 val PER: 0.0811
2025-12-08 23:22:56,251: t15.2023.08.18 val PER: 0.0687
2025-12-08 23:22:56,251: t15.2023.08.20 val PER: 0.0564
2025-12-08 23:22:56,251: t15.2023.08.25 val PER: 0.0994
2025-12-08 23:22:56,251: t15.2023.08.27 val PER: 0.1302
2025-12-08 23:22:56,251: t15.2023.09.01 val PER: 0.0390
2025-12-08 23:22:56,251: t15.2023.09.03 val PER: 0.1140
2025-12-08 23:22:56,251: t15.2023.09.24 val PER: 0.0862
2025-12-08 23:22:56,252: t15.2023.09.29 val PER: 0.1187
2025-12-08 23:22:56,252: t15.2023.10.01 val PER: 0.1347
2025-12-08 23:22:56,252: t15.2023.10.06 val PER: 0.0700
2025-12-08 23:22:56,252: t15.2023.10.08 val PER: 0.1664
2025-12-08 23:22:56,252: t15.2023.10.13 val PER: 0.1552
2025-12-08 23:22:56,252: t15.2023.10.15 val PER: 0.1127
2025-12-08 23:22:56,252: t15.2023.10.20 val PER: 0.1678
2025-12-08 23:22:56,252: t15.2023.10.22 val PER: 0.1258
2025-12-08 23:22:56,252: t15.2023.11.03 val PER: 0.1730
2025-12-08 23:22:56,252: t15.2023.11.04 val PER: 0.0273
2025-12-08 23:22:56,252: t15.2023.11.17 val PER: 0.0202
2025-12-08 23:22:56,252: t15.2023.11.19 val PER: 0.0579
2025-12-08 23:22:56,252: t15.2023.11.26 val PER: 0.0486
2025-12-08 23:22:56,252: t15.2023.12.03 val PER: 0.0473
2025-12-08 23:22:56,252: t15.2023.12.08 val PER: 0.0353
2025-12-08 23:22:56,252: t15.2023.12.10 val PER: 0.0368
2025-12-08 23:22:56,252: t15.2023.12.17 val PER: 0.1154
2025-12-08 23:22:56,252: t15.2023.12.29 val PER: 0.0844
2025-12-08 23:22:56,252: t15.2024.02.25 val PER: 0.0562
2025-12-08 23:22:56,253: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:22:56,253: t15.2024.03.08 val PER: 0.1650
2025-12-08 23:22:56,253: t15.2024.03.15 val PER: 0.1739
2025-12-08 23:22:56,253: t15.2024.03.17 val PER: 0.0656
2025-12-08 23:22:56,253: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:22:56,253: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:22:56,253: t15.2024.05.10 val PER: 0.1055
2025-12-08 23:22:56,253: t15.2024.06.14 val PER: 0.1136
2025-12-08 23:22:56,253: t15.2024.07.19 val PER: 0.1490
2025-12-08 23:22:56,253: t15.2024.07.21 val PER: 0.0676
2025-12-08 23:22:56,253: t15.2024.07.28 val PER: 0.0691
2025-12-08 23:22:56,253: t15.2025.01.10 val PER: 0.2493
2025-12-08 23:22:56,253: t15.2025.01.12 val PER: 0.0701
2025-12-08 23:22:56,253: t15.2025.03.14 val PER: 0.2441
2025-12-08 23:22:56,253: t15.2025.03.16 val PER: 0.0969
2025-12-08 23:22:56,253: t15.2025.03.30 val PER: 0.2241
2025-12-08 23:22:56,253: t15.2025.04.13 val PER: 0.1797
2025-12-08 23:23:29,631: Train batch 94200: loss: 0.02 grad norm: 2.01 time: 0.098
2025-12-08 23:24:02,539: Train batch 94400: loss: 0.00 grad norm: 0.00 time: 0.083
2025-12-08 23:24:35,631: Train batch 94600: loss: 0.00 grad norm: 0.01 time: 0.107
2025-12-08 23:25:09,228: Train batch 94800: loss: 0.00 grad norm: 0.00 time: 0.135
2025-12-08 23:25:42,313: Train batch 95000: loss: 0.08 grad norm: 2.89 time: 0.167
2025-12-08 23:26:15,004: Train batch 95200: loss: 0.00 grad norm: 0.00 time: 0.092
2025-12-08 23:26:48,193: Train batch 95400: loss: 0.00 grad norm: 0.00 time: 0.153
2025-12-08 23:27:20,803: Train batch 95600: loss: 0.00 grad norm: 0.21 time: 0.122
2025-12-08 23:27:52,470: Train batch 95800: loss: 0.00 grad norm: 0.00 time: 0.084
2025-12-08 23:28:25,799: Train batch 96000: loss: 0.00 grad norm: 0.00 time: 0.163
2025-12-08 23:28:25,800: Running test after training batch: 96000
2025-12-08 23:28:36,431: Val batch 96000: PER (avg): 0.1052 CTC Loss (avg): 45.6285 time: 10.631
2025-12-08 23:28:36,432: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:28:36,432: t15.2023.08.13 val PER: 0.0728
2025-12-08 23:28:36,432: t15.2023.08.18 val PER: 0.0671
2025-12-08 23:28:36,432: t15.2023.08.20 val PER: 0.0572
2025-12-08 23:28:36,432: t15.2023.08.25 val PER: 0.0919
2025-12-08 23:28:36,432: t15.2023.08.27 val PER: 0.1238
2025-12-08 23:28:36,432: t15.2023.09.01 val PER: 0.0438
2025-12-08 23:28:36,432: t15.2023.09.03 val PER: 0.1045
2025-12-08 23:28:36,432: t15.2023.09.24 val PER: 0.0886
2025-12-08 23:28:36,432: t15.2023.09.29 val PER: 0.1225
2025-12-08 23:28:36,432: t15.2023.10.01 val PER: 0.1341
2025-12-08 23:28:36,432: t15.2023.10.06 val PER: 0.0764
2025-12-08 23:28:36,432: t15.2023.10.08 val PER: 0.1773
2025-12-08 23:28:36,432: t15.2023.10.13 val PER: 0.1458
2025-12-08 23:28:36,433: t15.2023.10.15 val PER: 0.1134
2025-12-08 23:28:36,433: t15.2023.10.20 val PER: 0.1644
2025-12-08 23:28:36,433: t15.2023.10.22 val PER: 0.1203
2025-12-08 23:28:36,433: t15.2023.11.03 val PER: 0.1818
2025-12-08 23:28:36,433: t15.2023.11.04 val PER: 0.0307
2025-12-08 23:28:36,433: t15.2023.11.17 val PER: 0.0280
2025-12-08 23:28:36,433: t15.2023.11.19 val PER: 0.0539
2025-12-08 23:28:36,433: t15.2023.11.26 val PER: 0.0464
2025-12-08 23:28:36,433: t15.2023.12.03 val PER: 0.0462
2025-12-08 23:28:36,433: t15.2023.12.08 val PER: 0.0306
2025-12-08 23:28:36,433: t15.2023.12.10 val PER: 0.0355
2025-12-08 23:28:36,433: t15.2023.12.17 val PER: 0.1154
2025-12-08 23:28:36,433: t15.2023.12.29 val PER: 0.0906
2025-12-08 23:28:36,433: t15.2024.02.25 val PER: 0.0562
2025-12-08 23:28:36,433: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:28:36,433: t15.2024.03.08 val PER: 0.1650
2025-12-08 23:28:36,433: t15.2024.03.15 val PER: 0.1676
2025-12-08 23:28:36,433: t15.2024.03.17 val PER: 0.0607
2025-12-08 23:28:36,433: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:28:36,434: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:28:36,434: t15.2024.05.10 val PER: 0.1100
2025-12-08 23:28:36,434: t15.2024.06.14 val PER: 0.1057
2025-12-08 23:28:36,434: t15.2024.07.19 val PER: 0.1615
2025-12-08 23:28:36,434: t15.2024.07.21 val PER: 0.0697
2025-12-08 23:28:36,434: t15.2024.07.28 val PER: 0.0625
2025-12-08 23:28:36,434: t15.2025.01.10 val PER: 0.2590
2025-12-08 23:28:36,434: t15.2025.01.12 val PER: 0.0600
2025-12-08 23:28:36,434: t15.2025.03.14 val PER: 0.2678
2025-12-08 23:28:36,434: t15.2025.03.16 val PER: 0.0995
2025-12-08 23:28:36,434: t15.2025.03.30 val PER: 0.2207
2025-12-08 23:28:36,434: t15.2025.04.13 val PER: 0.1755
2025-12-08 23:29:09,402: Train batch 96200: loss: 0.00 grad norm: 0.00 time: 0.117
2025-12-08 23:29:42,210: Train batch 96400: loss: 0.00 grad norm: 0.04 time: 0.094
2025-12-08 23:30:15,689: Train batch 96600: loss: 0.00 grad norm: 0.00 time: 0.140
2025-12-08 23:30:49,132: Train batch 96800: loss: 0.00 grad norm: 0.03 time: 0.088
2025-12-08 23:31:23,012: Train batch 97000: loss: 0.00 grad norm: 0.00 time: 0.113
2025-12-08 23:31:56,516: Train batch 97200: loss: 0.00 grad norm: 0.12 time: 0.140
2025-12-08 23:32:29,661: Train batch 97400: loss: 0.00 grad norm: 0.00 time: 0.129
2025-12-08 23:33:02,051: Train batch 97600: loss: 0.00 grad norm: 0.00 time: 0.093
2025-12-08 23:33:35,736: Train batch 97800: loss: 0.00 grad norm: 0.00 time: 0.119
2025-12-08 23:34:08,798: Train batch 98000: loss: 0.00 grad norm: 0.00 time: 0.128
2025-12-08 23:34:08,798: Running test after training batch: 98000
2025-12-08 23:34:19,063: Val batch 98000: PER (avg): 0.1036 CTC Loss (avg): 45.4450 time: 10.264
2025-12-08 23:34:19,063: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:34:19,063: t15.2023.08.13 val PER: 0.0759
2025-12-08 23:34:19,063: t15.2023.08.18 val PER: 0.0629
2025-12-08 23:34:19,063: t15.2023.08.20 val PER: 0.0604
2025-12-08 23:34:19,063: t15.2023.08.25 val PER: 0.0783
2025-12-08 23:34:19,063: t15.2023.08.27 val PER: 0.1238
2025-12-08 23:34:19,064: t15.2023.09.01 val PER: 0.0430
2025-12-08 23:34:19,064: t15.2023.09.03 val PER: 0.1045
2025-12-08 23:34:19,064: t15.2023.09.24 val PER: 0.0874
2025-12-08 23:34:19,064: t15.2023.09.29 val PER: 0.1200
2025-12-08 23:34:19,064: t15.2023.10.01 val PER: 0.1367
2025-12-08 23:34:19,064: t15.2023.10.06 val PER: 0.0732
2025-12-08 23:34:19,064: t15.2023.10.08 val PER: 0.1705
2025-12-08 23:34:19,064: t15.2023.10.13 val PER: 0.1505
2025-12-08 23:34:19,064: t15.2023.10.15 val PER: 0.1101
2025-12-08 23:34:19,064: t15.2023.10.20 val PER: 0.1678
2025-12-08 23:34:19,064: t15.2023.10.22 val PER: 0.1102
2025-12-08 23:34:19,064: t15.2023.11.03 val PER: 0.1689
2025-12-08 23:34:19,064: t15.2023.11.04 val PER: 0.0205
2025-12-08 23:34:19,064: t15.2023.11.17 val PER: 0.0233
2025-12-08 23:34:19,064: t15.2023.11.19 val PER: 0.0519
2025-12-08 23:34:19,064: t15.2023.11.26 val PER: 0.0478
2025-12-08 23:34:19,064: t15.2023.12.03 val PER: 0.0452
2025-12-08 23:34:19,064: t15.2023.12.08 val PER: 0.0300
2025-12-08 23:34:19,065: t15.2023.12.10 val PER: 0.0394
2025-12-08 23:34:19,065: t15.2023.12.17 val PER: 0.1227
2025-12-08 23:34:19,065: t15.2023.12.29 val PER: 0.0858
2025-12-08 23:34:19,065: t15.2024.02.25 val PER: 0.0590
2025-12-08 23:34:19,065: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:34:19,065: t15.2024.03.08 val PER: 0.1693
2025-12-08 23:34:19,065: t15.2024.03.15 val PER: 0.1732
2025-12-08 23:34:19,065: t15.2024.03.17 val PER: 0.0656
2025-12-08 23:34:19,065: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:34:19,065: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:34:19,065: t15.2024.05.10 val PER: 0.1100
2025-12-08 23:34:19,065: t15.2024.06.14 val PER: 0.0994
2025-12-08 23:34:19,065: t15.2024.07.19 val PER: 0.1543
2025-12-08 23:34:19,065: t15.2024.07.21 val PER: 0.0676
2025-12-08 23:34:19,065: t15.2024.07.28 val PER: 0.0603
2025-12-08 23:34:19,065: t15.2025.01.10 val PER: 0.2493
2025-12-08 23:34:19,065: t15.2025.01.12 val PER: 0.0616
2025-12-08 23:34:19,065: t15.2025.03.14 val PER: 0.2618
2025-12-08 23:34:19,065: t15.2025.03.16 val PER: 0.0982
2025-12-08 23:34:19,066: t15.2025.03.30 val PER: 0.2103
2025-12-08 23:34:19,066: t15.2025.04.13 val PER: 0.1740
2025-12-08 23:34:19,066: New best test PER 0.1041 --> 0.1036
2025-12-08 23:34:19,066: Checkpointing model
2025-12-08 23:34:20,215: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:34:52,417: Train batch 98200: loss: 0.00 grad norm: 0.00 time: 0.101
2025-12-08 23:35:26,417: Train batch 98400: loss: 0.00 grad norm: 0.38 time: 0.106
2025-12-08 23:35:59,754: Train batch 98600: loss: 0.00 grad norm: 0.00 time: 0.137
2025-12-08 23:36:33,660: Train batch 98800: loss: 0.00 grad norm: 0.00 time: 0.097
2025-12-08 23:37:08,090: Train batch 99000: loss: 0.00 grad norm: 0.01 time: 0.111
2025-12-08 23:37:42,328: Train batch 99200: loss: 0.00 grad norm: 0.01 time: 0.121
2025-12-08 23:38:14,872: Train batch 99400: loss: 0.00 grad norm: 0.00 time: 0.096
2025-12-08 23:38:47,640: Train batch 99600: loss: 0.00 grad norm: 0.01 time: 0.118
2025-12-08 23:39:21,227: Train batch 99800: loss: 0.00 grad norm: 0.00 time: 0.076
2025-12-08 23:39:54,199: Train batch 100000: loss: 0.00 grad norm: 0.00 time: 0.129
2025-12-08 23:39:54,199: Running test after training batch: 100000
2025-12-08 23:40:04,586: Val batch 100000: PER (avg): 0.1019 CTC Loss (avg): 45.9174 time: 10.386
2025-12-08 23:40:04,586: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:40:04,586: t15.2023.08.13 val PER: 0.0696
2025-12-08 23:40:04,586: t15.2023.08.18 val PER: 0.0629
2025-12-08 23:40:04,586: t15.2023.08.20 val PER: 0.0572
2025-12-08 23:40:04,586: t15.2023.08.25 val PER: 0.0768
2025-12-08 23:40:04,586: t15.2023.08.27 val PER: 0.1222
2025-12-08 23:40:04,586: t15.2023.09.01 val PER: 0.0422
2025-12-08 23:40:04,586: t15.2023.09.03 val PER: 0.1010
2025-12-08 23:40:04,586: t15.2023.09.24 val PER: 0.0837
2025-12-08 23:40:04,586: t15.2023.09.29 val PER: 0.1219
2025-12-08 23:40:04,586: t15.2023.10.01 val PER: 0.1321
2025-12-08 23:40:04,586: t15.2023.10.06 val PER: 0.0689
2025-12-08 23:40:04,587: t15.2023.10.08 val PER: 0.1746
2025-12-08 23:40:04,587: t15.2023.10.13 val PER: 0.1505
2025-12-08 23:40:04,587: t15.2023.10.15 val PER: 0.1167
2025-12-08 23:40:04,587: t15.2023.10.20 val PER: 0.1812
2025-12-08 23:40:04,587: t15.2023.10.22 val PER: 0.1036
2025-12-08 23:40:04,587: t15.2023.11.03 val PER: 0.1737
2025-12-08 23:40:04,587: t15.2023.11.04 val PER: 0.0205
2025-12-08 23:40:04,587: t15.2023.11.17 val PER: 0.0233
2025-12-08 23:40:04,587: t15.2023.11.19 val PER: 0.0499
2025-12-08 23:40:04,587: t15.2023.11.26 val PER: 0.0464
2025-12-08 23:40:04,587: t15.2023.12.03 val PER: 0.0431
2025-12-08 23:40:04,587: t15.2023.12.08 val PER: 0.0306
2025-12-08 23:40:04,587: t15.2023.12.10 val PER: 0.0263
2025-12-08 23:40:04,587: t15.2023.12.17 val PER: 0.1164
2025-12-08 23:40:04,587: t15.2023.12.29 val PER: 0.0872
2025-12-08 23:40:04,587: t15.2024.02.25 val PER: 0.0576
2025-12-08 23:40:04,587: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:40:04,587: t15.2024.03.08 val PER: 0.1650
2025-12-08 23:40:04,588: t15.2024.03.15 val PER: 0.1714
2025-12-08 23:40:04,588: t15.2024.03.17 val PER: 0.0656
2025-12-08 23:40:04,588: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:40:04,588: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:40:04,588: t15.2024.05.10 val PER: 0.1114
2025-12-08 23:40:04,588: t15.2024.06.14 val PER: 0.0946
2025-12-08 23:40:04,588: t15.2024.07.19 val PER: 0.1404
2025-12-08 23:40:04,588: t15.2024.07.21 val PER: 0.0662
2025-12-08 23:40:04,588: t15.2024.07.28 val PER: 0.0551
2025-12-08 23:40:04,588: t15.2025.01.10 val PER: 0.2438
2025-12-08 23:40:04,588: t15.2025.01.12 val PER: 0.0585
2025-12-08 23:40:04,588: t15.2025.03.14 val PER: 0.2574
2025-12-08 23:40:04,588: t15.2025.03.16 val PER: 0.0890
2025-12-08 23:40:04,588: t15.2025.03.30 val PER: 0.2184
2025-12-08 23:40:04,588: t15.2025.04.13 val PER: 0.1812
2025-12-08 23:40:04,588: New best test PER 0.1036 --> 0.1019
2025-12-08 23:40:04,588: Checkpointing model
2025-12-08 23:40:05,699: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:40:38,654: Train batch 100200: loss: 0.00 grad norm: 0.00 time: 0.143
2025-12-08 23:41:12,576: Train batch 100400: loss: 0.00 grad norm: 0.03 time: 0.138
2025-12-08 23:41:45,928: Train batch 100600: loss: 0.00 grad norm: 0.00 time: 0.087
2025-12-08 23:42:20,168: Train batch 100800: loss: 0.00 grad norm: 0.17 time: 0.133
2025-12-08 23:42:53,197: Train batch 101000: loss: 0.00 grad norm: 0.02 time: 0.129
2025-12-08 23:43:26,904: Train batch 101200: loss: 0.01 grad norm: 0.88 time: 0.143
2025-12-08 23:44:00,077: Train batch 101400: loss: 0.00 grad norm: 0.01 time: 0.150
2025-12-08 23:44:32,813: Train batch 101600: loss: 0.00 grad norm: 0.00 time: 0.137
2025-12-08 23:45:05,499: Train batch 101800: loss: 0.00 grad norm: 0.14 time: 0.114
2025-12-08 23:45:38,556: Train batch 102000: loss: 0.00 grad norm: 0.05 time: 0.109
2025-12-08 23:45:38,556: Running test after training batch: 102000
2025-12-08 23:45:49,248: Val batch 102000: PER (avg): 0.1017 CTC Loss (avg): 45.2979 time: 10.692
2025-12-08 23:45:49,249: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:45:49,249: t15.2023.08.13 val PER: 0.0686
2025-12-08 23:45:49,249: t15.2023.08.18 val PER: 0.0637
2025-12-08 23:45:49,249: t15.2023.08.20 val PER: 0.0572
2025-12-08 23:45:49,249: t15.2023.08.25 val PER: 0.0873
2025-12-08 23:45:49,249: t15.2023.08.27 val PER: 0.1254
2025-12-08 23:45:49,249: t15.2023.09.01 val PER: 0.0406
2025-12-08 23:45:49,249: t15.2023.09.03 val PER: 0.1069
2025-12-08 23:45:49,249: t15.2023.09.24 val PER: 0.0813
2025-12-08 23:45:49,249: t15.2023.09.29 val PER: 0.1238
2025-12-08 23:45:49,249: t15.2023.10.01 val PER: 0.1268
2025-12-08 23:45:49,249: t15.2023.10.06 val PER: 0.0667
2025-12-08 23:45:49,249: t15.2023.10.08 val PER: 0.1732
2025-12-08 23:45:49,249: t15.2023.10.13 val PER: 0.1490
2025-12-08 23:45:49,249: t15.2023.10.15 val PER: 0.1121
2025-12-08 23:45:49,250: t15.2023.10.20 val PER: 0.1745
2025-12-08 23:45:49,250: t15.2023.10.22 val PER: 0.0980
2025-12-08 23:45:49,250: t15.2023.11.03 val PER: 0.1703
2025-12-08 23:45:49,250: t15.2023.11.04 val PER: 0.0239
2025-12-08 23:45:49,250: t15.2023.11.17 val PER: 0.0249
2025-12-08 23:45:49,250: t15.2023.11.19 val PER: 0.0479
2025-12-08 23:45:49,250: t15.2023.11.26 val PER: 0.0478
2025-12-08 23:45:49,250: t15.2023.12.03 val PER: 0.0473
2025-12-08 23:45:49,250: t15.2023.12.08 val PER: 0.0326
2025-12-08 23:45:49,250: t15.2023.12.10 val PER: 0.0302
2025-12-08 23:45:49,250: t15.2023.12.17 val PER: 0.1133
2025-12-08 23:45:49,250: t15.2023.12.29 val PER: 0.0803
2025-12-08 23:45:49,250: t15.2024.02.25 val PER: 0.0562
2025-12-08 23:45:49,250: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:45:49,250: t15.2024.03.08 val PER: 0.1664
2025-12-08 23:45:49,250: t15.2024.03.15 val PER: 0.1682
2025-12-08 23:45:49,250: t15.2024.03.17 val PER: 0.0656
2025-12-08 23:45:49,250: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:45:49,250: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:45:49,251: t15.2024.05.10 val PER: 0.1010
2025-12-08 23:45:49,251: t15.2024.06.14 val PER: 0.0978
2025-12-08 23:45:49,251: t15.2024.07.19 val PER: 0.1490
2025-12-08 23:45:49,251: t15.2024.07.21 val PER: 0.0641
2025-12-08 23:45:49,251: t15.2024.07.28 val PER: 0.0603
2025-12-08 23:45:49,251: t15.2025.01.10 val PER: 0.2507
2025-12-08 23:45:49,251: t15.2025.01.12 val PER: 0.0577
2025-12-08 23:45:49,251: t15.2025.03.14 val PER: 0.2544
2025-12-08 23:45:49,251: t15.2025.03.16 val PER: 0.0969
2025-12-08 23:45:49,251: t15.2025.03.30 val PER: 0.2184
2025-12-08 23:45:49,251: t15.2025.04.13 val PER: 0.1769
2025-12-08 23:45:49,251: New best test PER 0.1019 --> 0.1017
2025-12-08 23:45:49,251: Checkpointing model
2025-12-08 23:45:50,462: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:46:23,247: Train batch 102200: loss: 0.00 grad norm: 0.00 time: 0.144
2025-12-08 23:46:55,909: Train batch 102400: loss: 0.00 grad norm: 0.00 time: 0.143
2025-12-08 23:47:28,046: Train batch 102600: loss: 0.00 grad norm: 0.00 time: 0.114
2025-12-08 23:48:02,069: Train batch 102800: loss: 0.00 grad norm: 0.00 time: 0.165
2025-12-08 23:48:34,819: Train batch 103000: loss: 0.00 grad norm: 0.00 time: 0.104
2025-12-08 23:49:08,719: Train batch 103200: loss: 0.02 grad norm: 1.77 time: 0.086
2025-12-08 23:49:41,854: Train batch 103400: loss: 0.00 grad norm: 0.00 time: 0.092
2025-12-08 23:50:14,572: Train batch 103600: loss: 0.00 grad norm: 0.00 time: 0.203
2025-12-08 23:50:46,642: Train batch 103800: loss: 0.00 grad norm: 0.00 time: 0.417
2025-12-08 23:51:23,141: Train batch 104000: loss: 0.00 grad norm: 0.03 time: 0.118
2025-12-08 23:51:23,142: Running test after training batch: 104000
2025-12-08 23:51:33,929: Val batch 104000: PER (avg): 0.1021 CTC Loss (avg): 46.4938 time: 10.787
2025-12-08 23:51:33,929: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:51:33,929: t15.2023.08.13 val PER: 0.0707
2025-12-08 23:51:33,929: t15.2023.08.18 val PER: 0.0645
2025-12-08 23:51:33,929: t15.2023.08.20 val PER: 0.0572
2025-12-08 23:51:33,929: t15.2023.08.25 val PER: 0.0783
2025-12-08 23:51:33,930: t15.2023.08.27 val PER: 0.1222
2025-12-08 23:51:33,930: t15.2023.09.01 val PER: 0.0414
2025-12-08 23:51:33,930: t15.2023.09.03 val PER: 0.1045
2025-12-08 23:51:33,930: t15.2023.09.24 val PER: 0.0850
2025-12-08 23:51:33,930: t15.2023.09.29 val PER: 0.1181
2025-12-08 23:51:33,930: t15.2023.10.01 val PER: 0.1334
2025-12-08 23:51:33,930: t15.2023.10.06 val PER: 0.0657
2025-12-08 23:51:33,930: t15.2023.10.08 val PER: 0.1691
2025-12-08 23:51:33,930: t15.2023.10.13 val PER: 0.1474
2025-12-08 23:51:33,930: t15.2023.10.15 val PER: 0.1107
2025-12-08 23:51:33,930: t15.2023.10.20 val PER: 0.1711
2025-12-08 23:51:33,930: t15.2023.10.22 val PER: 0.1091
2025-12-08 23:51:33,930: t15.2023.11.03 val PER: 0.1723
2025-12-08 23:51:33,930: t15.2023.11.04 val PER: 0.0239
2025-12-08 23:51:33,930: t15.2023.11.17 val PER: 0.0218
2025-12-08 23:51:33,930: t15.2023.11.19 val PER: 0.0459
2025-12-08 23:51:33,930: t15.2023.11.26 val PER: 0.0471
2025-12-08 23:51:33,930: t15.2023.12.03 val PER: 0.0441
2025-12-08 23:51:33,931: t15.2023.12.08 val PER: 0.0340
2025-12-08 23:51:33,931: t15.2023.12.10 val PER: 0.0302
2025-12-08 23:51:33,931: t15.2023.12.17 val PER: 0.1112
2025-12-08 23:51:33,931: t15.2023.12.29 val PER: 0.0837
2025-12-08 23:51:33,931: t15.2024.02.25 val PER: 0.0548
2025-12-08 23:51:33,931: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:51:33,931: t15.2024.03.08 val PER: 0.1664
2025-12-08 23:51:33,931: t15.2024.03.15 val PER: 0.1695
2025-12-08 23:51:33,931: t15.2024.03.17 val PER: 0.0697
2025-12-08 23:51:33,931: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:51:33,931: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:51:33,931: t15.2024.05.10 val PER: 0.0996
2025-12-08 23:51:33,931: t15.2024.06.14 val PER: 0.1025
2025-12-08 23:51:33,931: t15.2024.07.19 val PER: 0.1430
2025-12-08 23:51:33,931: t15.2024.07.21 val PER: 0.0621
2025-12-08 23:51:33,931: t15.2024.07.28 val PER: 0.0662
2025-12-08 23:51:33,931: t15.2025.01.10 val PER: 0.2507
2025-12-08 23:51:33,931: t15.2025.01.12 val PER: 0.0593
2025-12-08 23:51:33,931: t15.2025.03.14 val PER: 0.2544
2025-12-08 23:51:33,932: t15.2025.03.16 val PER: 0.1086
2025-12-08 23:51:33,932: t15.2025.03.30 val PER: 0.2161
2025-12-08 23:51:33,932: t15.2025.04.13 val PER: 0.1812
2025-12-08 23:52:02,359: Train batch 104200: loss: 0.00 grad norm: 0.00 time: 0.129
2025-12-08 23:52:30,324: Train batch 104400: loss: 0.00 grad norm: 0.00 time: 0.150
2025-12-08 23:52:59,064: Train batch 104600: loss: 0.00 grad norm: 0.00 time: 0.078
2025-12-08 23:53:27,215: Train batch 104800: loss: 0.00 grad norm: 0.00 time: 0.088
2025-12-08 23:53:56,107: Train batch 105000: loss: 0.00 grad norm: 0.00 time: 0.128
2025-12-08 23:54:25,283: Train batch 105200: loss: 0.00 grad norm: 0.00 time: 0.097
2025-12-08 23:54:54,664: Train batch 105400: loss: 0.00 grad norm: 0.00 time: 0.127
2025-12-08 23:55:24,044: Train batch 105600: loss: 0.00 grad norm: 0.00 time: 0.147
2025-12-08 23:55:53,365: Train batch 105800: loss: 0.00 grad norm: 0.01 time: 0.094
2025-12-08 23:56:22,464: Train batch 106000: loss: 0.00 grad norm: 0.01 time: 0.139
2025-12-08 23:56:22,464: Running test after training batch: 106000
2025-12-08 23:56:32,759: Val batch 106000: PER (avg): 0.1017 CTC Loss (avg): 46.8099 time: 10.294
2025-12-08 23:56:32,759: t15.2023.08.11 val PER: 1.0000
2025-12-08 23:56:32,759: t15.2023.08.13 val PER: 0.0748
2025-12-08 23:56:32,759: t15.2023.08.18 val PER: 0.0637
2025-12-08 23:56:32,759: t15.2023.08.20 val PER: 0.0548
2025-12-08 23:56:32,759: t15.2023.08.25 val PER: 0.0828
2025-12-08 23:56:32,759: t15.2023.08.27 val PER: 0.1158
2025-12-08 23:56:32,759: t15.2023.09.01 val PER: 0.0422
2025-12-08 23:56:32,759: t15.2023.09.03 val PER: 0.1045
2025-12-08 23:56:32,759: t15.2023.09.24 val PER: 0.0789
2025-12-08 23:56:32,759: t15.2023.09.29 val PER: 0.1161
2025-12-08 23:56:32,759: t15.2023.10.01 val PER: 0.1308
2025-12-08 23:56:32,759: t15.2023.10.06 val PER: 0.0700
2025-12-08 23:56:32,760: t15.2023.10.08 val PER: 0.1705
2025-12-08 23:56:32,760: t15.2023.10.13 val PER: 0.1458
2025-12-08 23:56:32,760: t15.2023.10.15 val PER: 0.1074
2025-12-08 23:56:32,760: t15.2023.10.20 val PER: 0.1846
2025-12-08 23:56:32,760: t15.2023.10.22 val PER: 0.1002
2025-12-08 23:56:32,760: t15.2023.11.03 val PER: 0.1730
2025-12-08 23:56:32,760: t15.2023.11.04 val PER: 0.0239
2025-12-08 23:56:32,760: t15.2023.11.17 val PER: 0.0202
2025-12-08 23:56:32,760: t15.2023.11.19 val PER: 0.0439
2025-12-08 23:56:32,760: t15.2023.11.26 val PER: 0.0500
2025-12-08 23:56:32,760: t15.2023.12.03 val PER: 0.0494
2025-12-08 23:56:32,760: t15.2023.12.08 val PER: 0.0300
2025-12-08 23:56:32,760: t15.2023.12.10 val PER: 0.0315
2025-12-08 23:56:32,760: t15.2023.12.17 val PER: 0.1112
2025-12-08 23:56:32,760: t15.2023.12.29 val PER: 0.0796
2025-12-08 23:56:32,760: t15.2024.02.25 val PER: 0.0590
2025-12-08 23:56:32,760: t15.2024.03.03 val PER: 1.0000
2025-12-08 23:56:32,760: t15.2024.03.08 val PER: 0.1622
2025-12-08 23:56:32,760: t15.2024.03.15 val PER: 0.1707
2025-12-08 23:56:32,761: t15.2024.03.17 val PER: 0.0669
2025-12-08 23:56:32,761: t15.2024.04.25 val PER: 1.0000
2025-12-08 23:56:32,761: t15.2024.04.28 val PER: 1.0000
2025-12-08 23:56:32,761: t15.2024.05.10 val PER: 0.1085
2025-12-08 23:56:32,761: t15.2024.06.14 val PER: 0.1041
2025-12-08 23:56:32,761: t15.2024.07.19 val PER: 0.1490
2025-12-08 23:56:32,761: t15.2024.07.21 val PER: 0.0669
2025-12-08 23:56:32,761: t15.2024.07.28 val PER: 0.0610
2025-12-08 23:56:32,761: t15.2025.01.10 val PER: 0.2479
2025-12-08 23:56:32,761: t15.2025.01.12 val PER: 0.0570
2025-12-08 23:56:32,761: t15.2025.03.14 val PER: 0.2574
2025-12-08 23:56:32,761: t15.2025.03.16 val PER: 0.1021
2025-12-08 23:56:32,761: t15.2025.03.30 val PER: 0.2172
2025-12-08 23:56:32,761: t15.2025.04.13 val PER: 0.1740
2025-12-08 23:56:32,761: New best test PER 0.1017 --> 0.1017
2025-12-08 23:56:32,761: Checkpointing model
2025-12-08 23:56:33,788: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-08 23:57:03,980: Train batch 106200: loss: 0.00 grad norm: 0.00 time: 0.093
2025-12-08 23:57:32,950: Train batch 106400: loss: 0.00 grad norm: 0.00 time: 0.077
2025-12-08 23:58:02,276: Train batch 106600: loss: 0.00 grad norm: 0.00 time: 0.113
2025-12-08 23:58:32,332: Train batch 106800: loss: 0.00 grad norm: 0.00 time: 0.141
2025-12-08 23:59:01,731: Train batch 107000: loss: 0.00 grad norm: 0.00 time: 0.126
2025-12-08 23:59:31,732: Train batch 107200: loss: 0.00 grad norm: 0.00 time: 0.125
2025-12-09 00:00:01,562: Train batch 107400: loss: 0.00 grad norm: 0.00 time: 0.154
2025-12-09 00:00:30,524: Train batch 107600: loss: 0.00 grad norm: 0.00 time: 0.113
2025-12-09 00:00:59,343: Train batch 107800: loss: 0.00 grad norm: 0.00 time: 0.101
2025-12-09 00:01:28,872: Train batch 108000: loss: 0.00 grad norm: 0.00 time: 0.121
2025-12-09 00:01:28,873: Running test after training batch: 108000
2025-12-09 00:01:39,078: Val batch 108000: PER (avg): 0.0999 CTC Loss (avg): 46.7499 time: 10.205
2025-12-09 00:01:39,079: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:01:39,079: t15.2023.08.13 val PER: 0.0696
2025-12-09 00:01:39,079: t15.2023.08.18 val PER: 0.0629
2025-12-09 00:01:39,079: t15.2023.08.20 val PER: 0.0508
2025-12-09 00:01:39,079: t15.2023.08.25 val PER: 0.0798
2025-12-09 00:01:39,079: t15.2023.08.27 val PER: 0.1158
2025-12-09 00:01:39,079: t15.2023.09.01 val PER: 0.0414
2025-12-09 00:01:39,079: t15.2023.09.03 val PER: 0.1057
2025-12-09 00:01:39,079: t15.2023.09.24 val PER: 0.0752
2025-12-09 00:01:39,079: t15.2023.09.29 val PER: 0.1142
2025-12-09 00:01:39,079: t15.2023.10.01 val PER: 0.1262
2025-12-09 00:01:39,079: t15.2023.10.06 val PER: 0.0667
2025-12-09 00:01:39,079: t15.2023.10.08 val PER: 0.1705
2025-12-09 00:01:39,079: t15.2023.10.13 val PER: 0.1443
2025-12-09 00:01:39,079: t15.2023.10.15 val PER: 0.1121
2025-12-09 00:01:39,079: t15.2023.10.20 val PER: 0.1846
2025-12-09 00:01:39,080: t15.2023.10.22 val PER: 0.1002
2025-12-09 00:01:39,080: t15.2023.11.03 val PER: 0.1716
2025-12-09 00:01:39,080: t15.2023.11.04 val PER: 0.0239
2025-12-09 00:01:39,080: t15.2023.11.17 val PER: 0.0202
2025-12-09 00:01:39,080: t15.2023.11.19 val PER: 0.0399
2025-12-09 00:01:39,080: t15.2023.11.26 val PER: 0.0464
2025-12-09 00:01:39,080: t15.2023.12.03 val PER: 0.0452
2025-12-09 00:01:39,080: t15.2023.12.08 val PER: 0.0273
2025-12-09 00:01:39,080: t15.2023.12.10 val PER: 0.0276
2025-12-09 00:01:39,080: t15.2023.12.17 val PER: 0.1071
2025-12-09 00:01:39,080: t15.2023.12.29 val PER: 0.0762
2025-12-09 00:01:39,080: t15.2024.02.25 val PER: 0.0590
2025-12-09 00:01:39,080: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:01:39,080: t15.2024.03.08 val PER: 0.1707
2025-12-09 00:01:39,080: t15.2024.03.15 val PER: 0.1695
2025-12-09 00:01:39,080: t15.2024.03.17 val PER: 0.0642
2025-12-09 00:01:39,080: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:01:39,081: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:01:39,081: t15.2024.05.10 val PER: 0.1040
2025-12-09 00:01:39,081: t15.2024.06.14 val PER: 0.1041
2025-12-09 00:01:39,081: t15.2024.07.19 val PER: 0.1463
2025-12-09 00:01:39,081: t15.2024.07.21 val PER: 0.0628
2025-12-09 00:01:39,081: t15.2024.07.28 val PER: 0.0596
2025-12-09 00:01:39,081: t15.2025.01.10 val PER: 0.2521
2025-12-09 00:01:39,081: t15.2025.01.12 val PER: 0.0608
2025-12-09 00:01:39,081: t15.2025.03.14 val PER: 0.2559
2025-12-09 00:01:39,081: t15.2025.03.16 val PER: 0.0969
2025-12-09 00:01:39,081: t15.2025.03.30 val PER: 0.2115
2025-12-09 00:01:39,081: t15.2025.04.13 val PER: 0.1698
2025-12-09 00:01:39,081: New best test PER 0.1017 --> 0.0999
2025-12-09 00:01:39,081: Checkpointing model
2025-12-09 00:01:40,194: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-09 00:02:09,329: Train batch 108200: loss: 0.00 grad norm: 0.00 time: 0.141
2025-12-09 00:02:38,550: Train batch 108400: loss: 0.00 grad norm: 0.00 time: 0.084
2025-12-09 00:03:07,957: Train batch 108600: loss: 0.00 grad norm: 0.02 time: 0.152
2025-12-09 00:03:38,068: Train batch 108800: loss: 0.00 grad norm: 0.00 time: 0.128
2025-12-09 00:04:06,932: Train batch 109000: loss: 0.00 grad norm: 0.00 time: 0.120
2025-12-09 00:04:36,714: Train batch 109200: loss: 0.00 grad norm: 0.00 time: 0.090
2025-12-09 00:05:06,547: Train batch 109400: loss: 0.00 grad norm: 0.00 time: 0.172
2025-12-09 00:05:35,588: Train batch 109600: loss: 0.00 grad norm: 0.00 time: 0.158
2025-12-09 00:06:05,430: Train batch 109800: loss: 0.00 grad norm: 0.00 time: 0.152
2025-12-09 00:06:35,200: Train batch 110000: loss: 0.00 grad norm: 0.00 time: 0.105
2025-12-09 00:06:35,200: Running test after training batch: 110000
2025-12-09 00:06:45,300: Val batch 110000: PER (avg): 0.0997 CTC Loss (avg): 45.8226 time: 10.100
2025-12-09 00:06:45,301: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:06:45,301: t15.2023.08.13 val PER: 0.0686
2025-12-09 00:06:45,301: t15.2023.08.18 val PER: 0.0620
2025-12-09 00:06:45,301: t15.2023.08.20 val PER: 0.0477
2025-12-09 00:06:45,301: t15.2023.08.25 val PER: 0.0768
2025-12-09 00:06:45,301: t15.2023.08.27 val PER: 0.1238
2025-12-09 00:06:45,301: t15.2023.09.01 val PER: 0.0406
2025-12-09 00:06:45,301: t15.2023.09.03 val PER: 0.1069
2025-12-09 00:06:45,301: t15.2023.09.24 val PER: 0.0789
2025-12-09 00:06:45,301: t15.2023.09.29 val PER: 0.1123
2025-12-09 00:06:45,301: t15.2023.10.01 val PER: 0.1222
2025-12-09 00:06:45,301: t15.2023.10.06 val PER: 0.0710
2025-12-09 00:06:45,301: t15.2023.10.08 val PER: 0.1651
2025-12-09 00:06:45,301: t15.2023.10.13 val PER: 0.1458
2025-12-09 00:06:45,302: t15.2023.10.15 val PER: 0.1127
2025-12-09 00:06:45,302: t15.2023.10.20 val PER: 0.1846
2025-12-09 00:06:45,302: t15.2023.10.22 val PER: 0.0969
2025-12-09 00:06:45,302: t15.2023.11.03 val PER: 0.1662
2025-12-09 00:06:45,302: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:06:45,302: t15.2023.11.17 val PER: 0.0187
2025-12-09 00:06:45,302: t15.2023.11.19 val PER: 0.0399
2025-12-09 00:06:45,302: t15.2023.11.26 val PER: 0.0464
2025-12-09 00:06:45,302: t15.2023.12.03 val PER: 0.0441
2025-12-09 00:06:45,302: t15.2023.12.08 val PER: 0.0273
2025-12-09 00:06:45,302: t15.2023.12.10 val PER: 0.0276
2025-12-09 00:06:45,302: t15.2023.12.17 val PER: 0.1123
2025-12-09 00:06:45,302: t15.2023.12.29 val PER: 0.0776
2025-12-09 00:06:45,302: t15.2024.02.25 val PER: 0.0548
2025-12-09 00:06:45,302: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:06:45,302: t15.2024.03.08 val PER: 0.1721
2025-12-09 00:06:45,302: t15.2024.03.15 val PER: 0.1682
2025-12-09 00:06:45,302: t15.2024.03.17 val PER: 0.0649
2025-12-09 00:06:45,303: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:06:45,303: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:06:45,303: t15.2024.05.10 val PER: 0.1055
2025-12-09 00:06:45,303: t15.2024.06.14 val PER: 0.1041
2025-12-09 00:06:45,303: t15.2024.07.19 val PER: 0.1437
2025-12-09 00:06:45,303: t15.2024.07.21 val PER: 0.0669
2025-12-09 00:06:45,303: t15.2024.07.28 val PER: 0.0596
2025-12-09 00:06:45,303: t15.2025.01.10 val PER: 0.2534
2025-12-09 00:06:45,303: t15.2025.01.12 val PER: 0.0624
2025-12-09 00:06:45,303: t15.2025.03.14 val PER: 0.2589
2025-12-09 00:06:45,303: t15.2025.03.16 val PER: 0.0942
2025-12-09 00:06:45,303: t15.2025.03.30 val PER: 0.2149
2025-12-09 00:06:45,303: t15.2025.04.13 val PER: 0.1655
2025-12-09 00:06:45,303: New best test PER 0.0999 --> 0.0997
2025-12-09 00:06:45,303: Checkpointing model
2025-12-09 00:06:46,396: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-09 00:07:15,834: Train batch 110200: loss: 0.00 grad norm: 0.02 time: 0.120
2025-12-09 00:07:45,632: Train batch 110400: loss: 0.00 grad norm: 0.00 time: 0.127
2025-12-09 00:08:15,532: Train batch 110600: loss: 0.00 grad norm: 0.00 time: 0.086
2025-12-09 00:08:44,770: Train batch 110800: loss: 0.00 grad norm: 0.00 time: 0.122
2025-12-09 00:09:14,120: Train batch 111000: loss: 0.00 grad norm: 0.00 time: 0.105
2025-12-09 00:09:44,202: Train batch 111200: loss: 0.00 grad norm: 0.00 time: 0.129
2025-12-09 00:10:13,666: Train batch 111400: loss: 0.00 grad norm: 0.00 time: 0.133
2025-12-09 00:10:42,875: Train batch 111600: loss: 0.00 grad norm: 0.00 time: 0.139
2025-12-09 00:11:12,121: Train batch 111800: loss: 0.00 grad norm: 0.00 time: 0.095
2025-12-09 00:11:40,842: Train batch 112000: loss: 0.00 grad norm: 0.00 time: 0.087
2025-12-09 00:11:40,842: Running test after training batch: 112000
2025-12-09 00:11:51,065: Val batch 112000: PER (avg): 0.1005 CTC Loss (avg): 46.5469 time: 10.223
2025-12-09 00:11:51,066: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:11:51,066: t15.2023.08.13 val PER: 0.0696
2025-12-09 00:11:51,066: t15.2023.08.18 val PER: 0.0612
2025-12-09 00:11:51,066: t15.2023.08.20 val PER: 0.0485
2025-12-09 00:11:51,066: t15.2023.08.25 val PER: 0.0813
2025-12-09 00:11:51,066: t15.2023.08.27 val PER: 0.1222
2025-12-09 00:11:51,066: t15.2023.09.01 val PER: 0.0406
2025-12-09 00:11:51,066: t15.2023.09.03 val PER: 0.1069
2025-12-09 00:11:51,066: t15.2023.09.24 val PER: 0.0813
2025-12-09 00:11:51,066: t15.2023.09.29 val PER: 0.1123
2025-12-09 00:11:51,066: t15.2023.10.01 val PER: 0.1275
2025-12-09 00:11:51,066: t15.2023.10.06 val PER: 0.0700
2025-12-09 00:11:51,066: t15.2023.10.08 val PER: 0.1651
2025-12-09 00:11:51,066: t15.2023.10.13 val PER: 0.1466
2025-12-09 00:11:51,067: t15.2023.10.15 val PER: 0.1140
2025-12-09 00:11:51,067: t15.2023.10.20 val PER: 0.1812
2025-12-09 00:11:51,067: t15.2023.10.22 val PER: 0.1024
2025-12-09 00:11:51,067: t15.2023.11.03 val PER: 0.1682
2025-12-09 00:11:51,067: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:11:51,067: t15.2023.11.17 val PER: 0.0187
2025-12-09 00:11:51,067: t15.2023.11.19 val PER: 0.0399
2025-12-09 00:11:51,067: t15.2023.11.26 val PER: 0.0442
2025-12-09 00:11:51,067: t15.2023.12.03 val PER: 0.0441
2025-12-09 00:11:51,067: t15.2023.12.08 val PER: 0.0306
2025-12-09 00:11:51,067: t15.2023.12.10 val PER: 0.0329
2025-12-09 00:11:51,067: t15.2023.12.17 val PER: 0.1143
2025-12-09 00:11:51,067: t15.2023.12.29 val PER: 0.0796
2025-12-09 00:11:51,067: t15.2024.02.25 val PER: 0.0534
2025-12-09 00:11:51,067: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:11:51,067: t15.2024.03.08 val PER: 0.1679
2025-12-09 00:11:51,067: t15.2024.03.15 val PER: 0.1670
2025-12-09 00:11:51,068: t15.2024.03.17 val PER: 0.0662
2025-12-09 00:11:51,068: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:11:51,068: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:11:51,068: t15.2024.05.10 val PER: 0.1040
2025-12-09 00:11:51,068: t15.2024.06.14 val PER: 0.1073
2025-12-09 00:11:51,068: t15.2024.07.19 val PER: 0.1496
2025-12-09 00:11:51,068: t15.2024.07.21 val PER: 0.0634
2025-12-09 00:11:51,068: t15.2024.07.28 val PER: 0.0588
2025-12-09 00:11:51,068: t15.2025.01.10 val PER: 0.2548
2025-12-09 00:11:51,068: t15.2025.01.12 val PER: 0.0608
2025-12-09 00:11:51,068: t15.2025.03.14 val PER: 0.2559
2025-12-09 00:11:51,068: t15.2025.03.16 val PER: 0.0929
2025-12-09 00:11:51,068: t15.2025.03.30 val PER: 0.2161
2025-12-09 00:11:51,068: t15.2025.04.13 val PER: 0.1712
2025-12-09 00:12:20,118: Train batch 112200: loss: 0.00 grad norm: 0.00 time: 0.087
2025-12-09 00:12:49,764: Train batch 112400: loss: 0.00 grad norm: 0.00 time: 0.109
2025-12-09 00:13:19,001: Train batch 112600: loss: 0.00 grad norm: 0.00 time: 0.128
2025-12-09 00:13:48,941: Train batch 112800: loss: 0.00 grad norm: 0.00 time: 0.148
2025-12-09 00:14:18,261: Train batch 113000: loss: 0.00 grad norm: 0.00 time: 0.117
2025-12-09 00:14:48,040: Train batch 113200: loss: 0.00 grad norm: 0.00 time: 0.173
2025-12-09 00:15:17,278: Train batch 113400: loss: 0.00 grad norm: 0.00 time: 0.157
2025-12-09 00:15:46,523: Train batch 113600: loss: 0.00 grad norm: 0.00 time: 0.114
2025-12-09 00:16:16,394: Train batch 113800: loss: 0.00 grad norm: 0.00 time: 0.127
2025-12-09 00:16:45,801: Train batch 114000: loss: 0.00 grad norm: 0.00 time: 0.195
2025-12-09 00:16:45,802: Running test after training batch: 114000
2025-12-09 00:16:56,083: Val batch 114000: PER (avg): 0.1001 CTC Loss (avg): 46.8697 time: 10.281
2025-12-09 00:16:56,083: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:16:56,083: t15.2023.08.13 val PER: 0.0696
2025-12-09 00:16:56,083: t15.2023.08.18 val PER: 0.0629
2025-12-09 00:16:56,083: t15.2023.08.20 val PER: 0.0477
2025-12-09 00:16:56,083: t15.2023.08.25 val PER: 0.0798
2025-12-09 00:16:56,083: t15.2023.08.27 val PER: 0.1206
2025-12-09 00:16:56,084: t15.2023.09.01 val PER: 0.0414
2025-12-09 00:16:56,084: t15.2023.09.03 val PER: 0.1069
2025-12-09 00:16:56,084: t15.2023.09.24 val PER: 0.0813
2025-12-09 00:16:56,084: t15.2023.09.29 val PER: 0.1142
2025-12-09 00:16:56,084: t15.2023.10.01 val PER: 0.1275
2025-12-09 00:16:56,084: t15.2023.10.06 val PER: 0.0689
2025-12-09 00:16:56,084: t15.2023.10.08 val PER: 0.1624
2025-12-09 00:16:56,084: t15.2023.10.13 val PER: 0.1482
2025-12-09 00:16:56,084: t15.2023.10.15 val PER: 0.1127
2025-12-09 00:16:56,084: t15.2023.10.20 val PER: 0.1779
2025-12-09 00:16:56,084: t15.2023.10.22 val PER: 0.0947
2025-12-09 00:16:56,084: t15.2023.11.03 val PER: 0.1696
2025-12-09 00:16:56,084: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:16:56,084: t15.2023.11.17 val PER: 0.0171
2025-12-09 00:16:56,084: t15.2023.11.19 val PER: 0.0399
2025-12-09 00:16:56,084: t15.2023.11.26 val PER: 0.0420
2025-12-09 00:16:56,084: t15.2023.12.03 val PER: 0.0431
2025-12-09 00:16:56,084: t15.2023.12.08 val PER: 0.0293
2025-12-09 00:16:56,085: t15.2023.12.10 val PER: 0.0302
2025-12-09 00:16:56,085: t15.2023.12.17 val PER: 0.1143
2025-12-09 00:16:56,085: t15.2023.12.29 val PER: 0.0796
2025-12-09 00:16:56,085: t15.2024.02.25 val PER: 0.0534
2025-12-09 00:16:56,085: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:16:56,085: t15.2024.03.08 val PER: 0.1636
2025-12-09 00:16:56,085: t15.2024.03.15 val PER: 0.1676
2025-12-09 00:16:56,085: t15.2024.03.17 val PER: 0.0656
2025-12-09 00:16:56,085: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:16:56,085: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:16:56,085: t15.2024.05.10 val PER: 0.0996
2025-12-09 00:16:56,085: t15.2024.06.14 val PER: 0.1025
2025-12-09 00:16:56,085: t15.2024.07.19 val PER: 0.1477
2025-12-09 00:16:56,085: t15.2024.07.21 val PER: 0.0641
2025-12-09 00:16:56,085: t15.2024.07.28 val PER: 0.0625
2025-12-09 00:16:56,085: t15.2025.01.10 val PER: 0.2548
2025-12-09 00:16:56,085: t15.2025.01.12 val PER: 0.0577
2025-12-09 00:16:56,085: t15.2025.03.14 val PER: 0.2589
2025-12-09 00:16:56,085: t15.2025.03.16 val PER: 0.0969
2025-12-09 00:16:56,086: t15.2025.03.30 val PER: 0.2207
2025-12-09 00:16:56,086: t15.2025.04.13 val PER: 0.1740
2025-12-09 00:17:25,062: Train batch 114200: loss: 0.00 grad norm: 0.00 time: 0.089
2025-12-09 00:17:54,188: Train batch 114400: loss: 0.00 grad norm: 0.00 time: 0.180
2025-12-09 00:18:24,044: Train batch 114600: loss: 0.00 grad norm: 0.02 time: 0.143
2025-12-09 00:18:53,271: Train batch 114800: loss: 0.00 grad norm: 0.00 time: 0.128
2025-12-09 00:19:23,122: Train batch 115000: loss: 0.00 grad norm: 0.00 time: 0.096
2025-12-09 00:19:52,880: Train batch 115200: loss: 0.00 grad norm: 0.00 time: 0.143
2025-12-09 00:20:21,748: Train batch 115400: loss: 0.00 grad norm: 0.00 time: 0.120
2025-12-09 00:20:50,767: Train batch 115600: loss: 0.00 grad norm: 0.00 time: 0.100
2025-12-09 00:21:19,880: Train batch 115800: loss: 0.00 grad norm: 0.00 time: 0.135
2025-12-09 00:21:49,573: Train batch 116000: loss: 0.00 grad norm: 0.00 time: 0.136
2025-12-09 00:21:49,574: Running test after training batch: 116000
2025-12-09 00:21:59,891: Val batch 116000: PER (avg): 0.1003 CTC Loss (avg): 47.4561 time: 10.317
2025-12-09 00:21:59,891: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:21:59,891: t15.2023.08.13 val PER: 0.0707
2025-12-09 00:21:59,891: t15.2023.08.18 val PER: 0.0620
2025-12-09 00:21:59,891: t15.2023.08.20 val PER: 0.0469
2025-12-09 00:21:59,891: t15.2023.08.25 val PER: 0.0828
2025-12-09 00:21:59,891: t15.2023.08.27 val PER: 0.1238
2025-12-09 00:21:59,892: t15.2023.09.01 val PER: 0.0414
2025-12-09 00:21:59,892: t15.2023.09.03 val PER: 0.1081
2025-12-09 00:21:59,892: t15.2023.09.24 val PER: 0.0789
2025-12-09 00:21:59,892: t15.2023.09.29 val PER: 0.1123
2025-12-09 00:21:59,892: t15.2023.10.01 val PER: 0.1242
2025-12-09 00:21:59,892: t15.2023.10.06 val PER: 0.0710
2025-12-09 00:21:59,892: t15.2023.10.08 val PER: 0.1583
2025-12-09 00:21:59,892: t15.2023.10.13 val PER: 0.1458
2025-12-09 00:21:59,892: t15.2023.10.15 val PER: 0.1121
2025-12-09 00:21:59,892: t15.2023.10.20 val PER: 0.1879
2025-12-09 00:21:59,892: t15.2023.10.22 val PER: 0.0991
2025-12-09 00:21:59,892: t15.2023.11.03 val PER: 0.1730
2025-12-09 00:21:59,892: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:21:59,892: t15.2023.11.17 val PER: 0.0171
2025-12-09 00:21:59,892: t15.2023.11.19 val PER: 0.0419
2025-12-09 00:21:59,892: t15.2023.11.26 val PER: 0.0428
2025-12-09 00:21:59,893: t15.2023.12.03 val PER: 0.0399
2025-12-09 00:21:59,893: t15.2023.12.08 val PER: 0.0306
2025-12-09 00:21:59,893: t15.2023.12.10 val PER: 0.0289
2025-12-09 00:21:59,893: t15.2023.12.17 val PER: 0.1154
2025-12-09 00:21:59,893: t15.2023.12.29 val PER: 0.0782
2025-12-09 00:21:59,893: t15.2024.02.25 val PER: 0.0590
2025-12-09 00:21:59,893: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:21:59,893: t15.2024.03.08 val PER: 0.1636
2025-12-09 00:21:59,893: t15.2024.03.15 val PER: 0.1701
2025-12-09 00:21:59,893: t15.2024.03.17 val PER: 0.0649
2025-12-09 00:21:59,893: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:21:59,893: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:21:59,893: t15.2024.05.10 val PER: 0.1040
2025-12-09 00:21:59,893: t15.2024.06.14 val PER: 0.1057
2025-12-09 00:21:59,893: t15.2024.07.19 val PER: 0.1457
2025-12-09 00:21:59,893: t15.2024.07.21 val PER: 0.0628
2025-12-09 00:21:59,893: t15.2024.07.28 val PER: 0.0610
2025-12-09 00:21:59,894: t15.2025.01.10 val PER: 0.2603
2025-12-09 00:21:59,894: t15.2025.01.12 val PER: 0.0570
2025-12-09 00:21:59,894: t15.2025.03.14 val PER: 0.2589
2025-12-09 00:21:59,894: t15.2025.03.16 val PER: 0.0982
2025-12-09 00:21:59,894: t15.2025.03.30 val PER: 0.2184
2025-12-09 00:21:59,894: t15.2025.04.13 val PER: 0.1740
2025-12-09 00:22:28,895: Train batch 116200: loss: 0.00 grad norm: 0.00 time: 0.145
2025-12-09 00:22:57,929: Train batch 1164
grad norm: 0.00 time: 0.125
2025-12-09 00:23:56,254: Train batch 116800: loss: 0.00 grad norm: 0.00 time: 0.119
2025-12-09 00:24:25,852: Train batch 117000: loss: 0.00 grad norm: 0.00 time: 0.115
2025-12-09 00:24:54,962: Train batch 117200: loss: 0.00 grad norm: 0.00 time: 0.117
2025-12-09 00:25:24,191: Train batch 117400: loss: 0.00 grad norm: 0.00 time: 0.114
2025-12-09 00:25:53,115: Train batch 117600: loss: 0.00 grad norm: 0.00 time: 0.143
2025-12-09 00:26:21,924: Train batch 117800: loss: 0.00 grad norm: 0.05 time: 0.127
2025-12-09 00:26:51,882: Train batch 118000: loss: 0.00 grad norm: 0.00 time: 0.152
2025-12-09 00:26:51,882: Running test after training batch: 118000
2025-12-09 00:27:02,039: Val batch 118000: PER (avg): 0.0995 CTC Loss (avg): 47.4462 time: 10.156
2025-12-09 00:27:02,039: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:27:02,039: t15.2023.08.13 val PER: 0.0696
2025-12-09 00:27:02,039: t15.2023.08.18 val PER: 0.0595
2025-12-09 00:27:02,039: t15.2023.08.20 val PER: 0.0461
2025-12-09 00:27:02,039: t15.2023.08.25 val PER: 0.0768
2025-12-09 00:27:02,039: t15.2023.08.27 val PER: 0.1238
2025-12-09 00:27:02,040: t15.2023.09.01 val PER: 0.0414
2025-12-09 00:27:02,040: t15.2023.09.03 val PER: 0.1069
2025-12-09 00:27:02,040: t15.2023.09.24 val PER: 0.0813
2025-12-09 00:27:02,040: t15.2023.09.29 val PER: 0.1123
2025-12-09 00:27:02,040: t15.2023.10.01 val PER: 0.1229
2025-12-09 00:27:02,040: t15.2023.10.06 val PER: 0.0657
2025-12-09 00:27:02,040: t15.2023.10.08 val PER: 0.1691
2025-12-09 00:27:02,040: t15.2023.10.13 val PER: 0.1427
2025-12-09 00:27:02,040: t15.2023.10.15 val PER: 0.1127
2025-12-09 00:27:02,040: t15.2023.10.20 val PER: 0.1678
2025-12-09 00:27:02,040: t15.2023.10.22 val PER: 0.1047
2025-12-09 00:27:02,040: t15.2023.11.03 val PER: 0.1696
2025-12-09 00:27:02,040: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:27:02,040: t15.2023.11.17 val PER: 0.0202
2025-12-09 00:27:02,040: t15.2023.11.19 val PER: 0.0439
2025-12-09 00:27:02,040: t15.2023.11.26 val PER: 0.0428
2025-12-09 00:27:02,041: t15.2023.12.03 val PER: 0.0399
2025-12-09 00:27:02,041: t15.2023.12.08 val PER: 0.0306
2025-12-09 00:27:02,041: t15.2023.12.10 val PER: 0.0329
2025-12-09 00:27:02,041: t15.2023.12.17 val PER: 0.1123
2025-12-09 00:27:02,041: t15.2023.12.29 val PER: 0.0789
2025-12-09 00:27:02,041: t15.2024.02.25 val PER: 0.0576
2025-12-09 00:27:02,041: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:27:02,041: t15.2024.03.08 val PER: 0.1650
2025-12-09 00:27:02,041: t15.2024.03.15 val PER: 0.1676
2025-12-09 00:27:02,041: t15.2024.03.17 val PER: 0.0669
2025-12-09 00:27:02,041: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:27:02,041: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:27:02,041: t15.2024.05.10 val PER: 0.0966
2025-12-09 00:27:02,041: t15.2024.06.14 val PER: 0.1025
2025-12-09 00:27:02,041: t15.2024.07.19 val PER: 0.1483
2025-12-09 00:27:02,041: t15.2024.07.21 val PER: 0.0621
2025-12-09 00:27:02,042: t15.2024.07.28 val PER: 0.0618
2025-12-09 00:27:02,042: t15.2025.01.10 val PER: 0.2521
2025-12-09 00:27:02,042: t15.2025.01.12 val PER: 0.0577
2025-12-09 00:27:02,042: t15.2025.03.14 val PER: 0.2500
2025-12-09 00:27:02,042: t15.2025.03.16 val PER: 0.0969
2025-12-09 00:27:02,042: t15.2025.03.30 val PER: 0.2161
2025-12-09 00:27:02,042: t15.2025.04.13 val PER: 0.1712
2025-12-09 00:27:02,042: New best test PER 0.0997 --> 0.0995
2025-12-09 00:27:02,042: Checkpointing model
2025-12-09 00:27:03,094: Saved model to checkpoint: trained_models/baseline_conformer_v1_20251208_190614/checkpoint/best_checkpoint
2025-12-09 00:27:31,286: Train batch 118200: loss: 0.00 grad norm: 0.00 time: 0.093
2025-12-09 00:28:00,296: Train batch 118400: loss: 0.00 grad norm: 0.00 time: 0.116
2025-12-09 00:28:29,516: Train batch 118600: loss: 0.00 grad norm: 0.03 time: 0.153
2025-12-09 00:28:58,987: Train batch 118800: loss: 0.00 grad norm: 0.00 time: 0.160
2025-12-09 00:29:28,543: Train batch 119000: loss: 0.00 grad norm: 0.00 time: 0.118
2025-12-09 00:29:57,899: Train batch 119200: loss: 0.00 grad norm: 0.00 time: 0.151
2025-12-09 00:30:27,194: Train batch 119400: loss: 0.00 grad norm: 0.00 time: 0.110
2025-12-09 00:30:56,442: Train batch 119600: loss: 0.00 grad norm: 0.00 time: 0.098
2025-12-09 00:31:25,900: Train batch 119800: loss: 0.00 grad norm: 0.00 time: 0.114
2025-12-09 00:31:55,601: Running test after training batch: 119999
2025-12-09 00:32:05,362: Val batch 119999: PER (avg): 0.0996 CTC Loss (avg): 47.7139 time: 9.761
2025-12-09 00:32:05,363: t15.2023.08.11 val PER: 1.0000
2025-12-09 00:32:05,363: t15.2023.08.13 val PER: 0.0707
2025-12-09 00:32:05,363: t15.2023.08.18 val PER: 0.0612
2025-12-09 00:32:05,363: t15.2023.08.20 val PER: 0.0469
2025-12-09 00:32:05,363: t15.2023.08.25 val PER: 0.0768
2025-12-09 00:32:05,363: t15.2023.08.27 val PER: 0.1206
2025-12-09 00:32:05,363: t15.2023.09.01 val PER: 0.0398
2025-12-09 00:32:05,363: t15.2023.09.03 val PER: 0.1081
2025-12-09 00:32:05,363: t15.2023.09.24 val PER: 0.0813
2025-12-09 00:32:05,363: t15.2023.09.29 val PER: 0.1123
2025-12-09 00:32:05,363: t15.2023.10.01 val PER: 0.1242
2025-12-09 00:32:05,363: t15.2023.10.06 val PER: 0.0646
2025-12-09 00:32:05,363: t15.2023.10.08 val PER: 0.1651
2025-12-09 00:32:05,363: t15.2023.10.13 val PER: 0.1474
2025-12-09 00:32:05,363: t15.2023.10.15 val PER: 0.1127
2025-12-09 00:32:05,363: t15.2023.10.20 val PER: 0.1812
2025-12-09 00:32:05,364: t15.2023.10.22 val PER: 0.1036
2025-12-09 00:32:05,364: t15.2023.11.03 val PER: 0.1682
2025-12-09 00:32:05,364: t15.2023.11.04 val PER: 0.0205
2025-12-09 00:32:05,364: t15.2023.11.17 val PER: 0.0202
2025-12-09 00:32:05,364: t15.2023.11.19 val PER: 0.0399
2025-12-09 00:32:05,364: t15.2023.11.26 val PER: 0.0442
2025-12-09 00:32:05,364: t15.2023.12.03 val PER: 0.0431
2025-12-09 00:32:05,364: t15.2023.12.08 val PER: 0.0306
2025-12-09 00:32:05,364: t15.2023.12.10 val PER: 0.0342
2025-12-09 00:32:05,364: t15.2023.12.17 val PER: 0.1133
2025-12-09 00:32:05,364: t15.2023.12.29 val PER: 0.0776
2025-12-09 00:32:05,364: t15.2024.02.25 val PER: 0.0548
2025-12-09 00:32:05,364: t15.2024.03.03 val PER: 1.0000
2025-12-09 00:32:05,364: t15.2024.03.08 val PER: 0.1607
2025-12-09 00:32:05,364: t15.2024.03.15 val PER: 0.1645
2025-12-09 00:32:05,364: t15.2024.03.17 val PER: 0.0656
2025-12-09 00:32:05,364: t15.2024.04.25 val PER: 1.0000
2025-12-09 00:32:05,364: t15.2024.04.28 val PER: 1.0000
2025-12-09 00:32:05,364: t15.2024.05.10 val PER: 0.0966
2025-12-09 00:32:05,365: t15.2024.06.14 val PER: 0.1057
2025-12-09 00:32:05,365: t15.2024.07.19 val PER: 0.1477
2025-12-09 00:32:05,365: t15.2024.07.21 val PER: 0.0641
2025-12-09 00:32:05,365: t15.2024.07.28 val PER: 0.0625
2025-12-09 00:32:05,365: t15.2025.01.10 val PER: 0.2534
2025-12-09 00:32:05,365: t15.2025.01.12 val PER: 0.0593
2025-12-09 00:32:05,365: t15.2025.03.14 val PER: 0.2500
2025-12-09 00:32:05,365: t15.2025.03.16 val PER: 0.0969
2025-12-09 00:32:05,365: t15.2025.03.30 val PER: 0.2172
2025-12-09 00:32:05,365: t15.2025.04.13 val PER: 0.1683
2025-12-09 00:32:05,445: Best avg val PER achieved: 0.09954
2025-12-09 00:32:05,445: Total training time: 325.69 minutes
