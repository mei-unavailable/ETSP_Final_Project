2025-12-11 17:06:34,547: Using device: cuda:0
2025-12-11 17:06:34,549: Initializing Conformer with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-11 17:06:34,996: Initialized RNN decoding model
2025-12-11 17:06:34,996: UNetEnhancedModel(
  (unet): NeuralUNet(
    (inc): DoubleConv(
      (double_conv): Sequential(
        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (down1): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (outc): OutConv(
      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (decoder): ConformerDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.4, inplace=False)
    (projection): Linear(in_features=7168, out_features=512, bias=True)
    (conformer): Conformer(
      (conformer_layers): ModuleList(
        (0-5): 6 x ConformerLayer(
          (ffn1): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_dropout): Dropout(p=0.4, inplace=False)
          (conv_module): _ConvolutionModule(
            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (sequential): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
              (1): GLU(dim=1)
              (2): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (4): SiLU()
              (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (6): Dropout(p=0.4, inplace=False)
            )
          )
          (ffn2): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (out): Linear(in_features=512, out_features=41, bias=True)
  )
)
2025-12-11 17:06:35,000: Model has 69,136,170 parameters
2025-12-11 17:06:35,000: Model has 11,819,520 day-specific parameters | 17.10% of total parameters
2025-12-11 17:06:43,436: Successfully initialized datasets
2025-12-11 17:06:45,184: Train batch 0: loss: 679.32 grad norm: 1290.63 time: 1.211
2025-12-11 17:06:45,184: Running test after training batch: 0
2025-12-11 17:07:13,618: Val batch 0: PER (avg): 2.8689 CTC Loss (avg): 655.8417 time: 28.433
2025-12-11 17:07:13,618: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:07:13,618: t15.2023.08.13 val PER: 2.4356
2025-12-11 17:07:13,618: t15.2023.08.18 val PER: 2.6580
2025-12-11 17:07:13,618: t15.2023.08.20 val PER: 2.4345
2025-12-11 17:07:13,618: t15.2023.08.25 val PER: 2.7816
2025-12-11 17:07:13,618: t15.2023.08.27 val PER: 2.3424
2025-12-11 17:07:13,618: t15.2023.09.01 val PER: 2.6948
2025-12-11 17:07:13,618: t15.2023.09.03 val PER: 2.6686
2025-12-11 17:07:13,618: t15.2023.09.24 val PER: 3.1189
2025-12-11 17:07:13,618: t15.2023.09.29 val PER: 3.0868
2025-12-11 17:07:13,618: t15.2023.10.01 val PER: 2.3329
2025-12-11 17:07:13,619: t15.2023.10.06 val PER: 3.1582
2025-12-11 17:07:13,619: t15.2023.10.08 val PER: 2.4344
2025-12-11 17:07:13,619: t15.2023.10.13 val PER: 2.3375
2025-12-11 17:07:13,619: t15.2023.10.15 val PER: 2.6935
2025-12-11 17:07:13,619: t15.2023.10.20 val PER: 2.9295
2025-12-11 17:07:13,619: t15.2023.10.22 val PER: 2.3686
2025-12-11 17:07:13,619: t15.2023.11.03 val PER: 3.4858
2025-12-11 17:07:13,619: t15.2023.11.04 val PER: 3.8601
2025-12-11 17:07:13,619: t15.2023.11.17 val PER: 4.1462
2025-12-11 17:07:13,619: t15.2023.11.19 val PER: 3.8942
2025-12-11 17:07:13,619: t15.2023.11.26 val PER: 2.9928
2025-12-11 17:07:13,619: t15.2023.12.03 val PER: 2.9118
2025-12-11 17:07:13,619: t15.2023.12.08 val PER: 2.9407
2025-12-11 17:07:13,619: t15.2023.12.10 val PER: 3.2694
2025-12-11 17:07:13,619: t15.2023.12.17 val PER: 2.6445
2025-12-11 17:07:13,619: t15.2023.12.29 val PER: 3.0865
2025-12-11 17:07:13,619: t15.2024.02.25 val PER: 2.7444
2025-12-11 17:07:13,619: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:07:13,619: t15.2024.03.08 val PER: 2.8947
2025-12-11 17:07:13,620: t15.2024.03.15 val PER: 2.5266
2025-12-11 17:07:13,620: t15.2024.03.17 val PER: 2.8194
2025-12-11 17:07:13,620: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:07:13,620: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:07:13,620: t15.2024.05.10 val PER: 2.8053
2025-12-11 17:07:13,620: t15.2024.06.14 val PER: 3.0915
2025-12-11 17:07:13,620: t15.2024.07.19 val PER: 2.0145
2025-12-11 17:07:13,620: t15.2024.07.21 val PER: 3.2186
2025-12-11 17:07:13,620: t15.2024.07.28 val PER: 3.3743
2025-12-11 17:07:13,620: t15.2025.01.10 val PER: 2.2920
2025-12-11 17:07:13,620: t15.2025.01.12 val PER: 3.6012
2025-12-11 17:07:13,620: t15.2025.03.14 val PER: 1.9734
2025-12-11 17:07:13,620: t15.2025.03.16 val PER: 3.8874
2025-12-11 17:07:13,620: t15.2025.03.30 val PER: 2.8575
2025-12-11 17:07:13,620: t15.2025.04.13 val PER: 3.4650
2025-12-11 17:07:13,620: New best test PER inf --> 2.8689
2025-12-11 17:07:13,620: Checkpointing model
2025-12-11 17:07:14,096: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251211_170634/checkpoint/best_checkpoint
2025-12-11 17:08:03,027: Train batch 200: loss: 84.89 grad norm: 145.64 time: 0.207
2025-12-11 17:08:50,156: Train batch 400: loss: 68.49 grad norm: 78.12 time: 0.223
2025-12-11 17:09:38,079: Train batch 600: loss: 67.61 grad norm: 67.87 time: 0.210
2025-12-11 17:10:25,019: Train batch 800: loss: 68.22 grad norm: 69.21 time: 0.306
2025-12-11 17:11:12,084: Train batch 1000: loss: 78.09 grad norm: 131.35 time: 0.266
2025-12-11 17:11:58,749: Train batch 1200: loss: 41.76 grad norm: 240.45 time: 0.174
2025-12-11 17:12:46,164: Train batch 1400: loss: 72.83 grad norm: 148.94 time: 0.214
2025-12-11 17:13:34,052: Train batch 1600: loss: 63.40 grad norm: 105.76 time: 0.196
2025-12-11 17:14:21,340: Train batch 1800: loss: 56.51 grad norm: 427.74 time: 0.197
2025-12-11 17:15:08,200: Train batch 2000: loss: 65.07 grad norm: 809.52 time: 0.226
2025-12-11 17:15:08,200: Running test after training batch: 2000
2025-12-11 17:15:34,689: Val batch 2000: PER (avg): 0.7292 CTC Loss (avg): 90.3879 time: 26.489
2025-12-11 17:15:34,689: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:15:34,689: t15.2023.08.13 val PER: 0.7391
2025-12-11 17:15:34,689: t15.2023.08.18 val PER: 0.7334
2025-12-11 17:15:34,689: t15.2023.08.20 val PER: 0.7117
2025-12-11 17:15:34,689: t15.2023.08.25 val PER: 0.7139
2025-12-11 17:15:34,689: t15.2023.08.27 val PER: 0.7540
2025-12-11 17:15:34,689: t15.2023.09.01 val PER: 0.6964
2025-12-11 17:15:34,689: t15.2023.09.03 val PER: 0.7304
2025-12-11 17:15:34,690: t15.2023.09.24 val PER: 0.6869
2025-12-11 17:15:34,690: t15.2023.09.29 val PER: 0.7128
2025-12-11 17:15:34,690: t15.2023.10.01 val PER: 0.7332
2025-12-11 17:15:34,690: t15.2023.10.06 val PER: 0.6954
2025-12-11 17:15:34,690: t15.2023.10.08 val PER: 0.7578
2025-12-11 17:15:34,690: t15.2023.10.13 val PER: 0.7308
2025-12-11 17:15:34,690: t15.2023.10.15 val PER: 0.7271
2025-12-11 17:15:34,690: t15.2023.10.20 val PER: 0.7081
2025-12-11 17:15:34,690: t15.2023.10.22 val PER: 0.7071
2025-12-11 17:15:34,690: t15.2023.11.03 val PER: 0.7191
2025-12-11 17:15:34,690: t15.2023.11.04 val PER: 0.6792
2025-12-11 17:15:34,690: t15.2023.11.17 val PER: 0.6672
2025-12-11 17:15:34,690: t15.2023.11.19 val PER: 0.6966
2025-12-11 17:15:34,690: t15.2023.11.26 val PER: 0.7326
2025-12-11 17:15:34,690: t15.2023.12.03 val PER: 0.7143
2025-12-11 17:15:34,690: t15.2023.12.08 val PER: 0.7084
2025-12-11 17:15:34,690: t15.2023.12.10 val PER: 0.6886
2025-12-11 17:15:34,690: t15.2023.12.17 val PER: 0.7588
2025-12-11 17:15:34,690: t15.2023.12.29 val PER: 0.7515
2025-12-11 17:15:34,691: t15.2024.02.25 val PER: 0.7177
2025-12-11 17:15:34,691: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:15:34,691: t15.2024.03.08 val PER: 0.7624
2025-12-11 17:15:34,691: t15.2024.03.15 val PER: 0.7430
2025-12-11 17:15:34,691: t15.2024.03.17 val PER: 0.7287
2025-12-11 17:15:34,691: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:15:34,691: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:15:34,691: t15.2024.05.10 val PER: 0.7132
2025-12-11 17:15:34,691: t15.2024.06.14 val PER: 0.6987
2025-12-11 17:15:34,691: t15.2024.07.19 val PER: 0.7805
2025-12-11 17:15:34,691: t15.2024.07.21 val PER: 0.7241
2025-12-11 17:15:34,691: t15.2024.07.28 val PER: 0.7140
2025-12-11 17:15:34,691: t15.2025.01.10 val PER: 0.8196
2025-12-11 17:15:34,691: t15.2025.01.12 val PER: 0.7352
2025-12-11 17:15:34,691: t15.2025.03.14 val PER: 0.8003
2025-12-11 17:15:34,691: t15.2025.03.16 val PER: 0.7199
2025-12-11 17:15:34,691: t15.2025.03.30 val PER: 0.7816
2025-12-11 17:15:34,691: t15.2025.04.13 val PER: 0.7518
2025-12-11 17:15:34,691: New best test PER 2.8689 --> 0.7292
2025-12-11 17:15:34,691: Checkpointing model
2025-12-11 17:15:35,851: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251211_170634/checkpoint/best_checkpoint
2025-12-11 17:16:23,580: Train batch 2200: loss: 90.20 grad norm: 314.44 time: 0.246
2025-12-11 17:17:10,587: Train batch 2400: loss: 88.52 grad norm: 428.81 time: 0.233
2025-12-11 17:17:56,989: Train batch 2600: loss: 82.41 grad norm: 368.00 time: 0.283
2025-12-11 17:18:43,815: Train batch 2800: loss: 74.96 grad norm: 1321.23 time: 0.210
2025-12-11 17:19:31,011: Train batch 3000: loss: 88.16 grad norm: 5607.08 time: 0.226
2025-12-11 17:20:17,775: Train batch 3200: loss: 85.91 grad norm: 77048.48 time: 0.276
2025-12-11 17:21:03,135: Train batch 3400: loss: 73.84 grad norm: 19389.69 time: 0.167
2025-12-11 17:21:49,400: Train batch 3600: loss: 67.63 grad norm: 9106.54 time: 0.254
2025-12-11 17:22:35,763: Train batch 3800: loss: 84.88 grad norm: 33036.15 time: 0.265
2025-12-11 17:23:23,174: Train batch 4000: loss: 67.47 grad norm: 831.97 time: 0.193
2025-12-11 17:23:23,174: Running test after training batch: 4000
2025-12-11 17:23:49,747: Val batch 4000: PER (avg): 0.8274 CTC Loss (avg): 110.7157 time: 26.573
2025-12-11 17:23:49,748: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:23:49,748: t15.2023.08.13 val PER: 0.9314
2025-12-11 17:23:49,748: t15.2023.08.18 val PER: 0.8114
2025-12-11 17:23:49,748: t15.2023.08.20 val PER: 0.8237
2025-12-11 17:23:49,748: t15.2023.08.25 val PER: 0.7620
2025-12-11 17:23:49,748: t15.2023.08.27 val PER: 0.8424
2025-12-11 17:23:49,748: t15.2023.09.01 val PER: 0.7192
2025-12-11 17:23:49,748: t15.2023.09.03 val PER: 0.8812
2025-12-11 17:23:49,748: t15.2023.09.24 val PER: 0.8155
2025-12-11 17:23:49,748: t15.2023.09.29 val PER: 0.7805
2025-12-11 17:23:49,748: t15.2023.10.01 val PER: 0.7853
2025-12-11 17:23:49,748: t15.2023.10.06 val PER: 0.8999
2025-12-11 17:23:49,748: t15.2023.10.08 val PER: 0.7889
2025-12-11 17:23:49,748: t15.2023.10.13 val PER: 0.8107
2025-12-11 17:23:49,748: t15.2023.10.15 val PER: 0.8003
2025-12-11 17:23:49,748: t15.2023.10.20 val PER: 0.7483
2025-12-11 17:23:49,748: t15.2023.10.22 val PER: 0.8987
2025-12-11 17:23:49,748: t15.2023.11.03 val PER: 0.8365
2025-12-11 17:23:49,749: t15.2023.11.04 val PER: 0.8805
2025-12-11 17:23:49,749: t15.2023.11.17 val PER: 0.7294
2025-12-11 17:23:49,749: t15.2023.11.19 val PER: 0.8463
2025-12-11 17:23:49,749: t15.2023.11.26 val PER: 0.8297
2025-12-11 17:23:49,749: t15.2023.12.03 val PER: 0.8298
2025-12-11 17:23:49,749: t15.2023.12.08 val PER: 0.8855
2025-12-11 17:23:49,749: t15.2023.12.10 val PER: 0.8699
2025-12-11 17:23:49,749: t15.2023.12.17 val PER: 0.8337
2025-12-11 17:23:49,749: t15.2023.12.29 val PER: 0.8723
2025-12-11 17:23:49,749: t15.2024.02.25 val PER: 0.7767
2025-12-11 17:23:49,749: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:23:49,749: t15.2024.03.08 val PER: 0.9018
2025-12-11 17:23:49,749: t15.2024.03.15 val PER: 0.7911
2025-12-11 17:23:49,749: t15.2024.03.17 val PER: 0.8110
2025-12-11 17:23:49,749: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:23:49,749: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:23:49,749: t15.2024.05.10 val PER: 0.8351
2025-12-11 17:23:49,749: t15.2024.06.14 val PER: 0.8864
2025-12-11 17:23:49,749: t15.2024.07.19 val PER: 0.8906
2025-12-11 17:23:49,750: t15.2024.07.21 val PER: 0.7676
2025-12-11 17:23:49,750: t15.2024.07.28 val PER: 0.8669
2025-12-11 17:23:49,750: t15.2025.01.10 val PER: 0.8512
2025-12-11 17:23:49,750: t15.2025.01.12 val PER: 0.7629
2025-12-11 17:23:49,750: t15.2025.03.14 val PER: 0.9024
2025-12-11 17:23:49,750: t15.2025.03.16 val PER: 0.7565
2025-12-11 17:23:49,750: t15.2025.03.30 val PER: 0.8356
2025-12-11 17:23:49,750: t15.2025.04.13 val PER: 0.8631
2025-12-11 17:24:34,902: Train batch 4200: loss: 62.37 grad norm: 8735.74 time: 0.180
2025-12-11 17:25:20,791: Train batch 4400: loss: 101.25 grad norm: 648.86 time: 0.262
2025-12-11 17:26:06,434: Train batch 4600: loss: 86.62 grad norm: 26693.87 time: 0.200
2025-12-11 17:26:52,882: Train batch 4800: loss: 81.24 grad norm: 9678.85 time: 0.208
2025-12-11 17:27:38,281: Train batch 5000: loss: 94.58 grad norm: 23297.02 time: 0.316
2025-12-11 17:28:26,005: Train batch 5200: loss: 85.66 grad norm: 96351.95 time: 0.204
2025-12-11 17:29:13,715: Train batch 5400: loss: 73.17 grad norm: 6284.24 time: 0.215
2025-12-11 17:30:00,760: Train batch 5600: loss: 80.37 grad norm: 27744.94 time: 0.198
2025-12-11 17:30:48,369: Train batch 5800: loss: 89.84 grad norm: 108145.10 time: 0.213
2025-12-11 17:31:33,887: Train batch 6000: loss: 66.45 grad norm: 504.99 time: 0.199
2025-12-11 17:31:33,887: Running test after training batch: 6000
2025-12-11 17:32:00,364: Val batch 6000: PER (avg): 0.9963 CTC Loss (avg): 157.9597 time: 26.477
2025-12-11 17:32:00,364: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:32:00,364: t15.2023.08.13 val PER: 0.9948
2025-12-11 17:32:00,364: t15.2023.08.18 val PER: 0.9950
2025-12-11 17:32:00,364: t15.2023.08.20 val PER: 0.9960
2025-12-11 17:32:00,364: t15.2023.08.25 val PER: 0.9955
2025-12-11 17:32:00,364: t15.2023.08.27 val PER: 0.9952
2025-12-11 17:32:00,364: t15.2023.09.01 val PER: 0.9943
2025-12-11 17:32:00,365: t15.2023.09.03 val PER: 0.9952
2025-12-11 17:32:00,365: t15.2023.09.24 val PER: 0.9964
2025-12-11 17:32:00,365: t15.2023.09.29 val PER: 0.9974
2025-12-11 17:32:00,365: t15.2023.10.01 val PER: 0.9967
2025-12-11 17:32:00,365: t15.2023.10.06 val PER: 0.9968
2025-12-11 17:32:00,365: t15.2023.10.08 val PER: 0.9973
2025-12-11 17:32:00,365: t15.2023.10.13 val PER: 0.9969
2025-12-11 17:32:00,365: t15.2023.10.15 val PER: 0.9967
2025-12-11 17:32:00,365: t15.2023.10.20 val PER: 0.9966
2025-12-11 17:32:00,365: t15.2023.10.22 val PER: 0.9967
2025-12-11 17:32:00,365: t15.2023.11.03 val PER: 0.9959
2025-12-11 17:32:00,365: t15.2023.11.04 val PER: 0.9932
2025-12-11 17:32:00,365: t15.2023.11.17 val PER: 0.9953
2025-12-11 17:32:00,365: t15.2023.11.19 val PER: 0.9960
2025-12-11 17:32:00,365: t15.2023.11.26 val PER: 0.9971
2025-12-11 17:32:00,365: t15.2023.12.03 val PER: 0.9968
2025-12-11 17:32:00,365: t15.2023.12.08 val PER: 0.9967
2025-12-11 17:32:00,365: t15.2023.12.10 val PER: 0.9961
2025-12-11 17:32:00,365: t15.2023.12.17 val PER: 0.9969
2025-12-11 17:32:00,366: t15.2023.12.29 val PER: 0.9952
2025-12-11 17:32:00,366: t15.2024.02.25 val PER: 0.9972
2025-12-11 17:32:00,366: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:32:00,366: t15.2024.03.08 val PER: 0.9972
2025-12-11 17:32:00,366: t15.2024.03.15 val PER: 0.9975
2025-12-11 17:32:00,366: t15.2024.03.17 val PER: 0.9965
2025-12-11 17:32:00,366: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:32:00,366: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:32:00,366: t15.2024.05.10 val PER: 0.9955
2025-12-11 17:32:00,366: t15.2024.06.14 val PER: 0.9937
2025-12-11 17:32:00,366: t15.2024.07.19 val PER: 0.9974
2025-12-11 17:32:00,366: t15.2024.07.21 val PER: 0.9972
2025-12-11 17:32:00,366: t15.2024.07.28 val PER: 0.9963
2025-12-11 17:32:00,366: t15.2025.01.10 val PER: 0.9972
2025-12-11 17:32:00,366: t15.2025.01.12 val PER: 0.9962
2025-12-11 17:32:00,366: t15.2025.03.14 val PER: 0.9970
2025-12-11 17:32:00,366: t15.2025.03.16 val PER: 0.9948
2025-12-11 17:32:00,366: t15.2025.03.30 val PER: 0.9954
2025-12-11 17:32:00,366: t15.2025.04.13 val PER: 0.9957
2025-12-11 17:32:47,450: Train batch 6200: loss: 71.11 grad norm: 787.97 time: 0.203
2025-12-11 17:33:34,107: Train batch 6400: loss: 78.94 grad norm: 295.68 time: 0.219
2025-12-11 17:34:21,406: Train batch 6600: loss: 77.81 grad norm: 21689.13 time: 0.249
2025-12-11 17:35:08,314: Train batch 6800: loss: 88.33 grad norm: 276413.81 time: 0.215
2025-12-11 17:35:55,644: Train batch 7000: loss: 73.91 grad norm: 3822.82 time: 0.194
2025-12-11 17:36:42,841: Train batch 7200: loss: 85.70 grad norm: 366365.41 time: 0.295
2025-12-11 17:37:29,526: Train batch 7400: loss: 76.43 grad norm: 2766336.25 time: 0.250
2025-12-11 17:38:16,775: Train batch 7600: loss: 94.31 grad norm: 577920.19 time: 0.229
2025-12-11 17:39:02,614: Train batch 7800: loss: 82.32 grad norm: 2502608.00 time: 0.266
2025-12-11 17:39:49,149: Train batch 8000: loss: 74.36 grad norm: 436369.06 time: 0.247
2025-12-11 17:39:49,149: Running test after training batch: 8000
2025-12-11 17:40:15,599: Val batch 8000: PER (avg): 0.9998 CTC Loss (avg): 173.9265 time: 26.449
2025-12-11 17:40:15,599: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.08.13 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.08.18 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.08.20 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.08.25 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.08.27 val PER: 0.9968
2025-12-11 17:40:15,599: t15.2023.09.01 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.09.03 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.09.24 val PER: 1.0000
2025-12-11 17:40:15,599: t15.2023.09.29 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.01 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.06 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.08 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.13 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.15 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.20 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.10.22 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.11.03 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.11.04 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.11.17 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.11.19 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.11.26 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.12.03 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.12.08 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.12.10 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.12.17 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2023.12.29 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2024.02.25 val PER: 1.0000
2025-12-11 17:40:15,600: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.03.08 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.03.15 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.03.17 val PER: 0.9993
2025-12-11 17:40:15,601: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.05.10 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.06.14 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.07.19 val PER: 0.9974
2025-12-11 17:40:15,601: t15.2024.07.21 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2024.07.28 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.01.10 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.01.12 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.03.14 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.03.16 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.03.30 val PER: 1.0000
2025-12-11 17:40:15,601: t15.2025.04.13 val PER: 1.0000
2025-12-11 17:41:01,350: Train batch 8200: loss: 81.16 grad norm: 14909.60 time: 0.209
2025-12-11 17:41:48,669: Train batch 8400: loss: 82.84 grad norm: 21645.37 time: 0.227
2025-12-11 17:42:35,523: Train batch 8600: loss: 78.88 grad norm: 628723.50 time: 0.213
2025-12-11 17:43:22,561: Train batch 8800: loss: 86.30 grad norm: 806996.06 time: 0.245
2025-12-11 17:44:09,319: Train batch 9000: loss: 84.81 grad norm: 298259.31 time: 0.221
2025-12-11 17:44:56,923: Train batch 9200: loss: 78.94 grad norm: 616.67 time: 0.310
2025-12-11 17:45:44,086: Train batch 9400: loss: 69.31 grad norm: 23556.67 time: 0.226
2025-12-11 17:46:30,255: Train batch 9600: loss: 86.61 grad norm: 20970.87 time: 0.228
2025-12-11 17:47:17,676: Train batch 9800: loss: 60.75 grad norm: 33146.71 time: 0.253
2025-12-11 17:48:04,754: Train batch 10000: loss: 81.45 grad norm: 145385.17 time: 0.212
2025-12-11 17:48:04,754: Running test after training batch: 10000
2025-12-11 17:48:31,203: Val batch 10000: PER (avg): 0.9945 CTC Loss (avg): 167.5894 time: 26.450
2025-12-11 17:48:31,204: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:48:31,204: t15.2023.08.13 val PER: 0.9667
2025-12-11 17:48:31,204: t15.2023.08.18 val PER: 1.0000
2025-12-11 17:48:31,204: t15.2023.08.20 val PER: 1.0000
2025-12-11 17:48:31,204: t15.2023.08.25 val PER: 0.9985
2025-12-11 17:48:31,204: t15.2023.08.27 val PER: 0.9662
2025-12-11 17:48:31,204: t15.2023.09.01 val PER: 0.9968
2025-12-11 17:48:31,204: t15.2023.09.03 val PER: 0.9988
2025-12-11 17:48:31,204: t15.2023.09.24 val PER: 0.9915
2025-12-11 17:48:31,204: t15.2023.09.29 val PER: 0.9987
2025-12-11 17:48:31,204: t15.2023.10.01 val PER: 0.9987
2025-12-11 17:48:31,204: t15.2023.10.06 val PER: 0.9935
2025-12-11 17:48:31,204: t15.2023.10.08 val PER: 0.9905
2025-12-11 17:48:31,204: t15.2023.10.13 val PER: 0.9938
2025-12-11 17:48:31,204: t15.2023.10.15 val PER: 0.9987
2025-12-11 17:48:31,205: t15.2023.10.20 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2023.10.22 val PER: 0.9978
2025-12-11 17:48:31,205: t15.2023.11.03 val PER: 0.9959
2025-12-11 17:48:31,205: t15.2023.11.04 val PER: 0.9932
2025-12-11 17:48:31,205: t15.2023.11.17 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2023.11.19 val PER: 0.9820
2025-12-11 17:48:31,205: t15.2023.11.26 val PER: 0.9971
2025-12-11 17:48:31,205: t15.2023.12.03 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2023.12.08 val PER: 0.9913
2025-12-11 17:48:31,205: t15.2023.12.10 val PER: 0.9947
2025-12-11 17:48:31,205: t15.2023.12.17 val PER: 0.9938
2025-12-11 17:48:31,205: t15.2023.12.29 val PER: 0.9870
2025-12-11 17:48:31,205: t15.2024.02.25 val PER: 0.9888
2025-12-11 17:48:31,205: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2024.03.08 val PER: 0.9957
2025-12-11 17:48:31,205: t15.2024.03.15 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2024.03.17 val PER: 0.9993
2025-12-11 17:48:31,205: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:48:31,205: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:48:31,206: t15.2024.05.10 val PER: 0.9985
2025-12-11 17:48:31,206: t15.2024.06.14 val PER: 0.9905
2025-12-11 17:48:31,206: t15.2024.07.19 val PER: 0.9947
2025-12-11 17:48:31,206: t15.2024.07.21 val PER: 0.9986
2025-12-11 17:48:31,206: t15.2024.07.28 val PER: 1.0000
2025-12-11 17:48:31,206: t15.2025.01.10 val PER: 0.9972
2025-12-11 17:48:31,206: t15.2025.01.12 val PER: 0.9915
2025-12-11 17:48:31,206: t15.2025.03.14 val PER: 1.0000
2025-12-11 17:48:31,206: t15.2025.03.16 val PER: 1.0000
2025-12-11 17:48:31,206: t15.2025.03.30 val PER: 0.9839
2025-12-11 17:48:31,206: t15.2025.04.13 val PER: 0.9800
2025-12-11 17:49:17,892: Train batch 10200: loss: 81.10 grad norm: 24663.25 time: 0.250
2025-12-11 17:50:03,598: Train batch 10400: loss: 94.18 grad norm: 32034.89 time: 0.284
2025-12-11 17:50:49,853: Train batch 10600: loss: 81.41 grad norm: 47002.10 time: 0.233
2025-12-11 17:51:36,996: Train batch 10800: loss: 82.51 grad norm: 67982.02 time: 0.197
2025-12-11 17:52:22,660: Train batch 11000: loss: 75.58 grad norm: 6558.83 time: 0.198
2025-12-11 17:53:09,407: Train batch 11200: loss: 66.95 grad norm: 13543.97 time: 0.176
2025-12-11 17:53:56,899: Train batch 11400: loss: 70.28 grad norm: 4241.43 time: 0.211
2025-12-11 17:54:43,161: Train batch 11600: loss: 71.31 grad norm: 87728.57 time: 0.201
2025-12-11 17:55:29,567: Train batch 11800: loss: 73.72 grad norm: 18980620.00 time: 0.225
2025-12-11 17:56:17,176: Train batch 12000: loss: 78.34 grad norm: 511304.34 time: 0.244
2025-12-11 17:56:17,177: Running test after training batch: 12000
2025-12-11 17:56:43,561: Val batch 12000: PER (avg): 0.9907 CTC Loss (avg): 174.1280 time: 26.385
2025-12-11 17:56:43,561: t15.2023.08.11 val PER: 1.0000
2025-12-11 17:56:43,561: t15.2023.08.13 val PER: 0.9938
2025-12-11 17:56:43,562: t15.2023.08.18 val PER: 0.9916
2025-12-11 17:56:43,562: t15.2023.08.20 val PER: 0.9817
2025-12-11 17:56:43,562: t15.2023.08.25 val PER: 0.9940
2025-12-11 17:56:43,562: t15.2023.08.27 val PER: 0.9823
2025-12-11 17:56:43,562: t15.2023.09.01 val PER: 0.9797
2025-12-11 17:56:43,562: t15.2023.09.03 val PER: 0.9964
2025-12-11 17:56:43,562: t15.2023.09.24 val PER: 0.9891
2025-12-11 17:56:43,562: t15.2023.09.29 val PER: 0.9879
2025-12-11 17:56:43,562: t15.2023.10.01 val PER: 0.9894
2025-12-11 17:56:43,562: t15.2023.10.06 val PER: 0.9860
2025-12-11 17:56:43,562: t15.2023.10.08 val PER: 1.0000
2025-12-11 17:56:43,562: t15.2023.10.13 val PER: 0.9907
2025-12-11 17:56:43,562: t15.2023.10.15 val PER: 0.9941
2025-12-11 17:56:43,562: t15.2023.10.20 val PER: 0.9899
2025-12-11 17:56:43,562: t15.2023.10.22 val PER: 0.9911
2025-12-11 17:56:43,562: t15.2023.11.03 val PER: 0.9864
2025-12-11 17:56:43,562: t15.2023.11.04 val PER: 0.9966
2025-12-11 17:56:43,562: t15.2023.11.17 val PER: 0.9922
2025-12-11 17:56:43,562: t15.2023.11.19 val PER: 0.9920
2025-12-11 17:56:43,563: t15.2023.11.26 val PER: 0.9783
2025-12-11 17:56:43,563: t15.2023.12.03 val PER: 0.9884
2025-12-11 17:56:43,563: t15.2023.12.08 val PER: 0.9973
2025-12-11 17:56:43,563: t15.2023.12.10 val PER: 0.9934
2025-12-11 17:56:43,563: t15.2023.12.17 val PER: 0.9948
2025-12-11 17:56:43,563: t15.2023.12.29 val PER: 0.9931
2025-12-11 17:56:43,563: t15.2024.02.25 val PER: 0.9916
2025-12-11 17:56:43,563: t15.2024.03.03 val PER: 1.0000
2025-12-11 17:56:43,563: t15.2024.03.08 val PER: 0.9872
2025-12-11 17:56:43,563: t15.2024.03.15 val PER: 0.9975
2025-12-11 17:56:43,563: t15.2024.03.17 val PER: 0.9937
2025-12-11 17:56:43,563: t15.2024.04.25 val PER: 1.0000
2025-12-11 17:56:43,563: t15.2024.04.28 val PER: 1.0000
2025-12-11 17:56:43,563: t15.2024.05.10 val PER: 0.9985
2025-12-11 17:56:43,563: t15.2024.06.14 val PER: 0.9874
2025-12-11 17:56:43,563: t15.2024.07.19 val PER: 0.9927
2025-12-11 17:56:43,563: t15.2024.07.21 val PER: 0.9931
2025-12-11 17:56:43,563: t15.2024.07.28 val PER: 0.9926
2025-12-11 17:56:43,563: t15.2025.01.10 val PER: 0.9835
2025-12-11 17:56:43,564: t15.2025.01.12 val PER: 0.9915
2025-12-11 17:56:43,564: t15.2025.03.14 val PER: 0.9882
2025-12-11 17:56:43,564: t15.2025.03.16 val PER: 0.9856
2025-12-11 17:56:43,564: t15.2025.03.30 val PER: 0.9966
2025-12-11 17:56:43,564: t15.2025.04.13 val PER: 0.9914
2025-12-11 17:57:30,132: Train batch 12200: loss: 84.28 grad norm: 101865.85 time: 0.213
2025-12-11 17:58:17,365: Train batch 12400: loss: 77.82 grad norm: 115463.71 time: 0.205
2025-12-11 17:59:03,143: Train batch 12600: loss: 76.09 grad norm: 190138.61 time: 0.213
2025-12-11 17:59:50,619: Train batch 12800: loss: 74.71 grad norm: 2077368.00 time: 0.269
2025-12-11 18:00:37,166: Train batch 13000: loss: 67.81 grad norm: 7083.43 time: 0.227
2025-12-11 18:01:25,277: Train batch 13200: loss: 78.57 grad norm: 72198.46 time: 0.225
2025-12-11 18:02:10,520: Train batch 13400: loss: 69.06 grad norm: 3331350.00 time: 0.173
2025-12-11 18:02:57,294: Train batch 13600: loss: 72.98 grad norm: 58416.82 time: 0.222
2025-12-11 18:03:46,087: Train batch 13800: loss: 65.79 grad norm: 3797493.50 time: 0.205
2025-12-11 18:04:32,573: Train batch 14000: loss: 79.41 grad norm: 1976574.88 time: 0.247
2025-12-11 18:04:32,574: Running test after training batch: 14000
2025-12-11 18:04:58,983: Val batch 14000: PER (avg): 0.9265 CTC Loss (avg): 198.1031 time: 26.410
2025-12-11 18:04:58,984: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:04:58,984: t15.2023.08.13 val PER: 0.9241
2025-12-11 18:04:58,984: t15.2023.08.18 val PER: 0.9070
2025-12-11 18:04:58,984: t15.2023.08.20 val PER: 0.9317
2025-12-11 18:04:58,984: t15.2023.08.25 val PER: 0.9352
2025-12-11 18:04:58,984: t15.2023.08.27 val PER: 0.9341
2025-12-11 18:04:58,984: t15.2023.09.01 val PER: 0.9286
2025-12-11 18:04:58,984: t15.2023.09.03 val PER: 0.9347
2025-12-11 18:04:58,984: t15.2023.09.24 val PER: 0.9223
2025-12-11 18:04:58,984: t15.2023.09.29 val PER: 0.9285
2025-12-11 18:04:58,984: t15.2023.10.01 val PER: 0.9267
2025-12-11 18:04:58,984: t15.2023.10.06 val PER: 0.8848
2025-12-11 18:04:58,984: t15.2023.10.08 val PER: 0.9526
2025-12-11 18:04:58,984: t15.2023.10.13 val PER: 0.9263
2025-12-11 18:04:58,984: t15.2023.10.15 val PER: 0.9492
2025-12-11 18:04:58,984: t15.2023.10.20 val PER: 0.9295
2025-12-11 18:04:58,984: t15.2023.10.22 val PER: 0.9232
2025-12-11 18:04:58,985: t15.2023.11.03 val PER: 0.9417
2025-12-11 18:04:58,985: t15.2023.11.04 val PER: 0.9113
2025-12-11 18:04:58,985: t15.2023.11.17 val PER: 0.9316
2025-12-11 18:04:58,985: t15.2023.11.19 val PER: 0.9242
2025-12-11 18:04:58,985: t15.2023.11.26 val PER: 0.9399
2025-12-11 18:04:58,985: t15.2023.12.03 val PER: 0.9296
2025-12-11 18:04:58,985: t15.2023.12.08 val PER: 0.9261
2025-12-11 18:04:58,985: t15.2023.12.10 val PER: 0.8883
2025-12-11 18:04:58,985: t15.2023.12.17 val PER: 0.9335
2025-12-11 18:04:58,985: t15.2023.12.29 val PER: 0.9307
2025-12-11 18:04:58,985: t15.2024.02.25 val PER: 0.9284
2025-12-11 18:04:58,985: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:04:58,985: t15.2024.03.08 val PER: 0.9147
2025-12-11 18:04:58,985: t15.2024.03.15 val PER: 0.9250
2025-12-11 18:04:58,985: t15.2024.03.17 val PER: 0.8835
2025-12-11 18:04:58,985: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:04:58,985: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:04:58,985: t15.2024.05.10 val PER: 0.9287
2025-12-11 18:04:58,985: t15.2024.06.14 val PER: 0.9274
2025-12-11 18:04:58,986: t15.2024.07.19 val PER: 0.9321
2025-12-11 18:04:58,986: t15.2024.07.21 val PER: 0.9386
2025-12-11 18:04:58,986: t15.2024.07.28 val PER: 0.9199
2025-12-11 18:04:58,986: t15.2025.01.10 val PER: 0.9380
2025-12-11 18:04:58,986: t15.2025.01.12 val PER: 0.9369
2025-12-11 18:04:58,986: t15.2025.03.14 val PER: 0.9068
2025-12-11 18:04:58,986: t15.2025.03.16 val PER: 0.9424
2025-12-11 18:04:58,986: t15.2025.03.30 val PER: 0.9138
2025-12-11 18:04:58,986: t15.2025.04.13 val PER: 0.9344
2025-12-11 18:05:46,296: Train batch 14200: loss: 76.24 grad norm: 75951.65 time: 0.249
2025-12-11 18:06:33,134: Train batch 14400: loss: 72.19 grad norm: 22305.49 time: 0.209
2025-12-11 18:07:20,261: Train batch 14600: loss: 82.96 grad norm: 2295.99 time: 0.170
2025-12-11 18:08:06,810: Train batch 14800: loss: 70.04 grad norm: 733290.19 time: 0.197
2025-12-11 18:08:48,999: Train batch 15000: loss: 84.73 grad norm: 11024.33 time: 0.224
2025-12-11 18:09:31,919: Train batch 15200: loss: 79.95 grad norm: 39254.57 time: 0.177
2025-12-11 18:10:18,636: Train batch 15400: loss: 80.49 grad norm: 118486.73 time: 0.250
2025-12-11 18:11:06,450: Train batch 15600: loss: 68.42 grad norm: 72519.70 time: 0.176
2025-12-11 18:11:53,474: Train batch 15800: loss: 63.95 grad norm: 3509.67 time: 0.220
2025-12-11 18:12:40,505: Train batch 16000: loss: 59.62 grad norm: 30165.44 time: 0.188
2025-12-11 18:12:40,505: Running test after training batch: 16000
2025-12-11 18:13:06,952: Val batch 16000: PER (avg): 0.9393 CTC Loss (avg): 212.8937 time: 26.447
2025-12-11 18:13:06,953: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:13:06,953: t15.2023.08.13 val PER: 0.9356
2025-12-11 18:13:06,953: t15.2023.08.18 val PER: 0.9329
2025-12-11 18:13:06,953: t15.2023.08.20 val PER: 0.9317
2025-12-11 18:13:06,953: t15.2023.08.25 val PER: 0.9367
2025-12-11 18:13:06,953: t15.2023.08.27 val PER: 0.9341
2025-12-11 18:13:06,953: t15.2023.09.01 val PER: 0.9326
2025-12-11 18:13:06,953: t15.2023.09.03 val PER: 0.9359
2025-12-11 18:13:06,953: t15.2023.09.24 val PER: 0.9296
2025-12-11 18:13:06,953: t15.2023.09.29 val PER: 0.9438
2025-12-11 18:13:06,953: t15.2023.10.01 val PER: 0.9445
2025-12-11 18:13:06,953: t15.2023.10.06 val PER: 0.9333
2025-12-11 18:13:06,953: t15.2023.10.08 val PER: 0.9594
2025-12-11 18:13:06,953: t15.2023.10.13 val PER: 0.9379
2025-12-11 18:13:06,953: t15.2023.10.15 val PER: 0.9492
2025-12-11 18:13:06,953: t15.2023.10.20 val PER: 0.9497
2025-12-11 18:13:06,954: t15.2023.10.22 val PER: 0.9376
2025-12-11 18:13:06,954: t15.2023.11.03 val PER: 0.9430
2025-12-11 18:13:06,954: t15.2023.11.04 val PER: 0.9113
2025-12-11 18:13:06,954: t15.2023.11.17 val PER: 0.9300
2025-12-11 18:13:06,954: t15.2023.11.19 val PER: 0.9301
2025-12-11 18:13:06,954: t15.2023.11.26 val PER: 0.9428
2025-12-11 18:13:06,954: t15.2023.12.03 val PER: 0.9359
2025-12-11 18:13:06,954: t15.2023.12.08 val PER: 0.9381
2025-12-11 18:13:06,954: t15.2023.12.10 val PER: 0.9396
2025-12-11 18:13:06,954: t15.2023.12.17 val PER: 0.9407
2025-12-11 18:13:06,954: t15.2023.12.29 val PER: 0.9410
2025-12-11 18:13:06,954: t15.2024.02.25 val PER: 0.9410
2025-12-11 18:13:06,954: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:13:06,954: t15.2024.03.08 val PER: 0.9431
2025-12-11 18:13:06,954: t15.2024.03.15 val PER: 0.9475
2025-12-11 18:13:06,954: t15.2024.03.17 val PER: 0.9379
2025-12-11 18:13:06,954: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:13:06,954: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:13:06,954: t15.2024.05.10 val PER: 0.9331
2025-12-11 18:13:06,955: t15.2024.06.14 val PER: 0.9290
2025-12-11 18:13:06,955: t15.2024.07.19 val PER: 0.9413
2025-12-11 18:13:06,955: t15.2024.07.21 val PER: 0.9455
2025-12-11 18:13:06,955: t15.2024.07.28 val PER: 0.9382
2025-12-11 18:13:06,955: t15.2025.01.10 val PER: 0.9408
2025-12-11 18:13:06,955: t15.2025.01.12 val PER: 0.9369
2025-12-11 18:13:06,955: t15.2025.03.14 val PER: 0.9364
2025-12-11 18:13:06,955: t15.2025.03.16 val PER: 0.9424
2025-12-11 18:13:06,955: t15.2025.03.30 val PER: 0.9414
2025-12-11 18:13:06,955: t15.2025.04.13 val PER: 0.9387
2025-12-11 18:13:54,316: Train batch 16200: loss: 95.83 grad norm: 87573.32 time: 0.295
2025-12-11 18:14:41,171: Train batch 16400: loss: 74.12 grad norm: 3349.86 time: 0.193
2025-12-11 18:15:27,751: Train batch 16600: loss: 81.93 grad norm: 640292.00 time: 0.268
2025-12-11 18:16:14,304: Train batch 16800: loss: 81.45 grad norm: 95787.88 time: 0.218
2025-12-11 18:17:01,574: Train batch 17000: loss: 82.23 grad norm: 34108.62 time: 0.200
2025-12-11 18:17:48,844: Train batch 17200: loss: 78.28 grad norm: 11430.00 time: 0.243
2025-12-11 18:18:37,554: Train batch 17400: loss: 72.21 grad norm: 3019.79 time: 0.229
2025-12-11 18:19:24,833: Train batch 17600: loss: 79.69 grad norm: 356020.03 time: 0.206
2025-12-11 18:20:11,252: Train batch 17800: loss: 85.28 grad norm: 3228.52 time: 0.217
2025-12-11 18:20:58,409: Train batch 18000: loss: 73.88 grad norm: 170623.61 time: 0.263
2025-12-11 18:20:58,409: Running test after training batch: 18000
2025-12-11 18:21:24,843: Val batch 18000: PER (avg): 1.0000 CTC Loss (avg): 160.4940 time: 26.434
2025-12-11 18:21:24,844: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.08.13 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.08.18 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.08.20 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.08.25 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.08.27 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.09.01 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.09.03 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.09.24 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.09.29 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.10.01 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.10.06 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.10.08 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.10.13 val PER: 1.0000
2025-12-11 18:21:24,844: t15.2023.10.15 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.10.20 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.10.22 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.11.03 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.11.04 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.11.17 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.11.19 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.11.26 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.12.03 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.12.08 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.12.10 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.12.17 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2023.12.29 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.02.25 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.03.08 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.03.15 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.03.17 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:21:24,845: t15.2024.05.10 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2024.06.14 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2024.07.19 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2024.07.21 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2024.07.28 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.01.10 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.01.12 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.03.14 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.03.16 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.03.30 val PER: 1.0000
2025-12-11 18:21:24,846: t15.2025.04.13 val PER: 1.0000
2025-12-11 18:22:12,176: Train batch 18200: loss: 88.31 grad norm: 36733.14 time: 0.231
2025-12-11 18:23:00,311: Train batch 18400: loss: 60.64 grad norm: 11828.02 time: 0.192
2025-12-11 18:23:46,983: Train batch 18600: loss: 72.47 grad norm: 37706.65 time: 0.216
2025-12-11 18:24:33,440: Train batch 18800: loss: 76.30 grad norm: 119061.29 time: 0.210
2025-12-11 18:25:20,558: Train batch 19000: loss: 52.03 grad norm: 7186804.50 time: 0.184
2025-12-11 18:26:08,117: Train batch 19200: loss: 93.73 grad norm: 61147.96 time: 0.239
2025-12-11 18:26:54,482: Train batch 19400: loss: 69.52 grad norm: 1517.57 time: 0.258
2025-12-11 18:27:41,601: Train batch 19600: loss: 72.77 grad norm: 130478.41 time: 0.224
2025-12-11 18:28:28,814: Train batch 19800: loss: 81.73 grad norm: 8146.33 time: 0.252
2025-12-11 18:29:15,417: Train batch 20000: loss: 81.25 grad norm: 2651.01 time: 0.215
2025-12-11 18:29:15,417: Running test after training batch: 20000
2025-12-11 18:29:42,126: Val batch 20000: PER (avg): 0.9504 CTC Loss (avg): 172.6439 time: 26.709
2025-12-11 18:29:42,126: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:29:42,126: t15.2023.08.13 val PER: 0.9491
2025-12-11 18:29:42,126: t15.2023.08.18 val PER: 0.9430
2025-12-11 18:29:42,126: t15.2023.08.20 val PER: 0.9476
2025-12-11 18:29:42,126: t15.2023.08.25 val PER: 0.9398
2025-12-11 18:29:42,126: t15.2023.08.27 val PER: 0.9405
2025-12-11 18:29:42,126: t15.2023.09.01 val PER: 0.9456
2025-12-11 18:29:42,127: t15.2023.09.03 val PER: 0.9430
2025-12-11 18:29:42,127: t15.2023.09.24 val PER: 0.9405
2025-12-11 18:29:42,127: t15.2023.09.29 val PER: 0.9541
2025-12-11 18:29:42,127: t15.2023.10.01 val PER: 0.9577
2025-12-11 18:29:42,127: t15.2023.10.06 val PER: 0.9397
2025-12-11 18:29:42,127: t15.2023.10.08 val PER: 0.9675
2025-12-11 18:29:42,127: t15.2023.10.13 val PER: 0.9511
2025-12-11 18:29:42,127: t15.2023.10.15 val PER: 0.9598
2025-12-11 18:29:42,127: t15.2023.10.20 val PER: 0.9597
2025-12-11 18:29:42,127: t15.2023.10.22 val PER: 0.9510
2025-12-11 18:29:42,127: t15.2023.11.03 val PER: 0.9573
2025-12-11 18:29:42,127: t15.2023.11.04 val PER: 0.9283
2025-12-11 18:29:42,127: t15.2023.11.17 val PER: 0.9393
2025-12-11 18:29:42,127: t15.2023.11.19 val PER: 0.9441
2025-12-11 18:29:42,127: t15.2023.11.26 val PER: 0.9587
2025-12-11 18:29:42,127: t15.2023.12.03 val PER: 0.9548
2025-12-11 18:29:42,127: t15.2023.12.08 val PER: 0.9501
2025-12-11 18:29:42,128: t15.2023.12.10 val PER: 0.9514
2025-12-11 18:29:42,128: t15.2023.12.17 val PER: 0.9563
2025-12-11 18:29:42,128: t15.2023.12.29 val PER: 0.9492
2025-12-11 18:29:42,128: t15.2024.02.25 val PER: 0.9551
2025-12-11 18:29:42,128: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:29:42,128: t15.2024.03.08 val PER: 0.9502
2025-12-11 18:29:42,128: t15.2024.03.15 val PER: 0.9531
2025-12-11 18:29:42,128: t15.2024.03.17 val PER: 0.9505
2025-12-11 18:29:42,128: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:29:42,128: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:29:42,128: t15.2024.05.10 val PER: 0.9435
2025-12-11 18:29:42,128: t15.2024.06.14 val PER: 0.9432
2025-12-11 18:29:42,128: t15.2024.07.19 val PER: 0.9499
2025-12-11 18:29:42,128: t15.2024.07.21 val PER: 0.9524
2025-12-11 18:29:42,128: t15.2024.07.28 val PER: 0.9478
2025-12-11 18:29:42,128: t15.2025.01.10 val PER: 0.9559
2025-12-11 18:29:42,128: t15.2025.01.12 val PER: 0.9453
2025-12-11 18:29:42,128: t15.2025.03.14 val PER: 0.9527
2025-12-11 18:29:42,128: t15.2025.03.16 val PER: 0.9516
2025-12-11 18:29:42,129: t15.2025.03.30 val PER: 0.9494
2025-12-11 18:29:42,129: t15.2025.04.13 val PER: 0.9486
2025-12-11 18:30:29,500: Train batch 20200: loss: 66.26 grad norm: 125905.69 time: 0.238
2025-12-11 18:31:15,209: Train batch 20400: loss: 94.86 grad norm: 50814.43 time: 0.248
2025-12-11 18:32:02,323: Train batch 20600: loss: 71.78 grad norm: 1334.21 time: 0.237
2025-12-11 18:32:49,434: Train batch 20800: loss: 80.26 grad norm: 7597.22 time: 0.250
2025-12-11 18:33:36,708: Train batch 21000: loss: 65.48 grad norm: 2936.37 time: 0.221
2025-12-11 18:34:23,383: Train batch 21200: loss: 61.79 grad norm: 3554.83 time: 0.217
2025-12-11 18:35:10,076: Train batch 21400: loss: 78.86 grad norm: 1580.88 time: 0.244
2025-12-11 18:35:56,843: Train batch 21600: loss: 68.78 grad norm: 4017.18 time: 0.196
2025-12-11 18:36:44,020: Train batch 21800: loss: 80.39 grad norm: 7888.99 time: 0.217
2025-12-11 18:37:31,076: Train batch 22000: loss: 75.11 grad norm: 10430.19 time: 0.199
2025-12-11 18:37:31,076: Running test after training batch: 22000
2025-12-11 18:37:57,641: Val batch 22000: PER (avg): 0.9923 CTC Loss (avg): 199.0166 time: 26.564
2025-12-11 18:37:57,641: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:37:57,641: t15.2023.08.13 val PER: 0.9896
2025-12-11 18:37:57,641: t15.2023.08.18 val PER: 0.9866
2025-12-11 18:37:57,641: t15.2023.08.20 val PER: 0.9929
2025-12-11 18:37:57,641: t15.2023.08.25 val PER: 0.9895
2025-12-11 18:37:57,641: t15.2023.08.27 val PER: 0.9871
2025-12-11 18:37:57,641: t15.2023.09.01 val PER: 0.9870
2025-12-11 18:37:57,641: t15.2023.09.03 val PER: 0.9893
2025-12-11 18:37:57,641: t15.2023.09.24 val PER: 0.9891
2025-12-11 18:37:57,641: t15.2023.09.29 val PER: 0.9943
2025-12-11 18:37:57,641: t15.2023.10.01 val PER: 0.9908
2025-12-11 18:37:57,641: t15.2023.10.06 val PER: 0.9935
2025-12-11 18:37:57,642: t15.2023.10.08 val PER: 0.9946
2025-12-11 18:37:57,642: t15.2023.10.13 val PER: 0.9938
2025-12-11 18:37:57,642: t15.2023.10.15 val PER: 0.9941
2025-12-11 18:37:57,642: t15.2023.10.20 val PER: 0.9933
2025-12-11 18:37:57,642: t15.2023.10.22 val PER: 0.9944
2025-12-11 18:37:57,642: t15.2023.11.03 val PER: 0.9932
2025-12-11 18:37:57,642: t15.2023.11.04 val PER: 0.9795
2025-12-11 18:37:57,642: t15.2023.11.17 val PER: 0.9891
2025-12-11 18:37:57,642: t15.2023.11.19 val PER: 0.9920
2025-12-11 18:37:57,642: t15.2023.11.26 val PER: 0.9949
2025-12-11 18:37:57,642: t15.2023.12.03 val PER: 0.9926
2025-12-11 18:37:57,642: t15.2023.12.08 val PER: 0.9933
2025-12-11 18:37:57,642: t15.2023.12.10 val PER: 0.9921
2025-12-11 18:37:57,642: t15.2023.12.17 val PER: 0.9938
2025-12-11 18:37:57,642: t15.2023.12.29 val PER: 0.9904
2025-12-11 18:37:57,642: t15.2024.02.25 val PER: 0.9944
2025-12-11 18:37:57,642: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:37:57,642: t15.2024.03.08 val PER: 0.9957
2025-12-11 18:37:57,642: t15.2024.03.15 val PER: 0.9950
2025-12-11 18:37:57,643: t15.2024.03.17 val PER: 0.9930
2025-12-11 18:37:57,643: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:37:57,643: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:37:57,643: t15.2024.05.10 val PER: 0.9911
2025-12-11 18:37:57,643: t15.2024.06.14 val PER: 0.9874
2025-12-11 18:37:57,643: t15.2024.07.19 val PER: 0.9954
2025-12-11 18:37:57,643: t15.2024.07.21 val PER: 0.9945
2025-12-11 18:37:57,643: t15.2024.07.28 val PER: 0.9941
2025-12-11 18:37:57,643: t15.2025.01.10 val PER: 0.9945
2025-12-11 18:37:57,643: t15.2025.01.12 val PER: 0.9923
2025-12-11 18:37:57,643: t15.2025.03.14 val PER: 0.9926
2025-12-11 18:37:57,643: t15.2025.03.16 val PER: 0.9908
2025-12-11 18:37:57,643: t15.2025.03.30 val PER: 0.9920
2025-12-11 18:37:57,643: t15.2025.04.13 val PER: 0.9914
2025-12-11 18:38:44,322: Train batch 22200: loss: 78.69 grad norm: 6351.22 time: 0.233
2025-12-11 18:39:31,638: Train batch 22400: loss: 79.13 grad norm: 4656.09 time: 0.201
2025-12-11 18:40:18,516: Train batch 22600: loss: 68.81 grad norm: 1991.40 time: 0.217
2025-12-11 18:41:06,660: Train batch 22800: loss: 70.25 grad norm: 10462.59 time: 0.242
2025-12-11 18:41:53,125: Train batch 23000: loss: 86.42 grad norm: 21157.56 time: 0.342
2025-12-11 18:42:39,829: Train batch 23200: loss: 99.72 grad norm: 46506.53 time: 0.245
2025-12-11 18:43:26,355: Train batch 23400: loss: 76.54 grad norm: 145865.33 time: 0.230
2025-12-11 18:44:12,532: Train batch 23600: loss: 86.25 grad norm: 11156.02 time: 0.235
2025-12-11 18:44:57,376: Train batch 23800: loss: 78.56 grad norm: 21587.11 time: 0.231
2025-12-11 18:45:44,664: Train batch 24000: loss: 76.31 grad norm: 3873.42 time: 0.209
2025-12-11 18:45:44,664: Running test after training batch: 24000
2025-12-11 18:46:11,005: Val batch 24000: PER (avg): 0.9504 CTC Loss (avg): 236.3383 time: 26.340
2025-12-11 18:46:11,005: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:46:11,005: t15.2023.08.13 val PER: 0.9491
2025-12-11 18:46:11,005: t15.2023.08.18 val PER: 0.9430
2025-12-11 18:46:11,006: t15.2023.08.20 val PER: 0.9476
2025-12-11 18:46:11,006: t15.2023.08.25 val PER: 0.9398
2025-12-11 18:46:11,006: t15.2023.08.27 val PER: 0.9405
2025-12-11 18:46:11,006: t15.2023.09.01 val PER: 0.9456
2025-12-11 18:46:11,006: t15.2023.09.03 val PER: 0.9430
2025-12-11 18:46:11,006: t15.2023.09.24 val PER: 0.9405
2025-12-11 18:46:11,006: t15.2023.09.29 val PER: 0.9541
2025-12-11 18:46:11,006: t15.2023.10.01 val PER: 0.9577
2025-12-11 18:46:11,006: t15.2023.10.06 val PER: 0.9397
2025-12-11 18:46:11,006: t15.2023.10.08 val PER: 0.9675
2025-12-11 18:46:11,006: t15.2023.10.13 val PER: 0.9511
2025-12-11 18:46:11,006: t15.2023.10.15 val PER: 0.9598
2025-12-11 18:46:11,006: t15.2023.10.20 val PER: 0.9597
2025-12-11 18:46:11,006: t15.2023.10.22 val PER: 0.9510
2025-12-11 18:46:11,006: t15.2023.11.03 val PER: 0.9573
2025-12-11 18:46:11,006: t15.2023.11.04 val PER: 0.9283
2025-12-11 18:46:11,006: t15.2023.11.17 val PER: 0.9393
2025-12-11 18:46:11,006: t15.2023.11.19 val PER: 0.9441
2025-12-11 18:46:11,007: t15.2023.11.26 val PER: 0.9587
2025-12-11 18:46:11,007: t15.2023.12.03 val PER: 0.9548
2025-12-11 18:46:11,007: t15.2023.12.08 val PER: 0.9501
2025-12-11 18:46:11,007: t15.2023.12.10 val PER: 0.9514
2025-12-11 18:46:11,007: t15.2023.12.17 val PER: 0.9563
2025-12-11 18:46:11,007: t15.2023.12.29 val PER: 0.9492
2025-12-11 18:46:11,007: t15.2024.02.25 val PER: 0.9551
2025-12-11 18:46:11,007: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:46:11,007: t15.2024.03.08 val PER: 0.9502
2025-12-11 18:46:11,007: t15.2024.03.15 val PER: 0.9531
2025-12-11 18:46:11,007: t15.2024.03.17 val PER: 0.9505
2025-12-11 18:46:11,007: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:46:11,007: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:46:11,007: t15.2024.05.10 val PER: 0.9435
2025-12-11 18:46:11,007: t15.2024.06.14 val PER: 0.9432
2025-12-11 18:46:11,007: t15.2024.07.19 val PER: 0.9499
2025-12-11 18:46:11,007: t15.2024.07.21 val PER: 0.9524
2025-12-11 18:46:11,007: t15.2024.07.28 val PER: 0.9478
2025-12-11 18:46:11,007: t15.2025.01.10 val PER: 0.9559
2025-12-11 18:46:11,008: t15.2025.01.12 val PER: 0.9453
2025-12-11 18:46:11,008: t15.2025.03.14 val PER: 0.9527
2025-12-11 18:46:11,008: t15.2025.03.16 val PER: 0.9516
2025-12-11 18:46:11,008: t15.2025.03.30 val PER: 0.9494
2025-12-11 18:46:11,008: t15.2025.04.13 val PER: 0.9486
2025-12-11 18:46:57,462: Train batch 24200: loss: 86.60 grad norm: 5700.40 time: 0.217
2025-12-11 18:47:44,141: Train batch 24400: loss: 71.59 grad norm: 7634.01 time: 0.209
2025-12-11 18:48:31,980: Train batch 24600: loss: 72.36 grad norm: 2979.63 time: 0.239
2025-12-11 18:49:20,320: Train batch 24800: loss: 72.05 grad norm: 58196.24 time: 0.179
2025-12-11 18:50:07,553: Train batch 25000: loss: 85.64 grad norm: 1125.47 time: 0.237
2025-12-11 18:50:55,030: Train batch 25200: loss: 78.86 grad norm: 4749.33 time: 0.203
2025-12-11 18:51:42,673: Train batch 25400: loss: 82.58 grad norm: 5920.89 time: 0.229
2025-12-11 18:52:29,933: Train batch 25600: loss: 69.08 grad norm: 8836.01 time: 0.234
2025-12-11 18:53:17,176: Train batch 25800: loss: 66.17 grad norm: 6438.54 time: 0.206
2025-12-11 18:54:02,957: Train batch 26000: loss: 86.12 grad norm: 1866.09 time: 0.207
2025-12-11 18:54:02,957: Running test after training batch: 26000
2025-12-11 18:54:29,493: Val batch 26000: PER (avg): 0.9504 CTC Loss (avg): 233.8837 time: 26.536
2025-12-11 18:54:29,494: t15.2023.08.11 val PER: 1.0000
2025-12-11 18:54:29,494: t15.2023.08.13 val PER: 0.9491
2025-12-11 18:54:29,494: t15.2023.08.18 val PER: 0.9430
2025-12-11 18:54:29,494: t15.2023.08.20 val PER: 0.9476
2025-12-11 18:54:29,494: t15.2023.08.25 val PER: 0.9398
2025-12-11 18:54:29,494: t15.2023.08.27 val PER: 0.9405
2025-12-11 18:54:29,494: t15.2023.09.01 val PER: 0.9456
2025-12-11 18:54:29,494: t15.2023.09.03 val PER: 0.9430
2025-12-11 18:54:29,494: t15.2023.09.24 val PER: 0.9405
2025-12-11 18:54:29,494: t15.2023.09.29 val PER: 0.9541
2025-12-11 18:54:29,494: t15.2023.10.01 val PER: 0.9577
2025-12-11 18:54:29,494: t15.2023.10.06 val PER: 0.9397
2025-12-11 18:54:29,494: t15.2023.10.08 val PER: 0.9675
2025-12-11 18:54:29,494: t15.2023.10.13 val PER: 0.9511
2025-12-11 18:54:29,494: t15.2023.10.15 val PER: 0.9598
2025-12-11 18:54:29,494: t15.2023.10.20 val PER: 0.9597
2025-12-11 18:54:29,495: t15.2023.10.22 val PER: 0.9510
2025-12-11 18:54:29,495: t15.2023.11.03 val PER: 0.9573
2025-12-11 18:54:29,495: t15.2023.11.04 val PER: 0.9283
2025-12-11 18:54:29,495: t15.2023.11.17 val PER: 0.9393
2025-12-11 18:54:29,495: t15.2023.11.19 val PER: 0.9441
2025-12-11 18:54:29,495: t15.2023.11.26 val PER: 0.9587
2025-12-11 18:54:29,495: t15.2023.12.03 val PER: 0.9548
2025-12-11 18:54:29,495: t15.2023.12.08 val PER: 0.9501
2025-12-11 18:54:29,495: t15.2023.12.10 val PER: 0.9514
2025-12-11 18:54:29,495: t15.2023.12.17 val PER: 0.9563
2025-12-11 18:54:29,495: t15.2023.12.29 val PER: 0.9492
2025-12-11 18:54:29,495: t15.2024.02.25 val PER: 0.9551
2025-12-11 18:54:29,495: t15.2024.03.03 val PER: 1.0000
2025-12-11 18:54:29,495: t15.2024.03.08 val PER: 0.9502
2025-12-11 18:54:29,495: t15.2024.03.15 val PER: 0.9531
2025-12-11 18:54:29,495: t15.2024.03.17 val PER: 0.9505
2025-12-11 18:54:29,495: t15.2024.04.25 val PER: 1.0000
2025-12-11 18:54:29,495: t15.2024.04.28 val PER: 1.0000
2025-12-11 18:54:29,495: t15.2024.05.10 val PER: 0.9435
2025-12-11 18:54:29,495: t15.2024.06.14 val PER: 0.9432
2025-12-11 18:54:29,496: t15.2024.07.19 val PER: 0.9499
2025-12-11 18:54:29,496: t15.2024.07.21 val PER: 0.9524
2025-12-11 18:54:29,496: t15.2024.07.28 val PER: 0.9478
2025-12-11 18:54:29,496: t15.2025.01.10 val PER: 0.9559
2025-12-11 18:54:29,496: t15.2025.01.12 val PER: 0.9453
2025-12-11 18:54:29,496: t15.2025.03.14 val PER: 0.9527
2025-12-11 18:54:29,496: t15.2025.03.16 val PER: 0.9516
2025-12-11 18:54:29,496: t15.2025.03.30 val PER: 0.9494
2025-12-11 18:54:29,496: t15.2025.04.13 val PER: 0.9486
2025-12-11 18:55:16,677: Train batch 26200: loss: 84.92 grad norm: 30243.15 time: 0.213
2025-12-11 18:56:04,347: Train batch 26400: loss: 68.79 grad norm: 19466.53 time: 0.209
2025-12-11 18:56:50,328: Train batch 26600: loss: 69.60 grad norm: 8810.31 time: 0.212
2025-12-11 18:57:36,207: Train batch 26800: loss: 106.53 grad norm: 2908.95 time: 0.265
2025-12-11 18:58:22,065: Train batch 27000: loss: 86.93 grad norm: 12337.17 time: 0.255
2025-12-11 18:59:09,778: Train batch 27200: loss: 76.60 grad norm: 8447.03 time: 0.248
2025-12-11 18:59:57,320: Train batch 27400: loss: 84.22 grad norm: 803464.81 time: 0.207
2025-12-11 19:00:44,322: Train batch 27600: loss: 74.07 grad norm: 19782.46 time: 0.193
2025-12-11 19:01:32,468: Train batch 27800: loss: 86.82 grad norm: 5342.32 time: 0.231
2025-12-11 19:02:19,353: Train batch 28000: loss: 78.24 grad norm: 25588.26 time: 0.214
2025-12-11 19:02:19,353: Running test after training batch: 28000
2025-12-11 19:02:45,890: Val batch 28000: PER (avg): 0.9389 CTC Loss (avg): 234.7529 time: 26.537
2025-12-11 19:02:45,890: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:02:45,890: t15.2023.08.13 val PER: 0.9356
2025-12-11 19:02:45,890: t15.2023.08.18 val PER: 0.9304
2025-12-11 19:02:45,891: t15.2023.08.20 val PER: 0.9317
2025-12-11 19:02:45,891: t15.2023.08.25 val PER: 0.9367
2025-12-11 19:02:45,891: t15.2023.08.27 val PER: 0.9341
2025-12-11 19:02:45,891: t15.2023.09.01 val PER: 0.9318
2025-12-11 19:02:45,891: t15.2023.09.03 val PER: 0.9359
2025-12-11 19:02:45,891: t15.2023.09.24 val PER: 0.9284
2025-12-11 19:02:45,891: t15.2023.09.29 val PER: 0.9438
2025-12-11 19:02:45,891: t15.2023.10.01 val PER: 0.9445
2025-12-11 19:02:45,891: t15.2023.10.06 val PER: 0.9333
2025-12-11 19:02:45,891: t15.2023.10.08 val PER: 0.9594
2025-12-11 19:02:45,891: t15.2023.10.13 val PER: 0.9379
2025-12-11 19:02:45,891: t15.2023.10.15 val PER: 0.9492
2025-12-11 19:02:45,891: t15.2023.10.20 val PER: 0.9497
2025-12-11 19:02:45,891: t15.2023.10.22 val PER: 0.9376
2025-12-11 19:02:45,891: t15.2023.11.03 val PER: 0.9430
2025-12-11 19:02:45,891: t15.2023.11.04 val PER: 0.9113
2025-12-11 19:02:45,891: t15.2023.11.17 val PER: 0.9300
2025-12-11 19:02:45,891: t15.2023.11.19 val PER: 0.9301
2025-12-11 19:02:45,892: t15.2023.11.26 val PER: 0.9420
2025-12-11 19:02:45,892: t15.2023.12.03 val PER: 0.9359
2025-12-11 19:02:45,892: t15.2023.12.08 val PER: 0.9381
2025-12-11 19:02:45,892: t15.2023.12.10 val PER: 0.9396
2025-12-11 19:02:45,892: t15.2023.12.17 val PER: 0.9407
2025-12-11 19:02:45,892: t15.2023.12.29 val PER: 0.9410
2025-12-11 19:02:45,892: t15.2024.02.25 val PER: 0.9410
2025-12-11 19:02:45,892: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:02:45,892: t15.2024.03.08 val PER: 0.9403
2025-12-11 19:02:45,892: t15.2024.03.15 val PER: 0.9443
2025-12-11 19:02:45,892: t15.2024.03.17 val PER: 0.9379
2025-12-11 19:02:45,892: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:02:45,892: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:02:45,892: t15.2024.05.10 val PER: 0.9331
2025-12-11 19:02:45,892: t15.2024.06.14 val PER: 0.9290
2025-12-11 19:02:45,892: t15.2024.07.19 val PER: 0.9413
2025-12-11 19:02:45,892: t15.2024.07.21 val PER: 0.9455
2025-12-11 19:02:45,892: t15.2024.07.28 val PER: 0.9375
2025-12-11 19:02:45,892: t15.2025.01.10 val PER: 0.9408
2025-12-11 19:02:45,892: t15.2025.01.12 val PER: 0.9369
2025-12-11 19:02:45,893: t15.2025.03.14 val PER: 0.9349
2025-12-11 19:02:45,893: t15.2025.03.16 val PER: 0.9424
2025-12-11 19:02:45,893: t15.2025.03.30 val PER: 0.9402
2025-12-11 19:02:45,893: t15.2025.04.13 val PER: 0.9387
2025-12-11 19:03:32,406: Train batch 28200: loss: 77.06 grad norm: 41053.56 time: 0.197
2025-12-11 19:04:19,492: Train batch 28400: loss: 71.86 grad norm: 6660.80 time: 0.214
2025-12-11 19:05:05,729: Train batch 28600: loss: 79.25 grad norm: 430911.25 time: 0.219
2025-12-11 19:05:52,858: Train batch 28800: loss: 89.79 grad norm: 47596.03 time: 0.282
2025-12-11 19:06:40,507: Train batch 29000: loss: 76.89 grad norm: 15244.04 time: 0.295
2025-12-11 19:07:28,014: Train batch 29200: loss: 64.61 grad norm: 5866.62 time: 0.239
2025-12-11 19:08:14,898: Train batch 29400: loss: 75.61 grad norm: 4327.70 time: 0.223
2025-12-11 19:09:00,674: Train batch 29600: loss: 75.16 grad norm: 2401.58 time: 0.183
2025-12-11 19:09:47,322: Train batch 29800: loss: 75.52 grad norm: 53185.81 time: 0.220
2025-12-11 19:10:33,631: Train batch 30000: loss: 69.66 grad norm: 1997.37 time: 0.175
2025-12-11 19:10:33,631: Running test after training batch: 30000
2025-12-11 19:11:00,423: Val batch 30000: PER (avg): 0.9504 CTC Loss (avg): 165.6828 time: 26.792
2025-12-11 19:11:00,423: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:11:00,423: t15.2023.08.13 val PER: 0.9491
2025-12-11 19:11:00,423: t15.2023.08.18 val PER: 0.9430
2025-12-11 19:11:00,423: t15.2023.08.20 val PER: 0.9476
2025-12-11 19:11:00,423: t15.2023.08.25 val PER: 0.9398
2025-12-11 19:11:00,423: t15.2023.08.27 val PER: 0.9405
2025-12-11 19:11:00,423: t15.2023.09.01 val PER: 0.9456
2025-12-11 19:11:00,423: t15.2023.09.03 val PER: 0.9430
2025-12-11 19:11:00,423: t15.2023.09.24 val PER: 0.9405
2025-12-11 19:11:00,423: t15.2023.09.29 val PER: 0.9541
2025-12-11 19:11:00,424: t15.2023.10.01 val PER: 0.9577
2025-12-11 19:11:00,424: t15.2023.10.06 val PER: 0.9397
2025-12-11 19:11:00,424: t15.2023.10.08 val PER: 0.9675
2025-12-11 19:11:00,424: t15.2023.10.13 val PER: 0.9511
2025-12-11 19:11:00,424: t15.2023.10.15 val PER: 0.9598
2025-12-11 19:11:00,424: t15.2023.10.20 val PER: 0.9597
2025-12-11 19:11:00,424: t15.2023.10.22 val PER: 0.9510
2025-12-11 19:11:00,424: t15.2023.11.03 val PER: 0.9573
2025-12-11 19:11:00,424: t15.2023.11.04 val PER: 0.9283
2025-12-11 19:11:00,424: t15.2023.11.17 val PER: 0.9393
2025-12-11 19:11:00,424: t15.2023.11.19 val PER: 0.9441
2025-12-11 19:11:00,424: t15.2023.11.26 val PER: 0.9587
2025-12-11 19:11:00,424: t15.2023.12.03 val PER: 0.9548
2025-12-11 19:11:00,424: t15.2023.12.08 val PER: 0.9501
2025-12-11 19:11:00,424: t15.2023.12.10 val PER: 0.9514
2025-12-11 19:11:00,424: t15.2023.12.17 val PER: 0.9563
2025-12-11 19:11:00,424: t15.2023.12.29 val PER: 0.9492
2025-12-11 19:11:00,424: t15.2024.02.25 val PER: 0.9551
2025-12-11 19:11:00,425: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:11:00,425: t15.2024.03.08 val PER: 0.9502
2025-12-11 19:11:00,425: t15.2024.03.15 val PER: 0.9531
2025-12-11 19:11:00,425: t15.2024.03.17 val PER: 0.9505
2025-12-11 19:11:00,425: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:11:00,425: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:11:00,425: t15.2024.05.10 val PER: 0.9435
2025-12-11 19:11:00,425: t15.2024.06.14 val PER: 0.9432
2025-12-11 19:11:00,425: t15.2024.07.19 val PER: 0.9499
2025-12-11 19:11:00,425: t15.2024.07.21 val PER: 0.9524
2025-12-11 19:11:00,425: t15.2024.07.28 val PER: 0.9478
2025-12-11 19:11:00,425: t15.2025.01.10 val PER: 0.9559
2025-12-11 19:11:00,425: t15.2025.01.12 val PER: 0.9453
2025-12-11 19:11:00,425: t15.2025.03.14 val PER: 0.9527
2025-12-11 19:11:00,425: t15.2025.03.16 val PER: 0.9516
2025-12-11 19:11:00,425: t15.2025.03.30 val PER: 0.9494
2025-12-11 19:11:00,425: t15.2025.04.13 val PER: 0.9486
2025-12-11 19:11:45,954: Train batch 30200: loss: 79.80 grad norm: 9901.39 time: 0.219
2025-12-11 19:12:32,681: Train batch 30400: loss: 79.61 grad norm: 14071.65 time: 0.223
2025-12-11 19:13:19,621: Train batch 30600: loss: 73.90 grad norm: 9119.89 time: 0.210
2025-12-11 19:14:06,240: Train batch 30800: loss: 75.96 grad norm: 100626.22 time: 0.222
2025-12-11 19:14:53,305: Train batch 31000: loss: 65.15 grad norm: 86948.25 time: 0.230
2025-12-11 19:15:39,128: Train batch 31200: loss: 66.81 grad norm: 2578603.75 time: 0.309
2025-12-11 19:16:25,914: Train batch 31400: loss: 76.89 grad norm: 37779.01 time: 0.256
2025-12-11 19:17:13,585: Train batch 31600: loss: 73.67 grad norm: 20223.45 time: 0.245
2025-12-11 19:18:01,297: Train batch 31800: loss: 85.68 grad norm: 59537.53 time: 0.249
2025-12-11 19:18:49,177: Train batch 32000: loss: 63.54 grad norm: 16036.94 time: 0.150
2025-12-11 19:18:49,177: Running test after training batch: 32000
2025-12-11 19:19:15,677: Val batch 32000: PER (avg): 0.9504 CTC Loss (avg): 197.1546 time: 26.500
2025-12-11 19:19:15,677: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:19:15,677: t15.2023.08.13 val PER: 0.9491
2025-12-11 19:19:15,677: t15.2023.08.18 val PER: 0.9430
2025-12-11 19:19:15,677: t15.2023.08.20 val PER: 0.9476
2025-12-11 19:19:15,678: t15.2023.08.25 val PER: 0.9398
2025-12-11 19:19:15,678: t15.2023.08.27 val PER: 0.9405
2025-12-11 19:19:15,678: t15.2023.09.01 val PER: 0.9456
2025-12-11 19:19:15,678: t15.2023.09.03 val PER: 0.9430
2025-12-11 19:19:15,678: t15.2023.09.24 val PER: 0.9405
2025-12-11 19:19:15,678: t15.2023.09.29 val PER: 0.9541
2025-12-11 19:19:15,678: t15.2023.10.01 val PER: 0.9577
2025-12-11 19:19:15,678: t15.2023.10.06 val PER: 0.9397
2025-12-11 19:19:15,678: t15.2023.10.08 val PER: 0.9675
2025-12-11 19:19:15,678: t15.2023.10.13 val PER: 0.9511
2025-12-11 19:19:15,678: t15.2023.10.15 val PER: 0.9598
2025-12-11 19:19:15,678: t15.2023.10.20 val PER: 0.9597
2025-12-11 19:19:15,678: t15.2023.10.22 val PER: 0.9510
2025-12-11 19:19:15,678: t15.2023.11.03 val PER: 0.9573
2025-12-11 19:19:15,678: t15.2023.11.04 val PER: 0.9283
2025-12-11 19:19:15,678: t15.2023.11.17 val PER: 0.9393
2025-12-11 19:19:15,678: t15.2023.11.19 val PER: 0.9441
2025-12-11 19:19:15,678: t15.2023.11.26 val PER: 0.9587
2025-12-11 19:19:15,678: t15.2023.12.03 val PER: 0.9548
2025-12-11 19:19:15,679: t15.2023.12.08 val PER: 0.9501
2025-12-11 19:19:15,679: t15.2023.12.10 val PER: 0.9514
2025-12-11 19:19:15,679: t15.2023.12.17 val PER: 0.9563
2025-12-11 19:19:15,679: t15.2023.12.29 val PER: 0.9492
2025-12-11 19:19:15,679: t15.2024.02.25 val PER: 0.9551
2025-12-11 19:19:15,679: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:19:15,679: t15.2024.03.08 val PER: 0.9502
2025-12-11 19:19:15,679: t15.2024.03.15 val PER: 0.9531
2025-12-11 19:19:15,679: t15.2024.03.17 val PER: 0.9505
2025-12-11 19:19:15,679: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:19:15,679: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:19:15,679: t15.2024.05.10 val PER: 0.9435
2025-12-11 19:19:15,679: t15.2024.06.14 val PER: 0.9432
2025-12-11 19:19:15,679: t15.2024.07.19 val PER: 0.9499
2025-12-11 19:19:15,679: t15.2024.07.21 val PER: 0.9524
2025-12-11 19:19:15,679: t15.2024.07.28 val PER: 0.9478
2025-12-11 19:19:15,679: t15.2025.01.10 val PER: 0.9559
2025-12-11 19:19:15,679: t15.2025.01.12 val PER: 0.9453
2025-12-11 19:19:15,680: t15.2025.03.14 val PER: 0.9527
2025-12-11 19:19:15,680: t15.2025.03.16 val PER: 0.9516
2025-12-11 19:19:15,680: t15.2025.03.30 val PER: 0.9494
2025-12-11 19:19:15,680: t15.2025.04.13 val PER: 0.9486
2025-12-11 19:20:02,659: Train batch 32200: loss: 64.55 grad norm: 4998.93 time: 0.218
2025-12-11 19:20:50,892: Train batch 32400: loss: 86.73 grad norm: 75379.70 time: 0.271
2025-12-11 19:21:38,447: Train batch 32600: loss: 69.86 grad norm: 109508.17 time: 0.263
2025-12-11 19:22:25,467: Train batch 32800: loss: 87.49 grad norm: 35170.22 time: 0.210
2025-12-11 19:23:12,817: Train batch 33000: loss: 65.76 grad norm: 184033.34 time: 0.187
2025-12-11 19:24:00,444: Train batch 33200: loss: 101.36 grad norm: 443809.31 time: 0.237
2025-12-11 19:24:48,099: Train batch 33400: loss: 78.88 grad norm: 985434.50 time: 0.216
2025-12-11 19:25:34,907: Train batch 33600: loss: 80.92 grad norm: 1172749.12 time: 0.278
2025-12-11 19:26:21,374: Train batch 33800: loss: 85.67 grad norm: 2482495.25 time: 0.264
2025-12-11 19:27:07,913: Train batch 34000: loss: 88.01 grad norm: 24651.41 time: 0.219
2025-12-11 19:27:07,913: Running test after training batch: 34000
2025-12-11 19:27:34,596: Val batch 34000: PER (avg): 1.0000 CTC Loss (avg): 187.4644 time: 26.682
2025-12-11 19:27:34,596: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.08.13 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.08.18 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.08.20 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.08.25 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.08.27 val PER: 1.0000
2025-12-11 19:27:34,596: t15.2023.09.01 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.09.03 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.09.24 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.09.29 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.01 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.06 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.08 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.13 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.15 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.20 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.10.22 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.11.03 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.11.04 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.11.17 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.11.19 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.11.26 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.12.03 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.12.08 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.12.10 val PER: 1.0000
2025-12-11 19:27:34,597: t15.2023.12.17 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2023.12.29 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.02.25 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.03.08 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.03.15 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.03.17 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.05.10 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.06.14 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.07.19 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.07.21 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2024.07.28 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.01.10 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.01.12 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.03.14 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.03.16 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.03.30 val PER: 1.0000
2025-12-11 19:27:34,598: t15.2025.04.13 val PER: 1.0000
2025-12-11 19:28:21,715: Train batch 34200: loss: 87.98 grad norm: 102729.93 time: 0.213
2025-12-11 19:29:08,752: Train batch 34400: loss: 88.99 grad norm: 6028.22 time: 0.252
2025-12-11 19:29:55,775: Train batch 34600: loss: 84.38 grad norm: 4071.32 time: 0.199
2025-12-11 19:30:43,031: Train batch 34800: loss: 76.79 grad norm: 22117.86 time: 0.215
2025-12-11 19:31:30,215: Train batch 35000: loss: 75.87 grad norm: 12261.61 time: 0.236
2025-12-11 19:32:17,809: Train batch 35200: loss: 86.49 grad norm: 31313.99 time: 0.353
2025-12-11 19:33:04,580: Train batch 35400: loss: 83.40 grad norm: 67056.91 time: 0.247
2025-12-11 19:33:51,663: Train batch 35600: loss: 94.49 grad norm: 300501.28 time: 0.351
2025-12-11 19:34:38,765: Train batch 35800: loss: 82.78 grad norm: 20183.50 time: 0.226
2025-12-11 19:35:25,846: Train batch 36000: loss: 79.43 grad norm: 20134.24 time: 0.280
2025-12-11 19:35:25,847: Running test after training batch: 36000
2025-12-11 19:35:52,391: Val batch 36000: PER (avg): 0.9504 CTC Loss (avg): 205.5794 time: 26.544
2025-12-11 19:35:52,391: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:35:52,391: t15.2023.08.13 val PER: 0.9491
2025-12-11 19:35:52,391: t15.2023.08.18 val PER: 0.9430
2025-12-11 19:35:52,391: t15.2023.08.20 val PER: 0.9476
2025-12-11 19:35:52,391: t15.2023.08.25 val PER: 0.9398
2025-12-11 19:35:52,391: t15.2023.08.27 val PER: 0.9405
2025-12-11 19:35:52,391: t15.2023.09.01 val PER: 0.9456
2025-12-11 19:35:52,391: t15.2023.09.03 val PER: 0.9430
2025-12-11 19:35:52,391: t15.2023.09.24 val PER: 0.9405
2025-12-11 19:35:52,391: t15.2023.09.29 val PER: 0.9541
2025-12-11 19:35:52,391: t15.2023.10.01 val PER: 0.9577
2025-12-11 19:35:52,391: t15.2023.10.06 val PER: 0.9397
2025-12-11 19:35:52,392: t15.2023.10.08 val PER: 0.9675
2025-12-11 19:35:52,392: t15.2023.10.13 val PER: 0.9511
2025-12-11 19:35:52,392: t15.2023.10.15 val PER: 0.9598
2025-12-11 19:35:52,392: t15.2023.10.20 val PER: 0.9597
2025-12-11 19:35:52,392: t15.2023.10.22 val PER: 0.9510
2025-12-11 19:35:52,392: t15.2023.11.03 val PER: 0.9573
2025-12-11 19:35:52,392: t15.2023.11.04 val PER: 0.9283
2025-12-11 19:35:52,392: t15.2023.11.17 val PER: 0.9393
2025-12-11 19:35:52,392: t15.2023.11.19 val PER: 0.9441
2025-12-11 19:35:52,392: t15.2023.11.26 val PER: 0.9587
2025-12-11 19:35:52,392: t15.2023.12.03 val PER: 0.9548
2025-12-11 19:35:52,392: t15.2023.12.08 val PER: 0.9501
2025-12-11 19:35:52,392: t15.2023.12.10 val PER: 0.9514
2025-12-11 19:35:52,392: t15.2023.12.17 val PER: 0.9563
2025-12-11 19:35:52,392: t15.2023.12.29 val PER: 0.9492
2025-12-11 19:35:52,392: t15.2024.02.25 val PER: 0.9551
2025-12-11 19:35:52,392: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:35:52,392: t15.2024.03.08 val PER: 0.9502
2025-12-11 19:35:52,392: t15.2024.03.15 val PER: 0.9531
2025-12-11 19:35:52,393: t15.2024.03.17 val PER: 0.9505
2025-12-11 19:35:52,393: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:35:52,393: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:35:52,393: t15.2024.05.10 val PER: 0.9435
2025-12-11 19:35:52,393: t15.2024.06.14 val PER: 0.9432
2025-12-11 19:35:52,393: t15.2024.07.19 val PER: 0.9499
2025-12-11 19:35:52,393: t15.2024.07.21 val PER: 0.9524
2025-12-11 19:35:52,393: t15.2024.07.28 val PER: 0.9478
2025-12-11 19:35:52,393: t15.2025.01.10 val PER: 0.9559
2025-12-11 19:35:52,393: t15.2025.01.12 val PER: 0.9453
2025-12-11 19:35:52,393: t15.2025.03.14 val PER: 0.9527
2025-12-11 19:35:52,393: t15.2025.03.16 val PER: 0.9516
2025-12-11 19:35:52,393: t15.2025.03.30 val PER: 0.9494
2025-12-11 19:35:52,393: t15.2025.04.13 val PER: 0.9486
2025-12-11 19:36:39,217: Train batch 36200: loss: 78.40 grad norm: 2060.53 time: 0.341
2025-12-11 19:37:25,892: Train batch 36400: loss: 82.68 grad norm: 4898.65 time: 0.243
2025-12-11 19:38:11,630: Train batch 36600: loss: 80.28 grad norm: 14265.86 time: 0.202
2025-12-11 19:38:59,208: Train batch 36800: loss: 91.73 grad norm: 27080.47 time: 0.215
2025-12-11 19:39:45,650: Train batch 37000: loss: 74.18 grad norm: 4601.87 time: 0.253
2025-12-11 19:40:31,103: Train batch 37200: loss: 89.10 grad norm: 38208.09 time: 0.215
2025-12-11 19:41:16,912: Train batch 37400: loss: 73.27 grad norm: 9831.29 time: 0.163
2025-12-11 19:42:04,105: Train batch 37600: loss: 81.69 grad norm: 7047.49 time: 0.203
2025-12-11 19:42:50,891: Train batch 37800: loss: 83.20 grad norm: 22232.39 time: 0.243
2025-12-11 19:43:37,944: Train batch 38000: loss: 87.97 grad norm: 40577.64 time: 0.220
2025-12-11 19:43:37,944: Running test after training batch: 38000
2025-12-11 19:44:04,431: Val batch 38000: PER (avg): 0.9504 CTC Loss (avg): 180.9642 time: 26.487
2025-12-11 19:44:04,431: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:44:04,431: t15.2023.08.13 val PER: 0.9491
2025-12-11 19:44:04,431: t15.2023.08.18 val PER: 0.9430
2025-12-11 19:44:04,431: t15.2023.08.20 val PER: 0.9476
2025-12-11 19:44:04,431: t15.2023.08.25 val PER: 0.9398
2025-12-11 19:44:04,431: t15.2023.08.27 val PER: 0.9405
2025-12-11 19:44:04,431: t15.2023.09.01 val PER: 0.9456
2025-12-11 19:44:04,431: t15.2023.09.03 val PER: 0.9430
2025-12-11 19:44:04,431: t15.2023.09.24 val PER: 0.9405
2025-12-11 19:44:04,432: t15.2023.09.29 val PER: 0.9541
2025-12-11 19:44:04,432: t15.2023.10.01 val PER: 0.9577
2025-12-11 19:44:04,432: t15.2023.10.06 val PER: 0.9397
2025-12-11 19:44:04,432: t15.2023.10.08 val PER: 0.9675
2025-12-11 19:44:04,432: t15.2023.10.13 val PER: 0.9511
2025-12-11 19:44:04,432: t15.2023.10.15 val PER: 0.9598
2025-12-11 19:44:04,432: t15.2023.10.20 val PER: 0.9597
2025-12-11 19:44:04,432: t15.2023.10.22 val PER: 0.9510
2025-12-11 19:44:04,432: t15.2023.11.03 val PER: 0.9573
2025-12-11 19:44:04,432: t15.2023.11.04 val PER: 0.9283
2025-12-11 19:44:04,432: t15.2023.11.17 val PER: 0.9393
2025-12-11 19:44:04,432: t15.2023.11.19 val PER: 0.9441
2025-12-11 19:44:04,432: t15.2023.11.26 val PER: 0.9587
2025-12-11 19:44:04,432: t15.2023.12.03 val PER: 0.9548
2025-12-11 19:44:04,432: t15.2023.12.08 val PER: 0.9501
2025-12-11 19:44:04,432: t15.2023.12.10 val PER: 0.9514
2025-12-11 19:44:04,432: t15.2023.12.17 val PER: 0.9563
2025-12-11 19:44:04,432: t15.2023.12.29 val PER: 0.9492
2025-12-11 19:44:04,432: t15.2024.02.25 val PER: 0.9551
2025-12-11 19:44:04,433: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:44:04,433: t15.2024.03.08 val PER: 0.9502
2025-12-11 19:44:04,433: t15.2024.03.15 val PER: 0.9531
2025-12-11 19:44:04,433: t15.2024.03.17 val PER: 0.9505
2025-12-11 19:44:04,433: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:44:04,433: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:44:04,433: t15.2024.05.10 val PER: 0.9435
2025-12-11 19:44:04,433: t15.2024.06.14 val PER: 0.9432
2025-12-11 19:44:04,433: t15.2024.07.19 val PER: 0.9499
2025-12-11 19:44:04,433: t15.2024.07.21 val PER: 0.9524
2025-12-11 19:44:04,433: t15.2024.07.28 val PER: 0.9478
2025-12-11 19:44:04,433: t15.2025.01.10 val PER: 0.9559
2025-12-11 19:44:04,433: t15.2025.01.12 val PER: 0.9453
2025-12-11 19:44:04,433: t15.2025.03.14 val PER: 0.9527
2025-12-11 19:44:04,433: t15.2025.03.16 val PER: 0.9516
2025-12-11 19:44:04,433: t15.2025.03.30 val PER: 0.9494
2025-12-11 19:44:04,433: t15.2025.04.13 val PER: 0.9486
2025-12-11 19:44:51,419: Train batch 38200: loss: 78.93 grad norm: 5214.03 time: 0.181
2025-12-11 19:45:37,950: Train batch 38400: loss: 67.85 grad norm: 27800.07 time: 0.319
2025-12-11 19:46:24,160: Train batch 38600: loss: 45.68 grad norm: 3283.38 time: 0.173
2025-12-11 19:47:10,400: Train batch 38800: loss: 81.19 grad norm: 256195.36 time: 0.214
2025-12-11 19:47:56,127: Train batch 39000: loss: 90.01 grad norm: 6724.17 time: 0.216
2025-12-11 19:48:43,527: Train batch 39200: loss: 71.99 grad norm: 19558.58 time: 0.231
2025-12-11 19:49:31,963: Train batch 39400: loss: 85.83 grad norm: 18984.07 time: 0.180
2025-12-11 19:50:19,182: Train batch 39600: loss: 91.85 grad norm: 3324.58 time: 0.234
2025-12-11 19:51:04,484: Train batch 39800: loss: 73.66 grad norm: 8726.70 time: 0.228
2025-12-11 19:51:51,470: Train batch 40000: loss: 86.05 grad norm: 3504.11 time: 0.245
2025-12-11 19:51:51,470: Running test after training batch: 40000
2025-12-11 19:52:17,922: Val batch 40000: PER (avg): 0.9504 CTC Loss (avg): 161.0228 time: 26.451
2025-12-11 19:52:17,922: t15.2023.08.11 val PER: 1.0000
2025-12-11 19:52:17,922: t15.2023.08.13 val PER: 0.9491
2025-12-11 19:52:17,922: t15.2023.08.18 val PER: 0.9430
2025-12-11 19:52:17,922: t15.2023.08.20 val PER: 0.9476
2025-12-11 19:52:17,922: t15.2023.08.25 val PER: 0.9398
2025-12-11 19:52:17,922: t15.2023.08.27 val PER: 0.9405
2025-12-11 19:52:17,922: t15.2023.09.01 val PER: 0.9456
2025-12-11 19:52:17,922: t15.2023.09.03 val PER: 0.9430
2025-12-11 19:52:17,922: t15.2023.09.24 val PER: 0.9405
2025-12-11 19:52:17,923: t15.2023.09.29 val PER: 0.9541
2025-12-11 19:52:17,923: t15.2023.10.01 val PER: 0.9577
2025-12-11 19:52:17,923: t15.2023.10.06 val PER: 0.9397
2025-12-11 19:52:17,923: t15.2023.10.08 val PER: 0.9675
2025-12-11 19:52:17,923: t15.2023.10.13 val PER: 0.9511
2025-12-11 19:52:17,923: t15.2023.10.15 val PER: 0.9598
2025-12-11 19:52:17,923: t15.2023.10.20 val PER: 0.9597
2025-12-11 19:52:17,923: t15.2023.10.22 val PER: 0.9510
2025-12-11 19:52:17,923: t15.2023.11.03 val PER: 0.9573
2025-12-11 19:52:17,923: t15.2023.11.04 val PER: 0.9283
2025-12-11 19:52:17,923: t15.2023.11.17 val PER: 0.9393
2025-12-11 19:52:17,923: t15.2023.11.19 val PER: 0.9441
2025-12-11 19:52:17,923: t15.2023.11.26 val PER: 0.9587
2025-12-11 19:52:17,923: t15.2023.12.03 val PER: 0.9548
2025-12-11 19:52:17,923: t15.2023.12.08 val PER: 0.9501
2025-12-11 19:52:17,923: t15.2023.12.10 val PER: 0.9514
2025-12-11 19:52:17,923: t15.2023.12.17 val PER: 0.9563
2025-12-11 19:52:17,923: t15.2023.12.29 val PER: 0.9492
2025-12-11 19:52:17,924: t15.2024.02.25 val PER: 0.9551
2025-12-11 19:52:17,924: t15.2024.03.03 val PER: 1.0000
2025-12-11 19:52:17,924: t15.2024.03.08 val PER: 0.9502
2025-12-11 19:52:17,924: t15.2024.03.15 val PER: 0.9531
2025-12-11 19:52:17,924: t15.2024.03.17 val PER: 0.9505
2025-12-11 19:52:17,924: t15.2024.04.25 val PER: 1.0000
2025-12-11 19:52:17,924: t15.2024.04.28 val PER: 1.0000
2025-12-11 19:52:17,924: t15.2024.05.10 val PER: 0.9435
2025-12-11 19:52:17,924: t15.2024.06.14 val PER: 0.9432
2025-12-11 19:52:17,924: t15.2024.07.19 val PER: 0.9499
2025-12-11 19:52:17,924: t15.2024.07.21 val PER: 0.9524
2025-12-11 19:52:17,924: t15.2024.07.28 val PER: 0.9478
2025-12-11 19:52:17,924: t15.2025.01.10 val PER: 0.9559
2025-12-11 19:52:17,924: t15.2025.01.12 val PER: 0.9453
2025-12-11 19:52:17,924: t15.2025.03.14 val PER: 0.9527
2025-12-11 19:52:17,924: t15.2025.03.16 val PER: 0.9516
2025-12-11 19:52:17,924: t15.2025.03.30 val PER: 0.9494
2025-12-11 19:52:17,924: t15.2025.04.13 val PER: 0.9486
2025-12-11 19:53:04,700: Train batch 40200: loss: 88.05 grad norm: 18031.22 time: 0.267
2025-12-11 19:53:51,498: Train batch 40400: loss: 67.25 grad norm: 2084.00 time: 0.215
2025-12-11 19:54:38,972: Train batch 40600: loss: 82.56 grad norm: 13776.30 time: 0.232
2025-12-11 19:55:26,388: Train batch 40800: loss: 77.09 grad norm: 2840.55 time: 0.255
2025-12-11 19:56:13,297: Train batch 41000: loss: 75.19 grad norm: 321660.31 time: 0.318
2025-12-11 19:57:00,176: Train batch 41200: loss: 85.47 grad norm: 18350.65 time: 0.224
2025-12-11 19:57:47,459: Train batch 41400: loss: 75.12 grad norm: 92523.59 time: 0.210
2025-12-11 19:58:34,900: Train batch 41600: loss: 81.39 grad norm: 1327.76 time: 0.222
2025-12-11 19:59:22,149: Train batch 41800: loss: 74.81 grad norm: 633.37 time: 0.184
2025-12-11 20:00:08,284: Train batch 42000: loss: 83.27 grad norm: 1216.95 time: 0.195
2025-12-11 20:00:08,284: Running test after training batch: 42000
2025-12-11 20:00:34,596: Val batch 42000: PER (avg): 0.9504 CTC Loss (avg): 169.2043 time: 26.312
2025-12-11 20:00:34,596: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:00:34,596: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:00:34,596: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:00:34,596: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:00:34,596: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:00:34,597: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:00:34,597: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:00:34,597: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:00:34,597: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:00:34,597: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:00:34,597: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:00:34,597: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:00:34,597: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:00:34,597: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:00:34,597: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:00:34,597: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:00:34,597: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:00:34,597: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:00:34,597: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:00:34,597: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:00:34,597: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:00:34,597: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:00:34,597: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:00:34,597: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:00:34,598: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:00:34,598: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:00:34,598: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:00:34,598: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:00:34,598: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:00:34,598: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:00:34,598: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:00:34,598: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:00:34,598: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:00:34,598: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:00:34,598: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:00:34,598: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:00:34,598: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:00:34,598: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:00:34,598: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:00:34,598: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:00:34,598: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:00:34,598: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:00:34,598: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:00:34,599: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:00:34,599: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:01:21,090: Train batch 42200: loss: 79.23 grad norm: 20170.41 time: 0.237
2025-12-11 20:02:08,288: Train batch 42400: loss: 73.38 grad norm: 6334.85 time: 0.257
2025-12-11 20:02:55,552: Train batch 42600: loss: 79.20 grad norm: 24974.10 time: 0.214
2025-12-11 20:03:42,395: Train batch 42800: loss: 84.32 grad norm: 24237.62 time: 0.207
2025-12-11 20:04:29,057: Train batch 43000: loss: 82.35 grad norm: 387.06 time: 0.191
2025-12-11 20:05:15,751: Train batch 43200: loss: 98.54 grad norm: 12646.30 time: 0.355
2025-12-11 20:06:03,157: Train batch 43400: loss: 69.94 grad norm: 1172.64 time: 0.199
2025-12-11 20:06:49,603: Train batch 43600: loss: 78.21 grad norm: 1827.51 time: 0.174
2025-12-11 20:07:36,993: Train batch 43800: loss: 74.03 grad norm: 240228.75 time: 0.305
2025-12-11 20:08:24,520: Train batch 44000: loss: 92.63 grad norm: 13424.25 time: 0.205
2025-12-11 20:08:24,520: Running test after training batch: 44000
2025-12-11 20:08:51,120: Val batch 44000: PER (avg): 0.9504 CTC Loss (avg): 139.5979 time: 26.599
2025-12-11 20:08:51,120: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:08:51,120: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:08:51,120: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:08:51,120: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:08:51,120: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:08:51,120: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:08:51,120: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:08:51,121: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:08:51,121: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:08:51,121: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:08:51,121: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:08:51,121: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:08:51,121: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:08:51,121: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:08:51,121: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:08:51,121: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:08:51,121: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:08:51,121: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:08:51,121: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:08:51,121: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:08:51,121: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:08:51,121: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:08:51,121: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:08:51,121: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:08:51,121: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:08:51,121: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:08:51,122: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:08:51,122: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:08:51,122: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:08:51,122: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:08:51,122: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:08:51,122: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:08:51,122: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:08:51,122: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:08:51,122: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:08:51,122: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:08:51,122: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:08:51,122: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:08:51,122: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:08:51,122: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:08:51,122: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:08:51,122: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:08:51,122: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:08:51,122: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:08:51,122: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:09:38,405: Train batch 44200: loss: 78.32 grad norm: 7204.93 time: 0.255
2025-12-11 20:10:24,320: Train batch 44400: loss: 76.74 grad norm: 45341.31 time: 0.232
2025-12-11 20:11:12,087: Train batch 44600: loss: 91.38 grad norm: 2229.28 time: 0.254
2025-12-11 20:11:58,770: Train batch 44800: loss: 72.75 grad norm: 7571.07 time: 0.237
2025-12-11 20:12:45,497: Train batch 45000: loss: 82.74 grad norm: 9852.88 time: 0.207
2025-12-11 20:13:32,453: Train batch 45200: loss: 85.65 grad norm: 8978.40 time: 0.247
2025-12-11 20:14:19,633: Train batch 45400: loss: 90.99 grad norm: 1138.32 time: 0.284
2025-12-11 20:15:06,579: Train batch 45600: loss: 70.12 grad norm: 8171.96 time: 0.242
2025-12-11 20:15:53,403: Train batch 45800: loss: 72.46 grad norm: 17070.91 time: 0.199
2025-12-11 20:16:41,223: Train batch 46000: loss: 76.36 grad norm: 22195.70 time: 0.259
2025-12-11 20:16:41,224: Running test after training batch: 46000
2025-12-11 20:17:07,632: Val batch 46000: PER (avg): 0.9504 CTC Loss (avg): 154.1859 time: 26.409
2025-12-11 20:17:07,633: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:17:07,633: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:17:07,633: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:17:07,633: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:17:07,633: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:17:07,633: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:17:07,633: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:17:07,633: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:17:07,633: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:17:07,633: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:17:07,633: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:17:07,633: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:17:07,633: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:17:07,633: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:17:07,633: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:17:07,634: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:17:07,634: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:17:07,634: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:17:07,634: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:17:07,634: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:17:07,634: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:17:07,634: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:17:07,634: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:17:07,634: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:17:07,634: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:17:07,634: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:17:07,634: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:17:07,634: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:17:07,634: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:17:07,634: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:17:07,634: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:17:07,634: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:17:07,634: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:17:07,634: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:17:07,635: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:17:07,635: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:17:07,635: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:17:07,635: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:17:07,635: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:17:07,635: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:17:07,635: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:17:07,635: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:17:07,635: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:17:07,635: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:17:07,635: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:17:54,898: Train batch 46200: loss: 73.24 grad norm: 497.51 time: 0.206
2025-12-11 20:18:41,342: Train batch 46400: loss: 64.15 grad norm: 29749.04 time: 0.190
2025-12-11 20:19:28,927: Train batch 46600: loss: 77.71 grad norm: 20378.69 time: 0.224
2025-12-11 20:20:17,836: Train batch 46800: loss: 72.68 grad norm: 5163.35 time: 0.308
2025-12-11 20:21:05,613: Train batch 47000: loss: 80.64 grad norm: 53608.84 time: 0.225
2025-12-11 20:21:52,282: Train batch 47200: loss: 81.43 grad norm: 1739.36 time: 0.234
2025-12-11 20:22:39,347: Train batch 47400: loss: 80.10 grad norm: 6387.91 time: 0.205
2025-12-11 20:23:26,876: Train batch 47600: loss: 73.05 grad norm: 79821.73 time: 0.199
2025-12-11 20:24:14,342: Train batch 47800: loss: 89.60 grad norm: 554779.44 time: 0.283
2025-12-11 20:25:01,257: Train batch 48000: loss: 77.10 grad norm: 1741.45 time: 0.243
2025-12-11 20:25:01,257: Running test after training batch: 48000
2025-12-11 20:25:27,632: Val batch 48000: PER (avg): 0.9504 CTC Loss (avg): 158.3583 time: 26.374
2025-12-11 20:25:27,632: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:25:27,632: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:25:27,632: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:25:27,632: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:25:27,632: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:25:27,632: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:25:27,632: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:25:27,632: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:25:27,632: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:25:27,633: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:25:27,633: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:25:27,633: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:25:27,633: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:25:27,633: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:25:27,633: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:25:27,633: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:25:27,633: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:25:27,633: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:25:27,633: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:25:27,633: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:25:27,633: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:25:27,633: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:25:27,633: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:25:27,633: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:25:27,633: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:25:27,633: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:25:27,633: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:25:27,633: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:25:27,634: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:25:27,634: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:25:27,634: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:25:27,634: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:25:27,634: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:25:27,634: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:25:27,634: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:25:27,634: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:25:27,634: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:25:27,634: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:25:27,634: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:25:27,634: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:25:27,634: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:25:27,634: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:25:27,634: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:25:27,634: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:25:27,634: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:26:14,670: Train batch 48200: loss: 71.19 grad norm: 9554.69 time: 0.158
2025-12-11 20:27:00,533: Train batch 48400: loss: 57.22 grad norm: 4180.79 time: 0.193
2025-12-11 20:27:48,001: Train batch 48600: loss: 70.25 grad norm: 3005.86 time: 0.202
2025-12-11 20:28:35,335: Train batch 48800: loss: 69.14 grad norm: 224.00 time: 0.205
2025-12-11 20:29:22,277: Train batch 49000: loss: 82.39 grad norm: 19811.39 time: 0.199
2025-12-11 20:30:10,336: Train batch 49200: loss: 79.11 grad norm: 650924.31 time: 0.215
2025-12-11 20:30:56,723: Train batch 49400: loss: 64.46 grad norm: 941.99 time: 0.204
2025-12-11 20:31:43,951: Train batch 49600: loss: 80.94 grad norm: 142899.70 time: 0.283
2025-12-11 20:32:29,940: Train batch 49800: loss: 67.42 grad norm: 846.51 time: 0.203
2025-12-11 20:33:17,402: Train batch 50000: loss: 76.57 grad norm: 428.58 time: 0.191
2025-12-11 20:33:17,402: Running test after training batch: 50000
2025-12-11 20:33:43,833: Val batch 50000: PER (avg): 0.9504 CTC Loss (avg): 162.1009 time: 26.431
2025-12-11 20:33:43,834: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:33:43,834: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:33:43,834: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:33:43,834: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:33:43,834: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:33:43,834: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:33:43,834: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:33:43,834: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:33:43,834: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:33:43,834: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:33:43,834: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:33:43,834: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:33:43,834: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:33:43,834: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:33:43,834: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:33:43,835: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:33:43,835: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:33:43,835: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:33:43,835: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:33:43,835: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:33:43,835: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:33:43,835: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:33:43,835: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:33:43,835: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:33:43,835: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:33:43,835: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:33:43,835: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:33:43,835: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:33:43,835: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:33:43,835: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:33:43,835: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:33:43,835: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:33:43,835: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:33:43,835: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:33:43,836: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:33:43,836: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:33:43,836: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:33:43,836: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:33:43,836: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:33:43,836: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:33:43,836: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:33:43,836: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:33:43,836: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:33:43,836: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:33:43,836: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:34:30,842: Train batch 50200: loss: 83.27 grad norm: 17461.52 time: 0.246
2025-12-11 20:35:15,269: Train batch 50400: loss: 73.20 grad norm: 117351.55 time: 0.317
2025-12-11 20:36:02,107: Train batch 50600: loss: 89.91 grad norm: 101738.09 time: 0.195
2025-12-11 20:36:49,997: Train batch 50800: loss: 70.52 grad norm: 1413.79 time: 0.341
2025-12-11 20:37:36,429: Train batch 51000: loss: 97.90 grad norm: 144026.36 time: 0.352
2025-12-11 20:38:22,232: Train batch 51200: loss: 76.50 grad norm: 12404.52 time: 0.227
2025-12-11 20:39:09,605: Train batch 51400: loss: 75.37 grad norm: 3434.02 time: 0.222
2025-12-11 20:39:56,910: Train batch 51600: loss: 94.11 grad norm: 1800.05 time: 0.253
2025-12-11 20:40:43,629: Train batch 51800: loss: 85.25 grad norm: 226.39 time: 0.195
2025-12-11 20:41:30,521: Train batch 52000: loss: 85.26 grad norm: 10674.44 time: 0.259
2025-12-11 20:41:30,521: Running test after training batch: 52000
2025-12-11 20:41:57,142: Val batch 52000: PER (avg): 0.9504 CTC Loss (avg): 168.7482 time: 26.621
2025-12-11 20:41:57,143: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:41:57,143: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:41:57,143: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:41:57,143: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:41:57,143: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:41:57,143: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:41:57,143: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:41:57,143: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:41:57,143: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:41:57,143: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:41:57,143: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:41:57,143: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:41:57,143: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:41:57,143: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:41:57,143: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:41:57,143: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:41:57,143: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:41:57,144: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:41:57,144: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:41:57,144: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:41:57,144: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:41:57,144: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:41:57,144: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:41:57,144: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:41:57,144: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:41:57,144: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:41:57,144: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:41:57,144: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:41:57,144: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:41:57,144: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:41:57,144: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:41:57,144: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:41:57,144: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:41:57,144: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:41:57,144: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:41:57,144: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:41:57,145: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:41:57,145: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:41:57,145: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:41:57,145: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:41:57,145: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:41:57,145: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:41:57,145: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:41:57,145: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:41:57,145: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:42:44,113: Train batch 52200: loss: 79.70 grad norm: 1354.99 time: 0.225
2025-12-11 20:43:30,222: Train batch 52400: loss: 82.62 grad norm: 2739.74 time: 0.236
2025-12-11 20:44:18,104: Train batch 52600: loss: 75.31 grad norm: 683.08 time: 0.212
2025-12-11 20:45:04,490: Train batch 52800: loss: 77.09 grad norm: 620.74 time: 0.175
2025-12-11 20:45:51,572: Train batch 53000: loss: 76.35 grad norm: 262.72 time: 0.242
2025-12-11 20:46:38,586: Train batch 53200: loss: 68.29 grad norm: 3979.93 time: 0.199
2025-12-11 20:47:26,517: Train batch 53400: loss: 61.47 grad norm: 1810.76 time: 0.191
2025-12-11 20:48:13,442: Train batch 53600: loss: 78.62 grad norm: 3755.29 time: 0.232
2025-12-11 20:48:59,130: Train batch 53800: loss: 77.93 grad norm: 3479.51 time: 0.195
2025-12-11 20:49:45,900: Train batch 54000: loss: 73.19 grad norm: 414.41 time: 0.236
2025-12-11 20:49:45,900: Running test after training batch: 54000
2025-12-11 20:50:12,437: Val batch 54000: PER (avg): 0.9504 CTC Loss (avg): 157.5439 time: 26.536
2025-12-11 20:50:12,437: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:50:12,437: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:50:12,437: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:50:12,437: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:50:12,437: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:50:12,437: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:50:12,437: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:50:12,437: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:50:12,437: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:50:12,437: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:50:12,438: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:50:12,438: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:50:12,438: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:50:12,438: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:50:12,438: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:50:12,438: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:50:12,438: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:50:12,438: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:50:12,438: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:50:12,438: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:50:12,438: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:50:12,438: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:50:12,438: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:50:12,438: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:50:12,438: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:50:12,438: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:50:12,438: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:50:12,438: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:50:12,438: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:50:12,439: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:50:12,439: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:50:12,439: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:50:12,439: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:50:12,439: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:50:12,439: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:50:12,439: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:50:12,439: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:50:12,439: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:50:12,439: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:50:12,439: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:50:12,439: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:50:12,439: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:50:12,439: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:50:12,439: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:50:12,439: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:51:00,198: Train batch 54200: loss: 81.73 grad norm: 843.75 time: 0.272
2025-12-11 20:51:46,963: Train batch 54400: loss: 76.35 grad norm: 903.00 time: 0.209
2025-12-11 20:52:33,670: Train batch 54600: loss: 82.44 grad norm: 473.49 time: 0.242
2025-12-11 20:53:18,895: Train batch 54800: loss: 62.56 grad norm: 86017.02 time: 0.210
2025-12-11 20:54:04,702: Train batch 55000: loss: 85.28 grad norm: 548.71 time: 0.209
2025-12-11 20:54:50,681: Train batch 55200: loss: 78.45 grad norm: 288.44 time: 0.210
2025-12-11 20:55:37,429: Train batch 55400: loss: 78.30 grad norm: 66324.13 time: 0.257
2025-12-11 20:56:25,018: Train batch 55600: loss: 70.75 grad norm: 2487.57 time: 0.230
2025-12-11 20:57:12,622: Train batch 55800: loss: 80.16 grad norm: 1088.70 time: 0.221
2025-12-11 20:58:00,981: Train batch 56000: loss: 78.50 grad norm: 7784.62 time: 0.235
2025-12-11 20:58:00,981: Running test after training batch: 56000
2025-12-11 20:58:27,887: Val batch 56000: PER (avg): 0.9504 CTC Loss (avg): 168.4290 time: 26.905
2025-12-11 20:58:27,887: t15.2023.08.11 val PER: 1.0000
2025-12-11 20:58:27,887: t15.2023.08.13 val PER: 0.9491
2025-12-11 20:58:27,887: t15.2023.08.18 val PER: 0.9430
2025-12-11 20:58:27,887: t15.2023.08.20 val PER: 0.9476
2025-12-11 20:58:27,887: t15.2023.08.25 val PER: 0.9398
2025-12-11 20:58:27,887: t15.2023.08.27 val PER: 0.9405
2025-12-11 20:58:27,888: t15.2023.09.01 val PER: 0.9456
2025-12-11 20:58:27,888: t15.2023.09.03 val PER: 0.9430
2025-12-11 20:58:27,888: t15.2023.09.24 val PER: 0.9405
2025-12-11 20:58:27,888: t15.2023.09.29 val PER: 0.9541
2025-12-11 20:58:27,888: t15.2023.10.01 val PER: 0.9577
2025-12-11 20:58:27,888: t15.2023.10.06 val PER: 0.9397
2025-12-11 20:58:27,888: t15.2023.10.08 val PER: 0.9675
2025-12-11 20:58:27,888: t15.2023.10.13 val PER: 0.9511
2025-12-11 20:58:27,888: t15.2023.10.15 val PER: 0.9598
2025-12-11 20:58:27,888: t15.2023.10.20 val PER: 0.9597
2025-12-11 20:58:27,888: t15.2023.10.22 val PER: 0.9510
2025-12-11 20:58:27,888: t15.2023.11.03 val PER: 0.9573
2025-12-11 20:58:27,888: t15.2023.11.04 val PER: 0.9283
2025-12-11 20:58:27,888: t15.2023.11.17 val PER: 0.9393
2025-12-11 20:58:27,888: t15.2023.11.19 val PER: 0.9441
2025-12-11 20:58:27,888: t15.2023.11.26 val PER: 0.9587
2025-12-11 20:58:27,888: t15.2023.12.03 val PER: 0.9548
2025-12-11 20:58:27,888: t15.2023.12.08 val PER: 0.9501
2025-12-11 20:58:27,888: t15.2023.12.10 val PER: 0.9514
2025-12-11 20:58:27,888: t15.2023.12.17 val PER: 0.9563
2025-12-11 20:58:27,889: t15.2023.12.29 val PER: 0.9492
2025-12-11 20:58:27,889: t15.2024.02.25 val PER: 0.9551
2025-12-11 20:58:27,889: t15.2024.03.03 val PER: 1.0000
2025-12-11 20:58:27,889: t15.2024.03.08 val PER: 0.9502
2025-12-11 20:58:27,889: t15.2024.03.15 val PER: 0.9531
2025-12-11 20:58:27,889: t15.2024.03.17 val PER: 0.9505
2025-12-11 20:58:27,889: t15.2024.04.25 val PER: 1.0000
2025-12-11 20:58:27,889: t15.2024.04.28 val PER: 1.0000
2025-12-11 20:58:27,889: t15.2024.05.10 val PER: 0.9435
2025-12-11 20:58:27,889: t15.2024.06.14 val PER: 0.9432
2025-12-11 20:58:27,889: t15.2024.07.19 val PER: 0.9499
2025-12-11 20:58:27,889: t15.2024.07.21 val PER: 0.9524
2025-12-11 20:58:27,889: t15.2024.07.28 val PER: 0.9478
2025-12-11 20:58:27,889: t15.2025.01.10 val PER: 0.9559
2025-12-11 20:58:27,889: t15.2025.01.12 val PER: 0.9453
2025-12-11 20:58:27,889: t15.2025.03.14 val PER: 0.9527
2025-12-11 20:58:27,889: t15.2025.03.16 val PER: 0.9516
2025-12-11 20:58:27,889: t15.2025.03.30 val PER: 0.9494
2025-12-11 20:58:27,889: t15.2025.04.13 val PER: 0.9486
2025-12-11 20:59:14,939: Train batch 56200: loss: 84.94 grad norm: 563.70 time: 0.215
2025-12-11 21:00:01,379: Train batch 56400: loss: 81.25 grad norm: 1804.16 time: 0.239
2025-12-11 21:00:48,738: Train batch 56600: loss: 57.68 grad norm: 6995.56 time: 0.193
2025-12-11 21:01:34,474: Train batch 56800: loss: 81.77 grad norm: 1123.35 time: 0.245
2025-12-11 21:02:21,461: Train batch 57000: loss: 79.77 grad norm: 34639.79 time: 0.216
2025-12-11 21:03:09,277: Train batch 57200: loss: 63.64 grad norm: 4756.66 time: 0.265
2025-12-11 21:03:56,118: Train batch 57400: loss: 74.86 grad norm: 9580.01 time: 0.208
2025-12-11 21:04:43,902: Train batch 57600: loss: 73.05 grad norm: 1738.59 time: 0.214
2025-12-11 21:05:31,093: Train batch 57800: loss: 73.26 grad norm: 546.43 time: 0.253
2025-12-11 21:06:18,788: Train batch 58000: loss: 77.50 grad norm: 410.27 time: 0.230
2025-12-11 21:06:18,788: Running test after training batch: 58000
2025-12-11 21:06:45,390: Val batch 58000: PER (avg): 0.9504 CTC Loss (avg): 154.0924 time: 26.601
2025-12-11 21:06:45,390: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:06:45,390: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:06:45,390: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:06:45,390: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:06:45,390: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:06:45,390: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:06:45,391: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:06:45,391: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:06:45,391: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:06:45,391: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:06:45,391: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:06:45,391: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:06:45,391: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:06:45,391: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:06:45,391: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:06:45,391: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:06:45,391: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:06:45,391: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:06:45,391: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:06:45,391: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:06:45,391: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:06:45,391: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:06:45,391: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:06:45,391: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:06:45,391: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:06:45,392: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:06:45,392: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:06:45,392: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:06:45,392: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:06:45,392: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:06:45,392: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:06:45,392: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:06:45,392: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:06:45,392: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:06:45,392: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:06:45,392: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:06:45,392: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:06:45,392: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:06:45,392: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:06:45,392: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:06:45,392: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:06:45,392: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:06:45,392: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:06:45,392: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:06:45,393: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:07:33,191: Train batch 58200: loss: 70.82 grad norm: 10571.49 time: 0.196
2025-12-11 21:08:19,883: Train batch 58400: loss: 74.59 grad norm: 23143.21 time: 0.208
2025-12-11 21:09:07,538: Train batch 58600: loss: 78.12 grad norm: 5901.87 time: 0.219
2025-12-11 21:09:54,703: Train batch 58800: loss: 72.71 grad norm: 25323.01 time: 0.244
2025-12-11 21:10:40,587: Train batch 59000: loss: 69.96 grad norm: 472.70 time: 0.306
2025-12-11 21:11:27,676: Train batch 59200: loss: 84.47 grad norm: 97.64 time: 0.283
2025-12-11 21:12:15,127: Train batch 59400: loss: 75.21 grad norm: 3279.58 time: 0.193
2025-12-11 21:13:04,590: Train batch 59600: loss: 79.75 grad norm: 365178.81 time: 0.206
2025-12-11 21:13:51,196: Train batch 59800: loss: 75.05 grad norm: 8888.41 time: 0.236
2025-12-11 21:14:37,475: Train batch 60000: loss: 65.95 grad norm: 768.99 time: 0.306
2025-12-11 21:14:37,475: Running test after training batch: 60000
2025-12-11 21:15:04,121: Val batch 60000: PER (avg): 0.9504 CTC Loss (avg): 130.1577 time: 26.646
2025-12-11 21:15:04,121: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:15:04,121: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:15:04,121: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:15:04,121: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:15:04,122: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:15:04,122: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:15:04,122: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:15:04,122: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:15:04,122: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:15:04,122: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:15:04,122: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:15:04,122: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:15:04,122: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:15:04,122: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:15:04,122: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:15:04,122: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:15:04,122: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:15:04,122: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:15:04,122: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:15:04,122: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:15:04,122: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:15:04,122: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:15:04,122: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:15:04,123: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:15:04,123: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:15:04,123: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:15:04,123: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:15:04,123: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:15:04,123: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:15:04,123: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:15:04,123: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:15:04,123: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:15:04,123: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:15:04,123: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:15:04,123: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:15:04,123: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:15:04,123: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:15:04,123: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:15:04,123: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:15:04,123: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:15:04,123: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:15:04,123: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:15:04,124: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:15:04,124: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:15:04,124: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:15:51,120: Train batch 60200: loss: 90.37 grad norm: 22351.19 time: 0.234
2025-12-11 21:16:35,109: Train batch 60400: loss: 61.75 grad norm: 211.91 time: 0.176
2025-12-11 21:17:21,666: Train batch 60600: loss: 75.75 grad norm: 2188.50 time: 0.202
2025-12-11 21:18:08,025: Train batch 60800: loss: 70.39 grad norm: 2250.17 time: 0.245
2025-12-11 21:18:55,783: Train batch 61000: loss: 77.52 grad norm: 3041.24 time: 0.204
2025-12-11 21:19:42,259: Train batch 61200: loss: 84.61 grad norm: 3127.67 time: 0.250
2025-12-11 21:20:29,910: Train batch 61400: loss: 75.02 grad norm: 827.02 time: 0.267
2025-12-11 21:21:17,788: Train batch 61600: loss: 85.19 grad norm: 492.68 time: 0.219
2025-12-11 21:22:04,595: Train batch 61800: loss: 58.37 grad norm: 1997.45 time: 0.173
2025-12-11 21:22:50,759: Train batch 62000: loss: 79.06 grad norm: 364.07 time: 0.181
2025-12-11 21:22:50,759: Running test after training batch: 62000
2025-12-11 21:23:17,375: Val batch 62000: PER (avg): 0.9649 CTC Loss (avg): 147.7948 time: 26.616
2025-12-11 21:23:17,375: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:23:17,375: t15.2023.08.13 val PER: 0.9636
2025-12-11 21:23:17,375: t15.2023.08.18 val PER: 0.9581
2025-12-11 21:23:17,375: t15.2023.08.20 val PER: 0.9619
2025-12-11 21:23:17,375: t15.2023.08.25 val PER: 0.9578
2025-12-11 21:23:17,376: t15.2023.08.27 val PER: 0.9582
2025-12-11 21:23:17,376: t15.2023.09.01 val PER: 0.9602
2025-12-11 21:23:17,376: t15.2023.09.03 val PER: 0.9584
2025-12-11 21:23:17,376: t15.2023.09.24 val PER: 0.9563
2025-12-11 21:23:17,376: t15.2023.09.29 val PER: 0.9675
2025-12-11 21:23:17,376: t15.2023.10.01 val PER: 0.9709
2025-12-11 21:23:17,376: t15.2023.10.06 val PER: 0.9612
2025-12-11 21:23:17,376: t15.2023.10.08 val PER: 0.9770
2025-12-11 21:23:17,376: t15.2023.10.13 val PER: 0.9651
2025-12-11 21:23:17,376: t15.2023.10.15 val PER: 0.9703
2025-12-11 21:23:17,376: t15.2023.10.20 val PER: 0.9698
2025-12-11 21:23:17,376: t15.2023.10.22 val PER: 0.9633
2025-12-11 21:23:17,376: t15.2023.11.03 val PER: 0.9661
2025-12-11 21:23:17,376: t15.2023.11.04 val PER: 0.9488
2025-12-11 21:23:17,376: t15.2023.11.17 val PER: 0.9596
2025-12-11 21:23:17,376: t15.2023.11.19 val PER: 0.9601
2025-12-11 21:23:17,376: t15.2023.11.26 val PER: 0.9681
2025-12-11 21:23:17,377: t15.2023.12.03 val PER: 0.9643
2025-12-11 21:23:17,377: t15.2023.12.08 val PER: 0.9660
2025-12-11 21:23:17,377: t15.2023.12.10 val PER: 0.9671
2025-12-11 21:23:17,377: t15.2023.12.17 val PER: 0.9678
2025-12-11 21:23:17,377: t15.2023.12.29 val PER: 0.9657
2025-12-11 21:23:17,377: t15.2024.02.25 val PER: 0.9677
2025-12-11 21:23:17,377: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:23:17,377: t15.2024.03.08 val PER: 0.9644
2025-12-11 21:23:17,377: t15.2024.03.15 val PER: 0.9694
2025-12-11 21:23:17,377: t15.2024.03.17 val PER: 0.9658
2025-12-11 21:23:17,377: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:23:17,377: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:23:17,377: t15.2024.05.10 val PER: 0.9584
2025-12-11 21:23:17,377: t15.2024.06.14 val PER: 0.9590
2025-12-11 21:23:17,377: t15.2024.07.19 val PER: 0.9670
2025-12-11 21:23:17,377: t15.2024.07.21 val PER: 0.9683
2025-12-11 21:23:17,378: t15.2024.07.28 val PER: 0.9647
2025-12-11 21:23:17,378: t15.2025.01.10 val PER: 0.9683
2025-12-11 21:23:17,378: t15.2025.01.12 val PER: 0.9638
2025-12-11 21:23:17,378: t15.2025.03.14 val PER: 0.9645
2025-12-11 21:23:17,378: t15.2025.03.16 val PER: 0.9660
2025-12-11 21:23:17,378: t15.2025.03.30 val PER: 0.9655
2025-12-11 21:23:17,378: t15.2025.04.13 val PER: 0.9643
2025-12-11 21:24:03,783: Train batch 62200: loss: 58.28 grad norm: 13971.25 time: 0.165
2025-12-11 21:24:51,202: Train batch 62400: loss: 70.96 grad norm: 351.44 time: 0.205
2025-12-11 21:25:37,373: Train batch 62600: loss: 79.73 grad norm: 11666.21 time: 0.199
2025-12-11 21:26:23,650: Train batch 62800: loss: 92.32 grad norm: 865.74 time: 0.248
2025-12-11 21:27:10,005: Train batch 63000: loss: 65.07 grad norm: 3051.46 time: 0.178
2025-12-11 21:27:57,497: Train batch 63200: loss: 69.92 grad norm: 1051.55 time: 0.229
2025-12-11 21:28:43,610: Train batch 63400: loss: 72.05 grad norm: 1581.77 time: 0.205
2025-12-11 21:29:30,654: Train batch 63600: loss: 62.98 grad norm: 574.19 time: 0.173
2025-12-11 21:30:17,984: Train batch 63800: loss: 87.12 grad norm: 402.16 time: 0.205
2025-12-11 21:31:04,745: Train batch 64000: loss: 69.04 grad norm: 2172.57 time: 0.233
2025-12-11 21:31:04,745: Running test after training batch: 64000
2025-12-11 21:31:31,290: Val batch 64000: PER (avg): 0.9504 CTC Loss (avg): 157.9686 time: 26.545
2025-12-11 21:31:31,290: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:31:31,290: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:31:31,290: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:31:31,290: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:31:31,290: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:31:31,291: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:31:31,291: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:31:31,291: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:31:31,291: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:31:31,291: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:31:31,291: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:31:31,291: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:31:31,291: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:31:31,291: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:31:31,291: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:31:31,291: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:31:31,291: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:31:31,291: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:31:31,291: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:31:31,291: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:31:31,291: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:31:31,291: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:31:31,291: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:31:31,291: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:31:31,292: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:31:31,292: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:31:31,292: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:31:31,292: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:31:31,292: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:31:31,292: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:31:31,292: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:31:31,292: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:31:31,292: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:31:31,292: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:31:31,292: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:31:31,292: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:31:31,292: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:31:31,292: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:31:31,292: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:31:31,292: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:31:31,292: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:31:31,292: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:31:31,293: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:31:31,293: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:31:31,293: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:32:17,511: Train batch 64200: loss: 78.46 grad norm: 1954.03 time: 0.249
2025-12-11 21:33:04,936: Train batch 64400: loss: 87.50 grad norm: 2806.34 time: 0.227
2025-12-11 21:33:52,048: Train batch 64600: loss: 75.48 grad norm: 3278.85 time: 0.385
2025-12-11 21:34:38,453: Train batch 64800: loss: 77.46 grad norm: 3746.20 time: 0.219
2025-12-11 21:35:24,543: Train batch 65000: loss: 76.71 grad norm: 572.55 time: 0.268
2025-12-11 21:36:11,996: Train batch 65200: loss: 69.33 grad norm: 1969.88 time: 0.196
2025-12-11 21:36:59,101: Train batch 65400: loss: 73.53 grad norm: 3408.05 time: 0.199
2025-12-11 21:37:45,333: Train batch 65600: loss: 77.37 grad norm: 398.60 time: 0.212
2025-12-11 21:38:30,583: Train batch 65800: loss: 63.17 grad norm: 3455.06 time: 0.186
2025-12-11 21:39:16,486: Train batch 66000: loss: 85.14 grad norm: 2332.08 time: 0.272
2025-12-11 21:39:16,487: Running test after training batch: 66000
2025-12-11 21:39:43,066: Val batch 66000: PER (avg): 0.9504 CTC Loss (avg): 149.6890 time: 26.579
2025-12-11 21:39:43,066: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:39:43,066: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:39:43,066: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:39:43,066: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:39:43,066: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:39:43,066: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:39:43,066: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:39:43,066: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:39:43,066: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:39:43,066: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:39:43,067: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:39:43,067: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:39:43,067: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:39:43,067: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:39:43,067: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:39:43,067: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:39:43,067: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:39:43,067: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:39:43,067: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:39:43,067: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:39:43,067: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:39:43,067: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:39:43,067: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:39:43,067: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:39:43,067: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:39:43,067: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:39:43,067: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:39:43,067: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:39:43,067: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:39:43,068: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:39:43,068: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:39:43,068: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:39:43,068: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:39:43,068: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:39:43,068: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:39:43,068: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:39:43,068: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:39:43,068: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:39:43,068: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:39:43,068: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:39:43,068: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:39:43,068: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:39:43,068: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:39:43,068: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:39:43,068: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:40:29,537: Train batch 66200: loss: 58.90 grad norm: 361.36 time: 0.188
2025-12-11 21:41:16,887: Train batch 66400: loss: 65.36 grad norm: 3713.92 time: 0.176
2025-12-11 21:42:03,553: Train batch 66600: loss: 71.85 grad norm: 563.57 time: 0.248
2025-12-11 21:42:50,922: Train batch 66800: loss: 82.44 grad norm: 1960.82 time: 0.233
2025-12-11 21:43:37,337: Train batch 67000: loss: 83.02 grad norm: 2236.21 time: 0.275
2025-12-11 21:44:24,717: Train batch 67200: loss: 76.38 grad norm: 1069.78 time: 0.247
2025-12-11 21:45:11,485: Train batch 67400: loss: 102.09 grad norm: 1491.54 time: 0.256
2025-12-11 21:45:57,329: Train batch 67600: loss: 84.41 grad norm: 4841.73 time: 0.250
2025-12-11 21:46:43,487: Train batch 67800: loss: 65.73 grad norm: 244.50 time: 0.207
2025-12-11 21:47:31,656: Train batch 68000: loss: 62.17 grad norm: 2537.74 time: 0.222
2025-12-11 21:47:31,656: Running test after training batch: 68000
2025-12-11 21:47:58,391: Val batch 68000: PER (avg): 0.9504 CTC Loss (avg): 142.2910 time: 26.735
2025-12-11 21:47:58,392: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:47:58,392: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:47:58,392: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:47:58,392: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:47:58,392: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:47:58,392: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:47:58,392: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:47:58,392: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:47:58,392: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:47:58,392: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:47:58,392: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:47:58,392: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:47:58,392: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:47:58,392: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:47:58,392: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:47:58,392: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:47:58,392: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:47:58,393: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:47:58,393: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:47:58,393: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:47:58,393: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:47:58,393: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:47:58,393: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:47:58,393: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:47:58,393: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:47:58,393: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:47:58,393: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:47:58,393: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:47:58,393: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:47:58,393: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:47:58,393: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:47:58,393: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:47:58,393: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:47:58,393: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:47:58,393: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:47:58,393: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:47:58,393: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:47:58,394: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:47:58,394: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:47:58,394: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:47:58,394: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:47:58,394: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:47:58,394: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:47:58,394: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:47:58,394: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:48:45,785: Train batch 68200: loss: 70.00 grad norm: 7030.25 time: 0.280
2025-12-11 21:49:32,396: Train batch 68400: loss: 72.61 grad norm: 1738.68 time: 0.220
2025-12-11 21:50:18,098: Train batch 68600: loss: 68.78 grad norm: 639.20 time: 0.200
2025-12-11 21:51:05,788: Train batch 68800: loss: 91.99 grad norm: 491.15 time: 0.252
2025-12-11 21:51:53,446: Train batch 69000: loss: 59.04 grad norm: 13981.18 time: 0.179
2025-12-11 21:52:41,271: Train batch 69200: loss: 95.66 grad norm: 4749.92 time: 0.225
2025-12-11 21:53:29,307: Train batch 69400: loss: 70.11 grad norm: 658.14 time: 0.188
2025-12-11 21:54:16,898: Train batch 69600: loss: 74.55 grad norm: 5095.10 time: 0.262
2025-12-11 21:55:03,698: Train batch 69800: loss: 68.86 grad norm: 18417.07 time: 0.203
2025-12-11 21:55:51,378: Train batch 70000: loss: 74.70 grad norm: 11727.72 time: 0.261
2025-12-11 21:55:51,379: Running test after training batch: 70000
2025-12-11 21:56:17,916: Val batch 70000: PER (avg): 0.9504 CTC Loss (avg): 143.9411 time: 26.537
2025-12-11 21:56:17,916: t15.2023.08.11 val PER: 1.0000
2025-12-11 21:56:17,916: t15.2023.08.13 val PER: 0.9491
2025-12-11 21:56:17,916: t15.2023.08.18 val PER: 0.9430
2025-12-11 21:56:17,916: t15.2023.08.20 val PER: 0.9476
2025-12-11 21:56:17,916: t15.2023.08.25 val PER: 0.9398
2025-12-11 21:56:17,916: t15.2023.08.27 val PER: 0.9405
2025-12-11 21:56:17,916: t15.2023.09.01 val PER: 0.9456
2025-12-11 21:56:17,916: t15.2023.09.03 val PER: 0.9430
2025-12-11 21:56:17,916: t15.2023.09.24 val PER: 0.9405
2025-12-11 21:56:17,916: t15.2023.09.29 val PER: 0.9541
2025-12-11 21:56:17,916: t15.2023.10.01 val PER: 0.9577
2025-12-11 21:56:17,916: t15.2023.10.06 val PER: 0.9397
2025-12-11 21:56:17,917: t15.2023.10.08 val PER: 0.9675
2025-12-11 21:56:17,917: t15.2023.10.13 val PER: 0.9511
2025-12-11 21:56:17,917: t15.2023.10.15 val PER: 0.9598
2025-12-11 21:56:17,917: t15.2023.10.20 val PER: 0.9597
2025-12-11 21:56:17,917: t15.2023.10.22 val PER: 0.9510
2025-12-11 21:56:17,917: t15.2023.11.03 val PER: 0.9573
2025-12-11 21:56:17,917: t15.2023.11.04 val PER: 0.9283
2025-12-11 21:56:17,917: t15.2023.11.17 val PER: 0.9393
2025-12-11 21:56:17,917: t15.2023.11.19 val PER: 0.9441
2025-12-11 21:56:17,917: t15.2023.11.26 val PER: 0.9587
2025-12-11 21:56:17,917: t15.2023.12.03 val PER: 0.9548
2025-12-11 21:56:17,917: t15.2023.12.08 val PER: 0.9501
2025-12-11 21:56:17,917: t15.2023.12.10 val PER: 0.9514
2025-12-11 21:56:17,917: t15.2023.12.17 val PER: 0.9563
2025-12-11 21:56:17,917: t15.2023.12.29 val PER: 0.9492
2025-12-11 21:56:17,917: t15.2024.02.25 val PER: 0.9551
2025-12-11 21:56:17,917: t15.2024.03.03 val PER: 1.0000
2025-12-11 21:56:17,917: t15.2024.03.08 val PER: 0.9502
2025-12-11 21:56:17,917: t15.2024.03.15 val PER: 0.9531
2025-12-11 21:56:17,918: t15.2024.03.17 val PER: 0.9505
2025-12-11 21:56:17,918: t15.2024.04.25 val PER: 1.0000
2025-12-11 21:56:17,918: t15.2024.04.28 val PER: 1.0000
2025-12-11 21:56:17,918: t15.2024.05.10 val PER: 0.9435
2025-12-11 21:56:17,918: t15.2024.06.14 val PER: 0.9432
2025-12-11 21:56:17,918: t15.2024.07.19 val PER: 0.9499
2025-12-11 21:56:17,918: t15.2024.07.21 val PER: 0.9524
2025-12-11 21:56:17,918: t15.2024.07.28 val PER: 0.9478
2025-12-11 21:56:17,918: t15.2025.01.10 val PER: 0.9559
2025-12-11 21:56:17,918: t15.2025.01.12 val PER: 0.9453
2025-12-11 21:56:17,918: t15.2025.03.14 val PER: 0.9527
2025-12-11 21:56:17,918: t15.2025.03.16 val PER: 0.9516
2025-12-11 21:56:17,918: t15.2025.03.30 val PER: 0.9494
2025-12-11 21:56:17,918: t15.2025.04.13 val PER: 0.9486
2025-12-11 21:57:04,821: Train batch 70200: loss: 62.89 grad norm: 8251.95 time: 0.229
2025-12-11 21:57:51,929: Train batch 70400: loss: 70.49 grad norm: 8980.85 time: 0.241
2025-12-11 21:58:39,932: Train batch 70600: loss: 91.01 grad norm: 15475.34 time: 0.267
2025-12-11 21:59:27,786: Train batch 70800: loss: 81.85 grad norm: 28526.37 time: 0.263
2025-12-11 22:00:14,306: Train batch 71000: loss: 88.54 grad norm: 1675.46 time: 0.250
2025-12-11 22:01:01,126: Train batch 71200: loss: 98.60 grad norm: 73094.81 time: 0.310
2025-12-11 22:01:49,405: Train batch 71400: loss: 71.29 grad norm: 26859.03 time: 0.241
2025-12-11 22:02:35,018: Train batch 71600: loss: 71.23 grad norm: 8776.30 time: 0.244
2025-12-11 22:03:21,182: Train batch 71800: loss: 77.01 grad norm: 29673.40 time: 0.208
2025-12-11 22:04:07,759: Train batch 72000: loss: 57.70 grad norm: 161043.02 time: 0.178
2025-12-11 22:04:07,759: Running test after training batch: 72000
2025-12-11 22:04:34,270: Val batch 72000: PER (avg): 0.9425 CTC Loss (avg): 146.0775 time: 26.511
2025-12-11 22:04:34,270: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:04:34,270: t15.2023.08.13 val PER: 0.9407
2025-12-11 22:04:34,270: t15.2023.08.18 val PER: 0.9288
2025-12-11 22:04:34,270: t15.2023.08.20 val PER: 0.9388
2025-12-11 22:04:34,270: t15.2023.08.25 val PER: 0.9277
2025-12-11 22:04:34,270: t15.2023.08.27 val PER: 0.9277
2025-12-11 22:04:34,270: t15.2023.09.01 val PER: 0.9326
2025-12-11 22:04:34,270: t15.2023.09.03 val PER: 0.9299
2025-12-11 22:04:34,271: t15.2023.09.24 val PER: 0.9296
2025-12-11 22:04:34,271: t15.2023.09.29 val PER: 0.9483
2025-12-11 22:04:34,271: t15.2023.10.01 val PER: 0.9478
2025-12-11 22:04:34,271: t15.2023.10.06 val PER: 0.9354
2025-12-11 22:04:34,271: t15.2023.10.08 val PER: 0.9621
2025-12-11 22:04:34,271: t15.2023.10.13 val PER: 0.9426
2025-12-11 22:04:34,271: t15.2023.10.15 val PER: 0.9539
2025-12-11 22:04:34,271: t15.2023.10.20 val PER: 0.9530
2025-12-11 22:04:34,271: t15.2023.10.22 val PER: 0.9443
2025-12-11 22:04:34,271: t15.2023.11.03 val PER: 0.9512
2025-12-11 22:04:34,271: t15.2023.11.04 val PER: 0.9147
2025-12-11 22:04:34,271: t15.2023.11.17 val PER: 0.9331
2025-12-11 22:04:34,271: t15.2023.11.19 val PER: 0.9381
2025-12-11 22:04:34,271: t15.2023.11.26 val PER: 0.9536
2025-12-11 22:04:34,271: t15.2023.12.03 val PER: 0.9475
2025-12-11 22:04:34,271: t15.2023.12.08 val PER: 0.9441
2025-12-11 22:04:34,271: t15.2023.12.10 val PER: 0.9448
2025-12-11 22:04:34,271: t15.2023.12.17 val PER: 0.9491
2025-12-11 22:04:34,271: t15.2023.12.29 val PER: 0.9375
2025-12-11 22:04:34,272: t15.2024.02.25 val PER: 0.9494
2025-12-11 22:04:34,272: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:04:34,272: t15.2024.03.08 val PER: 0.9445
2025-12-11 22:04:34,272: t15.2024.03.15 val PER: 0.9481
2025-12-11 22:04:34,272: t15.2024.03.17 val PER: 0.9414
2025-12-11 22:04:34,272: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:04:34,272: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:04:34,272: t15.2024.05.10 val PER: 0.9361
2025-12-11 22:04:34,272: t15.2024.06.14 val PER: 0.9338
2025-12-11 22:04:34,272: t15.2024.07.19 val PER: 0.9440
2025-12-11 22:04:34,272: t15.2024.07.21 val PER: 0.9469
2025-12-11 22:04:34,272: t15.2024.07.28 val PER: 0.9404
2025-12-11 22:04:34,272: t15.2025.01.10 val PER: 0.9504
2025-12-11 22:04:34,272: t15.2025.01.12 val PER: 0.9384
2025-12-11 22:04:34,272: t15.2025.03.14 val PER: 0.9453
2025-12-11 22:04:34,272: t15.2025.03.16 val PER: 0.9437
2025-12-11 22:04:34,272: t15.2025.03.30 val PER: 0.9414
2025-12-11 22:04:34,272: t15.2025.04.13 val PER: 0.9401
2025-12-11 22:05:19,732: Train batch 72200: loss: 84.55 grad norm: 16871.61 time: 0.230
2025-12-11 22:06:06,266: Train batch 72400: loss: 61.54 grad norm: 74136.43 time: 0.205
2025-12-11 22:06:53,131: Train batch 72600: loss: 75.30 grad norm: 13373.62 time: 0.206
2025-12-11 22:07:40,759: Train batch 72800: loss: 64.28 grad norm: 7897.85 time: 0.216
2025-12-11 22:08:27,855: Train batch 73000: loss: 75.58 grad norm: 14698.52 time: 0.224
2025-12-11 22:09:15,105: Train batch 73200: loss: 95.98 grad norm: 22991.22 time: 0.197
2025-12-11 22:10:02,976: Train batch 73400: loss: 84.91 grad norm: 9802.67 time: 0.281
2025-12-11 22:10:49,704: Train batch 73600: loss: 91.17 grad norm: 3740.34 time: 0.270
2025-12-11 22:11:36,334: Train batch 73800: loss: 72.72 grad norm: 6064.71 time: 0.200
2025-12-11 22:12:22,461: Train batch 74000: loss: 65.61 grad norm: 453502.66 time: 0.341
2025-12-11 22:12:22,461: Running test after training batch: 74000
2025-12-11 22:12:49,050: Val batch 74000: PER (avg): 0.9504 CTC Loss (avg): 141.1868 time: 26.589
2025-12-11 22:12:49,051: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:12:49,051: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:12:49,051: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:12:49,051: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:12:49,051: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:12:49,051: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:12:49,051: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:12:49,051: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:12:49,051: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:12:49,051: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:12:49,051: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:12:49,051: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:12:49,051: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:12:49,051: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:12:49,051: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:12:49,051: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:12:49,051: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:12:49,052: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:12:49,052: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:12:49,052: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:12:49,052: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:12:49,052: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:12:49,052: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:12:49,052: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:12:49,052: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:12:49,052: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:12:49,052: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:12:49,052: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:12:49,052: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:12:49,052: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:12:49,052: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:12:49,052: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:12:49,052: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:12:49,052: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:12:49,052: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:12:49,052: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:12:49,052: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:12:49,053: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:12:49,053: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:12:49,053: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:12:49,053: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:12:49,053: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:12:49,053: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:12:49,053: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:12:49,053: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:13:36,362: Train batch 74200: loss: 87.73 grad norm: 62750.07 time: 0.215
2025-12-11 22:14:23,225: Train batch 74400: loss: 76.80 grad norm: 7943.02 time: 0.235
2025-12-11 22:15:09,922: Train batch 74600: loss: 68.72 grad norm: 2941.86 time: 0.214
2025-12-11 22:15:56,977: Train batch 74800: loss: 87.42 grad norm: 12659209.00 time: 0.241
2025-12-11 22:16:43,672: Train batch 75000: loss: 77.20 grad norm: 8244.72 time: 0.204
2025-12-11 22:17:30,353: Train batch 75200: loss: 83.90 grad norm: 3534.38 time: 0.228
2025-12-11 22:18:16,957: Train batch 75400: loss: 84.50 grad norm: 9500.33 time: 0.269
2025-12-11 22:19:03,703: Train batch 75600: loss: 75.18 grad norm: 2769.98 time: 0.183
2025-12-11 22:19:50,825: Train batch 75800: loss: 72.98 grad norm: 22727.11 time: 0.349
2025-12-11 22:20:37,450: Train batch 76000: loss: 71.69 grad norm: 1395.03 time: 0.216
2025-12-11 22:20:37,450: Running test after training batch: 76000
2025-12-11 22:21:04,016: Val batch 76000: PER (avg): 0.9504 CTC Loss (avg): 151.1815 time: 26.566
2025-12-11 22:21:04,017: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:21:04,017: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:21:04,017: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:21:04,017: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:21:04,017: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:21:04,017: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:21:04,017: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:21:04,017: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:21:04,017: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:21:04,017: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:21:04,017: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:21:04,017: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:21:04,017: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:21:04,017: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:21:04,017: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:21:04,017: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:21:04,017: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:21:04,017: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:21:04,018: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:21:04,018: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:21:04,018: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:21:04,018: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:21:04,018: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:21:04,018: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:21:04,018: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:21:04,018: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:21:04,018: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:21:04,018: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:21:04,018: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:21:04,018: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:21:04,018: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:21:04,018: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:21:04,018: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:21:04,018: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:21:04,018: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:21:04,018: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:21:04,018: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:21:04,019: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:21:04,019: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:21:04,019: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:21:04,019: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:21:04,019: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:21:04,019: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:21:04,019: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:21:04,019: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:21:49,854: Train batch 76200: loss: 70.54 grad norm: 22833.10 time: 0.250
2025-12-11 22:22:36,518: Train batch 76400: loss: 73.86 grad norm: 1026.38 time: 0.201
2025-12-11 22:23:22,459: Train batch 76600: loss: 70.46 grad norm: 41662.44 time: 0.235
2025-12-11 22:24:09,672: Train batch 76800: loss: 90.89 grad norm: 227823.66 time: 0.205
2025-12-11 22:24:58,245: Train batch 77000: loss: 74.60 grad norm: 1831.28 time: 0.232
2025-12-11 22:25:44,941: Train batch 77200: loss: 62.99 grad norm: 36600.50 time: 0.203
2025-12-11 22:26:32,080: Train batch 77400: loss: 78.13 grad norm: 26603.80 time: 0.199
2025-12-11 22:27:18,951: Train batch 77600: loss: 69.40 grad norm: 1626.73 time: 0.185
2025-12-11 22:28:07,164: Train batch 77800: loss: 70.88 grad norm: 19668.68 time: 0.235
2025-12-11 22:28:53,507: Train batch 78000: loss: 72.16 grad norm: 38522.49 time: 0.211
2025-12-11 22:28:53,508: Running test after training batch: 78000
2025-12-11 22:29:20,099: Val batch 78000: PER (avg): 0.9504 CTC Loss (avg): 165.0990 time: 26.592
2025-12-11 22:29:20,100: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:29:20,100: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:29:20,100: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:29:20,100: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:29:20,100: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:29:20,100: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:29:20,100: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:29:20,100: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:29:20,100: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:29:20,100: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:29:20,100: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:29:20,100: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:29:20,100: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:29:20,100: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:29:20,100: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:29:20,100: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:29:20,100: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:29:20,101: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:29:20,101: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:29:20,101: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:29:20,101: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:29:20,101: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:29:20,101: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:29:20,101: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:29:20,101: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:29:20,101: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:29:20,101: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:29:20,101: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:29:20,101: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:29:20,101: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:29:20,101: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:29:20,101: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:29:20,101: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:29:20,101: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:29:20,101: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:29:20,101: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:29:20,102: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:29:20,102: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:29:20,102: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:29:20,102: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:29:20,102: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:29:20,102: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:29:20,102: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:29:20,102: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:29:20,102: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:30:07,403: Train batch 78200: loss: 78.88 grad norm: 6631.45 time: 0.225
2025-12-11 22:30:54,811: Train batch 78400: loss: 74.31 grad norm: 7722.44 time: 0.259
2025-12-11 22:31:41,122: Train batch 78600: loss: 65.53 grad norm: 3766.63 time: 0.215
2025-12-11 22:32:28,087: Train batch 78800: loss: 72.32 grad norm: 8113.19 time: 0.208
2025-12-11 22:33:15,565: Train batch 79000: loss: 65.13 grad norm: 36070.04 time: 0.255
2025-12-11 22:34:03,089: Train batch 79200: loss: 72.26 grad norm: 9838.62 time: 0.253
2025-12-11 22:34:50,458: Train batch 79400: loss: 84.99 grad norm: 2077.86 time: 0.246
2025-12-11 22:35:37,565: Train batch 79600: loss: 77.50 grad norm: 9265.10 time: 0.189
2025-12-11 22:36:25,491: Train batch 79800: loss: 75.40 grad norm: 4772.52 time: 0.197
2025-12-11 22:37:12,531: Train batch 80000: loss: 70.91 grad norm: 1918.84 time: 0.251
2025-12-11 22:37:12,531: Running test after training batch: 80000
2025-12-11 22:37:39,115: Val batch 80000: PER (avg): 0.9504 CTC Loss (avg): 158.6635 time: 26.584
2025-12-11 22:37:39,115: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:37:39,115: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:37:39,115: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:37:39,115: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:37:39,115: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:37:39,115: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:37:39,115: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:37:39,115: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:37:39,115: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:37:39,115: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:37:39,116: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:37:39,116: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:37:39,116: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:37:39,116: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:37:39,116: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:37:39,116: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:37:39,116: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:37:39,116: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:37:39,116: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:37:39,116: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:37:39,116: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:37:39,116: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:37:39,116: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:37:39,116: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:37:39,116: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:37:39,116: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:37:39,116: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:37:39,116: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:37:39,116: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:37:39,117: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:37:39,117: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:37:39,117: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:37:39,117: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:37:39,117: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:37:39,117: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:37:39,117: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:37:39,117: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:37:39,117: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:37:39,117: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:37:39,117: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:37:39,117: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:37:39,117: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:37:39,117: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:37:39,117: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:37:39,117: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:38:25,807: Train batch 80200: loss: 62.18 grad norm: 4057.91 time: 0.229
2025-12-11 22:39:13,228: Train batch 80400: loss: 71.35 grad norm: 1584.78 time: 0.199
2025-12-11 22:39:59,485: Train batch 80600: loss: 68.66 grad norm: 36183.86 time: 0.227
2025-12-11 22:40:46,176: Train batch 80800: loss: 73.03 grad norm: 16899.70 time: 0.190
2025-12-11 22:41:32,771: Train batch 81000: loss: 60.62 grad norm: 29078.05 time: 0.182
2025-12-11 22:42:20,264: Train batch 81200: loss: 68.37 grad norm: 26968.55 time: 0.225
2025-12-11 22:43:07,650: Train batch 81400: loss: 70.28 grad norm: 18264.81 time: 0.297
2025-12-11 22:43:54,820: Train batch 81600: loss: 88.81 grad norm: 35831.01 time: 0.343
2025-12-11 22:44:40,609: Train batch 81800: loss: 79.79 grad norm: 10860.68 time: 0.208
2025-12-11 22:45:26,897: Train batch 82000: loss: 73.55 grad norm: 86996.07 time: 0.279
2025-12-11 22:45:26,897: Running test after training batch: 82000
2025-12-11 22:45:53,579: Val batch 82000: PER (avg): 0.9504 CTC Loss (avg): 177.1155 time: 26.682
2025-12-11 22:45:53,579: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:45:53,579: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:45:53,579: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:45:53,579: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:45:53,579: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:45:53,579: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:45:53,580: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:45:53,580: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:45:53,580: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:45:53,580: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:45:53,580: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:45:53,580: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:45:53,580: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:45:53,580: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:45:53,580: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:45:53,580: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:45:53,580: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:45:53,580: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:45:53,580: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:45:53,580: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:45:53,580: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:45:53,580: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:45:53,580: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:45:53,580: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:45:53,580: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:45:53,580: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:45:53,581: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:45:53,581: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:45:53,581: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:45:53,581: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:45:53,581: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:45:53,581: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:45:53,581: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:45:53,581: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:45:53,581: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:45:53,581: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:45:53,581: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:45:53,581: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:45:53,581: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:45:53,581: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:45:53,581: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:45:53,581: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:45:53,581: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:45:53,581: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:45:53,581: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:46:41,752: Train batch 82200: loss: 82.20 grad norm: 657200.75 time: 0.204
2025-12-11 22:47:27,802: Train batch 82400: loss: 76.74 grad norm: 395038.84 time: 0.244
2025-12-11 22:48:14,749: Train batch 82600: loss: 73.37 grad norm: 1855.53 time: 0.209
2025-12-11 22:49:03,122: Train batch 82800: loss: 81.73 grad norm: 20946.45 time: 0.226
2025-12-11 22:49:48,750: Train batch 83000: loss: 75.53 grad norm: 3619293.00 time: 0.245
2025-12-11 22:50:35,974: Train batch 83200: loss: 50.72 grad norm: 3707.20 time: 0.230
2025-12-11 22:51:23,468: Train batch 83400: loss: 86.85 grad norm: 12842.78 time: 0.201
2025-12-11 22:52:09,662: Train batch 83600: loss: 92.25 grad norm: 34997.10 time: 0.253
2025-12-11 22:52:58,195: Train batch 83800: loss: 88.40 grad norm: 1310.44 time: 0.232
2025-12-11 22:53:44,817: Train batch 84000: loss: 89.21 grad norm: 132775.11 time: 0.243
2025-12-11 22:53:44,817: Running test after training batch: 84000
2025-12-11 22:54:11,694: Val batch 84000: PER (avg): 0.9504 CTC Loss (avg): 183.7470 time: 26.876
2025-12-11 22:54:11,695: t15.2023.08.11 val PER: 1.0000
2025-12-11 22:54:11,695: t15.2023.08.13 val PER: 0.9491
2025-12-11 22:54:11,695: t15.2023.08.18 val PER: 0.9430
2025-12-11 22:54:11,695: t15.2023.08.20 val PER: 0.9476
2025-12-11 22:54:11,695: t15.2023.08.25 val PER: 0.9398
2025-12-11 22:54:11,696: t15.2023.08.27 val PER: 0.9405
2025-12-11 22:54:11,696: t15.2023.09.01 val PER: 0.9456
2025-12-11 22:54:11,696: t15.2023.09.03 val PER: 0.9430
2025-12-11 22:54:11,696: t15.2023.09.24 val PER: 0.9405
2025-12-11 22:54:11,696: t15.2023.09.29 val PER: 0.9541
2025-12-11 22:54:11,696: t15.2023.10.01 val PER: 0.9577
2025-12-11 22:54:11,696: t15.2023.10.06 val PER: 0.9397
2025-12-11 22:54:11,696: t15.2023.10.08 val PER: 0.9675
2025-12-11 22:54:11,696: t15.2023.10.13 val PER: 0.9511
2025-12-11 22:54:11,696: t15.2023.10.15 val PER: 0.9598
2025-12-11 22:54:11,696: t15.2023.10.20 val PER: 0.9597
2025-12-11 22:54:11,696: t15.2023.10.22 val PER: 0.9510
2025-12-11 22:54:11,696: t15.2023.11.03 val PER: 0.9573
2025-12-11 22:54:11,696: t15.2023.11.04 val PER: 0.9283
2025-12-11 22:54:11,696: t15.2023.11.17 val PER: 0.9393
2025-12-11 22:54:11,696: t15.2023.11.19 val PER: 0.9441
2025-12-11 22:54:11,696: t15.2023.11.26 val PER: 0.9587
2025-12-11 22:54:11,696: t15.2023.12.03 val PER: 0.9548
2025-12-11 22:54:11,697: t15.2023.12.08 val PER: 0.9501
2025-12-11 22:54:11,697: t15.2023.12.10 val PER: 0.9514
2025-12-11 22:54:11,697: t15.2023.12.17 val PER: 0.9563
2025-12-11 22:54:11,697: t15.2023.12.29 val PER: 0.9492
2025-12-11 22:54:11,697: t15.2024.02.25 val PER: 0.9551
2025-12-11 22:54:11,697: t15.2024.03.03 val PER: 1.0000
2025-12-11 22:54:11,697: t15.2024.03.08 val PER: 0.9502
2025-12-11 22:54:11,697: t15.2024.03.15 val PER: 0.9531
2025-12-11 22:54:11,697: t15.2024.03.17 val PER: 0.9505
2025-12-11 22:54:11,697: t15.2024.04.25 val PER: 1.0000
2025-12-11 22:54:11,697: t15.2024.04.28 val PER: 1.0000
2025-12-11 22:54:11,697: t15.2024.05.10 val PER: 0.9435
2025-12-11 22:54:11,697: t15.2024.06.14 val PER: 0.9432
2025-12-11 22:54:11,697: t15.2024.07.19 val PER: 0.9499
2025-12-11 22:54:11,697: t15.2024.07.21 val PER: 0.9524
2025-12-11 22:54:11,697: t15.2024.07.28 val PER: 0.9478
2025-12-11 22:54:11,697: t15.2025.01.10 val PER: 0.9559
2025-12-11 22:54:11,697: t15.2025.01.12 val PER: 0.9453
2025-12-11 22:54:11,697: t15.2025.03.14 val PER: 0.9527
2025-12-11 22:54:11,698: t15.2025.03.16 val PER: 0.9516
2025-12-11 22:54:11,698: t15.2025.03.30 val PER: 0.9494
2025-12-11 22:54:11,698: t15.2025.04.13 val PER: 0.9486
2025-12-11 22:54:59,195: Train batch 84200: loss: 75.16 grad norm: 36888.69 time: 0.330
2025-12-11 22:55:45,994: Train batch 84400: loss: 59.66 grad norm: 19697.91 time: 0.201
2025-12-11 22:56:33,205: Train batch 84600: loss: 72.25 grad norm: 42718.17 time: 0.189
2025-12-11 22:57:20,899: Train batch 84800: loss: 72.11 grad norm: 11759.25 time: 0.250
2025-12-11 22:58:08,520: Train batch 85000: loss: 70.39 grad norm: 21967.22 time: 0.207
2025-12-11 22:58:54,792: Train batch 85200: loss: 80.81 grad norm: 59367.91 time: 0.272
2025-12-11 22:59:41,955: Train batch 85400: loss: 78.09 grad norm: 3271.91 time: 0.233
2025-12-11 23:00:29,378: Train batch 85600: loss: 82.17 grad norm: 162302.45 time: 0.217
2025-12-11 23:01:16,311: Train batch 85800: loss: 82.27 grad norm: 11957.84 time: 0.268
2025-12-11 23:02:02,102: Train batch 86000: loss: 101.90 grad norm: 192885.31 time: 0.265
2025-12-11 23:02:02,102: Running test after training batch: 86000
2025-12-11 23:02:28,839: Val batch 86000: PER (avg): 0.9504 CTC Loss (avg): 210.3342 time: 26.737
2025-12-11 23:02:28,839: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:02:28,839: t15.2023.08.13 val PER: 0.9491
2025-12-11 23:02:28,839: t15.2023.08.18 val PER: 0.9430
2025-12-11 23:02:28,839: t15.2023.08.20 val PER: 0.9476
2025-12-11 23:02:28,839: t15.2023.08.25 val PER: 0.9398
2025-12-11 23:02:28,839: t15.2023.08.27 val PER: 0.9405
2025-12-11 23:02:28,840: t15.2023.09.01 val PER: 0.9456
2025-12-11 23:02:28,840: t15.2023.09.03 val PER: 0.9430
2025-12-11 23:02:28,840: t15.2023.09.24 val PER: 0.9405
2025-12-11 23:02:28,840: t15.2023.09.29 val PER: 0.9541
2025-12-11 23:02:28,840: t15.2023.10.01 val PER: 0.9577
2025-12-11 23:02:28,840: t15.2023.10.06 val PER: 0.9397
2025-12-11 23:02:28,840: t15.2023.10.08 val PER: 0.9675
2025-12-11 23:02:28,840: t15.2023.10.13 val PER: 0.9511
2025-12-11 23:02:28,840: t15.2023.10.15 val PER: 0.9598
2025-12-11 23:02:28,840: t15.2023.10.20 val PER: 0.9597
2025-12-11 23:02:28,840: t15.2023.10.22 val PER: 0.9510
2025-12-11 23:02:28,840: t15.2023.11.03 val PER: 0.9573
2025-12-11 23:02:28,840: t15.2023.11.04 val PER: 0.9283
2025-12-11 23:02:28,840: t15.2023.11.17 val PER: 0.9393
2025-12-11 23:02:28,840: t15.2023.11.19 val PER: 0.9441
2025-12-11 23:02:28,840: t15.2023.11.26 val PER: 0.9587
2025-12-11 23:02:28,840: t15.2023.12.03 val PER: 0.9548
2025-12-11 23:02:28,840: t15.2023.12.08 val PER: 0.9501
2025-12-11 23:02:28,840: t15.2023.12.10 val PER: 0.9514
2025-12-11 23:02:28,841: t15.2023.12.17 val PER: 0.9563
2025-12-11 23:02:28,841: t15.2023.12.29 val PER: 0.9492
2025-12-11 23:02:28,841: t15.2024.02.25 val PER: 0.9551
2025-12-11 23:02:28,841: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:02:28,841: t15.2024.03.08 val PER: 0.9502
2025-12-11 23:02:28,841: t15.2024.03.15 val PER: 0.9531
2025-12-11 23:02:28,841: t15.2024.03.17 val PER: 0.9505
2025-12-11 23:02:28,841: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:02:28,841: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:02:28,841: t15.2024.05.10 val PER: 0.9435
2025-12-11 23:02:28,841: t15.2024.06.14 val PER: 0.9432
2025-12-11 23:02:28,841: t15.2024.07.19 val PER: 0.9499
2025-12-11 23:02:28,841: t15.2024.07.21 val PER: 0.9524
2025-12-11 23:02:28,841: t15.2024.07.28 val PER: 0.9478
2025-12-11 23:02:28,841: t15.2025.01.10 val PER: 0.9559
2025-12-11 23:02:28,841: t15.2025.01.12 val PER: 0.9453
2025-12-11 23:02:28,841: t15.2025.03.14 val PER: 0.9527
2025-12-11 23:02:28,841: t15.2025.03.16 val PER: 0.9516
2025-12-11 23:02:28,841: t15.2025.03.30 val PER: 0.9494
2025-12-11 23:02:28,842: t15.2025.04.13 val PER: 0.9486
2025-12-11 23:03:16,135: Train batch 86200: loss: 89.76 grad norm: 593.92 time: 0.247
2025-12-11 23:04:02,130: Train batch 86400: loss: 69.95 grad norm: 66327.93 time: 0.172
2025-12-11 23:04:48,970: Train batch 86600: loss: 72.92 grad norm: 3121.88 time: 0.204
2025-12-11 23:05:37,227: Train batch 86800: loss: 72.95 grad norm: 14186.80 time: 0.249
2025-12-11 23:06:23,780: Train batch 87000: loss: 81.90 grad norm: 12012.56 time: 0.322
2025-12-11 23:07:10,612: Train batch 87200: loss: 72.02 grad norm: 3472.69 time: 0.177
2025-12-11 23:07:55,313: Train batch 87400: loss: 80.25 grad norm: 4379.91 time: 0.268
2025-12-11 23:08:42,068: Train batch 87600: loss: 78.07 grad norm: 1152.14 time: 0.175
2025-12-11 23:09:28,133: Train batch 87800: loss: 83.21 grad norm: 27453.16 time: 0.247
2025-12-11 23:10:15,067: Train batch 88000: loss: 83.32 grad norm: 593.48 time: 0.200
2025-12-11 23:10:15,067: Running test after training batch: 88000
2025-12-11 23:10:41,648: Val batch 88000: PER (avg): 0.9504 CTC Loss (avg): 179.2986 time: 26.581
2025-12-11 23:10:41,648: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:10:41,648: t15.2023.08.13 val PER: 0.9491
2025-12-11 23:10:41,648: t15.2023.08.18 val PER: 0.9430
2025-12-11 23:10:41,648: t15.2023.08.20 val PER: 0.9476
2025-12-11 23:10:41,648: t15.2023.08.25 val PER: 0.9398
2025-12-11 23:10:41,648: t15.2023.08.27 val PER: 0.9405
2025-12-11 23:10:41,648: t15.2023.09.01 val PER: 0.9456
2025-12-11 23:10:41,648: t15.2023.09.03 val PER: 0.9430
2025-12-11 23:10:41,648: t15.2023.09.24 val PER: 0.9405
2025-12-11 23:10:41,648: t15.2023.09.29 val PER: 0.9541
2025-12-11 23:10:41,649: t15.2023.10.01 val PER: 0.9577
2025-12-11 23:10:41,649: t15.2023.10.06 val PER: 0.9397
2025-12-11 23:10:41,649: t15.2023.10.08 val PER: 0.9675
2025-12-11 23:10:41,649: t15.2023.10.13 val PER: 0.9511
2025-12-11 23:10:41,649: t15.2023.10.15 val PER: 0.9598
2025-12-11 23:10:41,649: t15.2023.10.20 val PER: 0.9597
2025-12-11 23:10:41,649: t15.2023.10.22 val PER: 0.9510
2025-12-11 23:10:41,649: t15.2023.11.03 val PER: 0.9573
2025-12-11 23:10:41,649: t15.2023.11.04 val PER: 0.9283
2025-12-11 23:10:41,649: t15.2023.11.17 val PER: 0.9393
2025-12-11 23:10:41,649: t15.2023.11.19 val PER: 0.9441
2025-12-11 23:10:41,649: t15.2023.11.26 val PER: 0.9587
2025-12-11 23:10:41,649: t15.2023.12.03 val PER: 0.9548
2025-12-11 23:10:41,649: t15.2023.12.08 val PER: 0.9501
2025-12-11 23:10:41,649: t15.2023.12.10 val PER: 0.9514
2025-12-11 23:10:41,649: t15.2023.12.17 val PER: 0.9563
2025-12-11 23:10:41,649: t15.2023.12.29 val PER: 0.9492
2025-12-11 23:10:41,649: t15.2024.02.25 val PER: 0.9551
2025-12-11 23:10:41,649: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:10:41,649: t15.2024.03.08 val PER: 0.9502
2025-12-11 23:10:41,650: t15.2024.03.15 val PER: 0.9531
2025-12-11 23:10:41,650: t15.2024.03.17 val PER: 0.9505
2025-12-11 23:10:41,650: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:10:41,650: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:10:41,650: t15.2024.05.10 val PER: 0.9435
2025-12-11 23:10:41,650: t15.2024.06.14 val PER: 0.9432
2025-12-11 23:10:41,650: t15.2024.07.19 val PER: 0.9499
2025-12-11 23:10:41,650: t15.2024.07.21 val PER: 0.9524
2025-12-11 23:10:41,650: t15.2024.07.28 val PER: 0.9478
2025-12-11 23:10:41,650: t15.2025.01.10 val PER: 0.9559
2025-12-11 23:10:41,650: t15.2025.01.12 val PER: 0.9453
2025-12-11 23:10:41,650: t15.2025.03.14 val PER: 0.9527
2025-12-11 23:10:41,650: t15.2025.03.16 val PER: 0.9516
2025-12-11 23:10:41,650: t15.2025.03.30 val PER: 0.9494
2025-12-11 23:10:41,650: t15.2025.04.13 val PER: 0.9486
2025-12-11 23:11:28,208: Train batch 88200: loss: 83.47 grad norm: 2132.53 time: 0.207
2025-12-11 23:12:15,208: Train batch 88400: loss: 74.76 grad norm: 12010.06 time: 0.235
2025-12-11 23:13:01,475: Train batch 88600: loss: 80.39 grad norm: 8114.07 time: 0.200
2025-12-11 23:13:48,193: Train batch 88800: loss: 79.43 grad norm: 1439.57 time: 0.237
2025-12-11 23:14:35,643: Train batch 89000: loss: 70.90 grad norm: 1806.34 time: 0.205
2025-12-11 23:15:22,793: Train batch 89200: loss: 73.73 grad norm: 7447.87 time: 0.283
2025-12-11 23:16:10,035: Train batch 89400: loss: 84.98 grad norm: 11927.20 time: 0.224
2025-12-11 23:16:56,848: Train batch 89600: loss: 86.59 grad norm: 5498.51 time: 0.207
2025-12-11 23:17:44,097: Train batch 89800: loss: 74.20 grad norm: 66614.06 time: 0.216
2025-12-11 23:18:31,514: Train batch 90000: loss: 77.19 grad norm: 378.16 time: 0.199
2025-12-11 23:18:31,514: Running test after training batch: 90000
2025-12-11 23:18:58,082: Val batch 90000: PER (avg): 0.9504 CTC Loss (avg): 157.0241 time: 26.567
2025-12-11 23:18:58,082: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:18:58,082: t15.2023.08.13 val PER: 0.9491
2025-12-11 23:18:58,082: t15.2023.08.18 val PER: 0.9430
2025-12-11 23:18:58,082: t15.2023.08.20 val PER: 0.9476
2025-12-11 23:18:58,082: t15.2023.08.25 val PER: 0.9398
2025-12-11 23:18:58,082: t15.2023.08.27 val PER: 0.9405
2025-12-11 23:18:58,082: t15.2023.09.01 val PER: 0.9456
2025-12-11 23:18:58,082: t15.2023.09.03 val PER: 0.9430
2025-12-11 23:18:58,082: t15.2023.09.24 val PER: 0.9405
2025-12-11 23:18:58,082: t15.2023.09.29 val PER: 0.9541
2025-12-11 23:18:58,082: t15.2023.10.01 val PER: 0.9577
2025-12-11 23:18:58,082: t15.2023.10.06 val PER: 0.9397
2025-12-11 23:18:58,083: t15.2023.10.08 val PER: 0.9675
2025-12-11 23:18:58,083: t15.2023.10.13 val PER: 0.9511
2025-12-11 23:18:58,083: t15.2023.10.15 val PER: 0.9598
2025-12-11 23:18:58,083: t15.2023.10.20 val PER: 0.9597
2025-12-11 23:18:58,083: t15.2023.10.22 val PER: 0.9510
2025-12-11 23:18:58,083: t15.2023.11.03 val PER: 0.9573
2025-12-11 23:18:58,083: t15.2023.11.04 val PER: 0.9283
2025-12-11 23:18:58,083: t15.2023.11.17 val PER: 0.9393
2025-12-11 23:18:58,083: t15.2023.11.19 val PER: 0.9441
2025-12-11 23:18:58,083: t15.2023.11.26 val PER: 0.9587
2025-12-11 23:18:58,083: t15.2023.12.03 val PER: 0.9548
2025-12-11 23:18:58,083: t15.2023.12.08 val PER: 0.9501
2025-12-11 23:18:58,083: t15.2023.12.10 val PER: 0.9514
2025-12-11 23:18:58,083: t15.2023.12.17 val PER: 0.9563
2025-12-11 23:18:58,083: t15.2023.12.29 val PER: 0.9492
2025-12-11 23:18:58,083: t15.2024.02.25 val PER: 0.9551
2025-12-11 23:18:58,083: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:18:58,083: t15.2024.03.08 val PER: 0.9502
2025-12-11 23:18:58,083: t15.2024.03.15 val PER: 0.9531
2025-12-11 23:18:58,084: t15.2024.03.17 val PER: 0.9505
2025-12-11 23:18:58,084: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:18:58,084: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:18:58,084: t15.2024.05.10 val PER: 0.9435
2025-12-11 23:18:58,084: t15.2024.06.14 val PER: 0.9432
2025-12-11 23:18:58,084: t15.2024.07.19 val PER: 0.9499
2025-12-11 23:18:58,084: t15.2024.07.21 val PER: 0.9524
2025-12-11 23:18:58,084: t15.2024.07.28 val PER: 0.9478
2025-12-11 23:18:58,084: t15.2025.01.10 val PER: 0.9559
2025-12-11 23:18:58,084: t15.2025.01.12 val PER: 0.9453
2025-12-11 23:18:58,084: t15.2025.03.14 val PER: 0.9527
2025-12-11 23:18:58,084: t15.2025.03.16 val PER: 0.9516
2025-12-11 23:18:58,084: t15.2025.03.30 val PER: 0.9494
2025-12-11 23:18:58,084: t15.2025.04.13 val PER: 0.9486
2025-12-11 23:19:46,030: Train batch 90200: loss: 72.15 grad norm: 5374.27 time: 0.202
2025-12-11 23:20:33,190: Train batch 90400: loss: 69.23 grad norm: 42470.55 time: 0.217
2025-12-11 23:21:20,466: Train batch 90600: loss: 82.83 grad norm: 294923.38 time: 0.245
2025-12-11 23:22:07,333: Train batch 90800: loss: 69.02 grad norm: 8116.25 time: 0.190
2025-12-11 23:22:54,060: Train batch 91000: loss: 81.09 grad norm: 18885.08 time: 0.272
2025-12-11 23:23:41,085: Train batch 91200: loss: 76.47 grad norm: 130457.20 time: 0.306
2025-12-11 23:24:28,688: Train batch 91400: loss: 74.79 grad norm: 42195.24 time: 0.208
2025-12-11 23:25:16,588: Train batch 91600: loss: 81.24 grad norm: 6085.75 time: 0.211
2025-12-11 23:26:03,120: Train batch 91800: loss: 83.59 grad norm: 1104.39 time: 0.238
2025-12-11 23:26:51,240: Train batch 92000: loss: 89.02 grad norm: 7273.74 time: 0.202
2025-12-11 23:26:51,241: Running test after training batch: 92000
2025-12-11 23:27:18,302: Val batch 92000: PER (avg): 0.9504 CTC Loss (avg): 172.2165 time: 27.061
2025-12-11 23:27:18,303: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:27:18,303: t15.2023.08.13 val PER: 0.9491
2025-12-11 23:27:18,303: t15.2023.08.18 val PER: 0.9430
2025-12-11 23:27:18,303: t15.2023.08.20 val PER: 0.9476
2025-12-11 23:27:18,303: t15.2023.08.25 val PER: 0.9398
2025-12-11 23:27:18,303: t15.2023.08.27 val PER: 0.9405
2025-12-11 23:27:18,303: t15.2023.09.01 val PER: 0.9456
2025-12-11 23:27:18,303: t15.2023.09.03 val PER: 0.9430
2025-12-11 23:27:18,303: t15.2023.09.24 val PER: 0.9405
2025-12-11 23:27:18,303: t15.2023.09.29 val PER: 0.9541
2025-12-11 23:27:18,303: t15.2023.10.01 val PER: 0.9577
2025-12-11 23:27:18,303: t15.2023.10.06 val PER: 0.9397
2025-12-11 23:27:18,303: t15.2023.10.08 val PER: 0.9675
2025-12-11 23:27:18,303: t15.2023.10.13 val PER: 0.9511
2025-12-11 23:27:18,304: t15.2023.10.15 val PER: 0.9598
2025-12-11 23:27:18,304: t15.2023.10.20 val PER: 0.9597
2025-12-11 23:27:18,304: t15.2023.10.22 val PER: 0.9510
2025-12-11 23:27:18,304: t15.2023.11.03 val PER: 0.9573
2025-12-11 23:27:18,304: t15.2023.11.04 val PER: 0.9283
2025-12-11 23:27:18,304: t15.2023.11.17 val PER: 0.9393
2025-12-11 23:27:18,304: t15.2023.11.19 val PER: 0.9441
2025-12-11 23:27:18,304: t15.2023.11.26 val PER: 0.9587
2025-12-11 23:27:18,304: t15.2023.12.03 val PER: 0.9548
2025-12-11 23:27:18,304: t15.2023.12.08 val PER: 0.9501
2025-12-11 23:27:18,304: t15.2023.12.10 val PER: 0.9514
2025-12-11 23:27:18,304: t15.2023.12.17 val PER: 0.9563
2025-12-11 23:27:18,304: t15.2023.12.29 val PER: 0.9492
2025-12-11 23:27:18,304: t15.2024.02.25 val PER: 0.9551
2025-12-11 23:27:18,304: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:27:18,304: t15.2024.03.08 val PER: 0.9502
2025-12-11 23:27:18,304: t15.2024.03.15 val PER: 0.9531
2025-12-11 23:27:18,304: t15.2024.03.17 val PER: 0.9505
2025-12-11 23:27:18,305: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:27:18,305: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:27:18,305: t15.2024.05.10 val PER: 0.9435
2025-12-11 23:27:18,305: t15.2024.06.14 val PER: 0.9432
2025-12-11 23:27:18,305: t15.2024.07.19 val PER: 0.9499
2025-12-11 23:27:18,305: t15.2024.07.21 val PER: 0.9524
2025-12-11 23:27:18,305: t15.2024.07.28 val PER: 0.9478
2025-12-11 23:27:18,305: t15.2025.01.10 val PER: 0.9559
2025-12-11 23:27:18,305: t15.2025.01.12 val PER: 0.9453
2025-12-11 23:27:18,305: t15.2025.03.14 val PER: 0.9527
2025-12-11 23:27:18,305: t15.2025.03.16 val PER: 0.9516
2025-12-11 23:27:18,305: t15.2025.03.30 val PER: 0.9494
2025-12-11 23:27:18,305: t15.2025.04.13 val PER: 0.9486
2025-12-11 23:28:04,136: Train batch 92200: loss: 69.90 grad norm: 11330.94 time: 0.188
2025-12-11 23:28:51,846: Train batch 92400: loss: 72.47 grad norm: 14808.70 time: 0.229
2025-12-11 23:29:39,788: Train batch 92600: loss: 88.41 grad norm: 1318.54 time: 0.220
2025-12-11 23:30:27,380: Train batch 92800: loss: 81.49 grad norm: 13709.25 time: 0.208
2025-12-11 23:31:14,708: Train batch 93000: loss: 85.60 grad norm: 4340.36 time: 0.230
2025-12-11 23:32:01,225: Train batch 93200: loss: 73.49 grad norm: 2145.18 time: 0.220
2025-12-11 23:32:48,857: Train batch 93400: loss: 84.80 grad norm: 18181.04 time: 0.209
2025-12-11 23:33:36,471: Train batch 93600: loss: 72.15 grad norm: 51135.30 time: 0.230
2025-12-11 23:34:22,213: Train batch 93800: loss: 80.60 grad norm: 2878.82 time: 0.249
2025-12-11 23:35:09,254: Train batch 94000: loss: 80.71 grad norm: 154672.69 time: 0.205
2025-12-11 23:35:09,255: Running test after training batch: 94000
2025-12-11 23:35:36,361: Val batch 94000: PER (avg): 0.9655 CTC Loss (avg): 193.6326 time: 27.106
2025-12-11 23:35:36,361: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:35:36,361: t15.2023.08.13 val PER: 0.9636
2025-12-11 23:35:36,361: t15.2023.08.18 val PER: 0.9589
2025-12-11 23:35:36,361: t15.2023.08.20 val PER: 0.9619
2025-12-11 23:35:36,361: t15.2023.08.25 val PER: 0.9623
2025-12-11 23:35:36,361: t15.2023.08.27 val PER: 0.9598
2025-12-11 23:35:36,362: t15.2023.09.01 val PER: 0.9602
2025-12-11 23:35:36,362: t15.2023.09.03 val PER: 0.9596
2025-12-11 23:35:36,362: t15.2023.09.24 val PER: 0.9575
2025-12-11 23:35:36,362: t15.2023.09.29 val PER: 0.9694
2025-12-11 23:35:36,362: t15.2023.10.01 val PER: 0.9709
2025-12-11 23:35:36,362: t15.2023.10.06 val PER: 0.9612
2025-12-11 23:35:36,362: t15.2023.10.08 val PER: 0.9770
2025-12-11 23:35:36,362: t15.2023.10.13 val PER: 0.9659
2025-12-11 23:35:36,362: t15.2023.10.15 val PER: 0.9710
2025-12-11 23:35:36,362: t15.2023.10.20 val PER: 0.9698
2025-12-11 23:35:36,362: t15.2023.10.22 val PER: 0.9633
2025-12-11 23:35:36,362: t15.2023.11.03 val PER: 0.9661
2025-12-11 23:35:36,362: t15.2023.11.04 val PER: 0.9488
2025-12-11 23:35:36,362: t15.2023.11.17 val PER: 0.9611
2025-12-11 23:35:36,362: t15.2023.11.19 val PER: 0.9601
2025-12-11 23:35:36,362: t15.2023.11.26 val PER: 0.9681
2025-12-11 23:35:36,362: t15.2023.12.03 val PER: 0.9643
2025-12-11 23:35:36,362: t15.2023.12.08 val PER: 0.9667
2025-12-11 23:35:36,362: t15.2023.12.10 val PER: 0.9671
2025-12-11 23:35:36,363: t15.2023.12.17 val PER: 0.9688
2025-12-11 23:35:36,363: t15.2023.12.29 val PER: 0.9657
2025-12-11 23:35:36,363: t15.2024.02.25 val PER: 0.9677
2025-12-11 23:35:36,363: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:35:36,363: t15.2024.03.08 val PER: 0.9659
2025-12-11 23:35:36,363: t15.2024.03.15 val PER: 0.9700
2025-12-11 23:35:36,363: t15.2024.03.17 val PER: 0.9665
2025-12-11 23:35:36,363: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:35:36,363: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:35:36,363: t15.2024.05.10 val PER: 0.9629
2025-12-11 23:35:36,363: t15.2024.06.14 val PER: 0.9606
2025-12-11 23:35:36,363: t15.2024.07.19 val PER: 0.9684
2025-12-11 23:35:36,363: t15.2024.07.21 val PER: 0.9683
2025-12-11 23:35:36,363: t15.2024.07.28 val PER: 0.9647
2025-12-11 23:35:36,363: t15.2025.01.10 val PER: 0.9683
2025-12-11 23:35:36,363: t15.2025.01.12 val PER: 0.9638
2025-12-11 23:35:36,363: t15.2025.03.14 val PER: 0.9645
2025-12-11 23:35:36,363: t15.2025.03.16 val PER: 0.9686
2025-12-11 23:35:36,363: t15.2025.03.30 val PER: 0.9655
2025-12-11 23:35:36,364: t15.2025.04.13 val PER: 0.9643
2025-12-11 23:36:23,630: Train batch 94200: loss: 92.89 grad norm: 29866.45 time: 0.222
2025-12-11 23:37:11,330: Train batch 94400: loss: 60.93 grad norm: 13218.07 time: 0.174
2025-12-11 23:37:57,815: Train batch 94600: loss: 69.64 grad norm: 882.64 time: 0.196
2025-12-11 23:38:45,451: Train batch 94800: loss: 80.59 grad norm: 18281.58 time: 0.229
2025-12-11 23:39:32,799: Train batch 95000: loss: 71.59 grad norm: 13398.63 time: 0.194
2025-12-11 23:40:19,224: Train batch 95200: loss: 83.08 grad norm: 3763.24 time: 0.263
2025-12-11 23:41:07,406: Train batch 95400: loss: 90.81 grad norm: 16889.62 time: 0.241
2025-12-11 23:41:55,243: Train batch 95600: loss: 78.21 grad norm: 100166.16 time: 0.227
2025-12-11 23:42:42,803: Train batch 95800: loss: 68.61 grad norm: 81430.97 time: 0.207
2025-12-11 23:43:30,608: Train batch 96000: loss: 82.33 grad norm: 3773.26 time: 0.221
2025-12-11 23:43:30,608: Running test after training batch: 96000
2025-12-11 23:43:57,409: Val batch 96000: PER (avg): 0.9655 CTC Loss (avg): 185.3206 time: 26.801
2025-12-11 23:43:57,410: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:43:57,410: t15.2023.08.13 val PER: 0.9636
2025-12-11 23:43:57,410: t15.2023.08.18 val PER: 0.9589
2025-12-11 23:43:57,410: t15.2023.08.20 val PER: 0.9619
2025-12-11 23:43:57,410: t15.2023.08.25 val PER: 0.9623
2025-12-11 23:43:57,410: t15.2023.08.27 val PER: 0.9598
2025-12-11 23:43:57,410: t15.2023.09.01 val PER: 0.9602
2025-12-11 23:43:57,410: t15.2023.09.03 val PER: 0.9596
2025-12-11 23:43:57,410: t15.2023.09.24 val PER: 0.9575
2025-12-11 23:43:57,410: t15.2023.09.29 val PER: 0.9694
2025-12-11 23:43:57,410: t15.2023.10.01 val PER: 0.9709
2025-12-11 23:43:57,410: t15.2023.10.06 val PER: 0.9612
2025-12-11 23:43:57,410: t15.2023.10.08 val PER: 0.9770
2025-12-11 23:43:57,410: t15.2023.10.13 val PER: 0.9659
2025-12-11 23:43:57,410: t15.2023.10.15 val PER: 0.9710
2025-12-11 23:43:57,410: t15.2023.10.20 val PER: 0.9698
2025-12-11 23:43:57,411: t15.2023.10.22 val PER: 0.9633
2025-12-11 23:43:57,411: t15.2023.11.03 val PER: 0.9661
2025-12-11 23:43:57,411: t15.2023.11.04 val PER: 0.9488
2025-12-11 23:43:57,411: t15.2023.11.17 val PER: 0.9611
2025-12-11 23:43:57,411: t15.2023.11.19 val PER: 0.9601
2025-12-11 23:43:57,411: t15.2023.11.26 val PER: 0.9681
2025-12-11 23:43:57,411: t15.2023.12.03 val PER: 0.9643
2025-12-11 23:43:57,411: t15.2023.12.08 val PER: 0.9667
2025-12-11 23:43:57,411: t15.2023.12.10 val PER: 0.9671
2025-12-11 23:43:57,411: t15.2023.12.17 val PER: 0.9688
2025-12-11 23:43:57,411: t15.2023.12.29 val PER: 0.9657
2025-12-11 23:43:57,411: t15.2024.02.25 val PER: 0.9677
2025-12-11 23:43:57,411: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:43:57,411: t15.2024.03.08 val PER: 0.9659
2025-12-11 23:43:57,411: t15.2024.03.15 val PER: 0.9700
2025-12-11 23:43:57,411: t15.2024.03.17 val PER: 0.9665
2025-12-11 23:43:57,411: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:43:57,411: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:43:57,412: t15.2024.05.10 val PER: 0.9629
2025-12-11 23:43:57,412: t15.2024.06.14 val PER: 0.9606
2025-12-11 23:43:57,412: t15.2024.07.19 val PER: 0.9684
2025-12-11 23:43:57,412: t15.2024.07.21 val PER: 0.9683
2025-12-11 23:43:57,412: t15.2024.07.28 val PER: 0.9647
2025-12-11 23:43:57,412: t15.2025.01.10 val PER: 0.9683
2025-12-11 23:43:57,412: t15.2025.01.12 val PER: 0.9638
2025-12-11 23:43:57,412: t15.2025.03.14 val PER: 0.9645
2025-12-11 23:43:57,412: t15.2025.03.16 val PER: 0.9686
2025-12-11 23:43:57,412: t15.2025.03.30 val PER: 0.9655
2025-12-11 23:43:57,412: t15.2025.04.13 val PER: 0.9643
2025-12-11 23:44:44,171: Train batch 96200: loss: 65.53 grad norm: 474.70 time: 0.237
2025-12-11 23:45:31,540: Train batch 96400: loss: 80.32 grad norm: 19030.20 time: 0.209
2025-12-11 23:46:19,984: Train batch 96600: loss: 70.82 grad norm: 4301.55 time: 0.235
2025-12-11 23:47:08,118: Train batch 96800: loss: 82.04 grad norm: 15957.38 time: 0.198
2025-12-11 23:47:55,804: Train batch 97000: loss: 83.39 grad norm: 7410.75 time: 0.280
2025-12-11 23:48:43,612: Train batch 97200: loss: 70.03 grad norm: 18092.46 time: 0.350
2025-12-11 23:49:31,552: Train batch 97400: loss: 89.79 grad norm: 4248.64 time: 0.333
2025-12-11 23:50:18,436: Train batch 97600: loss: 67.15 grad norm: 4486.59 time: 0.219
2025-12-11 23:51:05,218: Train batch 97800: loss: 75.26 grad norm: 1626.81 time: 0.256
2025-12-11 23:51:52,053: Train batch 98000: loss: 71.79 grad norm: 1803.36 time: 0.227
2025-12-11 23:51:52,053: Running test after training batch: 98000
2025-12-11 23:52:18,925: Val batch 98000: PER (avg): 0.9655 CTC Loss (avg): 152.0366 time: 26.872
2025-12-11 23:52:18,926: t15.2023.08.11 val PER: 1.0000
2025-12-11 23:52:18,926: t15.2023.08.13 val PER: 0.9636
2025-12-11 23:52:18,926: t15.2023.08.18 val PER: 0.9589
2025-12-11 23:52:18,926: t15.2023.08.20 val PER: 0.9619
2025-12-11 23:52:18,926: t15.2023.08.25 val PER: 0.9623
2025-12-11 23:52:18,926: t15.2023.08.27 val PER: 0.9598
2025-12-11 23:52:18,926: t15.2023.09.01 val PER: 0.9602
2025-12-11 23:52:18,926: t15.2023.09.03 val PER: 0.9596
2025-12-11 23:52:18,926: t15.2023.09.24 val PER: 0.9575
2025-12-11 23:52:18,926: t15.2023.09.29 val PER: 0.9694
2025-12-11 23:52:18,926: t15.2023.10.01 val PER: 0.9709
2025-12-11 23:52:18,927: t15.2023.10.06 val PER: 0.9612
2025-12-11 23:52:18,927: t15.2023.10.08 val PER: 0.9770
2025-12-11 23:52:18,927: t15.2023.10.13 val PER: 0.9659
2025-12-11 23:52:18,927: t15.2023.10.15 val PER: 0.9710
2025-12-11 23:52:18,927: t15.2023.10.20 val PER: 0.9698
2025-12-11 23:52:18,927: t15.2023.10.22 val PER: 0.9633
2025-12-11 23:52:18,927: t15.2023.11.03 val PER: 0.9661
2025-12-11 23:52:18,927: t15.2023.11.04 val PER: 0.9488
2025-12-11 23:52:18,927: t15.2023.11.17 val PER: 0.9611
2025-12-11 23:52:18,927: t15.2023.11.19 val PER: 0.9601
2025-12-11 23:52:18,927: t15.2023.11.26 val PER: 0.9681
2025-12-11 23:52:18,927: t15.2023.12.03 val PER: 0.9643
2025-12-11 23:52:18,927: t15.2023.12.08 val PER: 0.9667
2025-12-11 23:52:18,927: t15.2023.12.10 val PER: 0.9671
2025-12-11 23:52:18,927: t15.2023.12.17 val PER: 0.9688
2025-12-11 23:52:18,927: t15.2023.12.29 val PER: 0.9657
2025-12-11 23:52:18,927: t15.2024.02.25 val PER: 0.9677
2025-12-11 23:52:18,927: t15.2024.03.03 val PER: 1.0000
2025-12-11 23:52:18,927: t15.2024.03.08 val PER: 0.9659
2025-12-11 23:52:18,928: t15.2024.03.15 val PER: 0.9700
2025-12-11 23:52:18,928: t15.2024.03.17 val PER: 0.9665
2025-12-11 23:52:18,928: t15.2024.04.25 val PER: 1.0000
2025-12-11 23:52:18,928: t15.2024.04.28 val PER: 1.0000
2025-12-11 23:52:18,928: t15.2024.05.10 val PER: 0.9629
2025-12-11 23:52:18,928: t15.2024.06.14 val PER: 0.9606
2025-12-11 23:52:18,928: t15.2024.07.19 val PER: 0.9684
2025-12-11 23:52:18,928: t15.2024.07.21 val PER: 0.9683
2025-12-11 23:52:18,928: t15.2024.07.28 val PER: 0.9647
2025-12-11 23:52:18,928: t15.2025.01.10 val PER: 0.9683
2025-12-11 23:52:18,928: t15.2025.01.12 val PER: 0.9638
2025-12-11 23:52:18,928: t15.2025.03.14 val PER: 0.9645
2025-12-11 23:52:18,928: t15.2025.03.16 val PER: 0.9686
2025-12-11 23:52:18,928: t15.2025.03.30 val PER: 0.9655
2025-12-11 23:52:18,928: t15.2025.04.13 val PER: 0.9643
2025-12-11 23:53:07,338: Train batch 98200: loss: 78.45 grad norm: 804.58 time: 0.208
2025-12-11 23:53:55,999: Train batch 98400: loss: 82.26 grad norm: 6555.21 time: 0.195
2025-12-11 23:54:42,457: Train batch 98600: loss: 77.98 grad norm: 29697.02 time: 0.269
2025-12-11 23:55:30,418: Train batch 98800: loss: 71.50 grad norm: 33638.44 time: 0.226
2025-12-11 23:56:17,680: Train batch 99000: loss: 80.07 grad norm: 1114.96 time: 0.256
2025-12-11 23:57:05,373: Train batch 99200: loss: 72.00 grad norm: 5033.55 time: 0.189
2025-12-11 23:57:53,661: Train batch 99400: loss: 73.63 grad norm: 3254.29 time: 0.214
2025-12-11 23:58:39,951: Train batch 99600: loss: 79.76 grad norm: 8467.80 time: 0.192
2025-12-11 23:59:27,069: Train batch 99800: loss: 92.31 grad norm: 3902.48 time: 0.251
2025-12-12 00:00:14,304: Train batch 100000: loss: 77.10 grad norm: 179132.03 time: 0.221
2025-12-12 00:00:14,304: Running test after training batch: 100000
2025-12-12 00:00:41,147: Val batch 100000: PER (avg): 1.0000 CTC Loss (avg): 157.9837 time: 26.843
2025-12-12 00:00:41,147: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:00:41,147: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:00:41,147: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:00:41,147: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:00:41,147: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:00:41,147: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:00:41,148: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:00:41,149: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:00:41,150: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:00:41,150: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:00:41,150: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:01:27,647: Train batch 100200: loss: 80.46 grad norm: 4896.24 time: 0.226
2025-12-12 00:02:15,217: Train batch 100400: loss: 76.25 grad norm: 1647.90 time: 0.228
2025-12-12 00:03:01,716: Train batch 100600: loss: 71.61 grad norm: 11202.67 time: 0.189
2025-12-12 00:03:47,790: Train batch 100800: loss: 82.37 grad norm: 39726.78 time: 0.240
2025-12-12 00:04:35,622: Train batch 101000: loss: 84.40 grad norm: 1178.84 time: 0.223
2025-12-12 00:05:21,537: Train batch 101200: loss: 79.05 grad norm: 1544.31 time: 0.324
2025-12-12 00:06:08,822: Train batch 101400: loss: 65.61 grad norm: 1609.92 time: 0.222
2025-12-12 00:06:56,212: Train batch 101600: loss: 70.73 grad norm: 13675.93 time: 0.250
2025-12-12 00:07:44,959: Train batch 101800: loss: 82.00 grad norm: 349759.84 time: 0.229
2025-12-12 00:08:32,260: Train batch 102000: loss: 62.09 grad norm: 16175.25 time: 0.227
2025-12-12 00:08:32,260: Running test after training batch: 102000
2025-12-12 00:08:59,195: Val batch 102000: PER (avg): 1.0000 CTC Loss (avg): 162.3552 time: 26.935
2025-12-12 00:08:59,196: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:08:59,196: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:08:59,197: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:08:59,198: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:09:46,260: Train batch 102200: loss: 68.64 grad norm: 3785.46 time: 0.249
2025-12-12 00:10:31,960: Train batch 102400: loss: 85.75 grad norm: 1029.42 time: 0.233
2025-12-12 00:11:17,674: Train batch 102600: loss: 80.84 grad norm: 10119.31 time: 0.273
2025-12-12 00:12:04,501: Train batch 102800: loss: 77.67 grad norm: 1755.50 time: 0.268
2025-12-12 00:12:51,838: Train batch 103000: loss: 64.18 grad norm: 5408.91 time: 0.206
2025-12-12 00:13:38,272: Train batch 103200: loss: 70.10 grad norm: 995.76 time: 0.211
2025-12-12 00:14:26,349: Train batch 103400: loss: 70.20 grad norm: 4746.54 time: 0.178
2025-12-12 00:15:13,575: Train batch 103600: loss: 67.82 grad norm: 264169.88 time: 0.315
2025-12-12 00:15:59,459: Train batch 103800: loss: 72.95 grad norm: 2874.56 time: 0.199
2025-12-12 00:16:45,821: Train batch 104000: loss: 76.31 grad norm: 4571.91 time: 0.343
2025-12-12 00:16:45,821: Running test after training batch: 104000
2025-12-12 00:17:12,630: Val batch 104000: PER (avg): 1.0000 CTC Loss (avg): 165.5671 time: 26.809
2025-12-12 00:17:12,630: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:17:12,630: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:17:12,630: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:17:12,631: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:17:12,632: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:17:12,633: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:18:00,143: Train batch 104200: loss: 71.67 grad norm: 60390.88 time: 0.235
2025-12-12 00:18:46,467: Train batch 104400: loss: 77.80 grad norm: 653.08 time: 0.226
2025-12-12 00:19:31,474: Train batch 104600: loss: 76.45 grad norm: 12141.41 time: 0.224
2025-12-12 00:20:18,484: Train batch 104800: loss: 90.51 grad norm: 57306.21 time: 0.252
2025-12-12 00:21:04,849: Train batch 105000: loss: 77.67 grad norm: 13821.44 time: 0.225
2025-12-12 00:21:51,342: Train batch 105200: loss: 85.86 grad norm: 5767.44 time: 0.255
2025-12-12 00:22:38,305: Train batch 105400: loss: 74.32 grad norm: 4647.22 time: 0.202
2025-12-12 00:23:25,785: Train batch 105600: loss: 88.08 grad norm: 5144.02 time: 0.257
2025-12-12 00:24:13,386: Train batch 105800: loss: 69.32 grad norm: 15953.37 time: 0.210
2025-12-12 00:24:59,814: Train batch 106000: loss: 88.64 grad norm: 6982.72 time: 0.238
2025-12-12 00:24:59,814: Running test after training batch: 106000
2025-12-12 00:25:26,670: Val batch 106000: PER (avg): 1.0000 CTC Loss (avg): 143.8150 time: 26.855
2025-12-12 00:25:26,670: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:25:26,670: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:25:26,670: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:25:26,671: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:25:26,672: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:25:26,673: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:25:26,673: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:25:26,673: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:25:26,673: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:26:14,036: Train batch 106200: loss: 80.62 grad norm: 1197.40 time: 0.233
2025-12-12 00:27:01,277: Train batch 106400: loss: 79.66 grad norm: 2364.18 time: 0.219
2025-12-12 00:27:48,075: Train batch 106600: loss: 64.14 grad norm: 3872.72 time: 0.224
2025-12-12 00:28:34,808: Train batch 106800: loss: 69.90 grad norm: 216.45 time: 0.228
2025-12-12 00:29:22,980: Train batch 107000: loss: 85.76 grad norm: 2149.29 time: 0.283
2025-12-12 00:30:11,057: Train batch 107200: loss: 77.81 grad norm: 354.74 time: 0.229
2025-12-12 00:30:58,500: Train batch 107400: loss: 65.30 grad norm: 1422.67 time: 0.203
2025-12-12 00:31:45,280: Train batch 107600: loss: 68.18 grad norm: 1768.40 time: 0.214
2025-12-12 00:32:32,876: Train batch 107800: loss: 62.14 grad norm: 470.95 time: 0.203
2025-12-12 00:33:20,389: Train batch 108000: loss: 77.77 grad norm: 3195.05 time: 0.213
2025-12-12 00:33:20,390: Running test after training batch: 108000
2025-12-12 00:33:46,827: Val batch 108000: PER (avg): 1.0000 CTC Loss (avg): 143.2018 time: 26.437
2025-12-12 00:33:46,827: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:33:46,827: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:33:46,828: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:33:46,829: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:34:32,897: Train batch 108200: loss: 68.47 grad norm: 1954.22 time: 0.225
2025-12-12 00:35:20,947: Train batch 108400: loss: 67.04 grad norm: 37479.25 time: 0.231
2025-12-12 00:36:08,549: Train batch 108600: loss: 76.63 grad norm: 11525.49 time: 0.242
2025-12-12 00:36:56,032: Train batch 108800: loss: 79.33 grad norm: 2941.72 time: 0.201
2025-12-12 00:37:44,076: Train batch 109000: loss: 76.31 grad norm: 3593.12 time: 0.265
2025-12-12 00:38:31,590: Train batch 109200: loss: 83.31 grad norm: 7616.90 time: 0.278
2025-12-12 00:39:17,378: Train batch 109400: loss: 75.01 grad norm: 40724.16 time: 0.220
2025-12-12 00:40:05,543: Train batch 109600: loss: 86.06 grad norm: 6967.58 time: 0.204
2025-12-12 00:40:53,510: Train batch 109800: loss: 77.88 grad norm: 27783.14 time: 0.239
2025-12-12 00:41:41,160: Train batch 110000: loss: 92.68 grad norm: 36516.38 time: 0.254
2025-12-12 00:41:41,161: Running test after training batch: 110000
2025-12-12 00:42:07,941: Val batch 110000: PER (avg): 1.0000 CTC Loss (avg): 134.1166 time: 26.780
2025-12-12 00:42:07,941: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:42:07,941: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:42:07,942: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:42:07,943: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:42:55,349: Train batch 110200: loss: 71.93 grad norm: 20484.43 time: 0.215
2025-12-12 00:43:42,777: Train batch 110400: loss: 69.81 grad norm: 7032.81 time: 0.207
2025-12-12 00:44:31,152: Train batch 110600: loss: 90.98 grad norm: 29703.77 time: 0.387
2025-12-12 00:45:18,809: Train batch 110800: loss: 82.08 grad norm: 2333.21 time: 0.254
2025-12-12 00:46:05,739: Train batch 111000: loss: 80.90 grad norm: 15731.69 time: 0.218
2025-12-12 00:46:52,578: Train batch 111200: loss: 79.00 grad norm: 3545.41 time: 0.201
2025-12-12 00:47:39,497: Train batch 111400: loss: 66.87 grad norm: 7987.64 time: 0.237
2025-12-12 00:48:26,343: Train batch 111600: loss: 76.34 grad norm: 2758.09 time: 0.253
2025-12-12 00:49:13,015: Train batch 111800: loss: 76.95 grad norm: 28304.76 time: 0.224
2025-12-12 00:49:59,610: Train batch 112000: loss: 76.08 grad norm: 9686.00 time: 0.207
2025-12-12 00:49:59,610: Running test after training batch: 112000
2025-12-12 00:50:26,011: Val batch 112000: PER (avg): 1.0000 CTC Loss (avg): 137.0208 time: 26.400
2025-12-12 00:50:26,011: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:50:26,011: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:50:26,012: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:50:26,013: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:51:12,752: Train batch 112200: loss: 66.39 grad norm: 4515.49 time: 0.196
2025-12-12 00:51:59,984: Train batch 112400: loss: 82.47 grad norm: 1588.43 time: 0.255
2025-12-12 00:52:47,470: Train batch 112600: loss: 68.02 grad norm: 1781.68 time: 0.236
2025-12-12 00:53:35,225: Train batch 112800: loss: 86.64 grad norm: 3514.21 time: 0.264
2025-12-12 00:54:21,546: Train batch 113000: loss: 77.98 grad norm: 14026.01 time: 0.215
2025-12-12 00:55:08,418: Train batch 113200: loss: 74.85 grad norm: 4064.01 time: 0.184
2025-12-12 00:55:55,658: Train batch 113400: loss: 70.08 grad norm: 3272.97 time: 0.264
2025-12-12 00:56:43,965: Train batch 113600: loss: 66.75 grad norm: 49274.99 time: 0.225
2025-12-12 00:57:31,110: Train batch 113800: loss: 68.99 grad norm: 16573.96 time: 0.209
2025-12-12 00:58:17,732: Train batch 114000: loss: 63.56 grad norm: 369720.59 time: 0.276
2025-12-12 00:58:17,732: Running test after training batch: 114000
2025-12-12 00:58:44,233: Val batch 114000: PER (avg): 1.0000 CTC Loss (avg): 135.5406 time: 26.501
2025-12-12 00:58:44,233: t15.2023.08.11 val PER: 1.0000
2025-12-12 00:58:44,233: t15.2023.08.13 val PER: 1.0000
2025-12-12 00:58:44,233: t15.2023.08.18 val PER: 1.0000
2025-12-12 00:58:44,233: t15.2023.08.20 val PER: 1.0000
2025-12-12 00:58:44,233: t15.2023.08.25 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.08.27 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.09.01 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.09.03 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.09.24 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.09.29 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.01 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.06 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.08 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.13 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.15 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.20 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.10.22 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.11.03 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.11.04 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.11.17 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.11.19 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.11.26 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.12.03 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.12.08 val PER: 1.0000
2025-12-12 00:58:44,234: t15.2023.12.10 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2023.12.17 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2023.12.29 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.02.25 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.03.03 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.03.08 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.03.15 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.03.17 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.04.25 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.04.28 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.05.10 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.06.14 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.07.19 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.07.21 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2024.07.28 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2025.01.10 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2025.01.12 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2025.03.14 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2025.03.16 val PER: 1.0000
2025-12-12 00:58:44,235: t15.2025.03.30 val PER: 1.0000
2025-12-12 00:58:44,236: t15.2025.04.13 val PER: 1.0000
2025-12-12 00:59:30,502: Train batch 114200: loss: 70.30 grad norm: 941.22 time: 0.251
2025-12-12 01:00:17,380: Train batch 114400: loss: 82.37 grad norm: 9417.72 time: 0.253
2025-12-12 01:01:03,808: Train batch 114600: loss: 76.29 grad norm: 24261.95 time: 0.231
2025-12-12 01:01:51,208: Train batch 114800: loss: 75.44 grad norm: 3562.50 time: 0.188
2025-12-12 01:02:37,961: Train batch 115000: loss: 74.28 grad norm: 649.12 time: 0.172
2025-12-12 01:03:24,366: Train batch 115200: loss: 89.75 grad norm: 171305.09 time: 0.215
2025-12-12 01:04:12,331: Train batch 115400: loss: 98.03 grad norm: 160193.12 time: 0.236
2025-12-12 01:04:58,923: Train batch 115600: loss: 91.61 grad norm: 3407.32 time: 0.224
2025-12-12 01:05:46,359: Train batch 115800: loss: 71.67 grad norm: 1956.95 time: 0.179
2025-12-12 01:06:32,891: Train batch 116000: loss: 93.32 grad norm: 1983.59 time: 0.278
2025-12-12 01:06:32,891: Running test after training batch: 116000
2025-12-12 01:06:59,368: Val batch 116000: PER (avg): 1.0000 CTC Loss (avg): 133.5692 time: 26.477
2025-12-12 01:06:59,368: t15.2023.08.11 val PER: 1.0000
2025-12-12 01:06:59,368: t15.2023.08.13 val PER: 1.0000
2025-12-12 01:06:59,368: t15.2023.08.18 val PER: 1.0000
2025-12-12 01:06:59,368: t15.2023.08.20 val PER: 1.0000
2025-12-12 01:06:59,368: t15.2023.08.25 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.08.27 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.09.01 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.09.03 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.09.24 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.09.29 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.01 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.06 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.08 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.13 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.15 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.20 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.10.22 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.11.03 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.11.04 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.11.17 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.11.19 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.11.26 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.12.03 val PER: 1.0000
2025-12-12 01:06:59,369: t15.2023.12.08 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2023.12.10 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2023.12.17 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2023.12.29 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.02.25 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.03.03 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.03.08 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.03.15 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.03.17 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.04.25 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.04.28 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.05.10 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.06.14 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.07.19 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.07.21 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2024.07.28 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2025.01.10 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2025.01.12 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2025.03.14 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2025.03.16 val PER: 1.0000
2025-12-12 01:06:59,370: t15.2025.03.30 val PER: 1.0000
2025-12-12 01:06:59,371: t15.2025.04.13 val PER: 1.0000
2025-12-12 01:07:47,088: Train batch 116200: loss: 64.85 grad norm: 1073.21 time: 0.205
2025-12-12 01:08:34,293: Train batch 116400: loss: 75.03 grad norm: 828.32 time: 0.176
2025-12-12 01:09:21,576: Train batch 116600: loss: 72.09 grad norm: 138421.33 time: 0.211
2025-12-12 01:10:07,373: Train batch 116800: loss: 83.90 grad norm: 715.41 time: 0.220
2025-12-12 01:10:53,822: Train batch 117000: loss: 67.54 grad norm: 5968.28 time: 0.196
2025-12-12 01:11:39,667: Train batch 117200: loss: 87.35 grad norm: 21288.82 time: 0.229
2025-12-12 01:12:26,327: Train batch 117400: loss: 79.64 grad norm: 4418.56 time: 0.195
2025-12-12 01:13:13,260: Train batch 117600: loss: 75.77 grad norm: 11640.50 time: 0.231
2025-12-12 01:13:58,999: Train batch 117800: loss: 62.53 grad norm: 65771.03 time: 0.232
2025-12-12 01:14:46,824: Train batch 118000: loss: 70.82 grad norm: 8589.00 time: 0.183
2025-12-12 01:14:46,825: Running test after training batch: 118000
2025-12-12 01:15:13,599: Val batch 118000: PER (avg): 1.0000 CTC Loss (avg): 135.6114 time: 26.774
2025-12-12 01:15:13,599: t15.2023.08.11 val PER: 1.0000
2025-12-12 01:15:13,599: t15.2023.08.13 val PER: 1.0000
2025-12-12 01:15:13,599: t15.2023.08.18 val PER: 1.0000
2025-12-12 01:15:13,599: t15.2023.08.20 val PER: 1.0000
2025-12-12 01:15:13,599: t15.2023.08.25 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.08.27 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.09.01 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.09.03 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.09.24 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.09.29 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.01 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.06 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.08 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.13 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.15 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.20 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.10.22 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.11.03 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.11.04 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.11.17 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.11.19 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.11.26 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.12.03 val PER: 1.0000
2025-12-12 01:15:13,600: t15.2023.12.08 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2023.12.10 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2023.12.17 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2023.12.29 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.02.25 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.03.03 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.03.08 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.03.15 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.03.17 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.04.25 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.04.28 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.05.10 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.06.14 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.07.19 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.07.21 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2024.07.28 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2025.01.10 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2025.01.12 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2025.03.14 val PER: 1.0000
2025-12-12 01:15:13,601: t15.2025.03.16 val PER: 1.0000
2025-12-12 01:15:13,602: t15.2025.03.30 val PER: 1.0000
2025-12-12 01:15:13,602: t15.2025.04.13 val PER: 1.0000
2025-12-12 01:16:00,847: Train batch 118200: loss: 85.46 grad norm: 2040.80 time: 0.241
2025-12-12 01:16:46,609: Train batch 118400: loss: 73.26 grad norm: 12064.40 time: 0.224
2025-12-12 01:17:33,782: Train batch 118600: loss: 79.06 grad norm: 24070.18 time: 0.236
2025-12-12 01:18:20,204: Train batch 118800: loss: 87.42 grad norm: 3746.93 time: 0.255
2025-12-12 01:19:06,937: Train batch 119000: loss: 88.74 grad norm: 421.60 time: 0.211
2025-12-12 01:19:52,124: Train batch 119200: loss: 75.29 grad norm: 6029.66 time: 0.212
2025-12-12 01:20:38,999: Train batch 119400: loss: 64.28 grad norm: 4575.10 time: 0.173
2025-12-12 01:21:25,554: Train batch 119600: loss: 62.59 grad norm: 1428.28 time: 0.168
2025-12-12 01:22:11,365: Train batch 119800: loss: 79.98 grad norm: 6643.86 time: 0.189
2025-12-12 01:22:55,444: Running test after training batch: 119999
2025-12-12 01:23:21,835: Val batch 119999: PER (avg): 1.0000 CTC Loss (avg): 135.5075 time: 26.391
2025-12-12 01:23:21,836: t15.2023.08.11 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.08.13 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.08.18 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.08.20 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.08.25 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.08.27 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.09.01 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.09.03 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.09.24 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.09.29 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.01 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.06 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.08 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.13 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.15 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.20 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.10.22 val PER: 1.0000
2025-12-12 01:23:21,836: t15.2023.11.03 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.11.04 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.11.17 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.11.19 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.11.26 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.12.03 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.12.08 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.12.10 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.12.17 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2023.12.29 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.02.25 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.03.03 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.03.08 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.03.15 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.03.17 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.04.25 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.04.28 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.05.10 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.06.14 val PER: 1.0000
2025-12-12 01:23:21,837: t15.2024.07.19 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2024.07.21 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2024.07.28 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.01.10 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.01.12 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.03.14 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.03.16 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.03.30 val PER: 1.0000
2025-12-12 01:23:21,838: t15.2025.04.13 val PER: 1.0000
2025-12-12 01:23:21,909: Best avg val PER achieved: 0.72925
2025-12-12 01:23:21,909: Total training time: 496.64 minutes
