2025-12-15 18:24:27,066: Using device: cuda:0
2025-12-15 18:24:27,068: Initializing Conformer with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-15 18:24:27,462: Initialized RNN decoding model
2025-12-15 18:24:27,462: UNetEnhancedModel(
  (unet): NeuralUNet(
    (inc): DoubleConv(
      (double_conv): Sequential(
        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (down1): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (outc): OutConv(
      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (decoder): ConformerDecoder(
    (day_layers): ModuleList(
      (0-44): 45 x Sequential(
        (0): Linear(in_features=512, out_features=512, bias=True)
        (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (2): GELU(approximate='none')
        (3): Dropout(p=0.2, inplace=False)
      )
    )
    (input_proj): Sequential(
      (0): Linear(in_features=7168, out_features=256, bias=True)
      (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (2): Dropout(p=0.2, inplace=False)
    )
    (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (conformer): Conformer(
      (conformer_layers): ModuleList(
        (0-3): 4 x ConformerLayer(
          (ffn1): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=256, out_features=512, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.2, inplace=False)
              (4): Linear(in_features=512, out_features=256, bias=True)
              (5): Dropout(p=0.2, inplace=False)
            )
          )
          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
          )
          (self_attn_dropout): Dropout(p=0.2, inplace=False)
          (conv_module): _ConvolutionModule(
            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (sequential): Sequential(
              (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
              (1): GLU(dim=1)
              (2): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
              (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (4): SiLU()
              (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
              (6): Dropout(p=0.2, inplace=False)
            )
          )
          (ffn2): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=256, out_features=512, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.2, inplace=False)
              (4): Linear(in_features=512, out_features=256, bias=True)
              (5): Dropout(p=0.2, inplace=False)
            )
          )
          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (output_dropout): Dropout(p=0.1, inplace=False)
    (out): Linear(in_features=256, out_features=41, bias=True)
  )
)
2025-12-15 18:24:27,465: Model has 34,948,906 parameters
2025-12-15 18:24:27,466: Model has 11,865,600 day-specific parameters | 33.95% of total parameters
2025-12-15 18:24:37,644: Successfully initialized datasets
2025-12-15 18:24:39,499: Train batch 0: loss: 681.63 grad norm: 908.62 time: 1.439
2025-12-15 18:24:39,499: Running test after training batch: 0
2025-12-15 18:25:08,490: Val batch 0: PER (avg): 4.4215 CTC Loss (avg): 720.0045 time: 28.990
2025-12-15 18:25:08,490: t15.2023.08.11 val PER: 1.0000
2025-12-15 18:25:08,490: t15.2023.08.13 val PER: 3.8087
2025-12-15 18:25:08,490: t15.2023.08.18 val PER: 4.1844
2025-12-15 18:25:08,490: t15.2023.08.20 val PER: 4.1033
2025-12-15 18:25:08,490: t15.2023.08.25 val PER: 4.1386
2025-12-15 18:25:08,490: t15.2023.08.27 val PER: 3.5305
2025-12-15 18:25:08,490: t15.2023.09.01 val PER: 4.3231
2025-12-15 18:25:08,491: t15.2023.09.03 val PER: 4.0285
2025-12-15 18:25:08,491: t15.2023.09.24 val PER: 4.6833
2025-12-15 18:25:08,491: t15.2023.09.29 val PER: 4.5348
2025-12-15 18:25:08,491: t15.2023.10.01 val PER: 3.4254
2025-12-15 18:25:08,491: t15.2023.10.06 val PER: 4.6717
2025-12-15 18:25:08,491: t15.2023.10.08 val PER: 3.3153
2025-12-15 18:25:08,491: t15.2023.10.13 val PER: 4.3429
2025-12-15 18:25:08,491: t15.2023.10.15 val PER: 4.0666
2025-12-15 18:25:08,491: t15.2023.10.20 val PER: 4.3591
2025-12-15 18:25:08,491: t15.2023.10.22 val PER: 4.6147
2025-12-15 18:25:08,491: t15.2023.11.03 val PER: 4.7212
2025-12-15 18:25:08,491: t15.2023.11.04 val PER: 6.1980
2025-12-15 18:25:08,491: t15.2023.11.17 val PER: 6.0793
2025-12-15 18:25:08,491: t15.2023.11.19 val PER: 5.1417
2025-12-15 18:25:08,491: t15.2023.11.26 val PER: 4.6674
2025-12-15 18:25:08,491: t15.2023.12.03 val PER: 4.5389
2025-12-15 18:25:08,491: t15.2023.12.08 val PER: 4.8502
2025-12-15 18:25:08,491: t15.2023.12.10 val PER: 5.1537
2025-12-15 18:25:08,492: t15.2023.12.17 val PER: 4.3503
2025-12-15 18:25:08,492: t15.2023.12.29 val PER: 4.2807
2025-12-15 18:25:08,492: t15.2024.02.25 val PER: 4.5801
2025-12-15 18:25:08,492: t15.2024.03.03 val PER: 1.0000
2025-12-15 18:25:08,492: t15.2024.03.08 val PER: 4.0427
2025-12-15 18:25:08,492: t15.2024.03.15 val PER: 4.0369
2025-12-15 18:25:08,492: t15.2024.03.17 val PER: 4.5153
2025-12-15 18:25:08,492: t15.2024.04.25 val PER: 1.0000
2025-12-15 18:25:08,492: t15.2024.04.28 val PER: 1.0000
2025-12-15 18:25:08,492: t15.2024.05.10 val PER: 4.3120
2025-12-15 18:25:08,492: t15.2024.06.14 val PER: 5.1262
2025-12-15 18:25:08,492: t15.2024.07.19 val PER: 3.0567
2025-12-15 18:25:08,492: t15.2024.07.21 val PER: 4.9986
2025-12-15 18:25:08,492: t15.2024.07.28 val PER: 5.3324
2025-12-15 18:25:08,492: t15.2025.01.10 val PER: 3.4683
2025-12-15 18:25:08,492: t15.2025.01.12 val PER: 5.1917
2025-12-15 18:25:08,492: t15.2025.03.14 val PER: 3.3654
2025-12-15 18:25:08,492: t15.2025.03.16 val PER: 5.8914
2025-12-15 18:25:08,492: t15.2025.03.30 val PER: 4.4046
2025-12-15 18:25:08,493: t15.2025.04.13 val PER: 5.0428
2025-12-15 18:25:08,493: New best test PER inf --> 4.4215
2025-12-15 18:25:08,493: Checkpointing model
2025-12-15 18:25:08,660: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_182426/checkpoint/best_checkpoint
2025-12-15 18:25:54,147: Train batch 200: loss: 397.88 grad norm: 2056.58 time: 0.198
2025-12-15 18:26:39,994: Train batch 400: loss: 232.93 grad norm: 1699.89 time: 0.228
2025-12-15 18:27:26,698: Train batch 600: loss: 109.10 grad norm: 295.73 time: 0.207
2025-12-15 18:28:12,428: Train batch 800: loss: 98.10 grad norm: 23.20 time: 0.301
2025-12-15 18:28:58,310: Train batch 1000: loss: 104.87 grad norm: 22.61 time: 0.260
2025-12-15 18:29:42,291: Train batch 1200: loss: 61.15 grad norm: 10.17 time: 0.189
2025-12-15 18:30:28,670: Train batch 1400: loss: 91.48 grad norm: 33.59 time: 0.211
2025-12-15 18:31:15,001: Train batch 1600: loss: 81.31 grad norm: 30.88 time: 0.198
2025-12-15 18:32:00,936: Train batch 1800: loss: 65.93 grad norm: 28.84 time: 0.197
2025-12-15 18:32:46,170: Train batch 2000: loss: 79.23 grad norm: 25.78 time: 0.221
2025-12-15 18:32:46,170: Running test after training batch: 2000
2025-12-15 18:33:12,204: Val batch 2000: PER (avg): 0.7832 CTC Loss (avg): 118.3495 time: 26.034
2025-12-15 18:33:12,205: t15.2023.08.11 val PER: 1.0000
2025-12-15 18:33:12,205: t15.2023.08.13 val PER: 0.7640
2025-12-15 18:33:12,205: t15.2023.08.18 val PER: 0.7645
2025-12-15 18:33:12,205: t15.2023.08.20 val PER: 0.7689
2025-12-15 18:33:12,205: t15.2023.08.25 val PER: 0.7636
2025-12-15 18:33:12,205: t15.2023.08.27 val PER: 0.7781
2025-12-15 18:33:12,205: t15.2023.09.01 val PER: 0.7200
2025-12-15 18:33:12,205: t15.2023.09.03 val PER: 0.7494
2025-12-15 18:33:12,205: t15.2023.09.24 val PER: 0.7925
2025-12-15 18:33:12,205: t15.2023.09.29 val PER: 0.7658
2025-12-15 18:33:12,205: t15.2023.10.01 val PER: 0.7708
2025-12-15 18:33:12,205: t15.2023.10.06 val PER: 0.7987
2025-12-15 18:33:12,205: t15.2023.10.08 val PER: 0.7943
2025-12-15 18:33:12,205: t15.2023.10.13 val PER: 0.8123
2025-12-15 18:33:12,206: t15.2023.10.15 val PER: 0.7699
2025-12-15 18:33:12,206: t15.2023.10.20 val PER: 0.7718
2025-12-15 18:33:12,206: t15.2023.10.22 val PER: 0.7494
2025-12-15 18:33:12,206: t15.2023.11.03 val PER: 0.8270
2025-12-15 18:33:12,206: t15.2023.11.04 val PER: 0.7679
2025-12-15 18:33:12,206: t15.2023.11.17 val PER: 0.8212
2025-12-15 18:33:12,206: t15.2023.11.19 val PER: 0.7844
2025-12-15 18:33:12,206: t15.2023.11.26 val PER: 0.8152
2025-12-15 18:33:12,206: t15.2023.12.03 val PER: 0.7931
2025-12-15 18:33:12,206: t15.2023.12.08 val PER: 0.7763
2025-12-15 18:33:12,206: t15.2023.12.10 val PER: 0.7700
2025-12-15 18:33:12,206: t15.2023.12.17 val PER: 0.8191
2025-12-15 18:33:12,206: t15.2023.12.29 val PER: 0.8353
2025-12-15 18:33:12,206: t15.2024.02.25 val PER: 0.7612
2025-12-15 18:33:12,206: t15.2024.03.03 val PER: 1.0000
2025-12-15 18:33:12,206: t15.2024.03.08 val PER: 0.7809
2025-12-15 18:33:12,206: t15.2024.03.15 val PER: 0.8493
2025-12-15 18:33:12,206: t15.2024.03.17 val PER: 0.7824
2025-12-15 18:33:12,206: t15.2024.04.25 val PER: 1.0000
2025-12-15 18:33:12,207: t15.2024.04.28 val PER: 1.0000
2025-12-15 18:33:12,207: t15.2024.05.10 val PER: 0.7816
2025-12-15 18:33:12,207: t15.2024.06.14 val PER: 0.7681
2025-12-15 18:33:12,207: t15.2024.07.19 val PER: 0.7528
2025-12-15 18:33:12,207: t15.2024.07.21 val PER: 0.7586
2025-12-15 18:33:12,207: t15.2024.07.28 val PER: 0.7603
2025-12-15 18:33:12,207: t15.2025.01.10 val PER: 0.8140
2025-12-15 18:33:12,207: t15.2025.01.12 val PER: 0.7806
2025-12-15 18:33:12,207: t15.2025.03.14 val PER: 0.8195
2025-12-15 18:33:12,207: t15.2025.03.16 val PER: 0.7421
2025-12-15 18:33:12,207: t15.2025.03.30 val PER: 0.8138
2025-12-15 18:33:12,207: t15.2025.04.13 val PER: 0.7589
2025-12-15 18:33:12,207: New best test PER 4.4215 --> 0.7832
2025-12-15 18:33:12,207: Checkpointing model
2025-12-15 18:33:12,758: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_182426/checkpoint/best_checkpoint
2025-12-15 18:33:59,021: Train batch 2200: loss: 97.30 grad norm: 33.32 time: 0.240
2025-12-15 18:34:44,859: Train batch 2400: loss: 90.66 grad norm: 31.69 time: 0.237
2025-12-15 18:35:30,151: Train batch 2600: loss: 83.00 grad norm: 37.29 time: 0.267
2025-12-15 18:36:15,405: Train batch 2800: loss: 78.87 grad norm: 34.49 time: 0.208
2025-12-15 18:37:01,361: Train batch 3000: loss: 93.51 grad norm: 40.60 time: 0.224
2025-12-15 18:37:46,986: Train batch 3200: loss: 91.30 grad norm: 38.37 time: 0.272
2025-12-15 18:38:31,270: Train batch 3400: loss: 76.68 grad norm: 35.83 time: 0.167
2025-12-15 18:39:16,225: Train batch 3600: loss: 69.86 grad norm: 25.71 time: 0.242
2025-12-15 18:40:01,588: Train batch 3800: loss: 85.23 grad norm: 27.10 time: 0.247
2025-12-15 18:40:47,991: Train batch 4000: loss: 71.54 grad norm: 38.03 time: 0.194
2025-12-15 18:40:47,991: Running test after training batch: 4000
2025-12-15 18:41:13,930: Val batch 4000: PER (avg): 0.7678 CTC Loss (avg): 128.9887 time: 25.939
2025-12-15 18:41:13,931: t15.2023.08.11 val PER: 1.0000
2025-12-15 18:41:13,931: t15.2023.08.13 val PER: 0.7256
2025-12-15 18:41:13,931: t15.2023.08.18 val PER: 0.7209
2025-12-15 18:41:13,931: t15.2023.08.20 val PER: 0.7331
2025-12-15 18:41:13,931: t15.2023.08.25 val PER: 0.7620
2025-12-15 18:41:13,931: t15.2023.08.27 val PER: 0.8103
2025-12-15 18:41:13,931: t15.2023.09.01 val PER: 0.7451
2025-12-15 18:41:13,931: t15.2023.09.03 val PER: 0.7340
2025-12-15 18:41:13,931: t15.2023.09.24 val PER: 0.7706
2025-12-15 18:41:13,931: t15.2023.09.29 val PER: 0.7588
2025-12-15 18:41:13,931: t15.2023.10.01 val PER: 0.7371
2025-12-15 18:41:13,931: t15.2023.10.06 val PER: 0.7330
2025-12-15 18:41:13,931: t15.2023.10.08 val PER: 0.7984
2025-12-15 18:41:13,931: t15.2023.10.13 val PER: 0.8076
2025-12-15 18:41:13,931: t15.2023.10.15 val PER: 0.7989
2025-12-15 18:41:13,931: t15.2023.10.20 val PER: 0.7450
2025-12-15 18:41:13,931: t15.2023.10.22 val PER: 0.7806
2025-12-15 18:41:13,932: t15.2023.11.03 val PER: 0.8039
2025-12-15 18:41:13,932: t15.2023.11.04 val PER: 0.7372
2025-12-15 18:41:13,932: t15.2023.11.17 val PER: 0.7978
2025-12-15 18:41:13,932: t15.2023.11.19 val PER: 0.7485
2025-12-15 18:41:13,932: t15.2023.11.26 val PER: 0.8181
2025-12-15 18:41:13,932: t15.2023.12.03 val PER: 0.8046
2025-12-15 18:41:13,932: t15.2023.12.08 val PER: 0.7916
2025-12-15 18:41:13,932: t15.2023.12.10 val PER: 0.7727
2025-12-15 18:41:13,932: t15.2023.12.17 val PER: 0.8378
2025-12-15 18:41:13,932: t15.2023.12.29 val PER: 0.8023
2025-12-15 18:41:13,932: t15.2024.02.25 val PER: 0.7654
2025-12-15 18:41:13,932: t15.2024.03.03 val PER: 1.0000
2025-12-15 18:41:13,932: t15.2024.03.08 val PER: 0.7667
2025-12-15 18:41:13,932: t15.2024.03.15 val PER: 0.7917
2025-12-15 18:41:13,932: t15.2024.03.17 val PER: 0.7741
2025-12-15 18:41:13,932: t15.2024.04.25 val PER: 1.0000
2025-12-15 18:41:13,932: t15.2024.04.28 val PER: 1.0000
2025-12-15 18:41:13,932: t15.2024.05.10 val PER: 0.7756
2025-12-15 18:41:13,932: t15.2024.06.14 val PER: 0.7508
2025-12-15 18:41:13,933: t15.2024.07.19 val PER: 0.7238
2025-12-15 18:41:13,933: t15.2024.07.21 val PER: 0.7600
2025-12-15 18:41:13,933: t15.2024.07.28 val PER: 0.7772
2025-12-15 18:41:13,933: t15.2025.01.10 val PER: 0.7534
2025-12-15 18:41:13,933: t15.2025.01.12 val PER: 0.7306
2025-12-15 18:41:13,933: t15.2025.03.14 val PER: 0.7441
2025-12-15 18:41:13,933: t15.2025.03.16 val PER: 0.7238
2025-12-15 18:41:13,933: t15.2025.03.30 val PER: 0.7483
2025-12-15 18:41:13,933: t15.2025.04.13 val PER: 0.7290
2025-12-15 18:41:13,933: New best test PER 0.7832 --> 0.7678
2025-12-15 18:41:13,933: Checkpointing model
2025-12-15 18:41:14,465: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_182426/checkpoint/best_checkpoint
2025-12-15 18:41:58,930: Train batch 4200: loss: 63.92 grad norm: 24.24 time: 0.164
2025-12-15 18:42:44,003: Train batch 4400: loss: 100.37 grad norm: 41.85 time: 0.255
2025-12-15 18:43:28,691: Train batch 4600: loss: 90.96 grad norm: 35.51 time: 0.205
2025-12-15 18:44:14,238: Train batch 4800: loss: 82.29 grad norm: 26.36 time: 0.226
2025-12-15 18:45:00,155: Train batch 5000: loss: 95.78 grad norm: 56.23 time: 0.312
2025-12-15 18:45:47,107: Train batch 5200: loss: 85.48 grad norm: 36.06 time: 0.217
2025-12-15 18:46:34,489: Train batch 5400: loss: 72.66 grad norm: 28.99 time: 0.191
2025-12-15 18:47:20,669: Train batch 5600: loss: 80.63 grad norm: 43.18 time: 0.197
2025-12-15 18:48:06,502: Train batch 5800: loss: 92.74 grad norm: 40.08 time: 0.206
2025-12-15 18:48:51,438: Train batch 6000: loss: 65.25 grad norm: 38.67 time: 0.191
2025-12-15 18:48:51,438: Running test after training batch: 6000
2025-12-15 18:49:17,368: Val batch 6000: PER (avg): 0.7047 CTC Loss (avg): 116.1153 time: 25.930
2025-12-15 18:49:17,369: t15.2023.08.11 val PER: 1.0000
2025-12-15 18:49:17,369: t15.2023.08.13 val PER: 0.6778
2025-12-15 18:49:17,369: t15.2023.08.18 val PER: 0.7200
2025-12-15 18:49:17,369: t15.2023.08.20 val PER: 0.6855
2025-12-15 18:49:17,369: t15.2023.08.25 val PER: 0.7033
2025-12-15 18:49:17,369: t15.2023.08.27 val PER: 0.7299
2025-12-15 18:49:17,369: t15.2023.09.01 val PER: 0.6607
2025-12-15 18:49:17,369: t15.2023.09.03 val PER: 0.6544
2025-12-15 18:49:17,369: t15.2023.09.24 val PER: 0.6748
2025-12-15 18:49:17,369: t15.2023.09.29 val PER: 0.6771
2025-12-15 18:49:17,369: t15.2023.10.01 val PER: 0.7226
2025-12-15 18:49:17,369: t15.2023.10.06 val PER: 0.6771
2025-12-15 18:49:17,369: t15.2023.10.08 val PER: 0.6793
2025-12-15 18:49:17,369: t15.2023.10.13 val PER: 0.7300
2025-12-15 18:49:17,370: t15.2023.10.15 val PER: 0.7007
2025-12-15 18:49:17,370: t15.2023.10.20 val PER: 0.6812
2025-12-15 18:49:17,370: t15.2023.10.22 val PER: 0.6849
2025-12-15 18:49:17,370: t15.2023.11.03 val PER: 0.7273
2025-12-15 18:49:17,370: t15.2023.11.04 val PER: 0.7031
2025-12-15 18:49:17,370: t15.2023.11.17 val PER: 0.7045
2025-12-15 18:49:17,370: t15.2023.11.19 val PER: 0.6387
2025-12-15 18:49:17,370: t15.2023.11.26 val PER: 0.7471
2025-12-15 18:49:17,370: t15.2023.12.03 val PER: 0.7206
2025-12-15 18:49:17,370: t15.2023.12.08 val PER: 0.6951
2025-12-15 18:49:17,370: t15.2023.12.10 val PER: 0.6912
2025-12-15 18:49:17,370: t15.2023.12.17 val PER: 0.7682
2025-12-15 18:49:17,370: t15.2023.12.29 val PER: 0.7605
2025-12-15 18:49:17,370: t15.2024.02.25 val PER: 0.6980
2025-12-15 18:49:17,370: t15.2024.03.03 val PER: 1.0000
2025-12-15 18:49:17,370: t15.2024.03.08 val PER: 0.7240
2025-12-15 18:49:17,370: t15.2024.03.15 val PER: 0.7061
2025-12-15 18:49:17,370: t15.2024.03.17 val PER: 0.6834
2025-12-15 18:49:17,371: t15.2024.04.25 val PER: 1.0000
2025-12-15 18:49:17,371: t15.2024.04.28 val PER: 1.0000
2025-12-15 18:49:17,371: t15.2024.05.10 val PER: 0.7786
2025-12-15 18:49:17,371: t15.2024.06.14 val PER: 0.6861
2025-12-15 18:49:17,371: t15.2024.07.19 val PER: 0.7284
2025-12-15 18:49:17,371: t15.2024.07.21 val PER: 0.6621
2025-12-15 18:49:17,371: t15.2024.07.28 val PER: 0.6846
2025-12-15 18:49:17,371: t15.2025.01.10 val PER: 0.7231
2025-12-15 18:49:17,371: t15.2025.01.12 val PER: 0.6744
2025-12-15 18:49:17,371: t15.2025.03.14 val PER: 0.7530
2025-12-15 18:49:17,371: t15.2025.03.16 val PER: 0.6859
2025-12-15 18:49:17,371: t15.2025.03.30 val PER: 0.7345
2025-12-15 18:49:17,371: t15.2025.04.13 val PER: 0.7361
2025-12-15 18:49:17,371: New best test PER 0.7678 --> 0.7047
2025-12-15 18:49:17,371: Checkpointing model
2025-12-15 18:49:17,902: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_182426/checkpoint/best_checkpoint
2025-12-15 18:50:03,965: Train batch 6200: loss: 68.82 grad norm: 43.05 time: 0.191
2025-12-15 18:50:48,998: Train batch 6400: loss: 78.76 grad norm: 139.62 time: 0.205
2025-12-15 18:51:35,310: Train batch 6600: loss: 83.03 grad norm: 87.81 time: 0.241
2025-12-15 18:52:21,026: Train batch 6800: loss: 89.09 grad norm: 43.45 time: 0.209
2025-12-15 18:53:06,993: Train batch 7000: loss: 76.64 grad norm: 112.89 time: 0.180
2025-12-15 18:53:51,973: Train batch 7200: loss: 90.22 grad norm: 278.30 time: 0.290
2025-12-15 18:54:37,478: Train batch 7400: loss: 77.17 grad norm: 218.49 time: 0.244
2025-12-15 18:55:23,545: Train batch 7600: loss: 84.84 grad norm: 232.26 time: 0.218
2025-12-15 18:56:09,001: Train batch 7800: loss: 88.99 grad norm: 37.91 time: 0.277
2025-12-15 18:56:54,522: Train batch 8000: loss: 79.26 grad norm: 123.59 time: 0.246
2025-12-15 18:56:54,522: Running test after training batch: 8000
2025-12-15 18:57:20,367: Val batch 8000: PER (avg): 0.7191 CTC Loss (avg): 119.0977 time: 25.845
2025-12-15 18:57:20,367: t15.2023.08.11 val PER: 1.0000
2025-12-15 18:57:20,367: t15.2023.08.13 val PER: 0.7048
2025-12-15 18:57:20,367: t15.2023.08.18 val PER: 0.7217
2025-12-15 18:57:20,367: t15.2023.08.20 val PER: 0.6974
2025-12-15 18:57:20,367: t15.2023.08.25 val PER: 0.7319
2025-12-15 18:57:20,368: t15.2023.08.27 val PER: 0.7299
2025-12-15 18:57:20,368: t15.2023.09.01 val PER: 0.7078
2025-12-15 18:57:20,368: t15.2023.09.03 val PER: 0.7126
2025-12-15 18:57:20,368: t15.2023.09.24 val PER: 0.7160
2025-12-15 18:57:20,368: t15.2023.09.29 val PER: 0.6854
2025-12-15 18:57:20,368: t15.2023.10.01 val PER: 0.7186
2025-12-15 18:57:20,368: t15.2023.10.06 val PER: 0.6846
2025-12-15 18:57:20,368: t15.2023.10.08 val PER: 0.7226
2025-12-15 18:57:20,368: t15.2023.10.13 val PER: 0.7463
2025-12-15 18:57:20,368: t15.2023.10.15 val PER: 0.7297
2025-12-15 18:57:20,368: t15.2023.10.20 val PER: 0.6980
2025-12-15 18:57:20,368: t15.2023.10.22 val PER: 0.7283
2025-12-15 18:57:20,368: t15.2023.11.03 val PER: 0.7218
2025-12-15 18:57:20,368: t15.2023.11.04 val PER: 0.6758
2025-12-15 18:57:20,368: t15.2023.11.17 val PER: 0.6734
2025-12-15 18:57:20,368: t15.2023.11.19 val PER: 0.6687
2025-12-15 18:57:20,368: t15.2023.11.26 val PER: 0.7391
2025-12-15 18:57:20,368: t15.2023.12.03 val PER: 0.7017
2025-12-15 18:57:20,368: t15.2023.12.08 val PER: 0.7244
2025-12-15 18:57:20,369: t15.2023.12.10 val PER: 0.7109
2025-12-15 18:57:20,369: t15.2023.12.17 val PER: 0.7744
2025-12-15 18:57:20,369: t15.2023.12.29 val PER: 0.7687
2025-12-15 18:57:20,369: t15.2024.02.25 val PER: 0.6966
2025-12-15 18:57:20,369: t15.2024.03.03 val PER: 1.0000
2025-12-15 18:57:20,369: t15.2024.03.08 val PER: 0.6956
2025-12-15 18:57:20,369: t15.2024.03.15 val PER: 0.7692
2025-12-15 18:57:20,369: t15.2024.03.17 val PER: 0.7162
2025-12-15 18:57:20,369: t15.2024.04.25 val PER: 1.0000
2025-12-15 18:57:20,369: t15.2024.04.28 val PER: 1.0000
2025-12-15 18:57:20,369: t15.2024.05.10 val PER: 0.7207
2025-12-15 18:57:20,369: t15.2024.06.14 val PER: 0.6893
2025-12-15 18:57:20,369: t15.2024.07.19 val PER: 0.7416
2025-12-15 18:57:20,369: t15.2024.07.21 val PER: 0.6807
2025-12-15 18:57:20,369: t15.2024.07.28 val PER: 0.6801
2025-12-15 18:57:20,369: t15.2025.01.10 val PER: 0.7631
2025-12-15 18:57:20,369: t15.2025.01.12 val PER: 0.6875
2025-12-15 18:57:20,369: t15.2025.03.14 val PER: 0.7692
2025-12-15 18:57:20,369: t15.2025.03.16 val PER: 0.6832
2025-12-15 18:57:20,370: t15.2025.03.30 val PER: 0.7701
2025-12-15 18:57:20,370: t15.2025.04.13 val PER: 0.7261
2025-12-15 18:58:05,595: Train batch 8200: loss: 75.28 grad norm: 53.86 time: 0.200
2025-12-15 18:58:51,960: Train batch 8400: loss: 87.36 grad norm: 55.55 time: 0.210
2025-12-15 18:59:37,957: Train batch 8600: loss: 76.61 grad norm: 82.03 time: 0.211
2025-12-15 19:00:23,747: Train batch 8800: loss: 88.45 grad norm: 151.49 time: 0.233
2025-12-15 19:01:09,417: Train batch 9000: loss: 87.49 grad norm: 53.34 time: 0.215
2025-12-15 19:01:55,978: Train batch 9200: loss: 79.74 grad norm: 443.87 time: 0.305
2025-12-15 19:02:41,612: Train batch 9400: loss: 66.96 grad norm: 55.48 time: 0.221
2025-12-15 19:03:27,131: Train batch 9600: loss: 86.32 grad norm: 51.70 time: 0.225
2025-12-15 19:04:12,750: Train batch 9800: loss: 58.39 grad norm: 165.08 time: 0.244
2025-12-15 19:04:58,043: Train batch 10000: loss: 82.18 grad norm: 176.62 time: 0.212
2025-12-15 19:04:58,044: Running test after training batch: 10000
2025-12-15 19:05:23,856: Val batch 10000: PER (avg): 0.7269 CTC Loss (avg): 121.2102 time: 25.812
2025-12-15 19:05:23,856: t15.2023.08.11 val PER: 1.0000
2025-12-15 19:05:23,856: t15.2023.08.13 val PER: 0.7183
2025-12-15 19:05:23,856: t15.2023.08.18 val PER: 0.7209
2025-12-15 19:05:23,856: t15.2023.08.20 val PER: 0.6600
2025-12-15 19:05:23,856: t15.2023.08.25 val PER: 0.7319
2025-12-15 19:05:23,856: t15.2023.08.27 val PER: 0.7138
2025-12-15 19:05:23,856: t15.2023.09.01 val PER: 0.6567
2025-12-15 19:05:23,856: t15.2023.09.03 val PER: 0.7043
2025-12-15 19:05:23,856: t15.2023.09.24 val PER: 0.7209
2025-12-15 19:05:23,857: t15.2023.09.29 val PER: 0.6905
2025-12-15 19:05:23,857: t15.2023.10.01 val PER: 0.7252
2025-12-15 19:05:23,857: t15.2023.10.06 val PER: 0.6566
2025-12-15 19:05:23,857: t15.2023.10.08 val PER: 0.7415
2025-12-15 19:05:23,857: t15.2023.10.13 val PER: 0.7618
2025-12-15 19:05:23,857: t15.2023.10.15 val PER: 0.7297
2025-12-15 19:05:23,857: t15.2023.10.20 val PER: 0.6678
2025-12-15 19:05:23,857: t15.2023.10.22 val PER: 0.6949
2025-12-15 19:05:23,857: t15.2023.11.03 val PER: 0.7198
2025-12-15 19:05:23,857: t15.2023.11.04 val PER: 0.7713
2025-12-15 19:05:23,857: t15.2023.11.17 val PER: 0.6719
2025-12-15 19:05:23,857: t15.2023.11.19 val PER: 0.6407
2025-12-15 19:05:23,857: t15.2023.11.26 val PER: 0.7326
2025-12-15 19:05:23,857: t15.2023.12.03 val PER: 0.7027
2025-12-15 19:05:23,857: t15.2023.12.08 val PER: 0.7064
2025-12-15 19:05:23,857: t15.2023.12.10 val PER: 0.7201
2025-12-15 19:05:23,857: t15.2023.12.17 val PER: 0.7838
2025-12-15 19:05:23,857: t15.2023.12.29 val PER: 0.7227
2025-12-15 19:05:23,857: t15.2024.02.25 val PER: 0.6742
2025-12-15 19:05:23,858: t15.2024.03.03 val PER: 1.0000
2025-12-15 19:05:23,858: t15.2024.03.08 val PER: 0.7568
2025-12-15 19:05:23,858: t15.2024.03.15 val PER: 0.7336
2025-12-15 19:05:23,858: t15.2024.03.17 val PER: 0.7664
2025-12-15 19:05:23,858: t15.2024.04.25 val PER: 1.0000
2025-12-15 19:05:23,858: t15.2024.04.28 val PER: 1.0000
2025-12-15 19:05:23,858: t15.2024.05.10 val PER: 0.7311
2025-12-15 19:05:23,858: t15.2024.06.14 val PER: 0.6735
2025-12-15 19:05:23,858: t15.2024.07.19 val PER: 0.9071
2025-12-15 19:05:23,858: t15.2024.07.21 val PER: 0.6869
2025-12-15 19:05:23,858: t15.2024.07.28 val PER: 0.7059
2025-12-15 19:05:23,858: t15.2025.01.10 val PER: 0.7796
2025-12-15 19:05:23,858: t15.2025.01.12 val PER: 0.6859
2025-12-15 19:05:23,858: t15.2025.03.14 val PER: 0.8846
2025-12-15 19:05:23,858: t15.2025.03.16 val PER: 0.7055
2025-12-15 19:05:23,858: t15.2025.03.30 val PER: 0.8690
2025-12-15 19:05:23,858: t15.2025.04.13 val PER: 0.7247
2025-12-15 19:06:08,823: Train batch 10200: loss: 82.82 grad norm: 110.89 time: 0.248
2025-12-15 19:06:53,840: Train batch 10400: loss: 97.12 grad norm: 142.40 time: 0.295
2025-12-15 19:07:39,407: Train batch 10600: loss: 84.03 grad norm: 52.74 time: 0.228
2025-12-15 19:08:24,882: Train batch 10800: loss: 83.52 grad norm: 83.43 time: 0.190
2025-12-15 19:09:08,438: Train batch 11000: loss: 78.10 grad norm: 96.69 time: 0.173
2025-12-15 19:09:53,686: Train batch 11200: loss: 73.31 grad norm: 110.04 time: 0.170
2025-12-15 19:10:40,091: Train batch 11400: loss: 73.46 grad norm: 66.20 time: 0.205
2025-12-15 19:11:25,079: Train batch 11600: loss: 72.04 grad norm: 41.45 time: 0.196
2025-12-15 19:12:10,196: Train batch 11800: loss: 72.88 grad norm: 80.45 time: 0.230
2025-12-15 19:12:56,587: Train batch 12000: loss: 74.69 grad norm: 32.42 time: 0.240
2025-12-15 19:12:56,587: Running test after training batch: 12000
2025-12-15 19:13:22,526: Val batch 12000: PER (avg): 0.7318 CTC Loss (avg): 129.6117 time: 25.939
2025-12-15 19:13:22,526: t15.2023.08.11 val PER: 1.0000
2025-12-15 19:13:22,526: t15.2023.08.13 val PER: 0.6850
2025-12-15 19:13:22,526: t15.2023.08.18 val PER: 0.7452
2025-12-15 19:13:22,526: t15.2023.08.20 val PER: 0.6680
2025-12-15 19:13:22,526: t15.2023.08.25 val PER: 0.6943
2025-12-15 19:13:22,526: t15.2023.08.27 val PER: 0.7508
2025-12-15 19:13:22,526: t15.2023.09.01 val PER: 0.7021
2025-12-15 19:13:22,526: t15.2023.09.03 val PER: 0.7114
2025-12-15 19:13:22,526: t15.2023.09.24 val PER: 0.6833
2025-12-15 19:13:22,526: t15.2023.09.29 val PER: 0.6828
2025-12-15 19:13:22,526: t15.2023.10.01 val PER: 0.7398
2025-12-15 19:13:22,526: t15.2023.10.06 val PER: 0.6825
2025-12-15 19:13:22,526: t15.2023.10.08 val PER: 0.7442
2025-12-15 19:13:22,526: t15.2023.10.13 val PER: 0.7261
2025-12-15 19:13:22,527: t15.2023.10.15 val PER: 0.7001
2025-12-15 19:13:22,527: t15.2023.10.20 val PER: 0.6913
2025-12-15 19:13:22,527: t15.2023.10.22 val PER: 0.7372
2025-12-15 19:13:22,527: t15.2023.11.03 val PER: 0.7680
2025-12-15 19:13:22,527: t15.2023.11.04 val PER: 0.7065
2025-12-15 19:13:22,527: t15.2023.11.17 val PER: 0.6687
2025-12-15 19:13:22,527: t15.2023.11.19 val PER: 0.6826
2025-12-15 19:13:22,527: t15.2023.11.26 val PER: 0.7420
2025-12-15 19:13:22,527: t15.2023.12.03 val PER: 0.7489
2025-12-15 19:13:22,527: t15.2023.12.08 val PER: 0.7477
2025-12-15 19:13:22,527: t15.2023.12.10 val PER: 0.7543
2025-12-15 19:13:22,527: t15.2023.12.17 val PER: 0.8098
2025-12-15 19:13:22,527: t15.2023.12.29 val PER: 0.7522
2025-12-15 19:13:22,527: t15.2024.02.25 val PER: 0.6770
2025-12-15 19:13:22,527: t15.2024.03.03 val PER: 1.0000
2025-12-15 19:13:22,527: t15.2024.03.08 val PER: 0.7852
2025-12-15 19:13:22,527: t15.2024.03.15 val PER: 0.7780
2025-12-15 19:13:22,527: t15.2024.03.17 val PER: 0.7427
2025-12-15 19:13:22,528: t15.2024.04.25 val PER: 1.0000
2025-12-15 19:13:22,528: t15.2024.04.28 val PER: 1.0000
2025-12-15 19:13:22,528: t15.2024.05.10 val PER: 0.7281
2025-12-15 19:13:22,528: t15.2024.06.14 val PER: 0.6672
2025-12-15 19:13:22,528: t15.2024.07.19 val PER: 0.7983
2025-12-15 19:13:22,528: t15.2024.07.21 val PER: 0.6807
2025-12-15 19:13:22,528: t15.2024.07.28 val PER: 0.6757
2025-12-15 19:13:22,528: t15.2025.01.10 val PER: 0.8953
2025-12-15 19:13:22,528: t15.2025.01.12 val PER: 0.6821
2025-12-15 19:13:22,528: t15.2025.03.14 val PER: 0.8491
2025-12-15 19:13:22,528: t15.2025.03.16 val PER: 0.7016
2025-12-15 19:13:22,528: t15.2025.03.30 val PER: 0.8368
2025-12-15 19:13:22,528: t15.2025.04.13 val PER: 0.7689
2025-12-15 19:14:06,973: Train batch 12200: loss: 78.96 grad norm: 284.94 time: 0.217
2025-12-15 19:14:53,155: Train batch 12400: loss: 81.43 grad norm: 269.42 time: 0.206
2025-12-15 19:15:38,713: Train batch 12600: loss: 72.71 grad norm: 157.61 time: 0.211
2025-12-15 19:16:25,019: Train batch 12800: loss: 72.70 grad norm: 98.78 time: 0.265
2025-12-15 19:17:10,452: Train batch 13000: loss: 65.85 grad norm: 521.11 time: 0.219
2025-12-15 19:17:56,333: Train batch 13200: loss: 79.76 grad norm: 47.66 time: 0.221
2025-12-15 19:18:40,253: Train batch 13400: loss: 67.14 grad norm: 133.26 time: 0.166
2025-12-15 19:19:25,700: Train batch 13600: loss: 72.91 grad norm: 22.73 time: 0.215
2025-12-15 19:20:13,000: Train batch 13800: loss: 66.68 grad norm: 148.67 time: 0.200
2025-12-15 19:20:57,092: Train batch 14000: loss: 81.62 grad norm: 48.16 time: 0.223
2025-12-15 19:20:57,093: Running test after training batch: 14000
2025-12-15 19:21:23,155: Val batch 14000: PER (avg): 0.6931 CTC Loss (avg): 115.8987 time: 26.062
2025-12-15 19:21:23,155: t15.2023.08.11 val PER: 1.0000
2025-12-15 19:21:23,155: t15.2023.08.13 val PER: 0.6549
2025-12-15 19:21:23,155: t15.2023.08.18 val PER: 0.6555
2025-12-15 19:21:23,155: t15.2023.08.20 val PER: 0.6545
2025-12-15 19:21:23,155: t15.2023.08.25 val PER: 0.6958
2025-12-15 19:21:23,155: t15.2023.08.27 val PER: 0.7042
2025-12-15 19:21:23,155: t15.2023.09.01 val PER: 0.6672
2025-12-15 19:21:23,155: t15.2023.09.03 val PER: 0.6900
2025-12-15 19:21:23,155: t15.2023.09.24 val PER: 0.6796
2025-12-15 19:21:23,156: t15.2023.09.29 val PER: 0.6701
2025-12-15 19:21:23,156: t15.2023.10.01 val PER: 0.6823
2025-12-15 19:21:23,156: t15.2023.10.06 val PER: 0.6480
2025-12-15 19:21:23,156: t15.2023.10.08 val PER: 0.6996
2025-12-15 19:21:23,156: t15.2023.10.13 val PER: 0.7238
2025-12-15 19:21:23,156: t15.2023.10.15 val PER: 0.6816
2025-12-15 19:21:23,156: t15.2023.10.20 val PER: 0.6577
2025-12-15 19:21:23,156: t15.2023.10.22 val PER: 0.6637
2025-12-15 19:21:23,156: t15.2023.11.03 val PER: 0.6961
2025-12-15 19:21:23,156: t15.2023.11.04 val PER: 0.6075
2025-12-15 19:21:23,156: t15.2023.11.17 val PER: 0.6594
2025-12-15 19:21:23,156: t15.2023.11.19 val PER: 0.6148
2025-12-15 19:21:23,156: t15.2023.11.26 val PER: 0.6870
2025-12-15 19:21:23,156: t15.2023.12.03 val PER: 0.6996
2025-12-15 19:21:23,156: t15.2023.12.08 val PER: 0.6818
2025-12-15 19:21:23,156: t15.2023.12.10 val PER: 0.6794
2025-12-15 19:21:23,156: t15.2023.12.17 val PER: 0.7277
2025-12-15 19:21:23,156: t15.2023.12.29 val PER: 0.7330
2025-12-15 19:21:23,156: t15.2024.02.25 val PER: 0.6868
2025-12-15 19:21:23,157: t15.2024.03.03 val PER: 1.0000
2025-12-15 19:21:23,157: t15.2024.03.08 val PER: 0.7994
2025-12-15 19:21:23,157: t15.2024.03.15 val PER: 0.7280
2025-12-15 19:21:23,157: t15.2024.03.17 val PER: 0.7517
2025-12-15 19:21:23,157: t15.2024.04.25 val PER: 1.0000
2025-12-15 19:21:23,157: t15.2024.04.28 val PER: 1.0000
2025-12-15 19:21:23,157: t15.2024.05.10 val PER: 0.6969
2025-12-15 19:21:23,157: t15.2024.06.14 val PER: 0.6688
2025-12-15 19:21:23,157: t15.2024.07.19 val PER: 0.7304
2025-12-15 19:21:23,157: t15.2024.07.21 val PER: 0.6400
2025-12-15 19:21:23,157: t15.2024.07.28 val PER: 0.6588
2025-12-15 19:21:23,157: t15.2025.01.10 val PER: 0.7906
2025-12-15 19:21:23,157: t15.2025.01.12 val PER: 0.6751
2025-12-15 19:21:23,157: t15.2025.03.14 val PER: 0.7840
2025-12-15 19:21:23,157: t15.2025.03.16 val PER: 0.7042
2025-12-15 19:21:23,157: t15.2025.03.30 val PER: 0.7310
2025-12-15 19:21:23,157: t15.2025.04.13 val PER: 0.7076
2025-12-15 19:21:23,157: New best test PER 0.7047 --> 0.6931
2025-12-15 19:21:23,157: Checkpointing model
2025-12-15 19:21:23,719: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_182426/checkpoint/best_checkpoint
2025-12-15 19:22:10,057: Train batch 14200: loss: 74.08 grad norm: 268.84 time: 0.263
2025-12-15 19:22:55,479: Train batch 14400: loss: 69.00 grad norm: 41.58 time: 0.205
2025-12-15 19:23:41,435: Train batch 14600: loss: 82.99 grad norm: 44.98 time: 0.188
2025-12-15 19:24:28,002: Train batch 14800: loss: 68.88 grad norm: 64.47 time: 0.215
2025-12-15 19:25:12,916: Train batch 15000: loss: 79.85 grad norm: 26.23 time: 0.239
2025-12-15 19:25:58,113: Train batch 15200: loss: 77.33 grad norm: 23.65 time: 0.173
2025-12-15 19:26:43,757: Train batch 15400: loss: 80.43 grad norm: 203.07 time: 0.246
2025-12-15 19:27:29,890: Train batch 15600: loss: 70.82 grad norm: 43.82 time: 0.171
2025-12-15 19:28:14,781: Train batch 15800: loss: 61.17 grad norm: 278.48 time: 0.220
2025-12-15 19:29:01,072: Train batch 16000: loss: 60.12 grad norm: 166.01 time: 0.184
2025-12-15 19:29:01,073: Running test after training batch: 16000
2025-12-15 19:29:27,429: Val batch 16000: PER (avg): 0.7031 CTC Loss (avg): 117.8716 time: 26.356
2025-12-15 19:29:27,429: t15.2023.08.11 val PER: 1.0000
2025-12-15 19:29:27,429: t15.2023.08.13 val PER: 0.6445
2025-12-15 19:29:27,429: t15.2023.08.18 val PER: 0.6547
2025-12-15 19:29:27,429: t15.2023.08.20 val PER: 0.6346
2025-12-15 19:29:27,429: t15.2023.08.25 val PER: 0.6898
2025-12-15 19:29:27,429: t15.2023.08.27 val PER: 0.6865
2025-12-15 19:29:27,429: t15.2023.09.01 val PER: 0.6664
2025-12-15 19:29:27,429: t15.2023.09.03 val PER: 0.6817
2025-12-15 19:29:27,429: t15.2023.09.24 val PER: 0.6687
2025-12-15 19:29:27,429: t15.2023.09.29 val PER: 0.6828
2025-12-15 19:29:27,430: t15.2023.10.01 val PER: 0.7054
2025-12-15 19:29:27,430: t15.2023.10.06 val PER: 0.6663
2025-12-15 19:29:27,430: t15.2023.10.08 val PER: 0.7064
2025-12-15 19:29:27,430: t15.2023.10.13 val PER: 0.7300
2025-12-15 19:29:27,430: t15.2023.10.15 val PER: 0.7139
2025-12-15 19:29:27,430: t15.2023.10.20 val PER: 0.6946
2025-12-15 19:29:27,430: t15.2023.10.22 val PER: 0.6982
2025-12-15 19:29:27,430: t15.2023.11.03 val PER: 0.7083
2025-12-15 19:29:27,430: t15.2023.11.04 val PER: 0.6553
2025-12-15 19:29:27,430: t15.2023.11.17 val PER: 0.6625
2025-12-15 19:29:27,430: t15.2023.11.19 val PER: 0.6627
2025-12-15 19:29:27,430: t15.2023.11.26 val PER: 0.7058
2025-12-15 19:29:27,430: t15.2023.12.03 val PER: 0.7216
2025-12-15 19:29:27,430: t15.2023.12.08 val PER: 0.7044
2025-12-15 19:29:27,430: t15.2023.12.10 val PER: 0.7122
2025-12-15 19:29:27,430: t15.2023.12.17 val PER: 0.7599
2025-12-15 19:29:27,430: t15.2023.12.29 val PER: 0.7296
2025-12-15 19:29:27,430: t15.2024.02.25 val PER: 0.6756
2025-12-15 19:29:27,431: t15.2024.03.03 val PER: 1.0000
2025-12-15 19:29:27,431: t15.2024.03.08 val PER: 0.7269
2025-12-15 19:29:27,431: t15.2024.03.15 val PER: 0.7598
2025-12-15 19:29:27,431: t15.2024.03.17 val PER: 0.7232
2025-12-15 19:29:27,431: t15.2024.04.25 val PER: 1.0000
2025-12-15 19:29:27,431: t15.2024.04.28 val PER: 1.0000
2025-12-15 19:29:27,431: t15.2024.05.10 val PER: 0.6939
2025-12-15 19:29:27,431: t15.2024.06.14 val PER: 0.6719
2025-12-15 19:29:27,431: t15.2024.07.19 val PER: 0.7811
2025-12-15 19:29:27,431: t15.2024.07.21 val PER: 0.6697
2025-12-15 19:29:27,431: t15.2024.07.28 val PER: 0.6779
2025-12-15 19:29:27,431: t15.2025.01.10 val PER: 0.8003
2025-12-15 19:29:27,431: t15.2025.01.12 val PER: 0.6674
2025-12-15 19:29:27,431: t15.2025.03.14 val PER: 0.7855
2025-12-15 19:29:27,431: t15.2025.03.16 val PER: 0.6963
2025-12-15 19:29:27,431: t15.2025.03.30 val PER: 0.7506
2025-12-15 19:29:27,431: t15.2025.04.13 val PER: 0.7204
2025-12-15 19:30:13,752: Train batch 16200: loss: 94.60 grad norm: 193.13 time: 0.298
2025-12-15 19:30:59,525: Train batch 16400: loss: 74.96 grad norm: 63.98 time: 0.182
2025-12-15 19:31:45,260: Train batch 16600: loss: 80.87 grad norm: 23.35 time: 0.247
2025-12-15 19:32:31,331: Train batch 16800: loss: 81.14 grad norm: 624.64 time: 0.204
2025-12-15 19:33:17,957: Train batch 17000: loss: 81.73 grad norm: 247.06 time: 0.196
2025-12-15 19:34:03,490: Train batch 17200: loss: 70.33 grad norm: 354.92 time: 0.238
2025-12-15 19:34:50,210: Train batch 17400: loss: 76.06 grad norm: 100.41 time: 0.221
2025-12-15 19:35:36,261: Train batch 17600: loss: 78.84 grad norm: 170.41 time: 0.218
2025-12-15 19:36:22,073: Train batch 17800: loss: 85.39 grad norm: 55.04 time: 0.214
2025-12-15 19:37:08,560: Train batch 18000: loss: 73.24 grad norm: 57.17 time: 0.260
2025-12-15 19:37:08,560: Running test after training batch: 18000
