2025-12-08 06:41:29,349: Using device: cuda:0
2025-12-08 06:41:29,820: Using torch.compile
2025-12-08 06:41:30,473: Initialized RNN decoding model
2025-12-08 06:41:30,473: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-12-08 06:41:30,474: Model has 44,315,177 parameters
2025-12-08 06:41:30,474: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-12-08 06:41:39,666: Successfully initialized datasets
2025-12-08 06:41:44,932: Train batch 0: loss: 757.39 grad norm: 383.78 time: 3.908
2025-12-08 06:41:44,932: Running test after training batch: 0
2025-12-08 06:42:14,303: Val batch 0: PER (avg): 1.2633 CTC Loss (avg): 706.8855 time: 29.371
2025-12-08 06:42:14,303: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:42:14,303: t15.2023.08.13 val PER: 1.0956
2025-12-08 06:42:14,303: t15.2023.08.18 val PER: 1.2347
2025-12-08 06:42:14,303: t15.2023.08.20 val PER: 1.1485
2025-12-08 06:42:14,303: t15.2023.08.25 val PER: 1.2229
2025-12-08 06:42:14,303: t15.2023.08.27 val PER: 1.1897
2025-12-08 06:42:14,303: t15.2023.09.01 val PER: 1.2419
2025-12-08 06:42:14,303: t15.2023.09.03 val PER: 1.2399
2025-12-08 06:42:14,303: t15.2023.09.24 val PER: 1.2985
2025-12-08 06:42:14,304: t15.2023.09.29 val PER: 1.3197
2025-12-08 06:42:14,304: t15.2023.10.01 val PER: 1.0614
2025-12-08 06:42:14,304: t15.2023.10.06 val PER: 1.3089
2025-12-08 06:42:14,304: t15.2023.10.08 val PER: 1.0325
2025-12-08 06:42:14,304: t15.2023.10.13 val PER: 1.2234
2025-12-08 06:42:14,304: t15.2023.10.15 val PER: 1.2861
2025-12-08 06:42:14,304: t15.2023.10.20 val PER: 1.2383
2025-12-08 06:42:14,304: t15.2023.10.22 val PER: 1.3029
2025-12-08 06:42:14,304: t15.2023.11.03 val PER: 1.3467
2025-12-08 06:42:14,304: t15.2023.11.04 val PER: 1.6655
2025-12-08 06:42:14,304: t15.2023.11.17 val PER: 1.7263
2025-12-08 06:42:14,304: t15.2023.11.19 val PER: 1.5050
2025-12-08 06:42:14,304: t15.2023.11.26 val PER: 1.2572
2025-12-08 06:42:14,304: t15.2023.12.03 val PER: 1.2626
2025-12-08 06:42:14,304: t15.2023.12.08 val PER: 1.2923
2025-12-08 06:42:14,304: t15.2023.12.10 val PER: 1.3088
2025-12-08 06:42:14,304: t15.2023.12.17 val PER: 1.1746
2025-12-08 06:42:14,304: t15.2023.12.29 val PER: 1.2045
2025-12-08 06:42:14,305: t15.2024.02.25 val PER: 1.2556
2025-12-08 06:42:14,305: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:42:14,305: t15.2024.03.08 val PER: 1.2504
2025-12-08 06:42:14,305: t15.2024.03.15 val PER: 1.1851
2025-12-08 06:42:14,305: t15.2024.03.17 val PER: 1.2950
2025-12-08 06:42:14,305: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:42:14,305: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:42:14,305: t15.2024.05.10 val PER: 1.1605
2025-12-08 06:42:14,305: t15.2024.06.14 val PER: 1.4069
2025-12-08 06:42:14,305: t15.2024.07.19 val PER: 1.0224
2025-12-08 06:42:14,305: t15.2024.07.21 val PER: 1.4338
2025-12-08 06:42:14,305: t15.2024.07.28 val PER: 1.4169
2025-12-08 06:42:14,305: t15.2025.01.10 val PER: 0.9972
2025-12-08 06:42:14,305: t15.2025.01.12 val PER: 1.5712
2025-12-08 06:42:14,305: t15.2025.03.14 val PER: 1.0192
2025-12-08 06:42:14,305: t15.2025.03.16 val PER: 1.4660
2025-12-08 06:42:14,305: t15.2025.03.30 val PER: 1.2057
2025-12-08 06:42:14,305: t15.2025.04.13 val PER: 1.3210
2025-12-08 06:42:14,305: New best test PER inf --> 1.2633
2025-12-08 06:42:14,306: Checkpointing model
2025-12-08 06:42:14,757: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 06:42:43,863: Train batch 200: loss: 98.63 grad norm: 41.84 time: 0.147
2025-12-08 06:43:11,723: Train batch 400: loss: 74.06 grad norm: 46.48 time: 0.130
2025-12-08 06:43:40,271: Train batch 600: loss: 70.57 grad norm: 51.68 time: 0.117
2025-12-08 06:44:09,392: Train batch 800: loss: 37.34 grad norm: 37.79 time: 0.141
2025-12-08 06:44:38,417: Train batch 1000: loss: 32.29 grad norm: 38.61 time: 0.120
2025-12-08 06:45:06,455: Train batch 1200: loss: 32.23 grad norm: 35.49 time: 0.120
2025-12-08 06:45:34,579: Train batch 1400: loss: 30.17 grad norm: 40.77 time: 0.115
2025-12-08 06:46:03,146: Train batch 1600: loss: 23.05 grad norm: 37.25 time: 0.151
2025-12-08 06:46:31,196: Train batch 1800: loss: 26.70 grad norm: 44.65 time: 0.120
2025-12-08 06:47:00,012: Train batch 2000: loss: 18.38 grad norm: 35.07 time: 0.107
2025-12-08 06:47:00,012: Running test after training batch: 2000
2025-12-08 06:47:10,375: Val batch 2000: PER (avg): 0.2271 CTC Loss (avg): 22.8261 time: 10.362
2025-12-08 06:47:10,375: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:47:10,375: t15.2023.08.13 val PER: 0.1902
2025-12-08 06:47:10,375: t15.2023.08.18 val PER: 0.1769
2025-12-08 06:47:10,375: t15.2023.08.20 val PER: 0.1612
2025-12-08 06:47:10,376: t15.2023.08.25 val PER: 0.1521
2025-12-08 06:47:10,376: t15.2023.08.27 val PER: 0.2717
2025-12-08 06:47:10,376: t15.2023.09.01 val PER: 0.1396
2025-12-08 06:47:10,376: t15.2023.09.03 val PER: 0.2197
2025-12-08 06:47:10,376: t15.2023.09.24 val PER: 0.1881
2025-12-08 06:47:10,376: t15.2023.09.29 val PER: 0.2010
2025-12-08 06:47:10,376: t15.2023.10.01 val PER: 0.2371
2025-12-08 06:47:10,376: t15.2023.10.06 val PER: 0.1615
2025-12-08 06:47:10,376: t15.2023.10.08 val PER: 0.3031
2025-12-08 06:47:10,376: t15.2023.10.13 val PER: 0.2933
2025-12-08 06:47:10,376: t15.2023.10.15 val PER: 0.2195
2025-12-08 06:47:10,376: t15.2023.10.20 val PER: 0.2215
2025-12-08 06:47:10,376: t15.2023.10.22 val PER: 0.2049
2025-12-08 06:47:10,376: t15.2023.11.03 val PER: 0.2578
2025-12-08 06:47:10,376: t15.2023.11.04 val PER: 0.0444
2025-12-08 06:47:10,376: t15.2023.11.17 val PER: 0.1011
2025-12-08 06:47:10,376: t15.2023.11.19 val PER: 0.0978
2025-12-08 06:47:10,376: t15.2023.11.26 val PER: 0.2522
2025-12-08 06:47:10,377: t15.2023.12.03 val PER: 0.1954
2025-12-08 06:47:10,377: t15.2023.12.08 val PER: 0.2011
2025-12-08 06:47:10,377: t15.2023.12.10 val PER: 0.1656
2025-12-08 06:47:10,377: t15.2023.12.17 val PER: 0.2339
2025-12-08 06:47:10,377: t15.2023.12.29 val PER: 0.2258
2025-12-08 06:47:10,377: t15.2024.02.25 val PER: 0.1910
2025-12-08 06:47:10,377: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:47:10,377: t15.2024.03.08 val PER: 0.3030
2025-12-08 06:47:10,377: t15.2024.03.15 val PER: 0.3002
2025-12-08 06:47:10,377: t15.2024.03.17 val PER: 0.2308
2025-12-08 06:47:10,377: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:47:10,377: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:47:10,377: t15.2024.05.10 val PER: 0.2333
2025-12-08 06:47:10,377: t15.2024.06.14 val PER: 0.2271
2025-12-08 06:47:10,377: t15.2024.07.19 val PER: 0.3111
2025-12-08 06:47:10,377: t15.2024.07.21 val PER: 0.1600
2025-12-08 06:47:10,377: t15.2024.07.28 val PER: 0.2110
2025-12-08 06:47:10,377: t15.2025.01.10 val PER: 0.3485
2025-12-08 06:47:10,377: t15.2025.01.12 val PER: 0.2271
2025-12-08 06:47:10,378: t15.2025.03.14 val PER: 0.4142
2025-12-08 06:47:10,378: t15.2025.03.16 val PER: 0.2683
2025-12-08 06:47:10,378: t15.2025.03.30 val PER: 0.3460
2025-12-08 06:47:10,378: t15.2025.04.13 val PER: 0.2910
2025-12-08 06:47:10,378: New best test PER 1.2633 --> 0.2271
2025-12-08 06:47:10,378: Checkpointing model
2025-12-08 06:47:11,403: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 06:47:39,712: Train batch 2200: loss: 19.51 grad norm: 39.38 time: 0.115
2025-12-08 06:48:08,347: Train batch 2400: loss: 16.98 grad norm: 37.23 time: 0.182
2025-12-08 06:48:35,961: Train batch 2600: loss: 10.03 grad norm: 28.67 time: 0.098
2025-12-08 06:49:04,691: Train batch 2800: loss: 7.97 grad norm: 26.82 time: 0.140
2025-12-08 06:49:33,002: Train batch 3000: loss: 13.31 grad norm: 36.28 time: 0.174
2025-12-08 06:50:01,825: Train batch 3200: loss: 10.69 grad norm: 31.28 time: 0.092
2025-12-08 06:50:30,087: Train batch 3400: loss: 14.17 grad norm: 38.42 time: 0.179
2025-12-08 06:50:57,748: Train batch 3600: loss: 8.84 grad norm: 33.37 time: 0.104
2025-12-08 06:51:26,926: Train batch 3800: loss: 7.56 grad norm: 28.89 time: 0.185
2025-12-08 06:51:55,139: Train batch 4000: loss: 6.07 grad norm: 25.58 time: 0.117
2025-12-08 06:51:55,140: Running test after training batch: 4000
2025-12-08 06:52:05,632: Val batch 4000: PER (avg): 0.1594 CTC Loss (avg): 17.6067 time: 10.491
2025-12-08 06:52:05,632: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:52:05,632: t15.2023.08.13 val PER: 0.1071
2025-12-08 06:52:05,632: t15.2023.08.18 val PER: 0.1232
2025-12-08 06:52:05,632: t15.2023.08.20 val PER: 0.0985
2025-12-08 06:52:05,632: t15.2023.08.25 val PER: 0.1175
2025-12-08 06:52:05,632: t15.2023.08.27 val PER: 0.2026
2025-12-08 06:52:05,632: t15.2023.09.01 val PER: 0.0828
2025-12-08 06:52:05,632: t15.2023.09.03 val PER: 0.1532
2025-12-08 06:52:05,632: t15.2023.09.24 val PER: 0.1201
2025-12-08 06:52:05,632: t15.2023.09.29 val PER: 0.1327
2025-12-08 06:52:05,632: t15.2023.10.01 val PER: 0.1777
2025-12-08 06:52:05,632: t15.2023.10.06 val PER: 0.1163
2025-12-08 06:52:05,633: t15.2023.10.08 val PER: 0.2544
2025-12-08 06:52:05,633: t15.2023.10.13 val PER: 0.2141
2025-12-08 06:52:05,633: t15.2023.10.15 val PER: 0.1701
2025-12-08 06:52:05,633: t15.2023.10.20 val PER: 0.2013
2025-12-08 06:52:05,633: t15.2023.10.22 val PER: 0.1225
2025-12-08 06:52:05,633: t15.2023.11.03 val PER: 0.1791
2025-12-08 06:52:05,633: t15.2023.11.04 val PER: 0.0307
2025-12-08 06:52:05,633: t15.2023.11.17 val PER: 0.0467
2025-12-08 06:52:05,633: t15.2023.11.19 val PER: 0.0499
2025-12-08 06:52:05,633: t15.2023.11.26 val PER: 0.1442
2025-12-08 06:52:05,633: t15.2023.12.03 val PER: 0.1113
2025-12-08 06:52:05,633: t15.2023.12.08 val PER: 0.1172
2025-12-08 06:52:05,633: t15.2023.12.10 val PER: 0.0907
2025-12-08 06:52:05,633: t15.2023.12.17 val PER: 0.1549
2025-12-08 06:52:05,633: t15.2023.12.29 val PER: 0.1414
2025-12-08 06:52:05,633: t15.2024.02.25 val PER: 0.1222
2025-12-08 06:52:05,633: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:52:05,633: t15.2024.03.08 val PER: 0.2461
2025-12-08 06:52:05,633: t15.2024.03.15 val PER: 0.2264
2025-12-08 06:52:05,634: t15.2024.03.17 val PER: 0.1499
2025-12-08 06:52:05,634: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:52:05,634: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:52:05,634: t15.2024.05.10 val PER: 0.1605
2025-12-08 06:52:05,634: t15.2024.06.14 val PER: 0.1672
2025-12-08 06:52:05,634: t15.2024.07.19 val PER: 0.2340
2025-12-08 06:52:05,634: t15.2024.07.21 val PER: 0.0931
2025-12-08 06:52:05,634: t15.2024.07.28 val PER: 0.1588
2025-12-08 06:52:05,634: t15.2025.01.10 val PER: 0.2837
2025-12-08 06:52:05,634: t15.2025.01.12 val PER: 0.1640
2025-12-08 06:52:05,634: t15.2025.03.14 val PER: 0.3831
2025-12-08 06:52:05,634: t15.2025.03.16 val PER: 0.1963
2025-12-08 06:52:05,634: t15.2025.03.30 val PER: 0.2701
2025-12-08 06:52:05,634: t15.2025.04.13 val PER: 0.2282
2025-12-08 06:52:05,634: New best test PER 0.2271 --> 0.1594
2025-12-08 06:52:05,634: Checkpointing model
2025-12-08 06:52:06,715: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 06:52:34,869: Train batch 4200: loss: 6.38 grad norm: 31.34 time: 0.134
2025-12-08 06:53:03,470: Train batch 4400: loss: 5.19 grad norm: 21.15 time: 0.153
2025-12-08 06:53:32,298: Train batch 4600: loss: 3.22 grad norm: 21.58 time: 0.194
2025-12-08 06:54:01,456: Train batch 4800: loss: 7.05 grad norm: 35.84 time: 0.100
2025-12-08 06:54:30,621: Train batch 5000: loss: 5.03 grad norm: 23.84 time: 0.135
2025-12-08 06:54:59,881: Train batch 5200: loss: 7.01 grad norm: 37.25 time: 0.168
2025-12-08 06:55:29,535: Train batch 5400: loss: 5.26 grad norm: 30.26 time: 0.109
2025-12-08 06:55:58,321: Train batch 5600: loss: 2.30 grad norm: 17.53 time: 0.102
2025-12-08 06:56:26,646: Train batch 5800: loss: 3.03 grad norm: 21.30 time: 0.129
2025-12-08 06:56:55,995: Train batch 6000: loss: 4.19 grad norm: 23.40 time: 0.162
2025-12-08 06:56:55,996: Running test after training batch: 6000
2025-12-08 06:57:06,768: Val batch 6000: PER (avg): 0.1393 CTC Loss (avg): 17.5422 time: 10.771
2025-12-08 06:57:06,768: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:57:06,768: t15.2023.08.13 val PER: 0.1008
2025-12-08 06:57:06,768: t15.2023.08.18 val PER: 0.1031
2025-12-08 06:57:06,768: t15.2023.08.20 val PER: 0.0763
2025-12-08 06:57:06,769: t15.2023.08.25 val PER: 0.0934
2025-12-08 06:57:06,769: t15.2023.08.27 val PER: 0.1511
2025-12-08 06:57:06,769: t15.2023.09.01 val PER: 0.0649
2025-12-08 06:57:06,769: t15.2023.09.03 val PER: 0.1508
2025-12-08 06:57:06,769: t15.2023.09.24 val PER: 0.1226
2025-12-08 06:57:06,769: t15.2023.09.29 val PER: 0.1238
2025-12-08 06:57:06,769: t15.2023.10.01 val PER: 0.1625
2025-12-08 06:57:06,769: t15.2023.10.06 val PER: 0.0936
2025-12-08 06:57:06,769: t15.2023.10.08 val PER: 0.2111
2025-12-08 06:57:06,769: t15.2023.10.13 val PER: 0.1932
2025-12-08 06:57:06,769: t15.2023.10.15 val PER: 0.1430
2025-12-08 06:57:06,769: t15.2023.10.20 val PER: 0.1846
2025-12-08 06:57:06,769: t15.2023.10.22 val PER: 0.1292
2025-12-08 06:57:06,769: t15.2023.11.03 val PER: 0.1716
2025-12-08 06:57:06,769: t15.2023.11.04 val PER: 0.0273
2025-12-08 06:57:06,769: t15.2023.11.17 val PER: 0.0358
2025-12-08 06:57:06,770: t15.2023.11.19 val PER: 0.0499
2025-12-08 06:57:06,770: t15.2023.11.26 val PER: 0.1138
2025-12-08 06:57:06,770: t15.2023.12.03 val PER: 0.1008
2025-12-08 06:57:06,770: t15.2023.12.08 val PER: 0.0905
2025-12-08 06:57:06,770: t15.2023.12.10 val PER: 0.0670
2025-12-08 06:57:06,770: t15.2023.12.17 val PER: 0.1299
2025-12-08 06:57:06,770: t15.2023.12.29 val PER: 0.1105
2025-12-08 06:57:06,770: t15.2024.02.25 val PER: 0.0969
2025-12-08 06:57:06,770: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:57:06,770: t15.2024.03.08 val PER: 0.1849
2025-12-08 06:57:06,770: t15.2024.03.15 val PER: 0.2133
2025-12-08 06:57:06,770: t15.2024.03.17 val PER: 0.1241
2025-12-08 06:57:06,770: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:57:06,770: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:57:06,770: t15.2024.05.10 val PER: 0.1575
2025-12-08 06:57:06,770: t15.2024.06.14 val PER: 0.1483
2025-12-08 06:57:06,771: t15.2024.07.19 val PER: 0.1931
2025-12-08 06:57:06,771: t15.2024.07.21 val PER: 0.0862
2025-12-08 06:57:06,771: t15.2024.07.28 val PER: 0.1169
2025-12-08 06:57:06,771: t15.2025.01.10 val PER: 0.2631
2025-12-08 06:57:06,771: t15.2025.01.12 val PER: 0.1393
2025-12-08 06:57:06,771: t15.2025.03.14 val PER: 0.3595
2025-12-08 06:57:06,771: t15.2025.03.16 val PER: 0.1872
2025-12-08 06:57:06,771: t15.2025.03.30 val PER: 0.2575
2025-12-08 06:57:06,771: t15.2025.04.13 val PER: 0.2168
2025-12-08 06:57:06,771: New best test PER 0.1594 --> 0.1393
2025-12-08 06:57:06,771: Checkpointing model
2025-12-08 06:57:07,890: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 06:57:37,443: Train batch 6200: loss: 3.13 grad norm: 21.99 time: 0.131
2025-12-08 06:58:06,667: Train batch 6400: loss: 3.26 grad norm: 20.39 time: 0.099
2025-12-08 06:58:36,549: Train batch 6600: loss: 4.09 grad norm: 26.03 time: 0.120
2025-12-08 06:59:05,027: Train batch 6800: loss: 3.67 grad norm: 22.19 time: 0.103
2025-12-08 06:59:34,186: Train batch 7000: loss: 2.67 grad norm: 20.85 time: 0.086
2025-12-08 07:00:02,781: Train batch 7200: loss: 2.73 grad norm: 24.25 time: 0.136
2025-12-08 07:00:32,221: Train batch 7400: loss: 2.30 grad norm: 24.66 time: 0.138
2025-12-08 07:01:01,155: Train batch 7600: loss: 5.24 grad norm: 29.05 time: 0.106
2025-12-08 07:01:30,594: Train batch 7800: loss: 4.85 grad norm: 27.11 time: 0.191
2025-12-08 07:01:58,918: Train batch 8000: loss: 2.05 grad norm: 23.31 time: 0.096
2025-12-08 07:01:58,919: Running test after training batch: 8000
2025-12-08 07:02:09,426: Val batch 8000: PER (avg): 0.1308 CTC Loss (avg): 18.1487 time: 10.507
2025-12-08 07:02:09,426: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:02:09,426: t15.2023.08.13 val PER: 0.0915
2025-12-08 07:02:09,426: t15.2023.08.18 val PER: 0.0838
2025-12-08 07:02:09,426: t15.2023.08.20 val PER: 0.0786
2025-12-08 07:02:09,426: t15.2023.08.25 val PER: 0.0964
2025-12-08 07:02:09,426: t15.2023.08.27 val PER: 0.1672
2025-12-08 07:02:09,426: t15.2023.09.01 val PER: 0.0649
2025-12-08 07:02:09,427: t15.2023.09.03 val PER: 0.1354
2025-12-08 07:02:09,427: t15.2023.09.24 val PER: 0.1092
2025-12-08 07:02:09,427: t15.2023.09.29 val PER: 0.1142
2025-12-08 07:02:09,427: t15.2023.10.01 val PER: 0.1645
2025-12-08 07:02:09,427: t15.2023.10.06 val PER: 0.0915
2025-12-08 07:02:09,427: t15.2023.10.08 val PER: 0.2043
2025-12-08 07:02:09,427: t15.2023.10.13 val PER: 0.1683
2025-12-08 07:02:09,427: t15.2023.10.15 val PER: 0.1318
2025-12-08 07:02:09,427: t15.2023.10.20 val PER: 0.1846
2025-12-08 07:02:09,427: t15.2023.10.22 val PER: 0.1080
2025-12-08 07:02:09,427: t15.2023.11.03 val PER: 0.1581
2025-12-08 07:02:09,427: t15.2023.11.04 val PER: 0.0205
2025-12-08 07:02:09,427: t15.2023.11.17 val PER: 0.0233
2025-12-08 07:02:09,427: t15.2023.11.19 val PER: 0.0499
2025-12-08 07:02:09,427: t15.2023.11.26 val PER: 0.0790
2025-12-08 07:02:09,427: t15.2023.12.03 val PER: 0.0704
2025-12-08 07:02:09,428: t15.2023.12.08 val PER: 0.0752
2025-12-08 07:02:09,428: t15.2023.12.10 val PER: 0.0683
2025-12-08 07:02:09,428: t15.2023.12.17 val PER: 0.1341
2025-12-08 07:02:09,428: t15.2023.12.29 val PER: 0.1030
2025-12-08 07:02:09,428: t15.2024.02.25 val PER: 0.0941
2025-12-08 07:02:09,428: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:02:09,428: t15.2024.03.08 val PER: 0.1892
2025-12-08 07:02:09,428: t15.2024.03.15 val PER: 0.2039
2025-12-08 07:02:09,428: t15.2024.03.17 val PER: 0.1144
2025-12-08 07:02:09,428: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:02:09,428: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:02:09,428: t15.2024.05.10 val PER: 0.1426
2025-12-08 07:02:09,428: t15.2024.06.14 val PER: 0.1435
2025-12-08 07:02:09,428: t15.2024.07.19 val PER: 0.2024
2025-12-08 07:02:09,428: t15.2024.07.21 val PER: 0.0731
2025-12-08 07:02:09,428: t15.2024.07.28 val PER: 0.1140
2025-12-08 07:02:09,428: t15.2025.01.10 val PER: 0.2534
2025-12-08 07:02:09,429: t15.2025.01.12 val PER: 0.1370
2025-12-08 07:02:09,429: t15.2025.03.14 val PER: 0.3669
2025-12-08 07:02:09,429: t15.2025.03.16 val PER: 0.1636
2025-12-08 07:02:09,429: t15.2025.03.30 val PER: 0.2494
2025-12-08 07:02:09,429: t15.2025.04.13 val PER: 0.2097
2025-12-08 07:02:09,429: New best test PER 0.1393 --> 0.1308
2025-12-08 07:02:09,429: Checkpointing model
2025-12-08 07:02:10,428: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:02:38,291: Train batch 8200: loss: 1.92 grad norm: 18.13 time: 0.111
2025-12-08 07:03:05,630: Train batch 8400: loss: 3.66 grad norm: 28.63 time: 0.128
2025-12-08 07:03:34,782: Train batch 8600: loss: 2.73 grad norm: 22.02 time: 0.166
2025-12-08 07:04:03,374: Train batch 8800: loss: 1.70 grad norm: 25.53 time: 0.113
2025-12-08 07:04:31,847: Train batch 9000: loss: 4.37 grad norm: 31.88 time: 0.114
2025-12-08 07:05:00,103: Train batch 9200: loss: 2.92 grad norm: 23.67 time: 0.112
2025-12-08 07:05:28,995: Train batch 9400: loss: 2.90 grad norm: 25.65 time: 0.141
2025-12-08 07:05:56,900: Train batch 9600: loss: 2.09 grad norm: 28.44 time: 0.166
2025-12-08 07:06:24,908: Train batch 9800: loss: 0.74 grad norm: 13.65 time: 0.095
2025-12-08 07:06:53,453: Train batch 10000: loss: 0.52 grad norm: 12.18 time: 0.141
2025-12-08 07:06:53,453: Running test after training batch: 10000
2025-12-08 07:07:04,086: Val batch 10000: PER (avg): 0.1281 CTC Loss (avg): 18.8813 time: 10.633
2025-12-08 07:07:04,086: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:07:04,086: t15.2023.08.13 val PER: 0.0967
2025-12-08 07:07:04,086: t15.2023.08.18 val PER: 0.0796
2025-12-08 07:07:04,086: t15.2023.08.20 val PER: 0.0659
2025-12-08 07:07:04,086: t15.2023.08.25 val PER: 0.0858
2025-12-08 07:07:04,086: t15.2023.08.27 val PER: 0.1608
2025-12-08 07:07:04,086: t15.2023.09.01 val PER: 0.0511
2025-12-08 07:07:04,087: t15.2023.09.03 val PER: 0.1271
2025-12-08 07:07:04,087: t15.2023.09.24 val PER: 0.1056
2025-12-08 07:07:04,087: t15.2023.09.29 val PER: 0.1251
2025-12-08 07:07:04,087: t15.2023.10.01 val PER: 0.1513
2025-12-08 07:07:04,087: t15.2023.10.06 val PER: 0.0850
2025-12-08 07:07:04,087: t15.2023.10.08 val PER: 0.1989
2025-12-08 07:07:04,087: t15.2023.10.13 val PER: 0.1792
2025-12-08 07:07:04,087: t15.2023.10.15 val PER: 0.1312
2025-12-08 07:07:04,087: t15.2023.10.20 val PER: 0.1812
2025-12-08 07:07:04,087: t15.2023.10.22 val PER: 0.1136
2025-12-08 07:07:04,087: t15.2023.11.03 val PER: 0.1540
2025-12-08 07:07:04,087: t15.2023.11.04 val PER: 0.0205
2025-12-08 07:07:04,087: t15.2023.11.17 val PER: 0.0187
2025-12-08 07:07:04,087: t15.2023.11.19 val PER: 0.0339
2025-12-08 07:07:04,087: t15.2023.11.26 val PER: 0.0862
2025-12-08 07:07:04,087: t15.2023.12.03 val PER: 0.0777
2025-12-08 07:07:04,088: t15.2023.12.08 val PER: 0.0726
2025-12-08 07:07:04,088: t15.2023.12.10 val PER: 0.0618
2025-12-08 07:07:04,088: t15.2023.12.17 val PER: 0.1154
2025-12-08 07:07:04,088: t15.2023.12.29 val PER: 0.1002
2025-12-08 07:07:04,088: t15.2024.02.25 val PER: 0.0927
2025-12-08 07:07:04,088: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:07:04,088: t15.2024.03.08 val PER: 0.1949
2025-12-08 07:07:04,088: t15.2024.03.15 val PER: 0.1970
2025-12-08 07:07:04,088: t15.2024.03.17 val PER: 0.1179
2025-12-08 07:07:04,088: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:07:04,088: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:07:04,088: t15.2024.05.10 val PER: 0.1412
2025-12-08 07:07:04,088: t15.2024.06.14 val PER: 0.1278
2025-12-08 07:07:04,088: t15.2024.07.19 val PER: 0.1885
2025-12-08 07:07:04,088: t15.2024.07.21 val PER: 0.0793
2025-12-08 07:07:04,088: t15.2024.07.28 val PER: 0.1132
2025-12-08 07:07:04,088: t15.2025.01.10 val PER: 0.2590
2025-12-08 07:07:04,089: t15.2025.01.12 val PER: 0.1255
2025-12-08 07:07:04,089: t15.2025.03.14 val PER: 0.3595
2025-12-08 07:07:04,089: t15.2025.03.16 val PER: 0.1571
2025-12-08 07:07:04,089: t15.2025.03.30 val PER: 0.2644
2025-12-08 07:07:04,089: t15.2025.04.13 val PER: 0.2126
2025-12-08 07:07:04,089: New best test PER 0.1308 --> 0.1281
2025-12-08 07:07:04,089: Checkpointing model
2025-12-08 07:07:05,270: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:07:33,235: Train batch 10200: loss: 2.49 grad norm: 20.04 time: 0.112
2025-12-08 07:08:01,318: Train batch 10400: loss: 1.16 grad norm: 15.37 time: 0.175
2025-12-08 07:08:29,788: Train batch 10600: loss: 0.87 grad norm: 16.37 time: 0.120
2025-12-08 07:08:58,480: Train batch 10800: loss: 1.60 grad norm: 15.00 time: 0.104
2025-12-08 07:09:26,519: Train batch 11000: loss: 1.46 grad norm: 19.24 time: 0.146
2025-12-08 07:09:55,228: Train batch 11200: loss: 2.30 grad norm: 30.45 time: 0.205
2025-12-08 07:10:23,796: Train batch 11400: loss: 0.60 grad norm: 10.29 time: 0.099
2025-12-08 07:10:52,637: Train batch 11600: loss: 0.49 grad norm: 20.32 time: 0.147
2025-12-08 07:11:20,996: Train batch 11800: loss: 2.16 grad norm: 19.51 time: 0.119
2025-12-08 07:11:49,474: Train batch 12000: loss: 1.24 grad norm: 14.43 time: 0.083
2025-12-08 07:11:49,475: Running test after training batch: 12000
2025-12-08 07:11:59,948: Val batch 12000: PER (avg): 0.1248 CTC Loss (avg): 19.6232 time: 10.473
2025-12-08 07:11:59,948: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:11:59,948: t15.2023.08.13 val PER: 0.0811
2025-12-08 07:11:59,948: t15.2023.08.18 val PER: 0.0813
2025-12-08 07:11:59,948: t15.2023.08.20 val PER: 0.0627
2025-12-08 07:11:59,948: t15.2023.08.25 val PER: 0.0783
2025-12-08 07:11:59,948: t15.2023.08.27 val PER: 0.1608
2025-12-08 07:11:59,948: t15.2023.09.01 val PER: 0.0609
2025-12-08 07:11:59,948: t15.2023.09.03 val PER: 0.1176
2025-12-08 07:11:59,948: t15.2023.09.24 val PER: 0.0934
2025-12-08 07:11:59,949: t15.2023.09.29 val PER: 0.1206
2025-12-08 07:11:59,949: t15.2023.10.01 val PER: 0.1572
2025-12-08 07:11:59,949: t15.2023.10.06 val PER: 0.0710
2025-12-08 07:11:59,949: t15.2023.10.08 val PER: 0.2057
2025-12-08 07:11:59,949: t15.2023.10.13 val PER: 0.1715
2025-12-08 07:11:59,949: t15.2023.10.15 val PER: 0.1272
2025-12-08 07:11:59,949: t15.2023.10.20 val PER: 0.1644
2025-12-08 07:11:59,949: t15.2023.10.22 val PER: 0.1158
2025-12-08 07:11:59,949: t15.2023.11.03 val PER: 0.1513
2025-12-08 07:11:59,949: t15.2023.11.04 val PER: 0.0205
2025-12-08 07:11:59,949: t15.2023.11.17 val PER: 0.0280
2025-12-08 07:11:59,949: t15.2023.11.19 val PER: 0.0319
2025-12-08 07:11:59,949: t15.2023.11.26 val PER: 0.0804
2025-12-08 07:11:59,949: t15.2023.12.03 val PER: 0.0651
2025-12-08 07:11:59,949: t15.2023.12.08 val PER: 0.0759
2025-12-08 07:11:59,949: t15.2023.12.10 val PER: 0.0539
2025-12-08 07:11:59,949: t15.2023.12.17 val PER: 0.1237
2025-12-08 07:11:59,949: t15.2023.12.29 val PER: 0.0933
2025-12-08 07:11:59,949: t15.2024.02.25 val PER: 0.0955
2025-12-08 07:11:59,950: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:11:59,950: t15.2024.03.08 val PER: 0.1878
2025-12-08 07:11:59,950: t15.2024.03.15 val PER: 0.1945
2025-12-08 07:11:59,950: t15.2024.03.17 val PER: 0.1039
2025-12-08 07:11:59,950: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:11:59,950: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:11:59,950: t15.2024.05.10 val PER: 0.1308
2025-12-08 07:11:59,950: t15.2024.06.14 val PER: 0.1293
2025-12-08 07:11:59,950: t15.2024.07.19 val PER: 0.1806
2025-12-08 07:11:59,950: t15.2024.07.21 val PER: 0.0855
2025-12-08 07:11:59,950: t15.2024.07.28 val PER: 0.1022
2025-12-08 07:11:59,950: t15.2025.01.10 val PER: 0.2686
2025-12-08 07:11:59,950: t15.2025.01.12 val PER: 0.1263
2025-12-08 07:11:59,950: t15.2025.03.14 val PER: 0.3550
2025-12-08 07:11:59,950: t15.2025.03.16 val PER: 0.1754
2025-12-08 07:11:59,950: t15.2025.03.30 val PER: 0.2425
2025-12-08 07:11:59,950: t15.2025.04.13 val PER: 0.1983
2025-12-08 07:11:59,950: New best test PER 0.1281 --> 0.1248
2025-12-08 07:11:59,950: Checkpointing model
2025-12-08 07:12:01,065: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:12:28,722: Train batch 12200: loss: 0.81 grad norm: 13.91 time: 0.098
2025-12-08 07:12:57,263: Train batch 12400: loss: 0.50 grad norm: 14.39 time: 0.095
2025-12-08 07:13:25,457: Train batch 12600: loss: 0.89 grad norm: 13.32 time: 0.088
2025-12-08 07:13:54,047: Train batch 12800: loss: 0.52 grad norm: 8.00 time: 0.117
2025-12-08 07:14:22,347: Train batch 13000: loss: 0.78 grad norm: 23.95 time: 0.136
2025-12-08 07:14:50,246: Train batch 13200: loss: 0.85 grad norm: 17.23 time: 0.126
2025-12-08 07:15:17,918: Train batch 13400: loss: 1.07 grad norm: 19.34 time: 0.090
2025-12-08 07:15:46,217: Train batch 13600: loss: 0.77 grad norm: 29.30 time: 0.104
2025-12-08 07:16:14,430: Train batch 13800: loss: 0.77 grad norm: 11.37 time: 0.125
2025-12-08 07:16:42,750: Train batch 14000: loss: 1.93 grad norm: 31.10 time: 0.166
2025-12-08 07:16:42,750: Running test after training batch: 14000
2025-12-08 07:16:53,298: Val batch 14000: PER (avg): 0.1262 CTC Loss (avg): 20.5638 time: 10.548
2025-12-08 07:16:53,298: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:16:53,298: t15.2023.08.13 val PER: 0.0936
2025-12-08 07:16:53,298: t15.2023.08.18 val PER: 0.0721
2025-12-08 07:16:53,298: t15.2023.08.20 val PER: 0.0699
2025-12-08 07:16:53,298: t15.2023.08.25 val PER: 0.0904
2025-12-08 07:16:53,299: t15.2023.08.27 val PER: 0.1656
2025-12-08 07:16:53,299: t15.2023.09.01 val PER: 0.0633
2025-12-08 07:16:53,299: t15.2023.09.03 val PER: 0.1283
2025-12-08 07:16:53,299: t15.2023.09.24 val PER: 0.0934
2025-12-08 07:16:53,299: t15.2023.09.29 val PER: 0.1213
2025-12-08 07:16:53,299: t15.2023.10.01 val PER: 0.1572
2025-12-08 07:16:53,299: t15.2023.10.06 val PER: 0.0818
2025-12-08 07:16:53,299: t15.2023.10.08 val PER: 0.1746
2025-12-08 07:16:53,299: t15.2023.10.13 val PER: 0.1722
2025-12-08 07:16:53,299: t15.2023.10.15 val PER: 0.1246
2025-12-08 07:16:53,299: t15.2023.10.20 val PER: 0.1779
2025-12-08 07:16:53,299: t15.2023.10.22 val PER: 0.1203
2025-12-08 07:16:53,299: t15.2023.11.03 val PER: 0.1594
2025-12-08 07:16:53,299: t15.2023.11.04 val PER: 0.0273
2025-12-08 07:16:53,299: t15.2023.11.17 val PER: 0.0342
2025-12-08 07:16:53,299: t15.2023.11.19 val PER: 0.0279
2025-12-08 07:16:53,299: t15.2023.11.26 val PER: 0.0826
2025-12-08 07:16:53,299: t15.2023.12.03 val PER: 0.0767
2025-12-08 07:16:53,300: t15.2023.12.08 val PER: 0.0626
2025-12-08 07:16:53,300: t15.2023.12.10 val PER: 0.0591
2025-12-08 07:16:53,300: t15.2023.12.17 val PER: 0.1019
2025-12-08 07:16:53,300: t15.2023.12.29 val PER: 0.0879
2025-12-08 07:16:53,300: t15.2024.02.25 val PER: 0.0829
2025-12-08 07:16:53,300: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:16:53,300: t15.2024.03.08 val PER: 0.1849
2025-12-08 07:16:53,300: t15.2024.03.15 val PER: 0.1907
2025-12-08 07:16:53,300: t15.2024.03.17 val PER: 0.1011
2025-12-08 07:16:53,300: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:16:53,300: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:16:53,300: t15.2024.05.10 val PER: 0.1278
2025-12-08 07:16:53,300: t15.2024.06.14 val PER: 0.1420
2025-12-08 07:16:53,300: t15.2024.07.19 val PER: 0.1951
2025-12-08 07:16:53,300: t15.2024.07.21 val PER: 0.0821
2025-12-08 07:16:53,300: t15.2024.07.28 val PER: 0.1059
2025-12-08 07:16:53,300: t15.2025.01.10 val PER: 0.2686
2025-12-08 07:16:53,300: t15.2025.01.12 val PER: 0.1301
2025-12-08 07:16:53,300: t15.2025.03.14 val PER: 0.3536
2025-12-08 07:16:53,301: t15.2025.03.16 val PER: 0.1688
2025-12-08 07:16:53,301: t15.2025.03.30 val PER: 0.2667
2025-12-08 07:16:53,301: t15.2025.04.13 val PER: 0.2268
2025-12-08 07:17:21,076: Train batch 14200: loss: 1.19 grad norm: 13.22 time: 0.100
2025-12-08 07:17:48,924: Train batch 14400: loss: 2.02 grad norm: 33.52 time: 0.114
2025-12-08 07:18:18,076: Train batch 14600: loss: 0.97 grad norm: 15.03 time: 0.143
2025-12-08 07:18:46,131: Train batch 14800: loss: 0.39 grad norm: 9.02 time: 0.119
2025-12-08 07:19:14,393: Train batch 15000: loss: 1.26 grad norm: 17.36 time: 0.150
2025-12-08 07:19:42,414: Train batch 15200: loss: 0.93 grad norm: 14.40 time: 0.112
2025-12-08 07:20:11,403: Train batch 15400: loss: 1.04 grad norm: 19.67 time: 0.094
2025-12-08 07:20:39,359: Train batch 15600: loss: 0.85 grad norm: 20.27 time: 0.167
2025-12-08 07:21:07,904: Train batch 15800: loss: 0.63 grad norm: 12.20 time: 0.144
2025-12-08 07:21:36,710: Train batch 16000: loss: 0.64 grad norm: 20.53 time: 0.117
2025-12-08 07:21:36,711: Running test after training batch: 16000
2025-12-08 07:21:47,117: Val batch 16000: PER (avg): 0.1214 CTC Loss (avg): 20.5257 time: 10.406
2025-12-08 07:21:47,118: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:21:47,118: t15.2023.08.13 val PER: 0.0884
2025-12-08 07:21:47,118: t15.2023.08.18 val PER: 0.0704
2025-12-08 07:21:47,118: t15.2023.08.20 val PER: 0.0707
2025-12-08 07:21:47,118: t15.2023.08.25 val PER: 0.0753
2025-12-08 07:21:47,118: t15.2023.08.27 val PER: 0.1576
2025-12-08 07:21:47,118: t15.2023.09.01 val PER: 0.0609
2025-12-08 07:21:47,118: t15.2023.09.03 val PER: 0.1366
2025-12-08 07:21:47,118: t15.2023.09.24 val PER: 0.1056
2025-12-08 07:21:47,118: t15.2023.09.29 val PER: 0.1181
2025-12-08 07:21:47,118: t15.2023.10.01 val PER: 0.1486
2025-12-08 07:21:47,118: t15.2023.10.06 val PER: 0.0818
2025-12-08 07:21:47,118: t15.2023.10.08 val PER: 0.1854
2025-12-08 07:21:47,119: t15.2023.10.13 val PER: 0.1652
2025-12-08 07:21:47,119: t15.2023.10.15 val PER: 0.1246
2025-12-08 07:21:47,119: t15.2023.10.20 val PER: 0.1711
2025-12-08 07:21:47,119: t15.2023.10.22 val PER: 0.1069
2025-12-08 07:21:47,119: t15.2023.11.03 val PER: 0.1608
2025-12-08 07:21:47,119: t15.2023.11.04 val PER: 0.0171
2025-12-08 07:21:47,119: t15.2023.11.17 val PER: 0.0249
2025-12-08 07:21:47,119: t15.2023.11.19 val PER: 0.0359
2025-12-08 07:21:47,119: t15.2023.11.26 val PER: 0.0667
2025-12-08 07:21:47,119: t15.2023.12.03 val PER: 0.0630
2025-12-08 07:21:47,119: t15.2023.12.08 val PER: 0.0606
2025-12-08 07:21:47,119: t15.2023.12.10 val PER: 0.0473
2025-12-08 07:21:47,119: t15.2023.12.17 val PER: 0.1019
2025-12-08 07:21:47,119: t15.2023.12.29 val PER: 0.0981
2025-12-08 07:21:47,119: t15.2024.02.25 val PER: 0.0815
2025-12-08 07:21:47,119: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:21:47,119: t15.2024.03.08 val PER: 0.1906
2025-12-08 07:21:47,119: t15.2024.03.15 val PER: 0.1982
2025-12-08 07:21:47,120: t15.2024.03.17 val PER: 0.1088
2025-12-08 07:21:47,120: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:21:47,120: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:21:47,120: t15.2024.05.10 val PER: 0.1278
2025-12-08 07:21:47,120: t15.2024.06.14 val PER: 0.1246
2025-12-08 07:21:47,120: t15.2024.07.19 val PER: 0.1800
2025-12-08 07:21:47,120: t15.2024.07.21 val PER: 0.0731
2025-12-08 07:21:47,120: t15.2024.07.28 val PER: 0.0926
2025-12-08 07:21:47,120: t15.2025.01.10 val PER: 0.2493
2025-12-08 07:21:47,120: t15.2025.01.12 val PER: 0.1239
2025-12-08 07:21:47,120: t15.2025.03.14 val PER: 0.3166
2025-12-08 07:21:47,120: t15.2025.03.16 val PER: 0.1675
2025-12-08 07:21:47,120: t15.2025.03.30 val PER: 0.2379
2025-12-08 07:21:47,120: t15.2025.04.13 val PER: 0.2140
2025-12-08 07:21:47,120: New best test PER 0.1248 --> 0.1214
2025-12-08 07:21:47,120: Checkpointing model
2025-12-08 07:21:48,224: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:22:16,047: Train batch 16200: loss: 1.10 grad norm: 17.48 time: 0.101
2025-12-08 07:22:44,265: Train batch 16400: loss: 1.22 grad norm: 20.71 time: 0.146
2025-12-08 07:23:12,011: Train batch 16600: loss: 0.47 grad norm: 14.35 time: 0.104
2025-12-08 07:23:39,897: Train batch 16800: loss: 1.00 grad norm: 20.92 time: 0.100
2025-12-08 07:24:08,802: Train batch 17000: loss: 0.62 grad norm: 11.03 time: 0.155
2025-12-08 07:24:36,738: Train batch 17200: loss: 0.38 grad norm: 8.78 time: 0.109
2025-12-08 07:25:05,355: Train batch 17400: loss: 0.92 grad norm: 16.43 time: 0.128
2025-12-08 07:25:32,672: Train batch 17600: loss: 0.33 grad norm: 12.18 time: 0.101
2025-12-08 07:26:00,815: Train batch 17800: loss: 0.50 grad norm: 17.37 time: 0.077
2025-12-08 07:26:29,479: Train batch 18000: loss: 0.72 grad norm: 13.00 time: 0.192
2025-12-08 07:26:29,480: Running test after training batch: 18000
2025-12-08 07:26:39,949: Val batch 18000: PER (avg): 0.1199 CTC Loss (avg): 21.3251 time: 10.469
2025-12-08 07:26:39,950: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:26:39,950: t15.2023.08.13 val PER: 0.0780
2025-12-08 07:26:39,950: t15.2023.08.18 val PER: 0.0645
2025-12-08 07:26:39,950: t15.2023.08.20 val PER: 0.0635
2025-12-08 07:26:39,950: t15.2023.08.25 val PER: 0.0813
2025-12-08 07:26:39,950: t15.2023.08.27 val PER: 0.1559
2025-12-08 07:26:39,950: t15.2023.09.01 val PER: 0.0511
2025-12-08 07:26:39,950: t15.2023.09.03 val PER: 0.1366
2025-12-08 07:26:39,950: t15.2023.09.24 val PER: 0.1032
2025-12-08 07:26:39,950: t15.2023.09.29 val PER: 0.1161
2025-12-08 07:26:39,950: t15.2023.10.01 val PER: 0.1513
2025-12-08 07:26:39,950: t15.2023.10.06 val PER: 0.0883
2025-12-08 07:26:39,950: t15.2023.10.08 val PER: 0.1881
2025-12-08 07:26:39,950: t15.2023.10.13 val PER: 0.1528
2025-12-08 07:26:39,950: t15.2023.10.15 val PER: 0.1226
2025-12-08 07:26:39,950: t15.2023.10.20 val PER: 0.1846
2025-12-08 07:26:39,951: t15.2023.10.22 val PER: 0.1047
2025-12-08 07:26:39,951: t15.2023.11.03 val PER: 0.1588
2025-12-08 07:26:39,951: t15.2023.11.04 val PER: 0.0171
2025-12-08 07:26:39,951: t15.2023.11.17 val PER: 0.0327
2025-12-08 07:26:39,951: t15.2023.11.19 val PER: 0.0359
2025-12-08 07:26:39,951: t15.2023.11.26 val PER: 0.0667
2025-12-08 07:26:39,951: t15.2023.12.03 val PER: 0.0746
2025-12-08 07:26:39,951: t15.2023.12.08 val PER: 0.0599
2025-12-08 07:26:39,951: t15.2023.12.10 val PER: 0.0604
2025-12-08 07:26:39,951: t15.2023.12.17 val PER: 0.1071
2025-12-08 07:26:39,951: t15.2023.12.29 val PER: 0.0947
2025-12-08 07:26:39,951: t15.2024.02.25 val PER: 0.0815
2025-12-08 07:26:39,951: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:26:39,951: t15.2024.03.08 val PER: 0.1792
2025-12-08 07:26:39,951: t15.2024.03.15 val PER: 0.1932
2025-12-08 07:26:39,951: t15.2024.03.17 val PER: 0.0914
2025-12-08 07:26:39,952: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:26:39,952: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:26:39,952: t15.2024.05.10 val PER: 0.1352
2025-12-08 07:26:39,952: t15.2024.06.14 val PER: 0.1199
2025-12-08 07:26:39,952: t15.2024.07.19 val PER: 0.1701
2025-12-08 07:26:39,952: t15.2024.07.21 val PER: 0.0690
2025-12-08 07:26:39,952: t15.2024.07.28 val PER: 0.1051
2025-12-08 07:26:39,952: t15.2025.01.10 val PER: 0.2521
2025-12-08 07:26:39,952: t15.2025.01.12 val PER: 0.1155
2025-12-08 07:26:39,952: t15.2025.03.14 val PER: 0.3166
2025-12-08 07:26:39,952: t15.2025.03.16 val PER: 0.1623
2025-12-08 07:26:39,952: t15.2025.03.30 val PER: 0.2494
2025-12-08 07:26:39,952: t15.2025.04.13 val PER: 0.2183
2025-12-08 07:26:39,952: New best test PER 0.1214 --> 0.1199
2025-12-08 07:26:39,952: Checkpointing model
2025-12-08 07:26:41,056: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:27:09,266: Train batch 18200: loss: 0.50 grad norm: 17.86 time: 0.099
2025-12-08 07:27:37,546: Train batch 18400: loss: 0.70 grad norm: 10.72 time: 0.171
2025-12-08 07:28:05,632: Train batch 18600: loss: 0.53 grad norm: 13.61 time: 0.159
2025-12-08 07:28:33,577: Train batch 18800: loss: 0.58 grad norm: 10.57 time: 0.106
2025-12-08 07:29:02,050: Train batch 19000: loss: 0.97 grad norm: 13.37 time: 0.133
2025-12-08 07:29:30,437: Train batch 19200: loss: 0.47 grad norm: 12.68 time: 0.167
2025-12-08 07:29:58,973: Train batch 19400: loss: 0.86 grad norm: 13.45 time: 0.148
2025-12-08 07:30:26,138: Train batch 19600: loss: 0.71 grad norm: 11.18 time: 0.090
2025-12-08 07:30:54,181: Train batch 19800: loss: 0.32 grad norm: 12.96 time: 0.104
2025-12-08 07:31:21,941: Train batch 20000: loss: 0.10 grad norm: 3.04 time: 0.100
2025-12-08 07:31:21,942: Running test after training batch: 20000
2025-12-08 07:31:32,337: Val batch 20000: PER (avg): 0.1179 CTC Loss (avg): 21.1764 time: 10.395
2025-12-08 07:31:32,337: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:31:32,337: t15.2023.08.13 val PER: 0.0780
2025-12-08 07:31:32,337: t15.2023.08.18 val PER: 0.0763
2025-12-08 07:31:32,337: t15.2023.08.20 val PER: 0.0627
2025-12-08 07:31:32,337: t15.2023.08.25 val PER: 0.0783
2025-12-08 07:31:32,337: t15.2023.08.27 val PER: 0.1495
2025-12-08 07:31:32,337: t15.2023.09.01 val PER: 0.0471
2025-12-08 07:31:32,337: t15.2023.09.03 val PER: 0.1342
2025-12-08 07:31:32,338: t15.2023.09.24 val PER: 0.0947
2025-12-08 07:31:32,338: t15.2023.09.29 val PER: 0.1104
2025-12-08 07:31:32,338: t15.2023.10.01 val PER: 0.1473
2025-12-08 07:31:32,338: t15.2023.10.06 val PER: 0.0775
2025-12-08 07:31:32,338: t15.2023.10.08 val PER: 0.1773
2025-12-08 07:31:32,338: t15.2023.10.13 val PER: 0.1544
2025-12-08 07:31:32,338: t15.2023.10.15 val PER: 0.1233
2025-12-08 07:31:32,338: t15.2023.10.20 val PER: 0.1779
2025-12-08 07:31:32,338: t15.2023.10.22 val PER: 0.0924
2025-12-08 07:31:32,338: t15.2023.11.03 val PER: 0.1533
2025-12-08 07:31:32,338: t15.2023.11.04 val PER: 0.0273
2025-12-08 07:31:32,338: t15.2023.11.17 val PER: 0.0140
2025-12-08 07:31:32,338: t15.2023.11.19 val PER: 0.0339
2025-12-08 07:31:32,338: t15.2023.11.26 val PER: 0.0688
2025-12-08 07:31:32,338: t15.2023.12.03 val PER: 0.0882
2025-12-08 07:31:32,338: t15.2023.12.08 val PER: 0.0606
2025-12-08 07:31:32,338: t15.2023.12.10 val PER: 0.0539
2025-12-08 07:31:32,338: t15.2023.12.17 val PER: 0.0946
2025-12-08 07:31:32,338: t15.2023.12.29 val PER: 0.0844
2025-12-08 07:31:32,339: t15.2024.02.25 val PER: 0.0829
2025-12-08 07:31:32,339: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:31:32,339: t15.2024.03.08 val PER: 0.1707
2025-12-08 07:31:32,339: t15.2024.03.15 val PER: 0.1820
2025-12-08 07:31:32,339: t15.2024.03.17 val PER: 0.1060
2025-12-08 07:31:32,339: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:31:32,339: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:31:32,339: t15.2024.05.10 val PER: 0.1263
2025-12-08 07:31:32,339: t15.2024.06.14 val PER: 0.1420
2025-12-08 07:31:32,339: t15.2024.07.19 val PER: 0.1819
2025-12-08 07:31:32,339: t15.2024.07.21 val PER: 0.0745
2025-12-08 07:31:32,339: t15.2024.07.28 val PER: 0.0868
2025-12-08 07:31:32,339: t15.2025.01.10 val PER: 0.2617
2025-12-08 07:31:32,339: t15.2025.01.12 val PER: 0.1116
2025-12-08 07:31:32,339: t15.2025.03.14 val PER: 0.3269
2025-12-08 07:31:32,339: t15.2025.03.16 val PER: 0.1688
2025-12-08 07:31:32,339: t15.2025.03.30 val PER: 0.2448
2025-12-08 07:31:32,339: t15.2025.04.13 val PER: 0.1954
2025-12-08 07:31:32,339: New best test PER 0.1199 --> 0.1179
2025-12-08 07:31:32,339: Checkpointing model
2025-12-08 07:31:33,383: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:32:01,792: Train batch 20200: loss: 0.60 grad norm: 31.29 time: 0.130
2025-12-08 07:32:29,640: Train batch 20400: loss: 0.35 grad norm: 7.97 time: 0.158
2025-12-08 07:32:57,221: Train batch 20600: loss: 0.62 grad norm: 15.15 time: 0.119
2025-12-08 07:33:24,654: Train batch 20800: loss: 0.29 grad norm: 8.84 time: 0.104
2025-12-08 07:33:53,571: Train batch 21000: loss: 0.35 grad norm: 12.42 time: 0.105
2025-12-08 07:34:22,144: Train batch 21200: loss: 0.46 grad norm: 8.38 time: 0.111
2025-12-08 07:34:50,388: Train batch 21400: loss: 0.62 grad norm: 12.90 time: 0.138
2025-12-08 07:35:18,326: Train batch 21600: loss: 0.64 grad norm: 13.20 time: 0.144
2025-12-08 07:35:46,018: Train batch 21800: loss: 0.48 grad norm: 12.63 time: 0.111
2025-12-08 07:36:14,288: Train batch 22000: loss: 0.18 grad norm: 6.43 time: 0.147
2025-12-08 07:36:14,289: Running test after training batch: 22000
2025-12-08 07:36:24,775: Val batch 22000: PER (avg): 0.1178 CTC Loss (avg): 21.8210 time: 10.487
2025-12-08 07:36:24,776: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:36:24,776: t15.2023.08.13 val PER: 0.0894
2025-12-08 07:36:24,776: t15.2023.08.18 val PER: 0.0712
2025-12-08 07:36:24,776: t15.2023.08.20 val PER: 0.0659
2025-12-08 07:36:24,776: t15.2023.08.25 val PER: 0.0798
2025-12-08 07:36:24,776: t15.2023.08.27 val PER: 0.1559
2025-12-08 07:36:24,776: t15.2023.09.01 val PER: 0.0503
2025-12-08 07:36:24,776: t15.2023.09.03 val PER: 0.1342
2025-12-08 07:36:24,776: t15.2023.09.24 val PER: 0.0874
2025-12-08 07:36:24,776: t15.2023.09.29 val PER: 0.1161
2025-12-08 07:36:24,776: t15.2023.10.01 val PER: 0.1506
2025-12-08 07:36:24,777: t15.2023.10.06 val PER: 0.0786
2025-12-08 07:36:24,777: t15.2023.10.08 val PER: 0.1759
2025-12-08 07:36:24,777: t15.2023.10.13 val PER: 0.1730
2025-12-08 07:36:24,777: t15.2023.10.15 val PER: 0.1279
2025-12-08 07:36:24,777: t15.2023.10.20 val PER: 0.1711
2025-12-08 07:36:24,777: t15.2023.10.22 val PER: 0.1080
2025-12-08 07:36:24,777: t15.2023.11.03 val PER: 0.1526
2025-12-08 07:36:24,777: t15.2023.11.04 val PER: 0.0171
2025-12-08 07:36:24,777: t15.2023.11.17 val PER: 0.0156
2025-12-08 07:36:24,777: t15.2023.11.19 val PER: 0.0220
2025-12-08 07:36:24,777: t15.2023.11.26 val PER: 0.0674
2025-12-08 07:36:24,777: t15.2023.12.03 val PER: 0.0515
2025-12-08 07:36:24,777: t15.2023.12.08 val PER: 0.0626
2025-12-08 07:36:24,777: t15.2023.12.10 val PER: 0.0420
2025-12-08 07:36:24,777: t15.2023.12.17 val PER: 0.1040
2025-12-08 07:36:24,777: t15.2023.12.29 val PER: 0.0858
2025-12-08 07:36:24,777: t15.2024.02.25 val PER: 0.0857
2025-12-08 07:36:24,777: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:36:24,777: t15.2024.03.08 val PER: 0.1821
2025-12-08 07:36:24,778: t15.2024.03.15 val PER: 0.1889
2025-12-08 07:36:24,778: t15.2024.03.17 val PER: 0.0990
2025-12-08 07:36:24,778: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:36:24,778: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:36:24,778: t15.2024.05.10 val PER: 0.1204
2025-12-08 07:36:24,778: t15.2024.06.14 val PER: 0.1309
2025-12-08 07:36:24,778: t15.2024.07.19 val PER: 0.1760
2025-12-08 07:36:24,778: t15.2024.07.21 val PER: 0.0690
2025-12-08 07:36:24,778: t15.2024.07.28 val PER: 0.0868
2025-12-08 07:36:24,778: t15.2025.01.10 val PER: 0.2590
2025-12-08 07:36:24,778: t15.2025.01.12 val PER: 0.1001
2025-12-08 07:36:24,778: t15.2025.03.14 val PER: 0.3328
2025-12-08 07:36:24,778: t15.2025.03.16 val PER: 0.1466
2025-12-08 07:36:24,778: t15.2025.03.30 val PER: 0.2506
2025-12-08 07:36:24,778: t15.2025.04.13 val PER: 0.2111
2025-12-08 07:36:24,778: New best test PER 0.1179 --> 0.1178
2025-12-08 07:36:24,778: Checkpointing model
2025-12-08 07:36:25,799: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:36:53,667: Train batch 22200: loss: 0.64 grad norm: 15.49 time: 0.130
2025-12-08 07:37:22,421: Train batch 22400: loss: 1.18 grad norm: 11.57 time: 0.097
2025-12-08 07:37:51,359: Train batch 22600: loss: 0.83 grad norm: 14.81 time: 0.117
2025-12-08 07:38:19,709: Train batch 22800: loss: 0.43 grad norm: 9.02 time: 0.105
2025-12-08 07:38:47,879: Train batch 23000: loss: 0.39 grad norm: 9.05 time: 0.094
2025-12-08 07:39:16,399: Train batch 23200: loss: 0.47 grad norm: 17.41 time: 0.094
2025-12-08 07:39:45,371: Train batch 23400: loss: 0.04 grad norm: 3.70 time: 0.102
2025-12-08 07:40:13,514: Train batch 23600: loss: 0.94 grad norm: 14.45 time: 0.129
2025-12-08 07:40:41,827: Train batch 23800: loss: 0.32 grad norm: 9.86 time: 0.153
2025-12-08 07:41:09,981: Train batch 24000: loss: 0.22 grad norm: 17.66 time: 0.122
2025-12-08 07:41:09,981: Running test after training batch: 24000
2025-12-08 07:41:20,546: Val batch 24000: PER (avg): 0.1187 CTC Loss (avg): 21.9866 time: 10.565
2025-12-08 07:41:20,546: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:41:20,546: t15.2023.08.13 val PER: 0.0769
2025-12-08 07:41:20,546: t15.2023.08.18 val PER: 0.0780
2025-12-08 07:41:20,546: t15.2023.08.20 val PER: 0.0635
2025-12-08 07:41:20,546: t15.2023.08.25 val PER: 0.0873
2025-12-08 07:41:20,547: t15.2023.08.27 val PER: 0.1415
2025-12-08 07:41:20,547: t15.2023.09.01 val PER: 0.0495
2025-12-08 07:41:20,547: t15.2023.09.03 val PER: 0.1366
2025-12-08 07:41:20,547: t15.2023.09.24 val PER: 0.0910
2025-12-08 07:41:20,547: t15.2023.09.29 val PER: 0.1104
2025-12-08 07:41:20,547: t15.2023.10.01 val PER: 0.1473
2025-12-08 07:41:20,547: t15.2023.10.06 val PER: 0.0710
2025-12-08 07:41:20,547: t15.2023.10.08 val PER: 0.1813
2025-12-08 07:41:20,547: t15.2023.10.13 val PER: 0.1606
2025-12-08 07:41:20,547: t15.2023.10.15 val PER: 0.1384
2025-12-08 07:41:20,547: t15.2023.10.20 val PER: 0.1342
2025-12-08 07:41:20,547: t15.2023.10.22 val PER: 0.1069
2025-12-08 07:41:20,547: t15.2023.11.03 val PER: 0.1547
2025-12-08 07:41:20,547: t15.2023.11.04 val PER: 0.0137
2025-12-08 07:41:20,547: t15.2023.11.17 val PER: 0.0233
2025-12-08 07:41:20,547: t15.2023.11.19 val PER: 0.0180
2025-12-08 07:41:20,547: t15.2023.11.26 val PER: 0.0725
2025-12-08 07:41:20,547: t15.2023.12.03 val PER: 0.0830
2025-12-08 07:41:20,548: t15.2023.12.08 val PER: 0.0586
2025-12-08 07:41:20,548: t15.2023.12.10 val PER: 0.0499
2025-12-08 07:41:20,548: t15.2023.12.17 val PER: 0.0925
2025-12-08 07:41:20,548: t15.2023.12.29 val PER: 0.0961
2025-12-08 07:41:20,548: t15.2024.02.25 val PER: 0.0885
2025-12-08 07:41:20,548: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:41:20,548: t15.2024.03.08 val PER: 0.2006
2025-12-08 07:41:20,548: t15.2024.03.15 val PER: 0.1814
2025-12-08 07:41:20,548: t15.2024.03.17 val PER: 0.0955
2025-12-08 07:41:20,548: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:41:20,548: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:41:20,548: t15.2024.05.10 val PER: 0.1248
2025-12-08 07:41:20,548: t15.2024.06.14 val PER: 0.1356
2025-12-08 07:41:20,548: t15.2024.07.19 val PER: 0.1721
2025-12-08 07:41:20,548: t15.2024.07.21 val PER: 0.0710
2025-12-08 07:41:20,548: t15.2024.07.28 val PER: 0.0934
2025-12-08 07:41:20,548: t15.2025.01.10 val PER: 0.2603
2025-12-08 07:41:20,548: t15.2025.01.12 val PER: 0.1201
2025-12-08 07:41:20,548: t15.2025.03.14 val PER: 0.3018
2025-12-08 07:41:20,549: t15.2025.03.16 val PER: 0.1610
2025-12-08 07:41:20,549: t15.2025.03.30 val PER: 0.2448
2025-12-08 07:41:20,549: t15.2025.04.13 val PER: 0.2197
2025-12-08 07:41:49,616: Train batch 24200: loss: 0.25 grad norm: 5.75 time: 0.077
2025-12-08 07:42:17,656: Train batch 24400: loss: 0.45 grad norm: 13.63 time: 0.104
2025-12-08 07:42:46,601: Train batch 24600: loss: 0.37 grad norm: 11.19 time: 0.142
2025-12-08 07:43:14,682: Train batch 24800: loss: 0.28 grad norm: 7.84 time: 0.114
2025-12-08 07:43:42,834: Train batch 25000: loss: 0.51 grad norm: 14.06 time: 0.132
2025-12-08 07:44:11,050: Train batch 25200: loss: 0.73 grad norm: 20.11 time: 0.072
2025-12-08 07:44:40,073: Train batch 25400: loss: 0.30 grad norm: 12.25 time: 0.126
2025-12-08 07:45:08,251: Train batch 25600: loss: 0.29 grad norm: 10.85 time: 0.127
2025-12-08 07:45:36,998: Train batch 25800: loss: 0.18 grad norm: 5.47 time: 0.124
2025-12-08 07:46:05,276: Train batch 26000: loss: 0.35 grad norm: 9.47 time: 0.121
2025-12-08 07:46:05,277: Running test after training batch: 26000
2025-12-08 07:46:15,681: Val batch 26000: PER (avg): 0.1192 CTC Loss (avg): 22.9837 time: 10.405
2025-12-08 07:46:15,682: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:46:15,682: t15.2023.08.13 val PER: 0.0904
2025-12-08 07:46:15,682: t15.2023.08.18 val PER: 0.0754
2025-12-08 07:46:15,682: t15.2023.08.20 val PER: 0.0588
2025-12-08 07:46:15,682: t15.2023.08.25 val PER: 0.0904
2025-12-08 07:46:15,682: t15.2023.08.27 val PER: 0.1768
2025-12-08 07:46:15,682: t15.2023.09.01 val PER: 0.0487
2025-12-08 07:46:15,682: t15.2023.09.03 val PER: 0.1164
2025-12-08 07:46:15,682: t15.2023.09.24 val PER: 0.1019
2025-12-08 07:46:15,682: t15.2023.09.29 val PER: 0.1123
2025-12-08 07:46:15,682: t15.2023.10.01 val PER: 0.1460
2025-12-08 07:46:15,682: t15.2023.10.06 val PER: 0.0947
2025-12-08 07:46:15,682: t15.2023.10.08 val PER: 0.1976
2025-12-08 07:46:15,682: t15.2023.10.13 val PER: 0.1544
2025-12-08 07:46:15,682: t15.2023.10.15 val PER: 0.1206
2025-12-08 07:46:15,682: t15.2023.10.20 val PER: 0.1611
2025-12-08 07:46:15,682: t15.2023.10.22 val PER: 0.0980
2025-12-08 07:46:15,683: t15.2023.11.03 val PER: 0.1567
2025-12-08 07:46:15,683: t15.2023.11.04 val PER: 0.0239
2025-12-08 07:46:15,683: t15.2023.11.17 val PER: 0.0249
2025-12-08 07:46:15,683: t15.2023.11.19 val PER: 0.0259
2025-12-08 07:46:15,683: t15.2023.11.26 val PER: 0.0754
2025-12-08 07:46:15,683: t15.2023.12.03 val PER: 0.0756
2025-12-08 07:46:15,683: t15.2023.12.08 val PER: 0.0666
2025-12-08 07:46:15,683: t15.2023.12.10 val PER: 0.0420
2025-12-08 07:46:15,683: t15.2023.12.17 val PER: 0.0936
2025-12-08 07:46:15,683: t15.2023.12.29 val PER: 0.0865
2025-12-08 07:46:15,683: t15.2024.02.25 val PER: 0.0857
2025-12-08 07:46:15,683: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:46:15,683: t15.2024.03.08 val PER: 0.1807
2025-12-08 07:46:15,683: t15.2024.03.15 val PER: 0.1895
2025-12-08 07:46:15,683: t15.2024.03.17 val PER: 0.1039
2025-12-08 07:46:15,683: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:46:15,683: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:46:15,683: t15.2024.05.10 val PER: 0.1233
2025-12-08 07:46:15,684: t15.2024.06.14 val PER: 0.1246
2025-12-08 07:46:15,684: t15.2024.07.19 val PER: 0.1674
2025-12-08 07:46:15,684: t15.2024.07.21 val PER: 0.0745
2025-12-08 07:46:15,684: t15.2024.07.28 val PER: 0.0846
2025-12-08 07:46:15,684: t15.2025.01.10 val PER: 0.2782
2025-12-08 07:46:15,684: t15.2025.01.12 val PER: 0.1109
2025-12-08 07:46:15,684: t15.2025.03.14 val PER: 0.3343
2025-12-08 07:46:15,684: t15.2025.03.16 val PER: 0.1754
2025-12-08 07:46:15,684: t15.2025.03.30 val PER: 0.2287
2025-12-08 07:46:15,684: t15.2025.04.13 val PER: 0.2068
2025-12-08 07:46:43,524: Train batch 26200: loss: 0.28 grad norm: 9.78 time: 0.105
2025-12-08 07:47:12,236: Train batch 26400: loss: 0.28 grad norm: 8.81 time: 0.088
2025-12-08 07:47:41,117: Train batch 26600: loss: 0.86 grad norm: 14.54 time: 0.164
2025-12-08 07:48:09,380: Train batch 26800: loss: 0.20 grad norm: 5.64 time: 0.115
2025-12-08 07:48:38,229: Train batch 27000: loss: 0.28 grad norm: 6.63 time: 0.103
2025-12-08 07:49:06,505: Train batch 27200: loss: 0.29 grad norm: 9.13 time: 0.104
2025-12-08 07:49:35,487: Train batch 27400: loss: 0.17 grad norm: 5.60 time: 0.125
2025-12-08 07:50:02,986: Train batch 27600: loss: 0.16 grad norm: 5.74 time: 0.097
2025-12-08 07:50:31,474: Train batch 27800: loss: 0.18 grad norm: 8.48 time: 0.104
2025-12-08 07:51:00,559: Train batch 28000: loss: 0.70 grad norm: 14.21 time: 0.134
2025-12-08 07:51:00,560: Running test after training batch: 28000
2025-12-08 07:51:11,002: Val batch 28000: PER (avg): 0.1160 CTC Loss (avg): 22.5428 time: 10.442
2025-12-08 07:51:11,003: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:51:11,003: t15.2023.08.13 val PER: 0.0728
2025-12-08 07:51:11,003: t15.2023.08.18 val PER: 0.0712
2025-12-08 07:51:11,003: t15.2023.08.20 val PER: 0.0612
2025-12-08 07:51:11,003: t15.2023.08.25 val PER: 0.0904
2025-12-08 07:51:11,003: t15.2023.08.27 val PER: 0.1768
2025-12-08 07:51:11,003: t15.2023.09.01 val PER: 0.0503
2025-12-08 07:51:11,003: t15.2023.09.03 val PER: 0.1342
2025-12-08 07:51:11,003: t15.2023.09.24 val PER: 0.0898
2025-12-08 07:51:11,003: t15.2023.09.29 val PER: 0.1104
2025-12-08 07:51:11,004: t15.2023.10.01 val PER: 0.1407
2025-12-08 07:51:11,004: t15.2023.10.06 val PER: 0.0807
2025-12-08 07:51:11,004: t15.2023.10.08 val PER: 0.1962
2025-12-08 07:51:11,004: t15.2023.10.13 val PER: 0.1552
2025-12-08 07:51:11,004: t15.2023.10.15 val PER: 0.1140
2025-12-08 07:51:11,004: t15.2023.10.20 val PER: 0.1577
2025-12-08 07:51:11,004: t15.2023.10.22 val PER: 0.1036
2025-12-08 07:51:11,004: t15.2023.11.03 val PER: 0.1506
2025-12-08 07:51:11,004: t15.2023.11.04 val PER: 0.0102
2025-12-08 07:51:11,004: t15.2023.11.17 val PER: 0.0171
2025-12-08 07:51:11,004: t15.2023.11.19 val PER: 0.0259
2025-12-08 07:51:11,004: t15.2023.11.26 val PER: 0.0572
2025-12-08 07:51:11,004: t15.2023.12.03 val PER: 0.0662
2025-12-08 07:51:11,004: t15.2023.12.08 val PER: 0.0513
2025-12-08 07:51:11,004: t15.2023.12.10 val PER: 0.0394
2025-12-08 07:51:11,004: t15.2023.12.17 val PER: 0.1019
2025-12-08 07:51:11,004: t15.2023.12.29 val PER: 0.0906
2025-12-08 07:51:11,004: t15.2024.02.25 val PER: 0.0913
2025-12-08 07:51:11,005: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:51:11,005: t15.2024.03.08 val PER: 0.1778
2025-12-08 07:51:11,005: t15.2024.03.15 val PER: 0.1657
2025-12-08 07:51:11,005: t15.2024.03.17 val PER: 0.0927
2025-12-08 07:51:11,005: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:51:11,005: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:51:11,005: t15.2024.05.10 val PER: 0.1189
2025-12-08 07:51:11,005: t15.2024.06.14 val PER: 0.1420
2025-12-08 07:51:11,005: t15.2024.07.19 val PER: 0.1819
2025-12-08 07:51:11,005: t15.2024.07.21 val PER: 0.0669
2025-12-08 07:51:11,005: t15.2024.07.28 val PER: 0.0934
2025-12-08 07:51:11,005: t15.2025.01.10 val PER: 0.2590
2025-12-08 07:51:11,005: t15.2025.01.12 val PER: 0.1232
2025-12-08 07:51:11,005: t15.2025.03.14 val PER: 0.3195
2025-12-08 07:51:11,005: t15.2025.03.16 val PER: 0.1505
2025-12-08 07:51:11,005: t15.2025.03.30 val PER: 0.2460
2025-12-08 07:51:11,005: t15.2025.04.13 val PER: 0.2154
2025-12-08 07:51:11,005: New best test PER 0.1178 --> 0.1160
2025-12-08 07:51:11,005: Checkpointing model
2025-12-08 07:51:11,634: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:51:40,835: Train batch 28200: loss: 0.34 grad norm: 8.99 time: 0.109
2025-12-08 07:52:08,813: Train batch 28400: loss: 0.14 grad norm: 5.05 time: 0.144
2025-12-08 07:52:36,853: Train batch 28600: loss: 0.26 grad norm: 7.54 time: 0.151
2025-12-08 07:53:04,926: Train batch 28800: loss: 0.52 grad norm: 12.41 time: 0.196
2025-12-08 07:53:33,018: Train batch 29000: loss: 0.20 grad norm: 8.23 time: 0.124
2025-12-08 07:54:00,994: Train batch 29200: loss: 0.13 grad norm: 6.99 time: 0.159
2025-12-08 07:54:29,145: Train batch 29400: loss: 0.29 grad norm: 9.11 time: 0.137
2025-12-08 07:54:57,551: Train batch 29600: loss: 0.20 grad norm: 7.20 time: 0.138
2025-12-08 07:55:26,281: Train batch 29800: loss: 0.07 grad norm: 3.70 time: 0.109
2025-12-08 07:55:54,657: Train batch 30000: loss: 0.18 grad norm: 7.33 time: 0.163
2025-12-08 07:55:54,658: Running test after training batch: 30000
2025-12-08 07:56:05,242: Val batch 30000: PER (avg): 0.1119 CTC Loss (avg): 22.2698 time: 10.584
2025-12-08 07:56:05,243: t15.2023.08.11 val PER: 1.0000
2025-12-08 07:56:05,243: t15.2023.08.13 val PER: 0.0634
2025-12-08 07:56:05,243: t15.2023.08.18 val PER: 0.0654
2025-12-08 07:56:05,243: t15.2023.08.20 val PER: 0.0564
2025-12-08 07:56:05,243: t15.2023.08.25 val PER: 0.0753
2025-12-08 07:56:05,243: t15.2023.08.27 val PER: 0.1559
2025-12-08 07:56:05,243: t15.2023.09.01 val PER: 0.0438
2025-12-08 07:56:05,243: t15.2023.09.03 val PER: 0.1259
2025-12-08 07:56:05,243: t15.2023.09.24 val PER: 0.0789
2025-12-08 07:56:05,243: t15.2023.09.29 val PER: 0.1193
2025-12-08 07:56:05,243: t15.2023.10.01 val PER: 0.1367
2025-12-08 07:56:05,243: t15.2023.10.06 val PER: 0.0721
2025-12-08 07:56:05,243: t15.2023.10.08 val PER: 0.1705
2025-12-08 07:56:05,244: t15.2023.10.13 val PER: 0.1575
2025-12-08 07:56:05,244: t15.2023.10.15 val PER: 0.1101
2025-12-08 07:56:05,244: t15.2023.10.20 val PER: 0.1812
2025-12-08 07:56:05,244: t15.2023.10.22 val PER: 0.0991
2025-12-08 07:56:05,244: t15.2023.11.03 val PER: 0.1486
2025-12-08 07:56:05,244: t15.2023.11.04 val PER: 0.0102
2025-12-08 07:56:05,244: t15.2023.11.17 val PER: 0.0249
2025-12-08 07:56:05,244: t15.2023.11.19 val PER: 0.0279
2025-12-08 07:56:05,244: t15.2023.11.26 val PER: 0.0601
2025-12-08 07:56:05,244: t15.2023.12.03 val PER: 0.0536
2025-12-08 07:56:05,244: t15.2023.12.08 val PER: 0.0606
2025-12-08 07:56:05,244: t15.2023.12.10 val PER: 0.0499
2025-12-08 07:56:05,244: t15.2023.12.17 val PER: 0.0884
2025-12-08 07:56:05,244: t15.2023.12.29 val PER: 0.0762
2025-12-08 07:56:05,244: t15.2024.02.25 val PER: 0.0787
2025-12-08 07:56:05,244: t15.2024.03.03 val PER: 1.0000
2025-12-08 07:56:05,244: t15.2024.03.08 val PER: 0.1707
2025-12-08 07:56:05,244: t15.2024.03.15 val PER: 0.1745
2025-12-08 07:56:05,244: t15.2024.03.17 val PER: 0.0941
2025-12-08 07:56:05,245: t15.2024.04.25 val PER: 1.0000
2025-12-08 07:56:05,245: t15.2024.04.28 val PER: 1.0000
2025-12-08 07:56:05,245: t15.2024.05.10 val PER: 0.1337
2025-12-08 07:56:05,245: t15.2024.06.14 val PER: 0.1356
2025-12-08 07:56:05,245: t15.2024.07.19 val PER: 0.1543
2025-12-08 07:56:05,245: t15.2024.07.21 val PER: 0.0697
2025-12-08 07:56:05,245: t15.2024.07.28 val PER: 0.0897
2025-12-08 07:56:05,245: t15.2025.01.10 val PER: 0.2452
2025-12-08 07:56:05,245: t15.2025.01.12 val PER: 0.1055
2025-12-08 07:56:05,245: t15.2025.03.14 val PER: 0.3314
2025-12-08 07:56:05,245: t15.2025.03.16 val PER: 0.1702
2025-12-08 07:56:05,245: t15.2025.03.30 val PER: 0.2276
2025-12-08 07:56:05,245: t15.2025.04.13 val PER: 0.2097
2025-12-08 07:56:05,245: New best test PER 0.1160 --> 0.1119
2025-12-08 07:56:05,245: Checkpointing model
2025-12-08 07:56:06,323: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 07:56:34,128: Train batch 30200: loss: 0.02 grad norm: 3.44 time: 0.080
2025-12-08 07:57:02,038: Train batch 30400: loss: 0.25 grad norm: 6.61 time: 0.098
2025-12-08 07:57:30,668: Train batch 30600: loss: 0.24 grad norm: 6.73 time: 0.154
2025-12-08 07:57:59,524: Train batch 30800: loss: 0.10 grad norm: 4.13 time: 0.115
2025-12-08 07:58:28,742: Train batch 31000: loss: 0.22 grad norm: 7.60 time: 0.098
2025-12-08 07:58:57,178: Train batch 31200: loss: 0.07 grad norm: 3.33 time: 0.086
2025-12-08 07:59:25,348: Train batch 31400: loss: 0.18 grad norm: 5.35 time: 0.105
2025-12-08 07:59:53,357: Train batch 31600: loss: 0.41 grad norm: 11.85 time: 0.096
2025-12-08 08:00:22,157: Train batch 31800: loss: 0.27 grad norm: 6.75 time: 0.103
2025-12-08 08:00:50,324: Train batch 32000: loss: 0.22 grad norm: 7.54 time: 0.092
2025-12-08 08:00:50,324: Running test after training batch: 32000
2025-12-08 08:01:00,705: Val batch 32000: PER (avg): 0.1160 CTC Loss (avg): 22.9571 time: 10.381
2025-12-08 08:01:00,706: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:01:00,706: t15.2023.08.13 val PER: 0.0707
2025-12-08 08:01:00,706: t15.2023.08.18 val PER: 0.0796
2025-12-08 08:01:00,706: t15.2023.08.20 val PER: 0.0572
2025-12-08 08:01:00,706: t15.2023.08.25 val PER: 0.0753
2025-12-08 08:01:00,706: t15.2023.08.27 val PER: 0.1431
2025-12-08 08:01:00,706: t15.2023.09.01 val PER: 0.0446
2025-12-08 08:01:00,706: t15.2023.09.03 val PER: 0.1200
2025-12-08 08:01:00,706: t15.2023.09.24 val PER: 0.0995
2025-12-08 08:01:00,706: t15.2023.09.29 val PER: 0.1142
2025-12-08 08:01:00,706: t15.2023.10.01 val PER: 0.1400
2025-12-08 08:01:00,706: t15.2023.10.06 val PER: 0.0818
2025-12-08 08:01:00,707: t15.2023.10.08 val PER: 0.1719
2025-12-08 08:01:00,707: t15.2023.10.13 val PER: 0.1443
2025-12-08 08:01:00,707: t15.2023.10.15 val PER: 0.1140
2025-12-08 08:01:00,707: t15.2023.10.20 val PER: 0.1678
2025-12-08 08:01:00,707: t15.2023.10.22 val PER: 0.1013
2025-12-08 08:01:00,707: t15.2023.11.03 val PER: 0.1540
2025-12-08 08:01:00,707: t15.2023.11.04 val PER: 0.0171
2025-12-08 08:01:00,707: t15.2023.11.17 val PER: 0.0218
2025-12-08 08:01:00,707: t15.2023.11.19 val PER: 0.0299
2025-12-08 08:01:00,707: t15.2023.11.26 val PER: 0.0623
2025-12-08 08:01:00,707: t15.2023.12.03 val PER: 0.0620
2025-12-08 08:01:00,707: t15.2023.12.08 val PER: 0.0486
2025-12-08 08:01:00,707: t15.2023.12.10 val PER: 0.0473
2025-12-08 08:01:00,707: t15.2023.12.17 val PER: 0.0946
2025-12-08 08:01:00,707: t15.2023.12.29 val PER: 0.0906
2025-12-08 08:01:00,707: t15.2024.02.25 val PER: 0.0730
2025-12-08 08:01:00,707: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:01:00,707: t15.2024.03.08 val PER: 0.1807
2025-12-08 08:01:00,708: t15.2024.03.15 val PER: 0.1901
2025-12-08 08:01:00,708: t15.2024.03.17 val PER: 0.0914
2025-12-08 08:01:00,708: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:01:00,708: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:01:00,708: t15.2024.05.10 val PER: 0.1308
2025-12-08 08:01:00,708: t15.2024.06.14 val PER: 0.1420
2025-12-08 08:01:00,708: t15.2024.07.19 val PER: 0.1819
2025-12-08 08:01:00,708: t15.2024.07.21 val PER: 0.0738
2025-12-08 08:01:00,708: t15.2024.07.28 val PER: 0.1000
2025-12-08 08:01:00,708: t15.2025.01.10 val PER: 0.2713
2025-12-08 08:01:00,708: t15.2025.01.12 val PER: 0.1055
2025-12-08 08:01:00,708: t15.2025.03.14 val PER: 0.3343
2025-12-08 08:01:00,708: t15.2025.03.16 val PER: 0.1545
2025-12-08 08:01:00,708: t15.2025.03.30 val PER: 0.2437
2025-12-08 08:01:00,708: t15.2025.04.13 val PER: 0.2211
2025-12-08 08:01:28,141: Train batch 32200: loss: 0.09 grad norm: 3.49 time: 0.137
2025-12-08 08:01:55,735: Train batch 32400: loss: 0.27 grad norm: 8.26 time: 0.111
2025-12-08 08:02:23,531: Train batch 32600: loss: 0.19 grad norm: 11.58 time: 0.122
2025-12-08 08:02:50,851: Train batch 32800: loss: 0.11 grad norm: 4.79 time: 0.156
2025-12-08 08:03:18,819: Train batch 33000: loss: 0.23 grad norm: 9.89 time: 0.104
2025-12-08 08:03:46,598: Train batch 33200: loss: 0.10 grad norm: 3.57 time: 0.083
2025-12-08 08:04:14,708: Train batch 33400: loss: 0.46 grad norm: 13.29 time: 0.156
2025-12-08 08:04:42,136: Train batch 33600: loss: 0.23 grad norm: 6.63 time: 0.139
2025-12-08 08:05:10,611: Train batch 33800: loss: 0.32 grad norm: 9.30 time: 0.164
2025-12-08 08:05:38,348: Train batch 34000: loss: 0.22 grad norm: 7.34 time: 0.158
2025-12-08 08:05:38,348: Running test after training batch: 34000
2025-12-08 08:05:49,206: Val batch 34000: PER (avg): 0.1112 CTC Loss (avg): 22.9570 time: 10.858
2025-12-08 08:05:49,207: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:05:49,208: t15.2023.08.13 val PER: 0.0728
2025-12-08 08:05:49,208: t15.2023.08.18 val PER: 0.0662
2025-12-08 08:05:49,208: t15.2023.08.20 val PER: 0.0620
2025-12-08 08:05:49,208: t15.2023.08.25 val PER: 0.0843
2025-12-08 08:05:49,208: t15.2023.08.27 val PER: 0.1511
2025-12-08 08:05:49,208: t15.2023.09.01 val PER: 0.0463
2025-12-08 08:05:49,208: t15.2023.09.03 val PER: 0.1235
2025-12-08 08:05:49,208: t15.2023.09.24 val PER: 0.0862
2025-12-08 08:05:49,208: t15.2023.09.29 val PER: 0.1085
2025-12-08 08:05:49,208: t15.2023.10.01 val PER: 0.1387
2025-12-08 08:05:49,208: t15.2023.10.06 val PER: 0.0689
2025-12-08 08:05:49,208: t15.2023.10.08 val PER: 0.1867
2025-12-08 08:05:49,208: t15.2023.10.13 val PER: 0.1528
2025-12-08 08:05:49,208: t15.2023.10.15 val PER: 0.1206
2025-12-08 08:05:49,208: t15.2023.10.20 val PER: 0.1745
2025-12-08 08:05:49,208: t15.2023.10.22 val PER: 0.0902
2025-12-08 08:05:49,208: t15.2023.11.03 val PER: 0.1425
2025-12-08 08:05:49,209: t15.2023.11.04 val PER: 0.0171
2025-12-08 08:05:49,209: t15.2023.11.17 val PER: 0.0171
2025-12-08 08:05:49,209: t15.2023.11.19 val PER: 0.0220
2025-12-08 08:05:49,209: t15.2023.11.26 val PER: 0.0529
2025-12-08 08:05:49,209: t15.2023.12.03 val PER: 0.0630
2025-12-08 08:05:49,209: t15.2023.12.08 val PER: 0.0566
2025-12-08 08:05:49,209: t15.2023.12.10 val PER: 0.0499
2025-12-08 08:05:49,209: t15.2023.12.17 val PER: 0.0946
2025-12-08 08:05:49,209: t15.2023.12.29 val PER: 0.0782
2025-12-08 08:05:49,209: t15.2024.02.25 val PER: 0.0716
2025-12-08 08:05:49,209: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:05:49,209: t15.2024.03.08 val PER: 0.1664
2025-12-08 08:05:49,209: t15.2024.03.15 val PER: 0.1770
2025-12-08 08:05:49,209: t15.2024.03.17 val PER: 0.1053
2025-12-08 08:05:49,209: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:05:49,209: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:05:49,209: t15.2024.05.10 val PER: 0.1189
2025-12-08 08:05:49,209: t15.2024.06.14 val PER: 0.1246
2025-12-08 08:05:49,209: t15.2024.07.19 val PER: 0.1595
2025-12-08 08:05:49,210: t15.2024.07.21 val PER: 0.0593
2025-12-08 08:05:49,210: t15.2024.07.28 val PER: 0.0956
2025-12-08 08:05:49,210: t15.2025.01.10 val PER: 0.2548
2025-12-08 08:05:49,210: t15.2025.01.12 val PER: 0.1008
2025-12-08 08:05:49,210: t15.2025.03.14 val PER: 0.3314
2025-12-08 08:05:49,210: t15.2025.03.16 val PER: 0.1322
2025-12-08 08:05:49,210: t15.2025.03.30 val PER: 0.2345
2025-12-08 08:05:49,210: t15.2025.04.13 val PER: 0.1969
2025-12-08 08:05:49,210: New best test PER 0.1119 --> 0.1112
2025-12-08 08:05:49,210: Checkpointing model
2025-12-08 08:05:50,319: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:06:18,336: Train batch 34200: loss: 0.14 grad norm: 7.63 time: 0.156
2025-12-08 08:06:45,414: Train batch 34400: loss: 0.16 grad norm: 12.17 time: 0.104
2025-12-08 08:07:12,972: Train batch 34600: loss: 0.29 grad norm: 8.82 time: 0.086
2025-12-08 08:07:40,408: Train batch 34800: loss: 0.25 grad norm: 10.55 time: 0.186
2025-12-08 08:08:07,922: Train batch 35000: loss: 0.21 grad norm: 10.08 time: 0.130
2025-12-08 08:08:35,005: Train batch 35200: loss: 0.14 grad norm: 5.75 time: 0.144
2025-12-08 08:09:02,842: Train batch 35400: loss: 0.11 grad norm: 6.07 time: 0.094
2025-12-08 08:09:31,501: Train batch 35600: loss: 0.09 grad norm: 4.69 time: 0.167
2025-12-08 08:09:59,056: Train batch 35800: loss: 0.18 grad norm: 4.99 time: 0.117
2025-12-08 08:10:26,129: Train batch 36000: loss: 0.08 grad norm: 4.38 time: 0.128
2025-12-08 08:10:26,130: Running test after training batch: 36000
2025-12-08 08:10:36,602: Val batch 36000: PER (avg): 0.1120 CTC Loss (avg): 23.1262 time: 10.473
2025-12-08 08:10:36,603: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:10:36,603: t15.2023.08.13 val PER: 0.0696
2025-12-08 08:10:36,603: t15.2023.08.18 val PER: 0.0721
2025-12-08 08:10:36,603: t15.2023.08.20 val PER: 0.0604
2025-12-08 08:10:36,603: t15.2023.08.25 val PER: 0.0858
2025-12-08 08:10:36,603: t15.2023.08.27 val PER: 0.1511
2025-12-08 08:10:36,603: t15.2023.09.01 val PER: 0.0446
2025-12-08 08:10:36,603: t15.2023.09.03 val PER: 0.1200
2025-12-08 08:10:36,603: t15.2023.09.24 val PER: 0.0789
2025-12-08 08:10:36,603: t15.2023.09.29 val PER: 0.1117
2025-12-08 08:10:36,603: t15.2023.10.01 val PER: 0.1466
2025-12-08 08:10:36,603: t15.2023.10.06 val PER: 0.0571
2025-12-08 08:10:36,603: t15.2023.10.08 val PER: 0.1786
2025-12-08 08:10:36,603: t15.2023.10.13 val PER: 0.1528
2025-12-08 08:10:36,603: t15.2023.10.15 val PER: 0.1134
2025-12-08 08:10:36,603: t15.2023.10.20 val PER: 0.1443
2025-12-08 08:10:36,604: t15.2023.10.22 val PER: 0.1047
2025-12-08 08:10:36,604: t15.2023.11.03 val PER: 0.1377
2025-12-08 08:10:36,604: t15.2023.11.04 val PER: 0.0034
2025-12-08 08:10:36,604: t15.2023.11.17 val PER: 0.0233
2025-12-08 08:10:36,604: t15.2023.11.19 val PER: 0.0379
2025-12-08 08:10:36,604: t15.2023.11.26 val PER: 0.0529
2025-12-08 08:10:36,604: t15.2023.12.03 val PER: 0.0620
2025-12-08 08:10:36,604: t15.2023.12.08 val PER: 0.0479
2025-12-08 08:10:36,604: t15.2023.12.10 val PER: 0.0420
2025-12-08 08:10:36,604: t15.2023.12.17 val PER: 0.0852
2025-12-08 08:10:36,604: t15.2023.12.29 val PER: 0.0755
2025-12-08 08:10:36,604: t15.2024.02.25 val PER: 0.0702
2025-12-08 08:10:36,604: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:10:36,604: t15.2024.03.08 val PER: 0.1721
2025-12-08 08:10:36,604: t15.2024.03.15 val PER: 0.1845
2025-12-08 08:10:36,604: t15.2024.03.17 val PER: 0.0893
2025-12-08 08:10:36,604: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:10:36,604: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:10:36,604: t15.2024.05.10 val PER: 0.1337
2025-12-08 08:10:36,605: t15.2024.06.14 val PER: 0.1262
2025-12-08 08:10:36,605: t15.2024.07.19 val PER: 0.1701
2025-12-08 08:10:36,605: t15.2024.07.21 val PER: 0.0697
2025-12-08 08:10:36,605: t15.2024.07.28 val PER: 0.0926
2025-12-08 08:10:36,605: t15.2025.01.10 val PER: 0.2521
2025-12-08 08:10:36,605: t15.2025.01.12 val PER: 0.1247
2025-12-08 08:10:36,605: t15.2025.03.14 val PER: 0.3299
2025-12-08 08:10:36,605: t15.2025.03.16 val PER: 0.1453
2025-12-08 08:10:36,605: t15.2025.03.30 val PER: 0.2299
2025-12-08 08:10:36,605: t15.2025.04.13 val PER: 0.2154
2025-12-08 08:11:03,368: Train batch 36200: loss: 0.59 grad norm: 13.70 time: 0.117
2025-12-08 08:11:30,954: Train batch 36400: loss: 0.62 grad norm: 19.79 time: 0.144
2025-12-08 08:11:58,337: Train batch 36600: loss: 0.11 grad norm: 5.41 time: 0.103
2025-12-08 08:12:25,835: Train batch 36800: loss: 0.21 grad norm: 6.76 time: 0.129
2025-12-08 08:12:53,979: Train batch 37000: loss: 0.16 grad norm: 6.56 time: 0.154
2025-12-08 08:13:21,529: Train batch 37200: loss: 0.38 grad norm: 12.74 time: 0.172
2025-12-08 08:13:49,131: Train batch 37400: loss: 0.09 grad norm: 4.63 time: 0.113
2025-12-08 08:14:16,875: Train batch 37600: loss: 0.21 grad norm: 14.40 time: 0.110
2025-12-08 08:14:44,300: Train batch 37800: loss: 0.25 grad norm: 22.71 time: 0.114
2025-12-08 08:15:11,703: Train batch 38000: loss: 0.17 grad norm: 5.34 time: 0.108
2025-12-08 08:15:11,703: Running test after training batch: 38000
2025-12-08 08:15:22,113: Val batch 38000: PER (avg): 0.1111 CTC Loss (avg): 22.7303 time: 10.410
2025-12-08 08:15:22,113: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:15:22,113: t15.2023.08.13 val PER: 0.0769
2025-12-08 08:15:22,113: t15.2023.08.18 val PER: 0.0662
2025-12-08 08:15:22,113: t15.2023.08.20 val PER: 0.0596
2025-12-08 08:15:22,114: t15.2023.08.25 val PER: 0.0723
2025-12-08 08:15:22,114: t15.2023.08.27 val PER: 0.1511
2025-12-08 08:15:22,114: t15.2023.09.01 val PER: 0.0373
2025-12-08 08:15:22,114: t15.2023.09.03 val PER: 0.1259
2025-12-08 08:15:22,114: t15.2023.09.24 val PER: 0.0947
2025-12-08 08:15:22,114: t15.2023.09.29 val PER: 0.1047
2025-12-08 08:15:22,114: t15.2023.10.01 val PER: 0.1413
2025-12-08 08:15:22,114: t15.2023.10.06 val PER: 0.0710
2025-12-08 08:15:22,114: t15.2023.10.08 val PER: 0.1759
2025-12-08 08:15:22,114: t15.2023.10.13 val PER: 0.1373
2025-12-08 08:15:22,114: t15.2023.10.15 val PER: 0.1167
2025-12-08 08:15:22,114: t15.2023.10.20 val PER: 0.1443
2025-12-08 08:15:22,114: t15.2023.10.22 val PER: 0.0991
2025-12-08 08:15:22,114: t15.2023.11.03 val PER: 0.1513
2025-12-08 08:15:22,114: t15.2023.11.04 val PER: 0.0205
2025-12-08 08:15:22,114: t15.2023.11.17 val PER: 0.0140
2025-12-08 08:15:22,114: t15.2023.11.19 val PER: 0.0359
2025-12-08 08:15:22,114: t15.2023.11.26 val PER: 0.0696
2025-12-08 08:15:22,114: t15.2023.12.03 val PER: 0.0662
2025-12-08 08:15:22,115: t15.2023.12.08 val PER: 0.0486
2025-12-08 08:15:22,115: t15.2023.12.10 val PER: 0.0420
2025-12-08 08:15:22,115: t15.2023.12.17 val PER: 0.1019
2025-12-08 08:15:22,115: t15.2023.12.29 val PER: 0.0769
2025-12-08 08:15:22,115: t15.2024.02.25 val PER: 0.0688
2025-12-08 08:15:22,115: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:15:22,115: t15.2024.03.08 val PER: 0.1664
2025-12-08 08:15:22,115: t15.2024.03.15 val PER: 0.1757
2025-12-08 08:15:22,115: t15.2024.03.17 val PER: 0.0927
2025-12-08 08:15:22,115: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:15:22,115: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:15:22,115: t15.2024.05.10 val PER: 0.1263
2025-12-08 08:15:22,115: t15.2024.06.14 val PER: 0.1388
2025-12-08 08:15:22,115: t15.2024.07.19 val PER: 0.1694
2025-12-08 08:15:22,115: t15.2024.07.21 val PER: 0.0572
2025-12-08 08:15:22,115: t15.2024.07.28 val PER: 0.0801
2025-12-08 08:15:22,115: t15.2025.01.10 val PER: 0.2479
2025-12-08 08:15:22,115: t15.2025.01.12 val PER: 0.1170
2025-12-08 08:15:22,115: t15.2025.03.14 val PER: 0.3299
2025-12-08 08:15:22,116: t15.2025.03.16 val PER: 0.1479
2025-12-08 08:15:22,116: t15.2025.03.30 val PER: 0.2253
2025-12-08 08:15:22,116: t15.2025.04.13 val PER: 0.2040
2025-12-08 08:15:22,116: New best test PER 0.1112 --> 0.1111
2025-12-08 08:15:22,116: Checkpointing model
2025-12-08 08:15:23,223: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:15:51,350: Train batch 38200: loss: 0.08 grad norm: 5.47 time: 0.138
2025-12-08 08:16:19,009: Train batch 38400: loss: 0.18 grad norm: 8.74 time: 0.141
2025-12-08 08:16:46,228: Train batch 38600: loss: 0.17 grad norm: 11.73 time: 0.133
2025-12-08 08:17:14,259: Train batch 38800: loss: 0.28 grad norm: 14.71 time: 0.162
2025-12-08 08:17:42,087: Train batch 39000: loss: 0.10 grad norm: 10.32 time: 0.136
2025-12-08 08:18:09,827: Train batch 39200: loss: 0.20 grad norm: 12.02 time: 0.094
2025-12-08 08:18:37,502: Train batch 39400: loss: 0.65 grad norm: 9.10 time: 0.099
2025-12-08 08:19:05,322: Train batch 39600: loss: 0.11 grad norm: 3.99 time: 0.090
2025-12-08 08:19:33,055: Train batch 39800: loss: 0.12 grad norm: 9.82 time: 0.086
2025-12-08 08:20:01,347: Train batch 40000: loss: 0.04 grad norm: 6.52 time: 0.133
2025-12-08 08:20:01,347: Running test after training batch: 40000
2025-12-08 08:20:11,864: Val batch 40000: PER (avg): 0.1114 CTC Loss (avg): 23.4888 time: 10.516
2025-12-08 08:20:11,864: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:20:11,864: t15.2023.08.13 val PER: 0.0686
2025-12-08 08:20:11,864: t15.2023.08.18 val PER: 0.0746
2025-12-08 08:20:11,865: t15.2023.08.20 val PER: 0.0612
2025-12-08 08:20:11,865: t15.2023.08.25 val PER: 0.0873
2025-12-08 08:20:11,865: t15.2023.08.27 val PER: 0.1592
2025-12-08 08:20:11,865: t15.2023.09.01 val PER: 0.0455
2025-12-08 08:20:11,865: t15.2023.09.03 val PER: 0.1093
2025-12-08 08:20:11,865: t15.2023.09.24 val PER: 0.0922
2025-12-08 08:20:11,865: t15.2023.09.29 val PER: 0.1078
2025-12-08 08:20:11,865: t15.2023.10.01 val PER: 0.1486
2025-12-08 08:20:11,865: t15.2023.10.06 val PER: 0.0700
2025-12-08 08:20:11,865: t15.2023.10.08 val PER: 0.1908
2025-12-08 08:20:11,865: t15.2023.10.13 val PER: 0.1497
2025-12-08 08:20:11,865: t15.2023.10.15 val PER: 0.1094
2025-12-08 08:20:11,865: t15.2023.10.20 val PER: 0.1812
2025-12-08 08:20:11,865: t15.2023.10.22 val PER: 0.0857
2025-12-08 08:20:11,865: t15.2023.11.03 val PER: 0.1520
2025-12-08 08:20:11,865: t15.2023.11.04 val PER: 0.0137
2025-12-08 08:20:11,865: t15.2023.11.17 val PER: 0.0233
2025-12-08 08:20:11,865: t15.2023.11.19 val PER: 0.0379
2025-12-08 08:20:11,865: t15.2023.11.26 val PER: 0.0587
2025-12-08 08:20:11,866: t15.2023.12.03 val PER: 0.0515
2025-12-08 08:20:11,866: t15.2023.12.08 val PER: 0.0453
2025-12-08 08:20:11,866: t15.2023.12.10 val PER: 0.0394
2025-12-08 08:20:11,866: t15.2023.12.17 val PER: 0.0946
2025-12-08 08:20:11,866: t15.2023.12.29 val PER: 0.0817
2025-12-08 08:20:11,866: t15.2024.02.25 val PER: 0.0801
2025-12-08 08:20:11,866: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:20:11,866: t15.2024.03.08 val PER: 0.1721
2025-12-08 08:20:11,866: t15.2024.03.15 val PER: 0.1745
2025-12-08 08:20:11,866: t15.2024.03.17 val PER: 0.0872
2025-12-08 08:20:11,866: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:20:11,866: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:20:11,866: t15.2024.05.10 val PER: 0.1159
2025-12-08 08:20:11,866: t15.2024.06.14 val PER: 0.1372
2025-12-08 08:20:11,866: t15.2024.07.19 val PER: 0.1707
2025-12-08 08:20:11,866: t15.2024.07.21 val PER: 0.0697
2025-12-08 08:20:11,866: t15.2024.07.28 val PER: 0.0904
2025-12-08 08:20:11,866: t15.2025.01.10 val PER: 0.2300
2025-12-08 08:20:11,866: t15.2025.01.12 val PER: 0.1039
2025-12-08 08:20:11,866: t15.2025.03.14 val PER: 0.3210
2025-12-08 08:20:11,867: t15.2025.03.16 val PER: 0.1531
2025-12-08 08:20:11,867: t15.2025.03.30 val PER: 0.2391
2025-12-08 08:20:11,867: t15.2025.04.13 val PER: 0.1926
2025-12-08 08:20:39,594: Train batch 40200: loss: 0.15 grad norm: 5.44 time: 0.099
2025-12-08 08:21:06,948: Train batch 40400: loss: 0.07 grad norm: 5.57 time: 0.103
2025-12-08 08:21:34,031: Train batch 40600: loss: 0.18 grad norm: 5.98 time: 0.173
2025-12-08 08:22:01,115: Train batch 40800: loss: 0.18 grad norm: 8.35 time: 0.129
2025-12-08 08:22:29,018: Train batch 41000: loss: 0.09 grad norm: 10.21 time: 0.118
2025-12-08 08:22:56,618: Train batch 41200: loss: 0.12 grad norm: 15.19 time: 0.167
2025-12-08 08:23:24,922: Train batch 41400: loss: 0.11 grad norm: 41.18 time: 0.128
2025-12-08 08:23:51,387: Train batch 41600: loss: 0.21 grad norm: 7.25 time: 0.122
2025-12-08 08:24:19,184: Train batch 41800: loss: 0.21 grad norm: 6.58 time: 0.093
2025-12-08 08:24:46,188: Train batch 42000: loss: 0.09 grad norm: 5.56 time: 0.118
2025-12-08 08:24:46,188: Running test after training batch: 42000
2025-12-08 08:24:56,630: Val batch 42000: PER (avg): 0.1093 CTC Loss (avg): 23.1648 time: 10.442
2025-12-08 08:24:56,630: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:24:56,630: t15.2023.08.13 val PER: 0.0696
2025-12-08 08:24:56,631: t15.2023.08.18 val PER: 0.0637
2025-12-08 08:24:56,631: t15.2023.08.20 val PER: 0.0516
2025-12-08 08:24:56,631: t15.2023.08.25 val PER: 0.0843
2025-12-08 08:24:56,631: t15.2023.08.27 val PER: 0.1238
2025-12-08 08:24:56,631: t15.2023.09.01 val PER: 0.0422
2025-12-08 08:24:56,631: t15.2023.09.03 val PER: 0.1069
2025-12-08 08:24:56,631: t15.2023.09.24 val PER: 0.0922
2025-12-08 08:24:56,631: t15.2023.09.29 val PER: 0.1053
2025-12-08 08:24:56,631: t15.2023.10.01 val PER: 0.1427
2025-12-08 08:24:56,631: t15.2023.10.06 val PER: 0.0581
2025-12-08 08:24:56,631: t15.2023.10.08 val PER: 0.1732
2025-12-08 08:24:56,631: t15.2023.10.13 val PER: 0.1451
2025-12-08 08:24:56,631: t15.2023.10.15 val PER: 0.1094
2025-12-08 08:24:56,631: t15.2023.10.20 val PER: 0.1611
2025-12-08 08:24:56,631: t15.2023.10.22 val PER: 0.1002
2025-12-08 08:24:56,631: t15.2023.11.03 val PER: 0.1567
2025-12-08 08:24:56,632: t15.2023.11.04 val PER: 0.0171
2025-12-08 08:24:56,632: t15.2023.11.17 val PER: 0.0218
2025-12-08 08:24:56,632: t15.2023.11.19 val PER: 0.0379
2025-12-08 08:24:56,632: t15.2023.11.26 val PER: 0.0601
2025-12-08 08:24:56,632: t15.2023.12.03 val PER: 0.0515
2025-12-08 08:24:56,632: t15.2023.12.08 val PER: 0.0499
2025-12-08 08:24:56,632: t15.2023.12.10 val PER: 0.0407
2025-12-08 08:24:56,632: t15.2023.12.17 val PER: 0.0915
2025-12-08 08:24:56,632: t15.2023.12.29 val PER: 0.0679
2025-12-08 08:24:56,632: t15.2024.02.25 val PER: 0.0787
2025-12-08 08:24:56,632: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:24:56,632: t15.2024.03.08 val PER: 0.1593
2025-12-08 08:24:56,632: t15.2024.03.15 val PER: 0.1695
2025-12-08 08:24:56,632: t15.2024.03.17 val PER: 0.0962
2025-12-08 08:24:56,632: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:24:56,632: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:24:56,632: t15.2024.05.10 val PER: 0.1308
2025-12-08 08:24:56,633: t15.2024.06.14 val PER: 0.1293
2025-12-08 08:24:56,633: t15.2024.07.19 val PER: 0.1635
2025-12-08 08:24:56,633: t15.2024.07.21 val PER: 0.0683
2025-12-08 08:24:56,633: t15.2024.07.28 val PER: 0.0831
2025-12-08 08:24:56,633: t15.2025.01.10 val PER: 0.2603
2025-12-08 08:24:56,633: t15.2025.01.12 val PER: 0.1109
2025-12-08 08:24:56,633: t15.2025.03.14 val PER: 0.3121
2025-12-08 08:24:56,633: t15.2025.03.16 val PER: 0.1453
2025-12-08 08:24:56,633: t15.2025.03.30 val PER: 0.2414
2025-12-08 08:24:56,633: t15.2025.04.13 val PER: 0.2054
2025-12-08 08:24:56,633: New best test PER 0.1111 --> 0.1093
2025-12-08 08:24:56,633: Checkpointing model
2025-12-08 08:24:57,729: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:25:25,641: Train batch 42200: loss: 0.08 grad norm: 69.70 time: 0.159
2025-12-08 08:25:52,342: Train batch 42400: loss: 0.14 grad norm: 4.73 time: 0.106
2025-12-08 08:26:19,885: Train batch 42600: loss: 0.41 grad norm: 6.83 time: 0.095
2025-12-08 08:26:47,795: Train batch 42800: loss: 0.15 grad norm: 9.76 time: 0.125
2025-12-08 08:27:15,003: Train batch 43000: loss: 0.08 grad norm: 3.95 time: 0.131
2025-12-08 08:27:42,474: Train batch 43200: loss: 0.11 grad norm: 5.22 time: 0.103
2025-12-08 08:28:10,012: Train batch 43400: loss: 0.06 grad norm: 4.24 time: 0.195
2025-12-08 08:28:37,532: Train batch 43600: loss: 0.06 grad norm: 4.26 time: 0.104
2025-12-08 08:29:05,347: Train batch 43800: loss: 0.18 grad norm: 8.16 time: 0.122
2025-12-08 08:29:32,719: Train batch 44000: loss: 0.11 grad norm: 4.34 time: 0.111
2025-12-08 08:29:32,721: Running test after training batch: 44000
2025-12-08 08:29:43,236: Val batch 44000: PER (avg): 0.1095 CTC Loss (avg): 23.4641 time: 10.515
2025-12-08 08:29:43,236: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:29:43,237: t15.2023.08.13 val PER: 0.0738
2025-12-08 08:29:43,237: t15.2023.08.18 val PER: 0.0662
2025-12-08 08:29:43,237: t15.2023.08.20 val PER: 0.0604
2025-12-08 08:29:43,237: t15.2023.08.25 val PER: 0.0783
2025-12-08 08:29:43,237: t15.2023.08.27 val PER: 0.1495
2025-12-08 08:29:43,237: t15.2023.09.01 val PER: 0.0381
2025-12-08 08:29:43,237: t15.2023.09.03 val PER: 0.1223
2025-12-08 08:29:43,237: t15.2023.09.24 val PER: 0.0910
2025-12-08 08:29:43,237: t15.2023.09.29 val PER: 0.1072
2025-12-08 08:29:43,237: t15.2023.10.01 val PER: 0.1361
2025-12-08 08:29:43,237: t15.2023.10.06 val PER: 0.0700
2025-12-08 08:29:43,237: t15.2023.10.08 val PER: 0.1786
2025-12-08 08:29:43,237: t15.2023.10.13 val PER: 0.1389
2025-12-08 08:29:43,237: t15.2023.10.15 val PER: 0.1074
2025-12-08 08:29:43,237: t15.2023.10.20 val PER: 0.1711
2025-12-08 08:29:43,237: t15.2023.10.22 val PER: 0.1024
2025-12-08 08:29:43,237: t15.2023.11.03 val PER: 0.1472
2025-12-08 08:29:43,237: t15.2023.11.04 val PER: 0.0137
2025-12-08 08:29:43,237: t15.2023.11.17 val PER: 0.0202
2025-12-08 08:29:43,238: t15.2023.11.19 val PER: 0.0339
2025-12-08 08:29:43,238: t15.2023.11.26 val PER: 0.0594
2025-12-08 08:29:43,238: t15.2023.12.03 val PER: 0.0557
2025-12-08 08:29:43,238: t15.2023.12.08 val PER: 0.0446
2025-12-08 08:29:43,238: t15.2023.12.10 val PER: 0.0434
2025-12-08 08:29:43,238: t15.2023.12.17 val PER: 0.0852
2025-12-08 08:29:43,238: t15.2023.12.29 val PER: 0.0721
2025-12-08 08:29:43,238: t15.2024.02.25 val PER: 0.0787
2025-12-08 08:29:43,238: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:29:43,238: t15.2024.03.08 val PER: 0.1593
2025-12-08 08:29:43,238: t15.2024.03.15 val PER: 0.1695
2025-12-08 08:29:43,238: t15.2024.03.17 val PER: 0.1004
2025-12-08 08:29:43,238: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:29:43,238: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:29:43,238: t15.2024.05.10 val PER: 0.1322
2025-12-08 08:29:43,238: t15.2024.06.14 val PER: 0.1435
2025-12-08 08:29:43,238: t15.2024.07.19 val PER: 0.1681
2025-12-08 08:29:43,238: t15.2024.07.21 val PER: 0.0662
2025-12-08 08:29:43,239: t15.2024.07.28 val PER: 0.0772
2025-12-08 08:29:43,239: t15.2025.01.10 val PER: 0.2645
2025-12-08 08:29:43,239: t15.2025.01.12 val PER: 0.1001
2025-12-08 08:29:43,239: t15.2025.03.14 val PER: 0.3018
2025-12-08 08:29:43,239: t15.2025.03.16 val PER: 0.1545
2025-12-08 08:29:43,239: t15.2025.03.30 val PER: 0.2356
2025-12-08 08:29:43,239: t15.2025.04.13 val PER: 0.1983
2025-12-08 08:30:11,563: Train batch 44200: loss: 0.07 grad norm: 3.64 time: 0.133
2025-12-08 08:30:39,856: Train batch 44400: loss: 0.21 grad norm: 4.80 time: 0.109
2025-12-08 08:31:07,286: Train batch 44600: loss: 0.12 grad norm: 5.77 time: 0.143
2025-12-08 08:31:34,773: Train batch 44800: loss: 0.07 grad norm: 3.10 time: 0.107
2025-12-08 08:32:02,102: Train batch 45000: loss: 0.15 grad norm: 9.20 time: 0.096
2025-12-08 08:32:29,198: Train batch 45200: loss: 0.25 grad norm: 23.87 time: 0.097
2025-12-08 08:32:57,117: Train batch 45400: loss: 0.09 grad norm: 4.06 time: 0.104
2025-12-08 08:33:24,673: Train batch 45600: loss: 0.16 grad norm: 6.33 time: 0.105
2025-12-08 08:33:52,583: Train batch 45800: loss: 0.13 grad norm: 5.19 time: 0.118
2025-12-08 08:34:20,477: Train batch 46000: loss: 0.14 grad norm: 7.57 time: 0.097
2025-12-08 08:34:20,478: Running test after training batch: 46000
2025-12-08 08:34:31,012: Val batch 46000: PER (avg): 0.1088 CTC Loss (avg): 23.1606 time: 10.534
2025-12-08 08:34:31,012: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:34:31,012: t15.2023.08.13 val PER: 0.0686
2025-12-08 08:34:31,012: t15.2023.08.18 val PER: 0.0637
2025-12-08 08:34:31,012: t15.2023.08.20 val PER: 0.0548
2025-12-08 08:34:31,012: t15.2023.08.25 val PER: 0.0633
2025-12-08 08:34:31,012: t15.2023.08.27 val PER: 0.1350
2025-12-08 08:34:31,012: t15.2023.09.01 val PER: 0.0398
2025-12-08 08:34:31,012: t15.2023.09.03 val PER: 0.1140
2025-12-08 08:34:31,013: t15.2023.09.24 val PER: 0.0825
2025-12-08 08:34:31,013: t15.2023.09.29 val PER: 0.1040
2025-12-08 08:34:31,013: t15.2023.10.01 val PER: 0.1387
2025-12-08 08:34:31,013: t15.2023.10.06 val PER: 0.0635
2025-12-08 08:34:31,013: t15.2023.10.08 val PER: 0.1664
2025-12-08 08:34:31,013: t15.2023.10.13 val PER: 0.1536
2025-12-08 08:34:31,013: t15.2023.10.15 val PER: 0.1121
2025-12-08 08:34:31,013: t15.2023.10.20 val PER: 0.1644
2025-12-08 08:34:31,013: t15.2023.10.22 val PER: 0.0947
2025-12-08 08:34:31,013: t15.2023.11.03 val PER: 0.1472
2025-12-08 08:34:31,013: t15.2023.11.04 val PER: 0.0102
2025-12-08 08:34:31,013: t15.2023.11.17 val PER: 0.0140
2025-12-08 08:34:31,013: t15.2023.11.19 val PER: 0.0259
2025-12-08 08:34:31,013: t15.2023.11.26 val PER: 0.0572
2025-12-08 08:34:31,013: t15.2023.12.03 val PER: 0.0651
2025-12-08 08:34:31,013: t15.2023.12.08 val PER: 0.0459
2025-12-08 08:34:31,013: t15.2023.12.10 val PER: 0.0329
2025-12-08 08:34:31,014: t15.2023.12.17 val PER: 0.0852
2025-12-08 08:34:31,014: t15.2023.12.29 val PER: 0.0741
2025-12-08 08:34:31,014: t15.2024.02.25 val PER: 0.0674
2025-12-08 08:34:31,014: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:34:31,014: t15.2024.03.08 val PER: 0.1849
2025-12-08 08:34:31,014: t15.2024.03.15 val PER: 0.1726
2025-12-08 08:34:31,014: t15.2024.03.17 val PER: 0.1011
2025-12-08 08:34:31,014: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:34:31,014: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:34:31,014: t15.2024.05.10 val PER: 0.1352
2025-12-08 08:34:31,014: t15.2024.06.14 val PER: 0.1230
2025-12-08 08:34:31,014: t15.2024.07.19 val PER: 0.1714
2025-12-08 08:34:31,014: t15.2024.07.21 val PER: 0.0614
2025-12-08 08:34:31,014: t15.2024.07.28 val PER: 0.0875
2025-12-08 08:34:31,014: t15.2025.01.10 val PER: 0.2452
2025-12-08 08:34:31,014: t15.2025.01.12 val PER: 0.1070
2025-12-08 08:34:31,015: t15.2025.03.14 val PER: 0.3077
2025-12-08 08:34:31,015: t15.2025.03.16 val PER: 0.1571
2025-12-08 08:34:31,015: t15.2025.03.30 val PER: 0.2471
2025-12-08 08:34:31,015: t15.2025.04.13 val PER: 0.1926
2025-12-08 08:34:31,015: New best test PER 0.1093 --> 0.1088
2025-12-08 08:34:31,015: Checkpointing model
2025-12-08 08:34:31,572: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:34:59,012: Train batch 46200: loss: 0.10 grad norm: 4.48 time: 0.112
2025-12-08 08:35:26,938: Train batch 46400: loss: 0.14 grad norm: 7.22 time: 0.142
2025-12-08 08:35:54,002: Train batch 46600: loss: 0.07 grad norm: 3.10 time: 0.077
2025-12-08 08:36:22,425: Train batch 46800: loss: 0.43 grad norm: 11.62 time: 0.118
2025-12-08 08:36:50,074: Train batch 47000: loss: 0.05 grad norm: 2.31 time: 0.119
2025-12-08 08:37:18,218: Train batch 47200: loss: 0.21 grad norm: 7.18 time: 0.164
2025-12-08 08:37:46,825: Train batch 47400: loss: 0.37 grad norm: 7.76 time: 0.168
2025-12-08 08:38:15,353: Train batch 47600: loss: 0.08 grad norm: 5.69 time: 0.103
2025-12-08 08:38:42,966: Train batch 47800: loss: 0.21 grad norm: 6.72 time: 0.080
2025-12-08 08:39:10,865: Train batch 48000: loss: 0.07 grad norm: 3.75 time: 0.125
2025-12-08 08:39:10,866: Running test after training batch: 48000
2025-12-08 08:39:21,506: Val batch 48000: PER (avg): 0.1083 CTC Loss (avg): 22.9803 time: 10.640
2025-12-08 08:39:21,507: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:39:21,507: t15.2023.08.13 val PER: 0.0717
2025-12-08 08:39:21,507: t15.2023.08.18 val PER: 0.0654
2025-12-08 08:39:21,507: t15.2023.08.20 val PER: 0.0572
2025-12-08 08:39:21,507: t15.2023.08.25 val PER: 0.0738
2025-12-08 08:39:21,507: t15.2023.08.27 val PER: 0.1431
2025-12-08 08:39:21,507: t15.2023.09.01 val PER: 0.0455
2025-12-08 08:39:21,507: t15.2023.09.03 val PER: 0.1152
2025-12-08 08:39:21,507: t15.2023.09.24 val PER: 0.0910
2025-12-08 08:39:21,507: t15.2023.09.29 val PER: 0.0944
2025-12-08 08:39:21,508: t15.2023.10.01 val PER: 0.1367
2025-12-08 08:39:21,508: t15.2023.10.06 val PER: 0.0689
2025-12-08 08:39:21,508: t15.2023.10.08 val PER: 0.1719
2025-12-08 08:39:21,508: t15.2023.10.13 val PER: 0.1466
2025-12-08 08:39:21,508: t15.2023.10.15 val PER: 0.1061
2025-12-08 08:39:21,508: t15.2023.10.20 val PER: 0.1678
2025-12-08 08:39:21,508: t15.2023.10.22 val PER: 0.1002
2025-12-08 08:39:21,508: t15.2023.11.03 val PER: 0.1520
2025-12-08 08:39:21,508: t15.2023.11.04 val PER: 0.0171
2025-12-08 08:39:21,508: t15.2023.11.17 val PER: 0.0187
2025-12-08 08:39:21,508: t15.2023.11.19 val PER: 0.0240
2025-12-08 08:39:21,508: t15.2023.11.26 val PER: 0.0529
2025-12-08 08:39:21,508: t15.2023.12.03 val PER: 0.0536
2025-12-08 08:39:21,508: t15.2023.12.08 val PER: 0.0453
2025-12-08 08:39:21,508: t15.2023.12.10 val PER: 0.0368
2025-12-08 08:39:21,508: t15.2023.12.17 val PER: 0.0863
2025-12-08 08:39:21,508: t15.2023.12.29 val PER: 0.0748
2025-12-08 08:39:21,508: t15.2024.02.25 val PER: 0.0758
2025-12-08 08:39:21,508: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:39:21,508: t15.2024.03.08 val PER: 0.1636
2025-12-08 08:39:21,509: t15.2024.03.15 val PER: 0.1695
2025-12-08 08:39:21,509: t15.2024.03.17 val PER: 0.0858
2025-12-08 08:39:21,509: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:39:21,509: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:39:21,509: t15.2024.05.10 val PER: 0.1248
2025-12-08 08:39:21,509: t15.2024.06.14 val PER: 0.1278
2025-12-08 08:39:21,509: t15.2024.07.19 val PER: 0.1536
2025-12-08 08:39:21,509: t15.2024.07.21 val PER: 0.0648
2025-12-08 08:39:21,509: t15.2024.07.28 val PER: 0.0956
2025-12-08 08:39:21,509: t15.2025.01.10 val PER: 0.2548
2025-12-08 08:39:21,509: t15.2025.01.12 val PER: 0.1124
2025-12-08 08:39:21,509: t15.2025.03.14 val PER: 0.3136
2025-12-08 08:39:21,509: t15.2025.03.16 val PER: 0.1453
2025-12-08 08:39:21,509: t15.2025.03.30 val PER: 0.2402
2025-12-08 08:39:21,509: t15.2025.04.13 val PER: 0.2097
2025-12-08 08:39:21,509: New best test PER 0.1088 --> 0.1083
2025-12-08 08:39:21,509: Checkpointing model
2025-12-08 08:39:22,607: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:39:50,430: Train batch 48200: loss: 0.24 grad norm: 7.83 time: 0.105
2025-12-08 08:40:18,446: Train batch 48400: loss: 0.03 grad norm: 2.06 time: 0.087
2025-12-08 08:40:46,252: Train batch 48600: loss: 0.14 grad norm: 5.92 time: 0.159
2025-12-08 08:41:14,002: Train batch 48800: loss: 0.11 grad norm: 4.17 time: 0.132
2025-12-08 08:41:41,199: Train batch 49000: loss: 0.04 grad norm: 1.88 time: 0.123
2025-12-08 08:42:08,596: Train batch 49200: loss: 0.05 grad norm: 3.10 time: 0.170
2025-12-08 08:42:36,720: Train batch 49400: loss: 0.10 grad norm: 5.32 time: 0.125
2025-12-08 08:43:04,506: Train batch 49600: loss: 0.07 grad norm: 3.35 time: 0.101
2025-12-08 08:43:33,015: Train batch 49800: loss: 0.11 grad norm: 11.47 time: 0.108
2025-12-08 08:44:00,793: Train batch 50000: loss: 0.06 grad norm: 3.08 time: 0.095
2025-12-08 08:44:00,793: Running test after training batch: 50000
2025-12-08 08:44:10,929: Val batch 50000: PER (avg): 0.1077 CTC Loss (avg): 23.0658 time: 10.135
2025-12-08 08:44:10,929: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:44:10,929: t15.2023.08.13 val PER: 0.0686
2025-12-08 08:44:10,929: t15.2023.08.18 val PER: 0.0553
2025-12-08 08:44:10,929: t15.2023.08.20 val PER: 0.0500
2025-12-08 08:44:10,929: t15.2023.08.25 val PER: 0.0708
2025-12-08 08:44:10,929: t15.2023.08.27 val PER: 0.1415
2025-12-08 08:44:10,930: t15.2023.09.01 val PER: 0.0430
2025-12-08 08:44:10,930: t15.2023.09.03 val PER: 0.1211
2025-12-08 08:44:10,930: t15.2023.09.24 val PER: 0.0825
2025-12-08 08:44:10,930: t15.2023.09.29 val PER: 0.1053
2025-12-08 08:44:10,930: t15.2023.10.01 val PER: 0.1341
2025-12-08 08:44:10,930: t15.2023.10.06 val PER: 0.0635
2025-12-08 08:44:10,930: t15.2023.10.08 val PER: 0.1813
2025-12-08 08:44:10,930: t15.2023.10.13 val PER: 0.1521
2025-12-08 08:44:10,930: t15.2023.10.15 val PER: 0.1121
2025-12-08 08:44:10,930: t15.2023.10.20 val PER: 0.2013
2025-12-08 08:44:10,930: t15.2023.10.22 val PER: 0.0924
2025-12-08 08:44:10,930: t15.2023.11.03 val PER: 0.1540
2025-12-08 08:44:10,930: t15.2023.11.04 val PER: 0.0137
2025-12-08 08:44:10,930: t15.2023.11.17 val PER: 0.0187
2025-12-08 08:44:10,930: t15.2023.11.19 val PER: 0.0319
2025-12-08 08:44:10,930: t15.2023.11.26 val PER: 0.0609
2025-12-08 08:44:10,930: t15.2023.12.03 val PER: 0.0504
2025-12-08 08:44:10,931: t15.2023.12.08 val PER: 0.0426
2025-12-08 08:44:10,931: t15.2023.12.10 val PER: 0.0381
2025-12-08 08:44:10,931: t15.2023.12.17 val PER: 0.0873
2025-12-08 08:44:10,931: t15.2023.12.29 val PER: 0.0830
2025-12-08 08:44:10,931: t15.2024.02.25 val PER: 0.0674
2025-12-08 08:44:10,931: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:44:10,931: t15.2024.03.08 val PER: 0.1778
2025-12-08 08:44:10,931: t15.2024.03.15 val PER: 0.1601
2025-12-08 08:44:10,931: t15.2024.03.17 val PER: 0.0927
2025-12-08 08:44:10,931: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:44:10,931: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:44:10,931: t15.2024.05.10 val PER: 0.1322
2025-12-08 08:44:10,931: t15.2024.06.14 val PER: 0.1388
2025-12-08 08:44:10,931: t15.2024.07.19 val PER: 0.1648
2025-12-08 08:44:10,931: t15.2024.07.21 val PER: 0.0545
2025-12-08 08:44:10,931: t15.2024.07.28 val PER: 0.0882
2025-12-08 08:44:10,931: t15.2025.01.10 val PER: 0.2397
2025-12-08 08:44:10,932: t15.2025.01.12 val PER: 0.1024
2025-12-08 08:44:10,932: t15.2025.03.14 val PER: 0.3269
2025-12-08 08:44:10,932: t15.2025.03.16 val PER: 0.1322
2025-12-08 08:44:10,932: t15.2025.03.30 val PER: 0.2253
2025-12-08 08:44:10,932: t15.2025.04.13 val PER: 0.1869
2025-12-08 08:44:10,932: New best test PER 0.1083 --> 0.1077
2025-12-08 08:44:10,932: Checkpointing model
2025-12-08 08:44:11,562: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:44:39,097: Train batch 50200: loss: 0.09 grad norm: 7.31 time: 0.153
2025-12-08 08:45:06,560: Train batch 50400: loss: 0.07 grad norm: 18.21 time: 0.160
2025-12-08 08:45:34,238: Train batch 50600: loss: 0.09 grad norm: 17.67 time: 0.098
2025-12-08 08:46:01,940: Train batch 50800: loss: 0.12 grad norm: 5.53 time: 0.115
2025-12-08 08:46:29,086: Train batch 51000: loss: 0.10 grad norm: 5.01 time: 0.138
2025-12-08 08:46:56,993: Train batch 51200: loss: 0.02 grad norm: 1.71 time: 0.101
2025-12-08 08:47:25,229: Train batch 51400: loss: 0.09 grad norm: 7.20 time: 0.105
2025-12-08 08:47:53,808: Train batch 51600: loss: 0.13 grad norm: 5.54 time: 0.137
2025-12-08 08:48:21,053: Train batch 51800: loss: 0.05 grad norm: 6.29 time: 0.152
2025-12-08 08:48:47,486: Train batch 52000: loss: 0.07 grad norm: 5.49 time: 0.082
2025-12-08 08:48:47,486: Running test after training batch: 52000
2025-12-08 08:48:57,771: Val batch 52000: PER (avg): 0.1072 CTC Loss (avg): 23.2072 time: 10.284
2025-12-08 08:48:57,771: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:48:57,771: t15.2023.08.13 val PER: 0.0748
2025-12-08 08:48:57,771: t15.2023.08.18 val PER: 0.0520
2025-12-08 08:48:57,771: t15.2023.08.20 val PER: 0.0508
2025-12-08 08:48:57,771: t15.2023.08.25 val PER: 0.0723
2025-12-08 08:48:57,771: t15.2023.08.27 val PER: 0.1672
2025-12-08 08:48:57,771: t15.2023.09.01 val PER: 0.0438
2025-12-08 08:48:57,771: t15.2023.09.03 val PER: 0.1116
2025-12-08 08:48:57,772: t15.2023.09.24 val PER: 0.0862
2025-12-08 08:48:57,772: t15.2023.09.29 val PER: 0.1085
2025-12-08 08:48:57,772: t15.2023.10.01 val PER: 0.1367
2025-12-08 08:48:57,772: t15.2023.10.06 val PER: 0.0646
2025-12-08 08:48:57,772: t15.2023.10.08 val PER: 0.1732
2025-12-08 08:48:57,772: t15.2023.10.13 val PER: 0.1513
2025-12-08 08:48:57,772: t15.2023.10.15 val PER: 0.1101
2025-12-08 08:48:57,772: t15.2023.10.20 val PER: 0.1443
2025-12-08 08:48:57,772: t15.2023.10.22 val PER: 0.0935
2025-12-08 08:48:57,772: t15.2023.11.03 val PER: 0.1465
2025-12-08 08:48:57,772: t15.2023.11.04 val PER: 0.0171
2025-12-08 08:48:57,772: t15.2023.11.17 val PER: 0.0187
2025-12-08 08:48:57,772: t15.2023.11.19 val PER: 0.0319
2025-12-08 08:48:57,772: t15.2023.11.26 val PER: 0.0674
2025-12-08 08:48:57,772: t15.2023.12.03 val PER: 0.0662
2025-12-08 08:48:57,772: t15.2023.12.08 val PER: 0.0459
2025-12-08 08:48:57,772: t15.2023.12.10 val PER: 0.0355
2025-12-08 08:48:57,772: t15.2023.12.17 val PER: 0.0842
2025-12-08 08:48:57,772: t15.2023.12.29 val PER: 0.0741
2025-12-08 08:48:57,773: t15.2024.02.25 val PER: 0.0758
2025-12-08 08:48:57,773: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:48:57,773: t15.2024.03.08 val PER: 0.1607
2025-12-08 08:48:57,773: t15.2024.03.15 val PER: 0.1814
2025-12-08 08:48:57,773: t15.2024.03.17 val PER: 0.0809
2025-12-08 08:48:57,773: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:48:57,773: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:48:57,773: t15.2024.05.10 val PER: 0.1159
2025-12-08 08:48:57,773: t15.2024.06.14 val PER: 0.1199
2025-12-08 08:48:57,773: t15.2024.07.19 val PER: 0.1595
2025-12-08 08:48:57,773: t15.2024.07.21 val PER: 0.0538
2025-12-08 08:48:57,773: t15.2024.07.28 val PER: 0.0904
2025-12-08 08:48:57,773: t15.2025.01.10 val PER: 0.2397
2025-12-08 08:48:57,773: t15.2025.01.12 val PER: 0.0985
2025-12-08 08:48:57,773: t15.2025.03.14 val PER: 0.3151
2025-12-08 08:48:57,773: t15.2025.03.16 val PER: 0.1348
2025-12-08 08:48:57,773: t15.2025.03.30 val PER: 0.2218
2025-12-08 08:48:57,773: t15.2025.04.13 val PER: 0.2040
2025-12-08 08:48:57,773: New best test PER 0.1077 --> 0.1072
2025-12-08 08:48:57,773: Checkpointing model
2025-12-08 08:48:58,937: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:49:26,099: Train batch 52200: loss: 0.04 grad norm: 2.48 time: 0.132
2025-12-08 08:49:53,751: Train batch 52400: loss: 0.06 grad norm: 4.53 time: 0.155
2025-12-08 08:50:21,211: Train batch 52600: loss: 0.13 grad norm: 10.00 time: 0.132
2025-12-08 08:50:49,439: Train batch 52800: loss: 0.05 grad norm: 7.25 time: 0.117
2025-12-08 08:51:17,680: Train batch 53000: loss: 0.04 grad norm: 3.27 time: 0.091
2025-12-08 08:51:45,882: Train batch 53200: loss: 0.36 grad norm: 18.69 time: 0.147
2025-12-08 08:52:12,377: Train batch 53400: loss: 0.06 grad norm: 3.48 time: 0.133
2025-12-08 08:52:40,731: Train batch 53600: loss: 0.04 grad norm: 4.61 time: 0.139
2025-12-08 08:53:08,017: Train batch 53800: loss: 0.13 grad norm: 6.15 time: 0.130
2025-12-08 08:53:35,843: Train batch 54000: loss: 0.06 grad norm: 5.52 time: 0.105
2025-12-08 08:53:35,844: Running test after training batch: 54000
2025-12-08 08:53:46,513: Val batch 54000: PER (avg): 0.1055 CTC Loss (avg): 22.5663 time: 10.669
2025-12-08 08:53:46,514: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:53:46,514: t15.2023.08.13 val PER: 0.0676
2025-12-08 08:53:46,514: t15.2023.08.18 val PER: 0.0712
2025-12-08 08:53:46,514: t15.2023.08.20 val PER: 0.0556
2025-12-08 08:53:46,514: t15.2023.08.25 val PER: 0.0708
2025-12-08 08:53:46,514: t15.2023.08.27 val PER: 0.1479
2025-12-08 08:53:46,514: t15.2023.09.01 val PER: 0.0455
2025-12-08 08:53:46,514: t15.2023.09.03 val PER: 0.1116
2025-12-08 08:53:46,514: t15.2023.09.24 val PER: 0.0850
2025-12-08 08:53:46,514: t15.2023.09.29 val PER: 0.1021
2025-12-08 08:53:46,514: t15.2023.10.01 val PER: 0.1453
2025-12-08 08:53:46,515: t15.2023.10.06 val PER: 0.0635
2025-12-08 08:53:46,515: t15.2023.10.08 val PER: 0.1678
2025-12-08 08:53:46,515: t15.2023.10.13 val PER: 0.1466
2025-12-08 08:53:46,515: t15.2023.10.15 val PER: 0.1134
2025-12-08 08:53:46,515: t15.2023.10.20 val PER: 0.1678
2025-12-08 08:53:46,515: t15.2023.10.22 val PER: 0.0947
2025-12-08 08:53:46,515: t15.2023.11.03 val PER: 0.1431
2025-12-08 08:53:46,515: t15.2023.11.04 val PER: 0.0137
2025-12-08 08:53:46,515: t15.2023.11.17 val PER: 0.0156
2025-12-08 08:53:46,515: t15.2023.11.19 val PER: 0.0259
2025-12-08 08:53:46,515: t15.2023.11.26 val PER: 0.0580
2025-12-08 08:53:46,515: t15.2023.12.03 val PER: 0.0494
2025-12-08 08:53:46,515: t15.2023.12.08 val PER: 0.0559
2025-12-08 08:53:46,515: t15.2023.12.10 val PER: 0.0434
2025-12-08 08:53:46,515: t15.2023.12.17 val PER: 0.0811
2025-12-08 08:53:46,515: t15.2023.12.29 val PER: 0.0782
2025-12-08 08:53:46,515: t15.2024.02.25 val PER: 0.0702
2025-12-08 08:53:46,516: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:53:46,516: t15.2024.03.08 val PER: 0.1522
2025-12-08 08:53:46,516: t15.2024.03.15 val PER: 0.1588
2025-12-08 08:53:46,516: t15.2024.03.17 val PER: 0.0844
2025-12-08 08:53:46,516: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:53:46,516: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:53:46,516: t15.2024.05.10 val PER: 0.1278
2025-12-08 08:53:46,516: t15.2024.06.14 val PER: 0.1167
2025-12-08 08:53:46,516: t15.2024.07.19 val PER: 0.1589
2025-12-08 08:53:46,516: t15.2024.07.21 val PER: 0.0566
2025-12-08 08:53:46,516: t15.2024.07.28 val PER: 0.0824
2025-12-08 08:53:46,516: t15.2025.01.10 val PER: 0.2259
2025-12-08 08:53:46,516: t15.2025.01.12 val PER: 0.0947
2025-12-08 08:53:46,516: t15.2025.03.14 val PER: 0.3180
2025-12-08 08:53:46,516: t15.2025.03.16 val PER: 0.1322
2025-12-08 08:53:46,516: t15.2025.03.30 val PER: 0.2299
2025-12-08 08:53:46,516: t15.2025.04.13 val PER: 0.1755
2025-12-08 08:53:46,517: New best test PER 0.1072 --> 0.1055
2025-12-08 08:53:46,517: Checkpointing model
2025-12-08 08:53:47,804: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 08:54:15,380: Train batch 54200: loss: 0.03 grad norm: 2.24 time: 0.104
2025-12-08 08:54:43,530: Train batch 54400: loss: 0.07 grad norm: 4.62 time: 0.153
2025-12-08 08:55:11,042: Train batch 54600: loss: 0.03 grad norm: 2.29 time: 0.130
2025-12-08 08:55:38,451: Train batch 54800: loss: 0.05 grad norm: 2.76 time: 0.139
2025-12-08 08:56:06,291: Train batch 55000: loss: 0.18 grad norm: 8.64 time: 0.179
2025-12-08 08:56:33,242: Train batch 55200: loss: 0.01 grad norm: 2.45 time: 0.087
2025-12-08 08:57:01,460: Train batch 55400: loss: 0.10 grad norm: 6.57 time: 0.153
2025-12-08 08:57:29,033: Train batch 55600: loss: 0.04 grad norm: 2.20 time: 0.111
2025-12-08 08:57:57,050: Train batch 55800: loss: 0.02 grad norm: 1.79 time: 0.118
2025-12-08 08:58:23,902: Train batch 56000: loss: 0.03 grad norm: 1.55 time: 0.082
2025-12-08 08:58:23,903: Running test after training batch: 56000
2025-12-08 08:58:34,311: Val batch 56000: PER (avg): 0.1072 CTC Loss (avg): 23.1052 time: 10.408
2025-12-08 08:58:34,311: t15.2023.08.11 val PER: 1.0000
2025-12-08 08:58:34,311: t15.2023.08.13 val PER: 0.0738
2025-12-08 08:58:34,311: t15.2023.08.18 val PER: 0.0662
2025-12-08 08:58:34,311: t15.2023.08.20 val PER: 0.0508
2025-12-08 08:58:34,311: t15.2023.08.25 val PER: 0.0678
2025-12-08 08:58:34,311: t15.2023.08.27 val PER: 0.1463
2025-12-08 08:58:34,311: t15.2023.09.01 val PER: 0.0422
2025-12-08 08:58:34,312: t15.2023.09.03 val PER: 0.1211
2025-12-08 08:58:34,312: t15.2023.09.24 val PER: 0.0886
2025-12-08 08:58:34,312: t15.2023.09.29 val PER: 0.1008
2025-12-08 08:58:34,312: t15.2023.10.01 val PER: 0.1387
2025-12-08 08:58:34,312: t15.2023.10.06 val PER: 0.0635
2025-12-08 08:58:34,312: t15.2023.10.08 val PER: 0.1881
2025-12-08 08:58:34,312: t15.2023.10.13 val PER: 0.1497
2025-12-08 08:58:34,312: t15.2023.10.15 val PER: 0.1127
2025-12-08 08:58:34,312: t15.2023.10.20 val PER: 0.1678
2025-12-08 08:58:34,312: t15.2023.10.22 val PER: 0.0969
2025-12-08 08:58:34,312: t15.2023.11.03 val PER: 0.1520
2025-12-08 08:58:34,312: t15.2023.11.04 val PER: 0.0205
2025-12-08 08:58:34,312: t15.2023.11.17 val PER: 0.0156
2025-12-08 08:58:34,312: t15.2023.11.19 val PER: 0.0319
2025-12-08 08:58:34,312: t15.2023.11.26 val PER: 0.0471
2025-12-08 08:58:34,312: t15.2023.12.03 val PER: 0.0483
2025-12-08 08:58:34,312: t15.2023.12.08 val PER: 0.0379
2025-12-08 08:58:34,313: t15.2023.12.10 val PER: 0.0368
2025-12-08 08:58:34,313: t15.2023.12.17 val PER: 0.0811
2025-12-08 08:58:34,313: t15.2023.12.29 val PER: 0.0741
2025-12-08 08:58:34,313: t15.2024.02.25 val PER: 0.0758
2025-12-08 08:58:34,313: t15.2024.03.03 val PER: 1.0000
2025-12-08 08:58:34,313: t15.2024.03.08 val PER: 0.1593
2025-12-08 08:58:34,313: t15.2024.03.15 val PER: 0.1732
2025-12-08 08:58:34,313: t15.2024.03.17 val PER: 0.0816
2025-12-08 08:58:34,313: t15.2024.04.25 val PER: 1.0000
2025-12-08 08:58:34,313: t15.2024.04.28 val PER: 1.0000
2025-12-08 08:58:34,313: t15.2024.05.10 val PER: 0.1218
2025-12-08 08:58:34,313: t15.2024.06.14 val PER: 0.1246
2025-12-08 08:58:34,313: t15.2024.07.19 val PER: 0.1661
2025-12-08 08:58:34,313: t15.2024.07.21 val PER: 0.0586
2025-12-08 08:58:34,313: t15.2024.07.28 val PER: 0.0831
2025-12-08 08:58:34,313: t15.2025.01.10 val PER: 0.2493
2025-12-08 08:58:34,314: t15.2025.01.12 val PER: 0.1062
2025-12-08 08:58:34,314: t15.2025.03.14 val PER: 0.3254
2025-12-08 08:58:34,314: t15.2025.03.16 val PER: 0.1401
2025-12-08 08:58:34,314: t15.2025.03.30 val PER: 0.2368
2025-12-08 08:58:34,314: t15.2025.04.13 val PER: 0.1897
2025-12-08 08:59:01,649: Train batch 56200: loss: 0.07 grad norm: 89.66 time: 0.134
2025-12-08 08:59:29,154: Train batch 56400: loss: 0.07 grad norm: 3.20 time: 0.145
2025-12-08 08:59:56,548: Train batch 56600: loss: 0.01 grad norm: 0.78 time: 0.121
2025-12-08 09:00:24,656: Train batch 56800: loss: 0.01 grad norm: 1.12 time: 0.132
2025-12-08 09:00:51,703: Train batch 57000: loss: 0.23 grad norm: 8.79 time: 0.151
2025-12-08 09:01:19,129: Train batch 57200: loss: 0.03 grad norm: 2.59 time: 0.133
2025-12-08 09:01:47,004: Train batch 57400: loss: 0.08 grad norm: 9.30 time: 0.095
2025-12-08 09:02:15,055: Train batch 57600: loss: 0.10 grad norm: 5.34 time: 0.140
2025-12-08 09:02:43,223: Train batch 57800: loss: 0.06 grad norm: 3.47 time: 0.145
2025-12-08 09:03:10,733: Train batch 58000: loss: 0.10 grad norm: 8.55 time: 0.157
2025-12-08 09:03:10,733: Running test after training batch: 58000
2025-12-08 09:03:21,173: Val batch 58000: PER (avg): 0.1064 CTC Loss (avg): 22.9140 time: 10.440
2025-12-08 09:03:21,174: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:03:21,174: t15.2023.08.13 val PER: 0.0707
2025-12-08 09:03:21,174: t15.2023.08.18 val PER: 0.0587
2025-12-08 09:03:21,174: t15.2023.08.20 val PER: 0.0556
2025-12-08 09:03:21,174: t15.2023.08.25 val PER: 0.0753
2025-12-08 09:03:21,174: t15.2023.08.27 val PER: 0.1447
2025-12-08 09:03:21,174: t15.2023.09.01 val PER: 0.0495
2025-12-08 09:03:21,174: t15.2023.09.03 val PER: 0.1116
2025-12-08 09:03:21,174: t15.2023.09.24 val PER: 0.0789
2025-12-08 09:03:21,174: t15.2023.09.29 val PER: 0.1053
2025-12-08 09:03:21,174: t15.2023.10.01 val PER: 0.1354
2025-12-08 09:03:21,174: t15.2023.10.06 val PER: 0.0667
2025-12-08 09:03:21,174: t15.2023.10.08 val PER: 0.1651
2025-12-08 09:03:21,174: t15.2023.10.13 val PER: 0.1427
2025-12-08 09:03:21,174: t15.2023.10.15 val PER: 0.1107
2025-12-08 09:03:21,174: t15.2023.10.20 val PER: 0.1745
2025-12-08 09:03:21,175: t15.2023.10.22 val PER: 0.0891
2025-12-08 09:03:21,175: t15.2023.11.03 val PER: 0.1499
2025-12-08 09:03:21,175: t15.2023.11.04 val PER: 0.0171
2025-12-08 09:03:21,175: t15.2023.11.17 val PER: 0.0202
2025-12-08 09:03:21,175: t15.2023.11.19 val PER: 0.0240
2025-12-08 09:03:21,175: t15.2023.11.26 val PER: 0.0529
2025-12-08 09:03:21,175: t15.2023.12.03 val PER: 0.0494
2025-12-08 09:03:21,175: t15.2023.12.08 val PER: 0.0539
2025-12-08 09:03:21,175: t15.2023.12.10 val PER: 0.0381
2025-12-08 09:03:21,175: t15.2023.12.17 val PER: 0.0842
2025-12-08 09:03:21,175: t15.2023.12.29 val PER: 0.0734
2025-12-08 09:03:21,175: t15.2024.02.25 val PER: 0.0646
2025-12-08 09:03:21,175: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:03:21,175: t15.2024.03.08 val PER: 0.1565
2025-12-08 09:03:21,175: t15.2024.03.15 val PER: 0.1570
2025-12-08 09:03:21,175: t15.2024.03.17 val PER: 0.0948
2025-12-08 09:03:21,175: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:03:21,175: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:03:21,175: t15.2024.05.10 val PER: 0.1114
2025-12-08 09:03:21,176: t15.2024.06.14 val PER: 0.1262
2025-12-08 09:03:21,176: t15.2024.07.19 val PER: 0.1661
2025-12-08 09:03:21,176: t15.2024.07.21 val PER: 0.0586
2025-12-08 09:03:21,176: t15.2024.07.28 val PER: 0.0919
2025-12-08 09:03:21,176: t15.2025.01.10 val PER: 0.2452
2025-12-08 09:03:21,176: t15.2025.01.12 val PER: 0.1008
2025-12-08 09:03:21,176: t15.2025.03.14 val PER: 0.3210
2025-12-08 09:03:21,176: t15.2025.03.16 val PER: 0.1453
2025-12-08 09:03:21,176: t15.2025.03.30 val PER: 0.2264
2025-12-08 09:03:21,176: t15.2025.04.13 val PER: 0.1926
2025-12-08 09:03:48,183: Train batch 58200: loss: 0.11 grad norm: 5.45 time: 0.106
2025-12-08 09:04:16,050: Train batch 58400: loss: 0.01 grad norm: 0.45 time: 0.131
2025-12-08 09:04:44,159: Train batch 58600: loss: 0.08 grad norm: 3.48 time: 0.130
2025-12-08 09:05:11,355: Train batch 58800: loss: 0.05 grad norm: 6.94 time: 0.100
2025-12-08 09:05:39,258: Train batch 59000: loss: 0.03 grad norm: 1.53 time: 0.072
2025-12-08 09:06:07,341: Train batch 59200: loss: 0.01 grad norm: 0.35 time: 0.125
2025-12-08 09:06:36,136: Train batch 59400: loss: 0.02 grad norm: 1.79 time: 0.097
2025-12-08 09:07:03,427: Train batch 59600: loss: 0.05 grad norm: 4.05 time: 0.131
2025-12-08 09:07:30,000: Train batch 59800: loss: 0.06 grad norm: 4.61 time: 0.124
2025-12-08 09:07:57,736: Train batch 60000: loss: 0.06 grad norm: 5.32 time: 0.150
2025-12-08 09:07:57,737: Running test after training batch: 60000
2025-12-08 09:08:08,214: Val batch 60000: PER (avg): 0.1063 CTC Loss (avg): 22.8982 time: 10.477
2025-12-08 09:08:08,214: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:08:08,214: t15.2023.08.13 val PER: 0.0686
2025-12-08 09:08:08,214: t15.2023.08.18 val PER: 0.0595
2025-12-08 09:08:08,215: t15.2023.08.20 val PER: 0.0580
2025-12-08 09:08:08,215: t15.2023.08.25 val PER: 0.0648
2025-12-08 09:08:08,215: t15.2023.08.27 val PER: 0.1431
2025-12-08 09:08:08,215: t15.2023.09.01 val PER: 0.0503
2025-12-08 09:08:08,215: t15.2023.09.03 val PER: 0.1057
2025-12-08 09:08:08,215: t15.2023.09.24 val PER: 0.0825
2025-12-08 09:08:08,215: t15.2023.09.29 val PER: 0.1040
2025-12-08 09:08:08,215: t15.2023.10.01 val PER: 0.1394
2025-12-08 09:08:08,215: t15.2023.10.06 val PER: 0.0603
2025-12-08 09:08:08,215: t15.2023.10.08 val PER: 0.1840
2025-12-08 09:08:08,215: t15.2023.10.13 val PER: 0.1404
2025-12-08 09:08:08,215: t15.2023.10.15 val PER: 0.1094
2025-12-08 09:08:08,215: t15.2023.10.20 val PER: 0.1611
2025-12-08 09:08:08,215: t15.2023.10.22 val PER: 0.0857
2025-12-08 09:08:08,215: t15.2023.11.03 val PER: 0.1452
2025-12-08 09:08:08,215: t15.2023.11.04 val PER: 0.0137
2025-12-08 09:08:08,216: t15.2023.11.17 val PER: 0.0187
2025-12-08 09:08:08,216: t15.2023.11.19 val PER: 0.0279
2025-12-08 09:08:08,216: t15.2023.11.26 val PER: 0.0565
2025-12-08 09:08:08,216: t15.2023.12.03 val PER: 0.0452
2025-12-08 09:08:08,216: t15.2023.12.08 val PER: 0.0419
2025-12-08 09:08:08,216: t15.2023.12.10 val PER: 0.0394
2025-12-08 09:08:08,216: t15.2023.12.17 val PER: 0.0832
2025-12-08 09:08:08,216: t15.2023.12.29 val PER: 0.0748
2025-12-08 09:08:08,216: t15.2024.02.25 val PER: 0.0688
2025-12-08 09:08:08,216: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:08:08,216: t15.2024.03.08 val PER: 0.1536
2025-12-08 09:08:08,216: t15.2024.03.15 val PER: 0.1695
2025-12-08 09:08:08,216: t15.2024.03.17 val PER: 0.0879
2025-12-08 09:08:08,216: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:08:08,216: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:08:08,216: t15.2024.05.10 val PER: 0.1248
2025-12-08 09:08:08,217: t15.2024.06.14 val PER: 0.1262
2025-12-08 09:08:08,217: t15.2024.07.19 val PER: 0.1655
2025-12-08 09:08:08,217: t15.2024.07.21 val PER: 0.0600
2025-12-08 09:08:08,217: t15.2024.07.28 val PER: 0.0846
2025-12-08 09:08:08,217: t15.2025.01.10 val PER: 0.2521
2025-12-08 09:08:08,217: t15.2025.01.12 val PER: 0.1024
2025-12-08 09:08:08,217: t15.2025.03.14 val PER: 0.3210
2025-12-08 09:08:08,217: t15.2025.03.16 val PER: 0.1466
2025-12-08 09:08:08,217: t15.2025.03.30 val PER: 0.2368
2025-12-08 09:08:08,217: t15.2025.04.13 val PER: 0.1854
2025-12-08 09:08:34,824: Train batch 60200: loss: 0.03 grad norm: 1.64 time: 0.136
2025-12-08 09:09:02,426: Train batch 60400: loss: 0.02 grad norm: 1.07 time: 0.129
2025-12-08 09:09:30,565: Train batch 60600: loss: 0.04 grad norm: 10.78 time: 0.129
2025-12-08 09:09:58,075: Train batch 60800: loss: 0.06 grad norm: 3.02 time: 0.113
2025-12-08 09:10:25,968: Train batch 61000: loss: 0.03 grad norm: 1.36 time: 0.090
2025-12-08 09:10:53,497: Train batch 61200: loss: 0.02 grad norm: 0.62 time: 0.099
2025-12-08 09:11:21,376: Train batch 61400: loss: 0.16 grad norm: 7.45 time: 0.140
2025-12-08 09:11:48,399: Train batch 61600: loss: 0.02 grad norm: 2.11 time: 0.107
2025-12-08 09:12:16,271: Train batch 61800: loss: 0.07 grad norm: 3.37 time: 0.092
2025-12-08 09:12:43,835: Train batch 62000: loss: 0.11 grad norm: 2.57 time: 0.086
2025-12-08 09:12:43,836: Running test after training batch: 62000
2025-12-08 09:12:54,524: Val batch 62000: PER (avg): 0.1064 CTC Loss (avg): 22.8432 time: 10.688
2025-12-08 09:12:54,524: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:12:54,524: t15.2023.08.13 val PER: 0.0717
2025-12-08 09:12:54,524: t15.2023.08.18 val PER: 0.0637
2025-12-08 09:12:54,524: t15.2023.08.20 val PER: 0.0461
2025-12-08 09:12:54,524: t15.2023.08.25 val PER: 0.0708
2025-12-08 09:12:54,524: t15.2023.08.27 val PER: 0.1511
2025-12-08 09:12:54,524: t15.2023.09.01 val PER: 0.0495
2025-12-08 09:12:54,525: t15.2023.09.03 val PER: 0.1093
2025-12-08 09:12:54,525: t15.2023.09.24 val PER: 0.0874
2025-12-08 09:12:54,525: t15.2023.09.29 val PER: 0.1027
2025-12-08 09:12:54,525: t15.2023.10.01 val PER: 0.1499
2025-12-08 09:12:54,525: t15.2023.10.06 val PER: 0.0624
2025-12-08 09:12:54,525: t15.2023.10.08 val PER: 0.1705
2025-12-08 09:12:54,525: t15.2023.10.13 val PER: 0.1482
2025-12-08 09:12:54,525: t15.2023.10.15 val PER: 0.1055
2025-12-08 09:12:54,525: t15.2023.10.20 val PER: 0.1644
2025-12-08 09:12:54,525: t15.2023.10.22 val PER: 0.0924
2025-12-08 09:12:54,525: t15.2023.11.03 val PER: 0.1506
2025-12-08 09:12:54,525: t15.2023.11.04 val PER: 0.0102
2025-12-08 09:12:54,525: t15.2023.11.17 val PER: 0.0171
2025-12-08 09:12:54,525: t15.2023.11.19 val PER: 0.0240
2025-12-08 09:12:54,525: t15.2023.11.26 val PER: 0.0493
2025-12-08 09:12:54,525: t15.2023.12.03 val PER: 0.0536
2025-12-08 09:12:54,525: t15.2023.12.08 val PER: 0.0426
2025-12-08 09:12:54,525: t15.2023.12.10 val PER: 0.0355
2025-12-08 09:12:54,526: t15.2023.12.17 val PER: 0.0800
2025-12-08 09:12:54,526: t15.2023.12.29 val PER: 0.0769
2025-12-08 09:12:54,526: t15.2024.02.25 val PER: 0.0716
2025-12-08 09:12:54,526: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:12:54,526: t15.2024.03.08 val PER: 0.1607
2025-12-08 09:12:54,526: t15.2024.03.15 val PER: 0.1632
2025-12-08 09:12:54,526: t15.2024.03.17 val PER: 0.0934
2025-12-08 09:12:54,526: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:12:54,526: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:12:54,526: t15.2024.05.10 val PER: 0.1204
2025-12-08 09:12:54,526: t15.2024.06.14 val PER: 0.1325
2025-12-08 09:12:54,526: t15.2024.07.19 val PER: 0.1516
2025-12-08 09:12:54,526: t15.2024.07.21 val PER: 0.0572
2025-12-08 09:12:54,526: t15.2024.07.28 val PER: 0.0816
2025-12-08 09:12:54,526: t15.2025.01.10 val PER: 0.2493
2025-12-08 09:12:54,526: t15.2025.01.12 val PER: 0.1062
2025-12-08 09:12:54,526: t15.2025.03.14 val PER: 0.3254
2025-12-08 09:12:54,526: t15.2025.03.16 val PER: 0.1492
2025-12-08 09:12:54,526: t15.2025.03.30 val PER: 0.2299
2025-12-08 09:12:54,527: t15.2025.04.13 val PER: 0.1869
2025-12-08 09:13:21,579: Train batch 62200: loss: 0.03 grad norm: 2.15 time: 0.125
2025-12-08 09:13:48,425: Train batch 62400: loss: 0.04 grad norm: 3.44 time: 0.176
2025-12-08 09:14:16,589: Train batch 62600: loss: 0.04 grad norm: 1.94 time: 0.121
2025-12-08 09:14:44,633: Train batch 62800: loss: 0.02 grad norm: 1.04 time: 0.161
2025-12-08 09:15:12,559: Train batch 63000: loss: 0.06 grad norm: 2.80 time: 0.180
2025-12-08 09:15:40,752: Train batch 63200: loss: 0.06 grad norm: 4.46 time: 0.131
2025-12-08 09:16:08,943: Train batch 63400: loss: 0.02 grad norm: 1.26 time: 0.157
2025-12-08 09:16:37,512: Train batch 63600: loss: 0.08 grad norm: 5.69 time: 0.176
2025-12-08 09:17:06,493: Train batch 63800: loss: 0.04 grad norm: 1.48 time: 0.107
2025-12-08 09:17:34,957: Train batch 64000: loss: 0.01 grad norm: 0.35 time: 0.113
2025-12-08 09:17:34,958: Running test after training batch: 64000
2025-12-08 09:17:45,598: Val batch 64000: PER (avg): 0.1052 CTC Loss (avg): 22.8508 time: 10.640
2025-12-08 09:17:45,598: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:17:45,598: t15.2023.08.13 val PER: 0.0769
2025-12-08 09:17:45,598: t15.2023.08.18 val PER: 0.0671
2025-12-08 09:17:45,598: t15.2023.08.20 val PER: 0.0548
2025-12-08 09:17:45,598: t15.2023.08.25 val PER: 0.0708
2025-12-08 09:17:45,598: t15.2023.08.27 val PER: 0.1447
2025-12-08 09:17:45,598: t15.2023.09.01 val PER: 0.0455
2025-12-08 09:17:45,598: t15.2023.09.03 val PER: 0.1140
2025-12-08 09:17:45,599: t15.2023.09.24 val PER: 0.0910
2025-12-08 09:17:45,599: t15.2023.09.29 val PER: 0.0983
2025-12-08 09:17:45,599: t15.2023.10.01 val PER: 0.1374
2025-12-08 09:17:45,599: t15.2023.10.06 val PER: 0.0635
2025-12-08 09:17:45,599: t15.2023.10.08 val PER: 0.1719
2025-12-08 09:17:45,599: t15.2023.10.13 val PER: 0.1497
2025-12-08 09:17:45,599: t15.2023.10.15 val PER: 0.1055
2025-12-08 09:17:45,599: t15.2023.10.20 val PER: 0.1611
2025-12-08 09:17:45,599: t15.2023.10.22 val PER: 0.0913
2025-12-08 09:17:45,599: t15.2023.11.03 val PER: 0.1445
2025-12-08 09:17:45,599: t15.2023.11.04 val PER: 0.0102
2025-12-08 09:17:45,599: t15.2023.11.17 val PER: 0.0156
2025-12-08 09:17:45,599: t15.2023.11.19 val PER: 0.0200
2025-12-08 09:17:45,599: t15.2023.11.26 val PER: 0.0507
2025-12-08 09:17:45,599: t15.2023.12.03 val PER: 0.0609
2025-12-08 09:17:45,599: t15.2023.12.08 val PER: 0.0439
2025-12-08 09:17:45,599: t15.2023.12.10 val PER: 0.0381
2025-12-08 09:17:45,599: t15.2023.12.17 val PER: 0.0780
2025-12-08 09:17:45,599: t15.2023.12.29 val PER: 0.0686
2025-12-08 09:17:45,600: t15.2024.02.25 val PER: 0.0716
2025-12-08 09:17:45,600: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:17:45,600: t15.2024.03.08 val PER: 0.1679
2025-12-08 09:17:45,600: t15.2024.03.15 val PER: 0.1670
2025-12-08 09:17:45,600: t15.2024.03.17 val PER: 0.0879
2025-12-08 09:17:45,600: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:17:45,600: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:17:45,600: t15.2024.05.10 val PER: 0.1144
2025-12-08 09:17:45,600: t15.2024.06.14 val PER: 0.1293
2025-12-08 09:17:45,600: t15.2024.07.19 val PER: 0.1529
2025-12-08 09:17:45,600: t15.2024.07.21 val PER: 0.0531
2025-12-08 09:17:45,600: t15.2024.07.28 val PER: 0.0868
2025-12-08 09:17:45,600: t15.2025.01.10 val PER: 0.2383
2025-12-08 09:17:45,600: t15.2025.01.12 val PER: 0.0947
2025-12-08 09:17:45,600: t15.2025.03.14 val PER: 0.3151
2025-12-08 09:17:45,600: t15.2025.03.16 val PER: 0.1479
2025-12-08 09:17:45,600: t15.2025.03.30 val PER: 0.2310
2025-12-08 09:17:45,600: t15.2025.04.13 val PER: 0.1883
2025-12-08 09:17:45,600: New best test PER 0.1055 --> 0.1052
2025-12-08 09:17:45,600: Checkpointing model
2025-12-08 09:17:46,214: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 09:18:15,034: Train batch 64200: loss: 0.05 grad norm: 14.53 time: 0.166
2025-12-08 09:18:43,702: Train batch 64400: loss: 0.02 grad norm: 1.42 time: 0.133
2025-12-08 09:19:12,747: Train batch 64600: loss: 0.01 grad norm: 0.74 time: 0.107
2025-12-08 09:19:41,556: Train batch 64800: loss: 0.03 grad norm: 1.89 time: 0.092
2025-12-08 09:20:09,568: Train batch 65000: loss: 0.05 grad norm: 2.92 time: 0.177
2025-12-08 09:20:38,172: Train batch 65200: loss: 0.05 grad norm: 2.67 time: 0.144
2025-12-08 09:21:06,429: Train batch 65400: loss: 0.03 grad norm: 2.41 time: 0.104
2025-12-08 09:21:35,685: Train batch 65600: loss: 0.01 grad norm: 0.54 time: 0.125
2025-12-08 09:22:04,829: Train batch 65800: loss: 0.03 grad norm: 2.24 time: 0.108
2025-12-08 09:22:33,939: Train batch 66000: loss: 0.02 grad norm: 1.12 time: 0.162
2025-12-08 09:22:33,940: Running test after training batch: 66000
2025-12-08 09:22:44,436: Val batch 66000: PER (avg): 0.1038 CTC Loss (avg): 22.5995 time: 10.496
2025-12-08 09:22:44,436: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:22:44,436: t15.2023.08.13 val PER: 0.0728
2025-12-08 09:22:44,436: t15.2023.08.18 val PER: 0.0637
2025-12-08 09:22:44,436: t15.2023.08.20 val PER: 0.0492
2025-12-08 09:22:44,436: t15.2023.08.25 val PER: 0.0738
2025-12-08 09:22:44,436: t15.2023.08.27 val PER: 0.1431
2025-12-08 09:22:44,437: t15.2023.09.01 val PER: 0.0414
2025-12-08 09:22:44,437: t15.2023.09.03 val PER: 0.0998
2025-12-08 09:22:44,437: t15.2023.09.24 val PER: 0.0765
2025-12-08 09:22:44,437: t15.2023.09.29 val PER: 0.0989
2025-12-08 09:22:44,437: t15.2023.10.01 val PER: 0.1347
2025-12-08 09:22:44,437: t15.2023.10.06 val PER: 0.0678
2025-12-08 09:22:44,437: t15.2023.10.08 val PER: 0.1746
2025-12-08 09:22:44,437: t15.2023.10.13 val PER: 0.1443
2025-12-08 09:22:44,437: t15.2023.10.15 val PER: 0.0982
2025-12-08 09:22:44,437: t15.2023.10.20 val PER: 0.1577
2025-12-08 09:22:44,437: t15.2023.10.22 val PER: 0.0958
2025-12-08 09:22:44,437: t15.2023.11.03 val PER: 0.1370
2025-12-08 09:22:44,437: t15.2023.11.04 val PER: 0.0137
2025-12-08 09:22:44,437: t15.2023.11.17 val PER: 0.0202
2025-12-08 09:22:44,437: t15.2023.11.19 val PER: 0.0220
2025-12-08 09:22:44,437: t15.2023.11.26 val PER: 0.0478
2025-12-08 09:22:44,437: t15.2023.12.03 val PER: 0.0462
2025-12-08 09:22:44,437: t15.2023.12.08 val PER: 0.0419
2025-12-08 09:22:44,438: t15.2023.12.10 val PER: 0.0420
2025-12-08 09:22:44,438: t15.2023.12.17 val PER: 0.0748
2025-12-08 09:22:44,438: t15.2023.12.29 val PER: 0.0734
2025-12-08 09:22:44,438: t15.2024.02.25 val PER: 0.0688
2025-12-08 09:22:44,438: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:22:44,438: t15.2024.03.08 val PER: 0.1636
2025-12-08 09:22:44,438: t15.2024.03.15 val PER: 0.1657
2025-12-08 09:22:44,438: t15.2024.03.17 val PER: 0.0858
2025-12-08 09:22:44,438: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:22:44,438: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:22:44,438: t15.2024.05.10 val PER: 0.1278
2025-12-08 09:22:44,438: t15.2024.06.14 val PER: 0.1215
2025-12-08 09:22:44,438: t15.2024.07.19 val PER: 0.1569
2025-12-08 09:22:44,438: t15.2024.07.21 val PER: 0.0517
2025-12-08 09:22:44,438: t15.2024.07.28 val PER: 0.0897
2025-12-08 09:22:44,438: t15.2025.01.10 val PER: 0.2534
2025-12-08 09:22:44,438: t15.2025.01.12 val PER: 0.1008
2025-12-08 09:22:44,438: t15.2025.03.14 val PER: 0.3092
2025-12-08 09:22:44,438: t15.2025.03.16 val PER: 0.1597
2025-12-08 09:22:44,439: t15.2025.03.30 val PER: 0.2184
2025-12-08 09:22:44,439: t15.2025.04.13 val PER: 0.1897
2025-12-08 09:22:44,439: New best test PER 0.1052 --> 0.1038
2025-12-08 09:22:44,439: Checkpointing model
2025-12-08 09:22:45,526: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 09:23:14,959: Train batch 66200: loss: 0.04 grad norm: 3.68 time: 0.133
2025-12-08 09:23:43,726: Train batch 66400: loss: 0.02 grad norm: 1.83 time: 0.098
2025-12-08 09:24:12,604: Train batch 66600: loss: 0.07 grad norm: 6.08 time: 0.124
2025-12-08 09:24:40,827: Train batch 66800: loss: 0.04 grad norm: 2.41 time: 0.115
2025-12-08 09:25:08,828: Train batch 67000: loss: 0.01 grad norm: 0.56 time: 0.187
2025-12-08 09:25:36,816: Train batch 67200: loss: 0.05 grad norm: 5.80 time: 0.095
2025-12-08 09:26:04,843: Train batch 67400: loss: 0.02 grad norm: 1.97 time: 0.108
2025-12-08 09:26:32,722: Train batch 67600: loss: 0.04 grad norm: 2.34 time: 0.096
2025-12-08 09:27:00,931: Train batch 67800: loss: 0.03 grad norm: 2.09 time: 0.167
2025-12-08 09:27:28,738: Train batch 68000: loss: 0.05 grad norm: 3.08 time: 0.076
2025-12-08 09:27:28,738: Running test after training batch: 68000
2025-12-08 09:27:39,326: Val batch 68000: PER (avg): 0.1040 CTC Loss (avg): 22.7910 time: 10.588
2025-12-08 09:27:39,328: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:27:39,328: t15.2023.08.13 val PER: 0.0696
2025-12-08 09:27:39,328: t15.2023.08.18 val PER: 0.0570
2025-12-08 09:27:39,328: t15.2023.08.20 val PER: 0.0500
2025-12-08 09:27:39,328: t15.2023.08.25 val PER: 0.0768
2025-12-08 09:27:39,328: t15.2023.08.27 val PER: 0.1479
2025-12-08 09:27:39,328: t15.2023.09.01 val PER: 0.0422
2025-12-08 09:27:39,328: t15.2023.09.03 val PER: 0.1045
2025-12-08 09:27:39,328: t15.2023.09.24 val PER: 0.0850
2025-12-08 09:27:39,328: t15.2023.09.29 val PER: 0.1034
2025-12-08 09:27:39,328: t15.2023.10.01 val PER: 0.1361
2025-12-08 09:27:39,328: t15.2023.10.06 val PER: 0.0710
2025-12-08 09:27:39,329: t15.2023.10.08 val PER: 0.1678
2025-12-08 09:27:39,329: t15.2023.10.13 val PER: 0.1474
2025-12-08 09:27:39,329: t15.2023.10.15 val PER: 0.1055
2025-12-08 09:27:39,329: t15.2023.10.20 val PER: 0.1745
2025-12-08 09:27:39,329: t15.2023.10.22 val PER: 0.0857
2025-12-08 09:27:39,329: t15.2023.11.03 val PER: 0.1438
2025-12-08 09:27:39,329: t15.2023.11.04 val PER: 0.0137
2025-12-08 09:27:39,329: t15.2023.11.17 val PER: 0.0140
2025-12-08 09:27:39,329: t15.2023.11.19 val PER: 0.0180
2025-12-08 09:27:39,329: t15.2023.11.26 val PER: 0.0500
2025-12-08 09:27:39,329: t15.2023.12.03 val PER: 0.0473
2025-12-08 09:27:39,329: t15.2023.12.08 val PER: 0.0433
2025-12-08 09:27:39,329: t15.2023.12.10 val PER: 0.0420
2025-12-08 09:27:39,329: t15.2023.12.17 val PER: 0.0863
2025-12-08 09:27:39,329: t15.2023.12.29 val PER: 0.0686
2025-12-08 09:27:39,329: t15.2024.02.25 val PER: 0.0730
2025-12-08 09:27:39,329: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:27:39,329: t15.2024.03.08 val PER: 0.1508
2025-12-08 09:27:39,329: t15.2024.03.15 val PER: 0.1632
2025-12-08 09:27:39,330: t15.2024.03.17 val PER: 0.0865
2025-12-08 09:27:39,330: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:27:39,330: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:27:39,330: t15.2024.05.10 val PER: 0.1263
2025-12-08 09:27:39,330: t15.2024.06.14 val PER: 0.1183
2025-12-08 09:27:39,330: t15.2024.07.19 val PER: 0.1582
2025-12-08 09:27:39,330: t15.2024.07.21 val PER: 0.0531
2025-12-08 09:27:39,330: t15.2024.07.28 val PER: 0.0868
2025-12-08 09:27:39,330: t15.2025.01.10 val PER: 0.2328
2025-12-08 09:27:39,330: t15.2025.01.12 val PER: 0.0855
2025-12-08 09:27:39,330: t15.2025.03.14 val PER: 0.3018
2025-12-08 09:27:39,330: t15.2025.03.16 val PER: 0.1571
2025-12-08 09:27:39,330: t15.2025.03.30 val PER: 0.2368
2025-12-08 09:27:39,330: t15.2025.04.13 val PER: 0.1997
2025-12-08 09:28:06,682: Train batch 68200: loss: 0.03 grad norm: 2.41 time: 0.131
2025-12-08 09:28:34,372: Train batch 68400: loss: 0.02 grad norm: 0.61 time: 0.083
2025-12-08 09:29:02,375: Train batch 68600: loss: 0.01 grad norm: 0.55 time: 0.111
2025-12-08 09:29:30,076: Train batch 68800: loss: 0.07 grad norm: 4.47 time: 0.132
2025-12-08 09:29:58,113: Train batch 69000: loss: 0.00 grad norm: 0.18 time: 0.166
2025-12-08 09:30:25,967: Train batch 69200: loss: 0.03 grad norm: 2.94 time: 0.108
2025-12-08 09:30:54,070: Train batch 69400: loss: 0.04 grad norm: 3.79 time: 0.119
2025-12-08 09:31:22,170: Train batch 69600: loss: 0.01 grad norm: 0.70 time: 0.138
2025-12-08 09:31:50,084: Train batch 69800: loss: 0.02 grad norm: 1.32 time: 0.124
2025-12-08 09:32:18,274: Train batch 70000: loss: 0.04 grad norm: 2.72 time: 0.130
2025-12-08 09:32:18,274: Running test after training batch: 70000
2025-12-08 09:32:28,882: Val batch 70000: PER (avg): 0.1043 CTC Loss (avg): 22.7878 time: 10.607
2025-12-08 09:32:28,882: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:32:28,882: t15.2023.08.13 val PER: 0.0686
2025-12-08 09:32:28,882: t15.2023.08.18 val PER: 0.0629
2025-12-08 09:32:28,883: t15.2023.08.20 val PER: 0.0516
2025-12-08 09:32:28,883: t15.2023.08.25 val PER: 0.0723
2025-12-08 09:32:28,883: t15.2023.08.27 val PER: 0.1431
2025-12-08 09:32:28,883: t15.2023.09.01 val PER: 0.0398
2025-12-08 09:32:28,883: t15.2023.09.03 val PER: 0.1057
2025-12-08 09:32:28,883: t15.2023.09.24 val PER: 0.0825
2025-12-08 09:32:28,883: t15.2023.09.29 val PER: 0.1021
2025-12-08 09:32:28,883: t15.2023.10.01 val PER: 0.1354
2025-12-08 09:32:28,883: t15.2023.10.06 val PER: 0.0667
2025-12-08 09:32:28,883: t15.2023.10.08 val PER: 0.1583
2025-12-08 09:32:28,883: t15.2023.10.13 val PER: 0.1443
2025-12-08 09:32:28,883: t15.2023.10.15 val PER: 0.1074
2025-12-08 09:32:28,883: t15.2023.10.20 val PER: 0.1678
2025-12-08 09:32:28,883: t15.2023.10.22 val PER: 0.0824
2025-12-08 09:32:28,883: t15.2023.11.03 val PER: 0.1479
2025-12-08 09:32:28,883: t15.2023.11.04 val PER: 0.0171
2025-12-08 09:32:28,883: t15.2023.11.17 val PER: 0.0171
2025-12-08 09:32:28,883: t15.2023.11.19 val PER: 0.0200
2025-12-08 09:32:28,883: t15.2023.11.26 val PER: 0.0507
2025-12-08 09:32:28,884: t15.2023.12.03 val PER: 0.0452
2025-12-08 09:32:28,884: t15.2023.12.08 val PER: 0.0433
2025-12-08 09:32:28,884: t15.2023.12.10 val PER: 0.0368
2025-12-08 09:32:28,884: t15.2023.12.17 val PER: 0.0842
2025-12-08 09:32:28,884: t15.2023.12.29 val PER: 0.0673
2025-12-08 09:32:28,884: t15.2024.02.25 val PER: 0.0758
2025-12-08 09:32:28,884: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:32:28,884: t15.2024.03.08 val PER: 0.1508
2025-12-08 09:32:28,884: t15.2024.03.15 val PER: 0.1614
2025-12-08 09:32:28,884: t15.2024.03.17 val PER: 0.0858
2025-12-08 09:32:28,884: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:32:28,884: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:32:28,884: t15.2024.05.10 val PER: 0.1174
2025-12-08 09:32:28,884: t15.2024.06.14 val PER: 0.1230
2025-12-08 09:32:28,884: t15.2024.07.19 val PER: 0.1641
2025-12-08 09:32:28,884: t15.2024.07.21 val PER: 0.0579
2025-12-08 09:32:28,884: t15.2024.07.28 val PER: 0.0853
2025-12-08 09:32:28,884: t15.2025.01.10 val PER: 0.2548
2025-12-08 09:32:28,884: t15.2025.01.12 val PER: 0.1024
2025-12-08 09:32:28,885: t15.2025.03.14 val PER: 0.3033
2025-12-08 09:32:28,885: t15.2025.03.16 val PER: 0.1597
2025-12-08 09:32:28,885: t15.2025.03.30 val PER: 0.2276
2025-12-08 09:32:28,885: t15.2025.04.13 val PER: 0.1869
2025-12-08 09:32:56,989: Train batch 70200: loss: 0.01 grad norm: 0.77 time: 0.125
2025-12-08 09:33:24,793: Train batch 70400: loss: 0.13 grad norm: 7.00 time: 0.201
2025-12-08 09:33:53,497: Train batch 70600: loss: 0.10 grad norm: 8.24 time: 0.165
2025-12-08 09:34:21,694: Train batch 70800: loss: 0.02 grad norm: 1.51 time: 0.154
2025-12-08 09:34:50,142: Train batch 71000: loss: 0.10 grad norm: 5.06 time: 0.176
2025-12-08 09:35:18,326: Train batch 71200: loss: 0.04 grad norm: 1.65 time: 0.102
2025-12-08 09:35:47,087: Train batch 71400: loss: 0.02 grad norm: 3.67 time: 0.114
2025-12-08 09:36:15,555: Train batch 71600: loss: 0.03 grad norm: 2.08 time: 0.101
2025-12-08 09:36:44,299: Train batch 71800: loss: 0.03 grad norm: 2.00 time: 0.144
2025-12-08 09:37:12,850: Train batch 72000: loss: 0.06 grad norm: 1.88 time: 0.155
2025-12-08 09:37:12,850: Running test after training batch: 72000
2025-12-08 09:37:23,746: Val batch 72000: PER (avg): 0.1045 CTC Loss (avg): 23.0456 time: 10.895
2025-12-08 09:37:23,746: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:37:23,746: t15.2023.08.13 val PER: 0.0738
2025-12-08 09:37:23,746: t15.2023.08.18 val PER: 0.0629
2025-12-08 09:37:23,746: t15.2023.08.20 val PER: 0.0469
2025-12-08 09:37:23,747: t15.2023.08.25 val PER: 0.0768
2025-12-08 09:37:23,747: t15.2023.08.27 val PER: 0.1399
2025-12-08 09:37:23,747: t15.2023.09.01 val PER: 0.0390
2025-12-08 09:37:23,747: t15.2023.09.03 val PER: 0.1164
2025-12-08 09:37:23,747: t15.2023.09.24 val PER: 0.0789
2025-12-08 09:37:23,747: t15.2023.09.29 val PER: 0.1015
2025-12-08 09:37:23,747: t15.2023.10.01 val PER: 0.1380
2025-12-08 09:37:23,747: t15.2023.10.06 val PER: 0.0721
2025-12-08 09:37:23,747: t15.2023.10.08 val PER: 0.1678
2025-12-08 09:37:23,747: t15.2023.10.13 val PER: 0.1342
2025-12-08 09:37:23,747: t15.2023.10.15 val PER: 0.1002
2025-12-08 09:37:23,747: t15.2023.10.20 val PER: 0.1611
2025-12-08 09:37:23,747: t15.2023.10.22 val PER: 0.0891
2025-12-08 09:37:23,747: t15.2023.11.03 val PER: 0.1350
2025-12-08 09:37:23,747: t15.2023.11.04 val PER: 0.0068
2025-12-08 09:37:23,747: t15.2023.11.17 val PER: 0.0187
2025-12-08 09:37:23,748: t15.2023.11.19 val PER: 0.0259
2025-12-08 09:37:23,748: t15.2023.11.26 val PER: 0.0435
2025-12-08 09:37:23,748: t15.2023.12.03 val PER: 0.0494
2025-12-08 09:37:23,748: t15.2023.12.08 val PER: 0.0393
2025-12-08 09:37:23,748: t15.2023.12.10 val PER: 0.0394
2025-12-08 09:37:23,748: t15.2023.12.17 val PER: 0.0800
2025-12-08 09:37:23,748: t15.2023.12.29 val PER: 0.0700
2025-12-08 09:37:23,748: t15.2024.02.25 val PER: 0.0660
2025-12-08 09:37:23,748: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:37:23,748: t15.2024.03.08 val PER: 0.1550
2025-12-08 09:37:23,748: t15.2024.03.15 val PER: 0.1695
2025-12-08 09:37:23,748: t15.2024.03.17 val PER: 0.0955
2025-12-08 09:37:23,748: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:37:23,748: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:37:23,748: t15.2024.05.10 val PER: 0.1204
2025-12-08 09:37:23,748: t15.2024.06.14 val PER: 0.1293
2025-12-08 09:37:23,748: t15.2024.07.19 val PER: 0.1628
2025-12-08 09:37:23,749: t15.2024.07.21 val PER: 0.0607
2025-12-08 09:37:23,749: t15.2024.07.28 val PER: 0.0875
2025-12-08 09:37:23,749: t15.2025.01.10 val PER: 0.2603
2025-12-08 09:37:23,749: t15.2025.01.12 val PER: 0.1032
2025-12-08 09:37:23,749: t15.2025.03.14 val PER: 0.3166
2025-12-08 09:37:23,749: t15.2025.03.16 val PER: 0.1453
2025-12-08 09:37:23,749: t15.2025.03.30 val PER: 0.2299
2025-12-08 09:37:23,749: t15.2025.04.13 val PER: 0.1826
2025-12-08 09:37:52,598: Train batch 72200: loss: 0.01 grad norm: 0.82 time: 0.154
2025-12-08 09:38:21,693: Train batch 72400: loss: 0.03 grad norm: 1.86 time: 0.135
2025-12-08 09:38:50,187: Train batch 72600: loss: 0.00 grad norm: 0.17 time: 0.148
2025-12-08 09:39:18,764: Train batch 72800: loss: 0.01 grad norm: 0.79 time: 0.094
2025-12-08 09:39:47,286: Train batch 73000: loss: 0.02 grad norm: 0.90 time: 0.119
2025-12-08 09:40:15,857: Train batch 73200: loss: 0.04 grad norm: 5.91 time: 0.118
2025-12-08 09:40:45,278: Train batch 73400: loss: 0.03 grad norm: 2.47 time: 0.187
2025-12-08 09:41:13,878: Train batch 73600: loss: 0.02 grad norm: 3.30 time: 0.216
2025-12-08 09:41:42,159: Train batch 73800: loss: 0.01 grad norm: 1.11 time: 0.102
2025-12-08 09:42:10,841: Train batch 74000: loss: 0.03 grad norm: 23.61 time: 0.099
2025-12-08 09:42:10,843: Running test after training batch: 74000
2025-12-08 09:42:21,700: Val batch 74000: PER (avg): 0.1040 CTC Loss (avg): 22.9224 time: 10.858
2025-12-08 09:42:21,701: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:42:21,701: t15.2023.08.13 val PER: 0.0696
2025-12-08 09:42:21,701: t15.2023.08.18 val PER: 0.0637
2025-12-08 09:42:21,701: t15.2023.08.20 val PER: 0.0548
2025-12-08 09:42:21,701: t15.2023.08.25 val PER: 0.0738
2025-12-08 09:42:21,701: t15.2023.08.27 val PER: 0.1431
2025-12-08 09:42:21,701: t15.2023.09.01 val PER: 0.0390
2025-12-08 09:42:21,701: t15.2023.09.03 val PER: 0.1176
2025-12-08 09:42:21,701: t15.2023.09.24 val PER: 0.0777
2025-12-08 09:42:21,701: t15.2023.09.29 val PER: 0.1040
2025-12-08 09:42:21,701: t15.2023.10.01 val PER: 0.1394
2025-12-08 09:42:21,701: t15.2023.10.06 val PER: 0.0657
2025-12-08 09:42:21,701: t15.2023.10.08 val PER: 0.1624
2025-12-08 09:42:21,701: t15.2023.10.13 val PER: 0.1490
2025-12-08 09:42:21,702: t15.2023.10.15 val PER: 0.1035
2025-12-08 09:42:21,702: t15.2023.10.20 val PER: 0.1678
2025-12-08 09:42:21,702: t15.2023.10.22 val PER: 0.0880
2025-12-08 09:42:21,702: t15.2023.11.03 val PER: 0.1438
2025-12-08 09:42:21,702: t15.2023.11.04 val PER: 0.0102
2025-12-08 09:42:21,702: t15.2023.11.17 val PER: 0.0109
2025-12-08 09:42:21,702: t15.2023.11.19 val PER: 0.0220
2025-12-08 09:42:21,702: t15.2023.11.26 val PER: 0.0536
2025-12-08 09:42:21,702: t15.2023.12.03 val PER: 0.0494
2025-12-08 09:42:21,702: t15.2023.12.08 val PER: 0.0413
2025-12-08 09:42:21,702: t15.2023.12.10 val PER: 0.0394
2025-12-08 09:42:21,702: t15.2023.12.17 val PER: 0.0769
2025-12-08 09:42:21,702: t15.2023.12.29 val PER: 0.0666
2025-12-08 09:42:21,702: t15.2024.02.25 val PER: 0.0646
2025-12-08 09:42:21,702: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:42:21,702: t15.2024.03.08 val PER: 0.1565
2025-12-08 09:42:21,702: t15.2024.03.15 val PER: 0.1588
2025-12-08 09:42:21,702: t15.2024.03.17 val PER: 0.0886
2025-12-08 09:42:21,703: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:42:21,703: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:42:21,703: t15.2024.05.10 val PER: 0.1144
2025-12-08 09:42:21,703: t15.2024.06.14 val PER: 0.1293
2025-12-08 09:42:21,703: t15.2024.07.19 val PER: 0.1655
2025-12-08 09:42:21,703: t15.2024.07.21 val PER: 0.0593
2025-12-08 09:42:21,703: t15.2024.07.28 val PER: 0.0831
2025-12-08 09:42:21,703: t15.2025.01.10 val PER: 0.2383
2025-12-08 09:42:21,703: t15.2025.01.12 val PER: 0.0970
2025-12-08 09:42:21,703: t15.2025.03.14 val PER: 0.3003
2025-12-08 09:42:21,703: t15.2025.03.16 val PER: 0.1518
2025-12-08 09:42:21,703: t15.2025.03.30 val PER: 0.2310
2025-12-08 09:42:21,703: t15.2025.04.13 val PER: 0.1812
2025-12-08 09:42:50,918: Train batch 74200: loss: 0.25 grad norm: 4.76 time: 0.169
2025-12-08 09:43:19,465: Train batch 74400: loss: 0.07 grad norm: 4.90 time: 0.139
2025-12-08 09:43:48,387: Train batch 74600: loss: 0.03 grad norm: 6.22 time: 0.184
2025-12-08 09:44:16,498: Train batch 74800: loss: 0.01 grad norm: 0.31 time: 0.111
2025-12-08 09:44:45,142: Train batch 75000: loss: 0.04 grad norm: 2.96 time: 0.122
2025-12-08 09:45:14,241: Train batch 75200: loss: 0.01 grad norm: 0.63 time: 0.142
2025-12-08 09:45:42,139: Train batch 75400: loss: 0.01 grad norm: 0.89 time: 0.116
2025-12-08 09:46:10,083: Train batch 75600: loss: 0.21 grad norm: 10.11 time: 0.096
2025-12-08 09:46:37,664: Train batch 75800: loss: 0.03 grad norm: 1.88 time: 0.184
2025-12-08 09:47:05,429: Train batch 76000: loss: 0.07 grad norm: 2.13 time: 0.124
2025-12-08 09:47:05,429: Running test after training batch: 76000
2025-12-08 09:47:16,141: Val batch 76000: PER (avg): 0.1033 CTC Loss (avg): 23.1547 time: 10.712
2025-12-08 09:47:16,142: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:47:16,142: t15.2023.08.13 val PER: 0.0738
2025-12-08 09:47:16,142: t15.2023.08.18 val PER: 0.0654
2025-12-08 09:47:16,142: t15.2023.08.20 val PER: 0.0540
2025-12-08 09:47:16,142: t15.2023.08.25 val PER: 0.0768
2025-12-08 09:47:16,142: t15.2023.08.27 val PER: 0.1447
2025-12-08 09:47:16,142: t15.2023.09.01 val PER: 0.0414
2025-12-08 09:47:16,142: t15.2023.09.03 val PER: 0.1152
2025-12-08 09:47:16,142: t15.2023.09.24 val PER: 0.0777
2025-12-08 09:47:16,142: t15.2023.09.29 val PER: 0.1002
2025-12-08 09:47:16,142: t15.2023.10.01 val PER: 0.1367
2025-12-08 09:47:16,142: t15.2023.10.06 val PER: 0.0635
2025-12-08 09:47:16,142: t15.2023.10.08 val PER: 0.1719
2025-12-08 09:47:16,142: t15.2023.10.13 val PER: 0.1458
2025-12-08 09:47:16,142: t15.2023.10.15 val PER: 0.0989
2025-12-08 09:47:16,143: t15.2023.10.20 val PER: 0.1611
2025-12-08 09:47:16,143: t15.2023.10.22 val PER: 0.0935
2025-12-08 09:47:16,143: t15.2023.11.03 val PER: 0.1404
2025-12-08 09:47:16,143: t15.2023.11.04 val PER: 0.0102
2025-12-08 09:47:16,143: t15.2023.11.17 val PER: 0.0124
2025-12-08 09:47:16,143: t15.2023.11.19 val PER: 0.0299
2025-12-08 09:47:16,143: t15.2023.11.26 val PER: 0.0500
2025-12-08 09:47:16,143: t15.2023.12.03 val PER: 0.0504
2025-12-08 09:47:16,143: t15.2023.12.08 val PER: 0.0360
2025-12-08 09:47:16,143: t15.2023.12.10 val PER: 0.0368
2025-12-08 09:47:16,143: t15.2023.12.17 val PER: 0.0759
2025-12-08 09:47:16,143: t15.2023.12.29 val PER: 0.0728
2025-12-08 09:47:16,143: t15.2024.02.25 val PER: 0.0618
2025-12-08 09:47:16,143: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:47:16,143: t15.2024.03.08 val PER: 0.1494
2025-12-08 09:47:16,143: t15.2024.03.15 val PER: 0.1714
2025-12-08 09:47:16,143: t15.2024.03.17 val PER: 0.0823
2025-12-08 09:47:16,143: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:47:16,143: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:47:16,144: t15.2024.05.10 val PER: 0.1144
2025-12-08 09:47:16,144: t15.2024.06.14 val PER: 0.1120
2025-12-08 09:47:16,144: t15.2024.07.19 val PER: 0.1575
2025-12-08 09:47:16,144: t15.2024.07.21 val PER: 0.0607
2025-12-08 09:47:16,144: t15.2024.07.28 val PER: 0.0757
2025-12-08 09:47:16,144: t15.2025.01.10 val PER: 0.2493
2025-12-08 09:47:16,144: t15.2025.01.12 val PER: 0.0931
2025-12-08 09:47:16,144: t15.2025.03.14 val PER: 0.3018
2025-12-08 09:47:16,144: t15.2025.03.16 val PER: 0.1531
2025-12-08 09:47:16,144: t15.2025.03.30 val PER: 0.2322
2025-12-08 09:47:16,144: t15.2025.04.13 val PER: 0.1854
2025-12-08 09:47:16,144: New best test PER 0.1038 --> 0.1033
2025-12-08 09:47:16,144: Checkpointing model
2025-12-08 09:47:17,292: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 09:47:45,473: Train batch 76200: loss: 0.01 grad norm: 0.85 time: 0.095
2025-12-08 09:48:13,218: Train batch 76400: loss: 0.02 grad norm: 1.00 time: 0.146
2025-12-08 09:48:40,719: Train batch 76600: loss: 0.04 grad norm: 3.45 time: 0.096
2025-12-08 09:49:08,443: Train batch 76800: loss: 0.02 grad norm: 1.55 time: 0.118
2025-12-08 09:49:36,116: Train batch 77000: loss: 0.04 grad norm: 3.63 time: 0.131
2025-12-08 09:50:03,738: Train batch 77200: loss: 0.04 grad norm: 3.23 time: 0.143
2025-12-08 09:50:31,204: Train batch 77400: loss: 0.01 grad norm: 1.53 time: 0.144
2025-12-08 09:50:58,426: Train batch 77600: loss: 0.03 grad norm: 7.31 time: 0.102
2025-12-08 09:51:26,252: Train batch 77800: loss: 0.03 grad norm: 3.23 time: 0.195
2025-12-08 09:51:54,727: Train batch 78000: loss: 0.01 grad norm: 0.34 time: 0.131
2025-12-08 09:51:54,728: Running test after training batch: 78000
2025-12-08 09:52:05,224: Val batch 78000: PER (avg): 0.1029 CTC Loss (avg): 23.0009 time: 10.495
2025-12-08 09:52:05,224: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:52:05,224: t15.2023.08.13 val PER: 0.0717
2025-12-08 09:52:05,224: t15.2023.08.18 val PER: 0.0637
2025-12-08 09:52:05,224: t15.2023.08.20 val PER: 0.0461
2025-12-08 09:52:05,224: t15.2023.08.25 val PER: 0.0708
2025-12-08 09:52:05,224: t15.2023.08.27 val PER: 0.1399
2025-12-08 09:52:05,224: t15.2023.09.01 val PER: 0.0414
2025-12-08 09:52:05,224: t15.2023.09.03 val PER: 0.1069
2025-12-08 09:52:05,224: t15.2023.09.24 val PER: 0.0850
2025-12-08 09:52:05,225: t15.2023.09.29 val PER: 0.1015
2025-12-08 09:52:05,225: t15.2023.10.01 val PER: 0.1387
2025-12-08 09:52:05,225: t15.2023.10.06 val PER: 0.0635
2025-12-08 09:52:05,225: t15.2023.10.08 val PER: 0.1705
2025-12-08 09:52:05,225: t15.2023.10.13 val PER: 0.1482
2025-12-08 09:52:05,225: t15.2023.10.15 val PER: 0.1022
2025-12-08 09:52:05,225: t15.2023.10.20 val PER: 0.1711
2025-12-08 09:52:05,225: t15.2023.10.22 val PER: 0.0902
2025-12-08 09:52:05,225: t15.2023.11.03 val PER: 0.1398
2025-12-08 09:52:05,225: t15.2023.11.04 val PER: 0.0171
2025-12-08 09:52:05,225: t15.2023.11.17 val PER: 0.0156
2025-12-08 09:52:05,225: t15.2023.11.19 val PER: 0.0220
2025-12-08 09:52:05,225: t15.2023.11.26 val PER: 0.0486
2025-12-08 09:52:05,225: t15.2023.12.03 val PER: 0.0504
2025-12-08 09:52:05,225: t15.2023.12.08 val PER: 0.0346
2025-12-08 09:52:05,225: t15.2023.12.10 val PER: 0.0342
2025-12-08 09:52:05,225: t15.2023.12.17 val PER: 0.0748
2025-12-08 09:52:05,225: t15.2023.12.29 val PER: 0.0686
2025-12-08 09:52:05,225: t15.2024.02.25 val PER: 0.0646
2025-12-08 09:52:05,226: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:52:05,226: t15.2024.03.08 val PER: 0.1550
2025-12-08 09:52:05,226: t15.2024.03.15 val PER: 0.1689
2025-12-08 09:52:05,226: t15.2024.03.17 val PER: 0.0816
2025-12-08 09:52:05,226: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:52:05,226: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:52:05,226: t15.2024.05.10 val PER: 0.1189
2025-12-08 09:52:05,226: t15.2024.06.14 val PER: 0.1183
2025-12-08 09:52:05,226: t15.2024.07.19 val PER: 0.1589
2025-12-08 09:52:05,226: t15.2024.07.21 val PER: 0.0586
2025-12-08 09:52:05,226: t15.2024.07.28 val PER: 0.0868
2025-12-08 09:52:05,226: t15.2025.01.10 val PER: 0.2534
2025-12-08 09:52:05,226: t15.2025.01.12 val PER: 0.0855
2025-12-08 09:52:05,226: t15.2025.03.14 val PER: 0.3003
2025-12-08 09:52:05,226: t15.2025.03.16 val PER: 0.1414
2025-12-08 09:52:05,226: t15.2025.03.30 val PER: 0.2299
2025-12-08 09:52:05,226: t15.2025.04.13 val PER: 0.1954
2025-12-08 09:52:05,226: New best test PER 0.1033 --> 0.1029
2025-12-08 09:52:05,226: Checkpointing model
2025-12-08 09:52:06,450: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 09:52:33,899: Train batch 78200: loss: 0.00 grad norm: 0.57 time: 0.095
2025-12-08 09:53:01,894: Train batch 78400: loss: 0.10 grad norm: 3.91 time: 0.119
2025-12-08 09:53:30,073: Train batch 78600: loss: 0.01 grad norm: 0.57 time: 0.098
2025-12-08 09:53:58,565: Train batch 78800: loss: 0.01 grad norm: 0.78 time: 0.090
2025-12-08 09:54:26,425: Train batch 79000: loss: 0.01 grad norm: 1.24 time: 0.104
2025-12-08 09:54:54,314: Train batch 79200: loss: 0.01 grad norm: 0.30 time: 0.123
2025-12-08 09:55:21,871: Train batch 79400: loss: 0.01 grad norm: 0.19 time: 0.136
2025-12-08 09:55:48,833: Train batch 79600: loss: 0.02 grad norm: 0.98 time: 0.097
2025-12-08 09:56:16,680: Train batch 79800: loss: 0.01 grad norm: 0.56 time: 0.136
2025-12-08 09:56:44,479: Train batch 80000: loss: 0.02 grad norm: 1.87 time: 0.135
2025-12-08 09:56:44,479: Running test after training batch: 80000
2025-12-08 09:56:55,240: Val batch 80000: PER (avg): 0.1015 CTC Loss (avg): 22.6854 time: 10.760
2025-12-08 09:56:55,241: t15.2023.08.11 val PER: 1.0000
2025-12-08 09:56:55,241: t15.2023.08.13 val PER: 0.0769
2025-12-08 09:56:55,241: t15.2023.08.18 val PER: 0.0587
2025-12-08 09:56:55,241: t15.2023.08.20 val PER: 0.0485
2025-12-08 09:56:55,241: t15.2023.08.25 val PER: 0.0753
2025-12-08 09:56:55,241: t15.2023.08.27 val PER: 0.1383
2025-12-08 09:56:55,241: t15.2023.09.01 val PER: 0.0398
2025-12-08 09:56:55,241: t15.2023.09.03 val PER: 0.1033
2025-12-08 09:56:55,241: t15.2023.09.24 val PER: 0.0825
2025-12-08 09:56:55,241: t15.2023.09.29 val PER: 0.0964
2025-12-08 09:56:55,241: t15.2023.10.01 val PER: 0.1321
2025-12-08 09:56:55,241: t15.2023.10.06 val PER: 0.0657
2025-12-08 09:56:55,241: t15.2023.10.08 val PER: 0.1597
2025-12-08 09:56:55,241: t15.2023.10.13 val PER: 0.1482
2025-12-08 09:56:55,241: t15.2023.10.15 val PER: 0.1042
2025-12-08 09:56:55,242: t15.2023.10.20 val PER: 0.1544
2025-12-08 09:56:55,242: t15.2023.10.22 val PER: 0.0913
2025-12-08 09:56:55,242: t15.2023.11.03 val PER: 0.1398
2025-12-08 09:56:55,242: t15.2023.11.04 val PER: 0.0137
2025-12-08 09:56:55,242: t15.2023.11.17 val PER: 0.0218
2025-12-08 09:56:55,242: t15.2023.11.19 val PER: 0.0200
2025-12-08 09:56:55,242: t15.2023.11.26 val PER: 0.0514
2025-12-08 09:56:55,242: t15.2023.12.03 val PER: 0.0515
2025-12-08 09:56:55,242: t15.2023.12.08 val PER: 0.0346
2025-12-08 09:56:55,242: t15.2023.12.10 val PER: 0.0342
2025-12-08 09:56:55,242: t15.2023.12.17 val PER: 0.0728
2025-12-08 09:56:55,242: t15.2023.12.29 val PER: 0.0645
2025-12-08 09:56:55,242: t15.2024.02.25 val PER: 0.0590
2025-12-08 09:56:55,242: t15.2024.03.03 val PER: 1.0000
2025-12-08 09:56:55,242: t15.2024.03.08 val PER: 0.1508
2025-12-08 09:56:55,242: t15.2024.03.15 val PER: 0.1645
2025-12-08 09:56:55,242: t15.2024.03.17 val PER: 0.0837
2025-12-08 09:56:55,242: t15.2024.04.25 val PER: 1.0000
2025-12-08 09:56:55,242: t15.2024.04.28 val PER: 1.0000
2025-12-08 09:56:55,243: t15.2024.05.10 val PER: 0.1114
2025-12-08 09:56:55,243: t15.2024.06.14 val PER: 0.1151
2025-12-08 09:56:55,243: t15.2024.07.19 val PER: 0.1595
2025-12-08 09:56:55,243: t15.2024.07.21 val PER: 0.0545
2025-12-08 09:56:55,243: t15.2024.07.28 val PER: 0.0838
2025-12-08 09:56:55,243: t15.2025.01.10 val PER: 0.2438
2025-12-08 09:56:55,243: t15.2025.01.12 val PER: 0.0893
2025-12-08 09:56:55,243: t15.2025.03.14 val PER: 0.3092
2025-12-08 09:56:55,243: t15.2025.03.16 val PER: 0.1361
2025-12-08 09:56:55,243: t15.2025.03.30 val PER: 0.2207
2025-12-08 09:56:55,243: t15.2025.04.13 val PER: 0.2011
2025-12-08 09:56:55,243: New best test PER 0.1029 --> 0.1015
2025-12-08 09:56:55,243: Checkpointing model
2025-12-08 09:56:56,576: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 09:57:24,681: Train batch 80200: loss: 0.07 grad norm: 6.17 time: 0.155
2025-12-08 09:57:53,190: Train batch 80400: loss: 0.02 grad norm: 1.30 time: 0.110
2025-12-08 09:58:20,626: Train batch 80600: loss: 0.01 grad norm: 0.51 time: 0.102
2025-12-08 09:58:48,982: Train batch 80800: loss: 0.07 grad norm: 14.31 time: 0.100
2025-12-08 09:59:16,866: Train batch 81000: loss: 0.04 grad norm: 2.42 time: 0.097
2025-12-08 09:59:44,275: Train batch 81200: loss: 0.02 grad norm: 2.07 time: 0.171
2025-12-08 10:00:12,030: Train batch 81400: loss: 0.02 grad norm: 1.70 time: 0.141
2025-12-08 10:00:40,272: Train batch 81600: loss: 0.03 grad norm: 1.73 time: 0.144
2025-12-08 10:01:07,720: Train batch 81800: loss: 0.01 grad norm: 0.40 time: 0.170
2025-12-08 10:01:36,018: Train batch 82000: loss: 0.01 grad norm: 0.69 time: 0.146
2025-12-08 10:01:36,018: Running test after training batch: 82000
2025-12-08 10:01:46,637: Val batch 82000: PER (avg): 0.1031 CTC Loss (avg): 22.8033 time: 10.618
2025-12-08 10:01:46,637: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:01:46,637: t15.2023.08.13 val PER: 0.0738
2025-12-08 10:01:46,637: t15.2023.08.18 val PER: 0.0612
2025-12-08 10:01:46,637: t15.2023.08.20 val PER: 0.0477
2025-12-08 10:01:46,637: t15.2023.08.25 val PER: 0.0798
2025-12-08 10:01:46,637: t15.2023.08.27 val PER: 0.1479
2025-12-08 10:01:46,637: t15.2023.09.01 val PER: 0.0422
2025-12-08 10:01:46,638: t15.2023.09.03 val PER: 0.1105
2025-12-08 10:01:46,638: t15.2023.09.24 val PER: 0.0728
2025-12-08 10:01:46,638: t15.2023.09.29 val PER: 0.1015
2025-12-08 10:01:46,638: t15.2023.10.01 val PER: 0.1334
2025-12-08 10:01:46,638: t15.2023.10.06 val PER: 0.0624
2025-12-08 10:01:46,638: t15.2023.10.08 val PER: 0.1773
2025-12-08 10:01:46,638: t15.2023.10.13 val PER: 0.1490
2025-12-08 10:01:46,638: t15.2023.10.15 val PER: 0.1028
2025-12-08 10:01:46,638: t15.2023.10.20 val PER: 0.1611
2025-12-08 10:01:46,638: t15.2023.10.22 val PER: 0.0880
2025-12-08 10:01:46,638: t15.2023.11.03 val PER: 0.1377
2025-12-08 10:01:46,638: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:01:46,638: t15.2023.11.17 val PER: 0.0202
2025-12-08 10:01:46,638: t15.2023.11.19 val PER: 0.0200
2025-12-08 10:01:46,638: t15.2023.11.26 val PER: 0.0536
2025-12-08 10:01:46,638: t15.2023.12.03 val PER: 0.0504
2025-12-08 10:01:46,638: t15.2023.12.08 val PER: 0.0366
2025-12-08 10:01:46,638: t15.2023.12.10 val PER: 0.0355
2025-12-08 10:01:46,639: t15.2023.12.17 val PER: 0.0759
2025-12-08 10:01:46,639: t15.2023.12.29 val PER: 0.0645
2025-12-08 10:01:46,639: t15.2024.02.25 val PER: 0.0688
2025-12-08 10:01:46,639: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:01:46,639: t15.2024.03.08 val PER: 0.1494
2025-12-08 10:01:46,639: t15.2024.03.15 val PER: 0.1632
2025-12-08 10:01:46,639: t15.2024.03.17 val PER: 0.0844
2025-12-08 10:01:46,639: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:01:46,639: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:01:46,639: t15.2024.05.10 val PER: 0.1085
2025-12-08 10:01:46,639: t15.2024.06.14 val PER: 0.1167
2025-12-08 10:01:46,639: t15.2024.07.19 val PER: 0.1641
2025-12-08 10:01:46,639: t15.2024.07.21 val PER: 0.0586
2025-12-08 10:01:46,639: t15.2024.07.28 val PER: 0.0846
2025-12-08 10:01:46,639: t15.2025.01.10 val PER: 0.2452
2025-12-08 10:01:46,639: t15.2025.01.12 val PER: 0.0947
2025-12-08 10:01:46,639: t15.2025.03.14 val PER: 0.3062
2025-12-08 10:01:46,639: t15.2025.03.16 val PER: 0.1466
2025-12-08 10:01:46,639: t15.2025.03.30 val PER: 0.2310
2025-12-08 10:01:46,639: t15.2025.04.13 val PER: 0.1940
2025-12-08 10:02:14,250: Train batch 82200: loss: 0.02 grad norm: 0.87 time: 0.128
2025-12-08 10:02:42,334: Train batch 82400: loss: 0.02 grad norm: 14.64 time: 0.096
2025-12-08 10:03:10,105: Train batch 82600: loss: 0.07 grad norm: 4.19 time: 0.142
2025-12-08 10:03:38,002: Train batch 82800: loss: 0.00 grad norm: 0.16 time: 0.163
2025-12-08 10:04:05,856: Train batch 83000: loss: 0.01 grad norm: 0.40 time: 0.105
2025-12-08 10:04:34,147: Train batch 83200: loss: 0.01 grad norm: 0.26 time: 0.177
2025-12-08 10:05:02,081: Train batch 83400: loss: 0.02 grad norm: 1.20 time: 0.108
2025-12-08 10:05:29,691: Train batch 83600: loss: 0.04 grad norm: 2.84 time: 0.106
2025-12-08 10:05:57,547: Train batch 83800: loss: 0.01 grad norm: 0.62 time: 0.138
2025-12-08 10:06:24,423: Train batch 84000: loss: 0.01 grad norm: 0.93 time: 0.149
2025-12-08 10:06:24,424: Running test after training batch: 84000
2025-12-08 10:06:34,803: Val batch 84000: PER (avg): 0.1026 CTC Loss (avg): 22.6768 time: 10.379
2025-12-08 10:06:34,803: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:06:34,803: t15.2023.08.13 val PER: 0.0707
2025-12-08 10:06:34,804: t15.2023.08.18 val PER: 0.0629
2025-12-08 10:06:34,804: t15.2023.08.20 val PER: 0.0508
2025-12-08 10:06:34,804: t15.2023.08.25 val PER: 0.0738
2025-12-08 10:06:34,804: t15.2023.08.27 val PER: 0.1447
2025-12-08 10:06:34,804: t15.2023.09.01 val PER: 0.0381
2025-12-08 10:06:34,804: t15.2023.09.03 val PER: 0.1069
2025-12-08 10:06:34,804: t15.2023.09.24 val PER: 0.0801
2025-12-08 10:06:34,804: t15.2023.09.29 val PER: 0.1034
2025-12-08 10:06:34,804: t15.2023.10.01 val PER: 0.1341
2025-12-08 10:06:34,804: t15.2023.10.06 val PER: 0.0635
2025-12-08 10:06:34,804: t15.2023.10.08 val PER: 0.1691
2025-12-08 10:06:34,804: t15.2023.10.13 val PER: 0.1427
2025-12-08 10:06:34,804: t15.2023.10.15 val PER: 0.1094
2025-12-08 10:06:34,804: t15.2023.10.20 val PER: 0.1577
2025-12-08 10:06:34,804: t15.2023.10.22 val PER: 0.0902
2025-12-08 10:06:34,804: t15.2023.11.03 val PER: 0.1404
2025-12-08 10:06:34,804: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:06:34,804: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:06:34,804: t15.2023.11.19 val PER: 0.0220
2025-12-08 10:06:34,805: t15.2023.11.26 val PER: 0.0507
2025-12-08 10:06:34,805: t15.2023.12.03 val PER: 0.0536
2025-12-08 10:06:34,805: t15.2023.12.08 val PER: 0.0386
2025-12-08 10:06:34,805: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:06:34,805: t15.2023.12.17 val PER: 0.0759
2025-12-08 10:06:34,805: t15.2023.12.29 val PER: 0.0666
2025-12-08 10:06:34,805: t15.2024.02.25 val PER: 0.0674
2025-12-08 10:06:34,805: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:06:34,805: t15.2024.03.08 val PER: 0.1550
2025-12-08 10:06:34,805: t15.2024.03.15 val PER: 0.1664
2025-12-08 10:06:34,805: t15.2024.03.17 val PER: 0.0844
2025-12-08 10:06:34,805: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:06:34,805: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:06:34,805: t15.2024.05.10 val PER: 0.1159
2025-12-08 10:06:34,805: t15.2024.06.14 val PER: 0.1183
2025-12-08 10:06:34,805: t15.2024.07.19 val PER: 0.1589
2025-12-08 10:06:34,805: t15.2024.07.21 val PER: 0.0517
2025-12-08 10:06:34,805: t15.2024.07.28 val PER: 0.0838
2025-12-08 10:06:34,805: t15.2025.01.10 val PER: 0.2493
2025-12-08 10:06:34,805: t15.2025.01.12 val PER: 0.0878
2025-12-08 10:06:34,806: t15.2025.03.14 val PER: 0.2973
2025-12-08 10:06:34,806: t15.2025.03.16 val PER: 0.1453
2025-12-08 10:06:34,806: t15.2025.03.30 val PER: 0.2287
2025-12-08 10:06:34,806: t15.2025.04.13 val PER: 0.1869
2025-12-08 10:07:02,129: Train batch 84200: loss: 0.10 grad norm: 2.68 time: 0.139
2025-12-08 10:07:29,695: Train batch 84400: loss: 0.01 grad norm: 0.47 time: 0.133
2025-12-08 10:07:57,814: Train batch 84600: loss: 0.02 grad norm: 1.09 time: 0.106
2025-12-08 10:08:26,562: Train batch 84800: loss: 0.01 grad norm: 0.97 time: 0.154
2025-12-08 10:08:54,318: Train batch 85000: loss: 0.00 grad norm: 0.18 time: 0.129
2025-12-08 10:09:22,141: Train batch 85200: loss: 0.01 grad norm: 0.34 time: 0.093
2025-12-08 10:09:50,758: Train batch 85400: loss: 0.02 grad norm: 0.88 time: 0.147
2025-12-08 10:10:19,974: Train batch 85600: loss: 0.03 grad norm: 10.31 time: 0.140
2025-12-08 10:10:48,506: Train batch 85800: loss: 0.00 grad norm: 0.32 time: 0.091
2025-12-08 10:11:16,091: Train batch 86000: loss: 0.01 grad norm: 0.25 time: 0.147
2025-12-08 10:11:16,091: Running test after training batch: 86000
2025-12-08 10:11:26,542: Val batch 86000: PER (avg): 0.1014 CTC Loss (avg): 22.8317 time: 10.451
2025-12-08 10:11:26,543: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:11:26,544: t15.2023.08.13 val PER: 0.0728
2025-12-08 10:11:26,544: t15.2023.08.18 val PER: 0.0595
2025-12-08 10:11:26,544: t15.2023.08.20 val PER: 0.0477
2025-12-08 10:11:26,544: t15.2023.08.25 val PER: 0.0738
2025-12-08 10:11:26,544: t15.2023.08.27 val PER: 0.1399
2025-12-08 10:11:26,544: t15.2023.09.01 val PER: 0.0398
2025-12-08 10:11:26,544: t15.2023.09.03 val PER: 0.1021
2025-12-08 10:11:26,544: t15.2023.09.24 val PER: 0.0740
2025-12-08 10:11:26,544: t15.2023.09.29 val PER: 0.1034
2025-12-08 10:11:26,544: t15.2023.10.01 val PER: 0.1341
2025-12-08 10:11:26,544: t15.2023.10.06 val PER: 0.0635
2025-12-08 10:11:26,544: t15.2023.10.08 val PER: 0.1637
2025-12-08 10:11:26,544: t15.2023.10.13 val PER: 0.1412
2025-12-08 10:11:26,544: t15.2023.10.15 val PER: 0.1022
2025-12-08 10:11:26,544: t15.2023.10.20 val PER: 0.1577
2025-12-08 10:11:26,544: t15.2023.10.22 val PER: 0.0902
2025-12-08 10:11:26,544: t15.2023.11.03 val PER: 0.1350
2025-12-08 10:11:26,544: t15.2023.11.04 val PER: 0.0137
2025-12-08 10:11:26,544: t15.2023.11.17 val PER: 0.0171
2025-12-08 10:11:26,545: t15.2023.11.19 val PER: 0.0200
2025-12-08 10:11:26,545: t15.2023.11.26 val PER: 0.0529
2025-12-08 10:11:26,545: t15.2023.12.03 val PER: 0.0525
2025-12-08 10:11:26,545: t15.2023.12.08 val PER: 0.0360
2025-12-08 10:11:26,545: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:11:26,545: t15.2023.12.17 val PER: 0.0759
2025-12-08 10:11:26,545: t15.2023.12.29 val PER: 0.0631
2025-12-08 10:11:26,545: t15.2024.02.25 val PER: 0.0646
2025-12-08 10:11:26,545: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:11:26,545: t15.2024.03.08 val PER: 0.1522
2025-12-08 10:11:26,545: t15.2024.03.15 val PER: 0.1595
2025-12-08 10:11:26,545: t15.2024.03.17 val PER: 0.0844
2025-12-08 10:11:26,545: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:11:26,545: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:11:26,545: t15.2024.05.10 val PER: 0.1070
2025-12-08 10:11:26,545: t15.2024.06.14 val PER: 0.1199
2025-12-08 10:11:26,545: t15.2024.07.19 val PER: 0.1575
2025-12-08 10:11:26,545: t15.2024.07.21 val PER: 0.0497
2025-12-08 10:11:26,545: t15.2024.07.28 val PER: 0.0816
2025-12-08 10:11:26,546: t15.2025.01.10 val PER: 0.2548
2025-12-08 10:11:26,546: t15.2025.01.12 val PER: 0.0916
2025-12-08 10:11:26,546: t15.2025.03.14 val PER: 0.2959
2025-12-08 10:11:26,546: t15.2025.03.16 val PER: 0.1492
2025-12-08 10:11:26,546: t15.2025.03.30 val PER: 0.2333
2025-12-08 10:11:26,546: t15.2025.04.13 val PER: 0.1983
2025-12-08 10:11:26,546: New best test PER 0.1015 --> 0.1014
2025-12-08 10:11:26,546: Checkpointing model
2025-12-08 10:11:27,855: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 10:11:55,640: Train batch 86200: loss: 0.04 grad norm: 2.98 time: 0.168
2025-12-08 10:12:22,957: Train batch 86400: loss: 0.01 grad norm: 1.11 time: 0.091
2025-12-08 10:12:49,795: Train batch 86600: loss: 0.03 grad norm: 28.72 time: 0.117
2025-12-08 10:13:16,847: Train batch 86800: loss: 0.01 grad norm: 0.46 time: 0.207
2025-12-08 10:13:44,606: Train batch 87000: loss: 0.01 grad norm: 0.94 time: 0.109
2025-12-08 10:14:13,043: Train batch 87200: loss: 0.01 grad norm: 0.67 time: 0.156
2025-12-08 10:14:40,390: Train batch 87400: loss: 0.04 grad norm: 2.95 time: 0.144
2025-12-08 10:15:07,484: Train batch 87600: loss: 0.01 grad norm: 1.15 time: 0.121
2025-12-08 10:15:36,126: Train batch 87800: loss: 0.01 grad norm: 0.92 time: 0.144
2025-12-08 10:16:03,988: Train batch 88000: loss: 0.00 grad norm: 0.27 time: 0.138
2025-12-08 10:16:03,988: Running test after training batch: 88000
2025-12-08 10:16:14,682: Val batch 88000: PER (avg): 0.1015 CTC Loss (avg): 22.8522 time: 10.694
2025-12-08 10:16:14,683: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:16:14,683: t15.2023.08.13 val PER: 0.0686
2025-12-08 10:16:14,683: t15.2023.08.18 val PER: 0.0612
2025-12-08 10:16:14,683: t15.2023.08.20 val PER: 0.0492
2025-12-08 10:16:14,683: t15.2023.08.25 val PER: 0.0768
2025-12-08 10:16:14,683: t15.2023.08.27 val PER: 0.1399
2025-12-08 10:16:14,683: t15.2023.09.01 val PER: 0.0398
2025-12-08 10:16:14,683: t15.2023.09.03 val PER: 0.1081
2025-12-08 10:16:14,683: t15.2023.09.24 val PER: 0.0813
2025-12-08 10:16:14,683: t15.2023.09.29 val PER: 0.1008
2025-12-08 10:16:14,683: t15.2023.10.01 val PER: 0.1367
2025-12-08 10:16:14,683: t15.2023.10.06 val PER: 0.0614
2025-12-08 10:16:14,683: t15.2023.10.08 val PER: 0.1610
2025-12-08 10:16:14,683: t15.2023.10.13 val PER: 0.1412
2025-12-08 10:16:14,683: t15.2023.10.15 val PER: 0.1022
2025-12-08 10:16:14,683: t15.2023.10.20 val PER: 0.1544
2025-12-08 10:16:14,683: t15.2023.10.22 val PER: 0.0902
2025-12-08 10:16:14,684: t15.2023.11.03 val PER: 0.1384
2025-12-08 10:16:14,684: t15.2023.11.04 val PER: 0.0102
2025-12-08 10:16:14,684: t15.2023.11.17 val PER: 0.0171
2025-12-08 10:16:14,684: t15.2023.11.19 val PER: 0.0200
2025-12-08 10:16:14,684: t15.2023.11.26 val PER: 0.0500
2025-12-08 10:16:14,684: t15.2023.12.03 val PER: 0.0515
2025-12-08 10:16:14,684: t15.2023.12.08 val PER: 0.0373
2025-12-08 10:16:14,684: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:16:14,684: t15.2023.12.17 val PER: 0.0696
2025-12-08 10:16:14,684: t15.2023.12.29 val PER: 0.0597
2025-12-08 10:16:14,684: t15.2024.02.25 val PER: 0.0632
2025-12-08 10:16:14,684: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:16:14,684: t15.2024.03.08 val PER: 0.1465
2025-12-08 10:16:14,684: t15.2024.03.15 val PER: 0.1645
2025-12-08 10:16:14,684: t15.2024.03.17 val PER: 0.0879
2025-12-08 10:16:14,684: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:16:14,684: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:16:14,684: t15.2024.05.10 val PER: 0.1055
2025-12-08 10:16:14,684: t15.2024.06.14 val PER: 0.1167
2025-12-08 10:16:14,685: t15.2024.07.19 val PER: 0.1569
2025-12-08 10:16:14,685: t15.2024.07.21 val PER: 0.0531
2025-12-08 10:16:14,685: t15.2024.07.28 val PER: 0.0882
2025-12-08 10:16:14,685: t15.2025.01.10 val PER: 0.2466
2025-12-08 10:16:14,685: t15.2025.01.12 val PER: 0.0970
2025-12-08 10:16:14,685: t15.2025.03.14 val PER: 0.2899
2025-12-08 10:16:14,685: t15.2025.03.16 val PER: 0.1414
2025-12-08 10:16:14,685: t15.2025.03.30 val PER: 0.2322
2025-12-08 10:16:14,685: t15.2025.04.13 val PER: 0.1969
2025-12-08 10:16:41,616: Train batch 88200: loss: 0.01 grad norm: 1.03 time: 0.091
2025-12-08 10:17:09,634: Train batch 88400: loss: 0.02 grad norm: 2.50 time: 0.096
2025-12-08 10:17:38,090: Train batch 88600: loss: 0.02 grad norm: 1.62 time: 0.248
2025-12-08 10:18:05,467: Train batch 88800: loss: 0.00 grad norm: 0.25 time: 0.078
2025-12-08 10:18:34,389: Train batch 89000: loss: 0.00 grad norm: 0.53 time: 0.152
2025-12-08 10:19:01,129: Train batch 89200: loss: 0.01 grad norm: 0.50 time: 0.222
2025-12-08 10:19:28,848: Train batch 89400: loss: 0.01 grad norm: 0.61 time: 0.092
2025-12-08 10:19:56,587: Train batch 89600: loss: 0.01 grad norm: 0.43 time: 0.090
2025-12-08 10:20:24,662: Train batch 89800: loss: 0.03 grad norm: 5.27 time: 0.134
2025-12-08 10:20:52,150: Train batch 90000: loss: 0.00 grad norm: 0.11 time: 0.126
2025-12-08 10:20:52,150: Running test after training batch: 90000
2025-12-08 10:21:02,553: Val batch 90000: PER (avg): 0.1011 CTC Loss (avg): 22.3534 time: 10.403
2025-12-08 10:21:02,554: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:21:02,554: t15.2023.08.13 val PER: 0.0707
2025-12-08 10:21:02,554: t15.2023.08.18 val PER: 0.0620
2025-12-08 10:21:02,554: t15.2023.08.20 val PER: 0.0500
2025-12-08 10:21:02,554: t15.2023.08.25 val PER: 0.0768
2025-12-08 10:21:02,554: t15.2023.08.27 val PER: 0.1367
2025-12-08 10:21:02,554: t15.2023.09.01 val PER: 0.0373
2025-12-08 10:21:02,554: t15.2023.09.03 val PER: 0.1081
2025-12-08 10:21:02,554: t15.2023.09.24 val PER: 0.0801
2025-12-08 10:21:02,554: t15.2023.09.29 val PER: 0.1015
2025-12-08 10:21:02,554: t15.2023.10.01 val PER: 0.1347
2025-12-08 10:21:02,555: t15.2023.10.06 val PER: 0.0667
2025-12-08 10:21:02,555: t15.2023.10.08 val PER: 0.1678
2025-12-08 10:21:02,555: t15.2023.10.13 val PER: 0.1458
2025-12-08 10:21:02,555: t15.2023.10.15 val PER: 0.1015
2025-12-08 10:21:02,555: t15.2023.10.20 val PER: 0.1510
2025-12-08 10:21:02,555: t15.2023.10.22 val PER: 0.0857
2025-12-08 10:21:02,555: t15.2023.11.03 val PER: 0.1330
2025-12-08 10:21:02,555: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:21:02,555: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:21:02,555: t15.2023.11.19 val PER: 0.0200
2025-12-08 10:21:02,555: t15.2023.11.26 val PER: 0.0543
2025-12-08 10:21:02,555: t15.2023.12.03 val PER: 0.0536
2025-12-08 10:21:02,555: t15.2023.12.08 val PER: 0.0373
2025-12-08 10:21:02,555: t15.2023.12.10 val PER: 0.0407
2025-12-08 10:21:02,555: t15.2023.12.17 val PER: 0.0717
2025-12-08 10:21:02,555: t15.2023.12.29 val PER: 0.0563
2025-12-08 10:21:02,556: t15.2024.02.25 val PER: 0.0632
2025-12-08 10:21:02,556: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:21:02,556: t15.2024.03.08 val PER: 0.1479
2025-12-08 10:21:02,556: t15.2024.03.15 val PER: 0.1639
2025-12-08 10:21:02,556: t15.2024.03.17 val PER: 0.0844
2025-12-08 10:21:02,556: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:21:02,556: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:21:02,556: t15.2024.05.10 val PER: 0.1025
2025-12-08 10:21:02,556: t15.2024.06.14 val PER: 0.1230
2025-12-08 10:21:02,556: t15.2024.07.19 val PER: 0.1582
2025-12-08 10:21:02,556: t15.2024.07.21 val PER: 0.0517
2025-12-08 10:21:02,556: t15.2024.07.28 val PER: 0.0860
2025-12-08 10:21:02,556: t15.2025.01.10 val PER: 0.2438
2025-12-08 10:21:02,556: t15.2025.01.12 val PER: 0.0955
2025-12-08 10:21:02,556: t15.2025.03.14 val PER: 0.2885
2025-12-08 10:21:02,556: t15.2025.03.16 val PER: 0.1374
2025-12-08 10:21:02,556: t15.2025.03.30 val PER: 0.2322
2025-12-08 10:21:02,557: t15.2025.04.13 val PER: 0.1883
2025-12-08 10:21:02,557: New best test PER 0.1014 --> 0.1011
2025-12-08 10:21:02,557: Checkpointing model
2025-12-08 10:21:03,614: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 10:21:30,788: Train batch 90200: loss: 0.01 grad norm: 0.61 time: 0.130
2025-12-08 10:21:58,435: Train batch 90400: loss: 0.04 grad norm: 1.84 time: 0.132
2025-12-08 10:22:25,794: Train batch 90600: loss: 0.04 grad norm: 3.80 time: 0.100
2025-12-08 10:22:53,643: Train batch 90800: loss: 0.05 grad norm: 3.76 time: 0.147
2025-12-08 10:23:21,215: Train batch 91000: loss: 0.01 grad norm: 1.28 time: 0.122
2025-12-08 10:23:48,903: Train batch 91200: loss: 0.05 grad norm: 3.71 time: 0.199
2025-12-08 10:24:17,169: Train batch 91400: loss: 0.07 grad norm: 3.67 time: 0.176
2025-12-08 10:24:44,979: Train batch 91600: loss: 0.00 grad norm: 0.09 time: 0.171
2025-12-08 10:25:12,169: Train batch 91800: loss: 0.01 grad norm: 0.21 time: 0.079
2025-12-08 10:25:40,336: Train batch 92000: loss: 0.02 grad norm: 1.43 time: 0.118
2025-12-08 10:25:40,337: Running test after training batch: 92000
2025-12-08 10:25:50,744: Val batch 92000: PER (avg): 0.1015 CTC Loss (avg): 22.4952 time: 10.407
2025-12-08 10:25:50,744: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:25:50,744: t15.2023.08.13 val PER: 0.0707
2025-12-08 10:25:50,744: t15.2023.08.18 val PER: 0.0595
2025-12-08 10:25:50,744: t15.2023.08.20 val PER: 0.0477
2025-12-08 10:25:50,744: t15.2023.08.25 val PER: 0.0768
2025-12-08 10:25:50,744: t15.2023.08.27 val PER: 0.1367
2025-12-08 10:25:50,744: t15.2023.09.01 val PER: 0.0414
2025-12-08 10:25:50,744: t15.2023.09.03 val PER: 0.1093
2025-12-08 10:25:50,744: t15.2023.09.24 val PER: 0.0716
2025-12-08 10:25:50,744: t15.2023.09.29 val PER: 0.1027
2025-12-08 10:25:50,744: t15.2023.10.01 val PER: 0.1347
2025-12-08 10:25:50,745: t15.2023.10.06 val PER: 0.0624
2025-12-08 10:25:50,745: t15.2023.10.08 val PER: 0.1691
2025-12-08 10:25:50,745: t15.2023.10.13 val PER: 0.1443
2025-12-08 10:25:50,745: t15.2023.10.15 val PER: 0.1042
2025-12-08 10:25:50,745: t15.2023.10.20 val PER: 0.1544
2025-12-08 10:25:50,745: t15.2023.10.22 val PER: 0.0924
2025-12-08 10:25:50,745: t15.2023.11.03 val PER: 0.1398
2025-12-08 10:25:50,745: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:25:50,745: t15.2023.11.17 val PER: 0.0187
2025-12-08 10:25:50,745: t15.2023.11.19 val PER: 0.0200
2025-12-08 10:25:50,745: t15.2023.11.26 val PER: 0.0514
2025-12-08 10:25:50,745: t15.2023.12.03 val PER: 0.0525
2025-12-08 10:25:50,745: t15.2023.12.08 val PER: 0.0386
2025-12-08 10:25:50,745: t15.2023.12.10 val PER: 0.0355
2025-12-08 10:25:50,745: t15.2023.12.17 val PER: 0.0717
2025-12-08 10:25:50,745: t15.2023.12.29 val PER: 0.0631
2025-12-08 10:25:50,745: t15.2024.02.25 val PER: 0.0632
2025-12-08 10:25:50,745: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:25:50,745: t15.2024.03.08 val PER: 0.1508
2025-12-08 10:25:50,746: t15.2024.03.15 val PER: 0.1576
2025-12-08 10:25:50,746: t15.2024.03.17 val PER: 0.0858
2025-12-08 10:25:50,746: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:25:50,746: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:25:50,746: t15.2024.05.10 val PER: 0.1144
2025-12-08 10:25:50,746: t15.2024.06.14 val PER: 0.1136
2025-12-08 10:25:50,746: t15.2024.07.19 val PER: 0.1569
2025-12-08 10:25:50,746: t15.2024.07.21 val PER: 0.0524
2025-12-08 10:25:50,746: t15.2024.07.28 val PER: 0.0868
2025-12-08 10:25:50,746: t15.2025.01.10 val PER: 0.2369
2025-12-08 10:25:50,746: t15.2025.01.12 val PER: 0.0931
2025-12-08 10:25:50,746: t15.2025.03.14 val PER: 0.2988
2025-12-08 10:25:50,746: t15.2025.03.16 val PER: 0.1440
2025-12-08 10:25:50,746: t15.2025.03.30 val PER: 0.2333
2025-12-08 10:25:50,746: t15.2025.04.13 val PER: 0.1883
2025-12-08 10:26:17,882: Train batch 92200: loss: 0.03 grad norm: 1.85 time: 0.097
2025-12-08 10:26:45,516: Train batch 92400: loss: 0.01 grad norm: 1.20 time: 0.137
2025-12-08 10:27:12,444: Train batch 92600: loss: 0.01 grad norm: 0.71 time: 0.120
2025-12-08 10:27:40,648: Train batch 92800: loss: 0.05 grad norm: 3.88 time: 0.116
2025-12-08 10:28:08,155: Train batch 93000: loss: 0.01 grad norm: 1.41 time: 0.135
2025-12-08 10:28:36,394: Train batch 93200: loss: 0.02 grad norm: 1.07 time: 0.158
2025-12-08 10:29:04,459: Train batch 93400: loss: 0.01 grad norm: 0.31 time: 0.109
2025-12-08 10:29:32,514: Train batch 93600: loss: 0.01 grad norm: 0.63 time: 0.134
2025-12-08 10:30:00,598: Train batch 93800: loss: 0.03 grad norm: 2.68 time: 0.102
2025-12-08 10:30:28,371: Train batch 94000: loss: 0.09 grad norm: 5.24 time: 0.167
2025-12-08 10:30:28,372: Running test after training batch: 94000
2025-12-08 10:30:38,821: Val batch 94000: PER (avg): 0.1013 CTC Loss (avg): 22.4452 time: 10.449
2025-12-08 10:30:38,821: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:30:38,821: t15.2023.08.13 val PER: 0.0696
2025-12-08 10:30:38,821: t15.2023.08.18 val PER: 0.0604
2025-12-08 10:30:38,821: t15.2023.08.20 val PER: 0.0485
2025-12-08 10:30:38,821: t15.2023.08.25 val PER: 0.0738
2025-12-08 10:30:38,821: t15.2023.08.27 val PER: 0.1383
2025-12-08 10:30:38,822: t15.2023.09.01 val PER: 0.0373
2025-12-08 10:30:38,822: t15.2023.09.03 val PER: 0.1128
2025-12-08 10:30:38,822: t15.2023.09.24 val PER: 0.0716
2025-12-08 10:30:38,822: t15.2023.09.29 val PER: 0.1034
2025-12-08 10:30:38,822: t15.2023.10.01 val PER: 0.1361
2025-12-08 10:30:38,822: t15.2023.10.06 val PER: 0.0592
2025-12-08 10:30:38,822: t15.2023.10.08 val PER: 0.1624
2025-12-08 10:30:38,822: t15.2023.10.13 val PER: 0.1435
2025-12-08 10:30:38,822: t15.2023.10.15 val PER: 0.1042
2025-12-08 10:30:38,822: t15.2023.10.20 val PER: 0.1644
2025-12-08 10:30:38,822: t15.2023.10.22 val PER: 0.0880
2025-12-08 10:30:38,822: t15.2023.11.03 val PER: 0.1364
2025-12-08 10:30:38,822: t15.2023.11.04 val PER: 0.0137
2025-12-08 10:30:38,822: t15.2023.11.17 val PER: 0.0171
2025-12-08 10:30:38,822: t15.2023.11.19 val PER: 0.0220
2025-12-08 10:30:38,822: t15.2023.11.26 val PER: 0.0486
2025-12-08 10:30:38,823: t15.2023.12.03 val PER: 0.0494
2025-12-08 10:30:38,823: t15.2023.12.08 val PER: 0.0360
2025-12-08 10:30:38,823: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:30:38,823: t15.2023.12.17 val PER: 0.0686
2025-12-08 10:30:38,823: t15.2023.12.29 val PER: 0.0625
2025-12-08 10:30:38,823: t15.2024.02.25 val PER: 0.0674
2025-12-08 10:30:38,823: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:30:38,823: t15.2024.03.08 val PER: 0.1522
2025-12-08 10:30:38,823: t15.2024.03.15 val PER: 0.1576
2025-12-08 10:30:38,823: t15.2024.03.17 val PER: 0.0837
2025-12-08 10:30:38,823: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:30:38,823: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:30:38,823: t15.2024.05.10 val PER: 0.1114
2025-12-08 10:30:38,823: t15.2024.06.14 val PER: 0.1199
2025-12-08 10:30:38,823: t15.2024.07.19 val PER: 0.1569
2025-12-08 10:30:38,823: t15.2024.07.21 val PER: 0.0538
2025-12-08 10:30:38,824: t15.2024.07.28 val PER: 0.0860
2025-12-08 10:30:38,824: t15.2025.01.10 val PER: 0.2507
2025-12-08 10:30:38,824: t15.2025.01.12 val PER: 0.0885
2025-12-08 10:30:38,824: t15.2025.03.14 val PER: 0.2973
2025-12-08 10:30:38,824: t15.2025.03.16 val PER: 0.1466
2025-12-08 10:30:38,824: t15.2025.03.30 val PER: 0.2402
2025-12-08 10:30:38,824: t15.2025.04.13 val PER: 0.1926
2025-12-08 10:31:06,166: Train batch 94200: loss: 0.01 grad norm: 0.36 time: 0.108
2025-12-08 10:31:33,263: Train batch 94400: loss: 0.01 grad norm: 0.89 time: 0.140
2025-12-08 10:32:00,832: Train batch 94600: loss: 0.01 grad norm: 0.19 time: 0.118
2025-12-08 10:32:28,313: Train batch 94800: loss: 0.01 grad norm: 0.68 time: 0.167
2025-12-08 10:32:56,537: Train batch 95000: loss: 0.06 grad norm: 2.84 time: 0.102
2025-12-08 10:33:24,101: Train batch 95200: loss: 0.02 grad norm: 0.98 time: 0.097
2025-12-08 10:33:52,148: Train batch 95400: loss: 0.01 grad norm: 0.42 time: 0.138
2025-12-08 10:34:20,203: Train batch 95600: loss: 0.12 grad norm: 1.69 time: 0.114
2025-12-08 10:34:47,370: Train batch 95800: loss: 0.01 grad norm: 0.55 time: 0.124
2025-12-08 10:35:14,372: Train batch 96000: loss: 0.01 grad norm: 0.56 time: 0.147
2025-12-08 10:35:14,373: Running test after training batch: 96000
2025-12-08 10:35:24,915: Val batch 96000: PER (avg): 0.1010 CTC Loss (avg): 22.6913 time: 10.543
2025-12-08 10:35:24,916: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:35:24,916: t15.2023.08.13 val PER: 0.0665
2025-12-08 10:35:24,916: t15.2023.08.18 val PER: 0.0595
2025-12-08 10:35:24,916: t15.2023.08.20 val PER: 0.0461
2025-12-08 10:35:24,916: t15.2023.08.25 val PER: 0.0768
2025-12-08 10:35:24,916: t15.2023.08.27 val PER: 0.1431
2025-12-08 10:35:24,916: t15.2023.09.01 val PER: 0.0406
2025-12-08 10:35:24,916: t15.2023.09.03 val PER: 0.1140
2025-12-08 10:35:24,916: t15.2023.09.24 val PER: 0.0752
2025-12-08 10:35:24,916: t15.2023.09.29 val PER: 0.1027
2025-12-08 10:35:24,916: t15.2023.10.01 val PER: 0.1354
2025-12-08 10:35:24,916: t15.2023.10.06 val PER: 0.0624
2025-12-08 10:35:24,916: t15.2023.10.08 val PER: 0.1637
2025-12-08 10:35:24,916: t15.2023.10.13 val PER: 0.1443
2025-12-08 10:35:24,917: t15.2023.10.15 val PER: 0.1048
2025-12-08 10:35:24,917: t15.2023.10.20 val PER: 0.1477
2025-12-08 10:35:24,917: t15.2023.10.22 val PER: 0.0913
2025-12-08 10:35:24,917: t15.2023.11.03 val PER: 0.1404
2025-12-08 10:35:24,917: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:35:24,917: t15.2023.11.17 val PER: 0.0171
2025-12-08 10:35:24,917: t15.2023.11.19 val PER: 0.0240
2025-12-08 10:35:24,917: t15.2023.11.26 val PER: 0.0507
2025-12-08 10:35:24,917: t15.2023.12.03 val PER: 0.0525
2025-12-08 10:35:24,917: t15.2023.12.08 val PER: 0.0379
2025-12-08 10:35:24,917: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:35:24,917: t15.2023.12.17 val PER: 0.0686
2025-12-08 10:35:24,917: t15.2023.12.29 val PER: 0.0604
2025-12-08 10:35:24,917: t15.2024.02.25 val PER: 0.0646
2025-12-08 10:35:24,917: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:35:24,917: t15.2024.03.08 val PER: 0.1565
2025-12-08 10:35:24,917: t15.2024.03.15 val PER: 0.1576
2025-12-08 10:35:24,917: t15.2024.03.17 val PER: 0.0837
2025-12-08 10:35:24,917: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:35:24,918: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:35:24,918: t15.2024.05.10 val PER: 0.1100
2025-12-08 10:35:24,918: t15.2024.06.14 val PER: 0.1215
2025-12-08 10:35:24,918: t15.2024.07.19 val PER: 0.1529
2025-12-08 10:35:24,918: t15.2024.07.21 val PER: 0.0524
2025-12-08 10:35:24,918: t15.2024.07.28 val PER: 0.0816
2025-12-08 10:35:24,918: t15.2025.01.10 val PER: 0.2493
2025-12-08 10:35:24,918: t15.2025.01.12 val PER: 0.0893
2025-12-08 10:35:24,918: t15.2025.03.14 val PER: 0.2944
2025-12-08 10:35:24,918: t15.2025.03.16 val PER: 0.1335
2025-12-08 10:35:24,918: t15.2025.03.30 val PER: 0.2333
2025-12-08 10:35:24,918: t15.2025.04.13 val PER: 0.1897
2025-12-08 10:35:24,918: New best test PER 0.1011 --> 0.1010
2025-12-08 10:35:24,918: Checkpointing model
2025-12-08 10:35:25,996: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 10:35:53,720: Train batch 96200: loss: 0.01 grad norm: 0.74 time: 0.164
2025-12-08 10:36:20,757: Train batch 96400: loss: 0.02 grad norm: 0.83 time: 0.104
2025-12-08 10:36:48,207: Train batch 96600: loss: 0.01 grad norm: 0.43 time: 0.156
2025-12-08 10:37:15,351: Train batch 96800: loss: 0.02 grad norm: 1.90 time: 0.113
2025-12-08 10:37:43,696: Train batch 97000: loss: 0.01 grad norm: 0.30 time: 0.115
2025-12-08 10:38:11,167: Train batch 97200: loss: 0.02 grad norm: 2.07 time: 0.162
2025-12-08 10:38:38,757: Train batch 97400: loss: 0.01 grad norm: 0.54 time: 0.146
2025-12-08 10:39:05,837: Train batch 97600: loss: 0.01 grad norm: 0.55 time: 0.129
2025-12-08 10:39:33,382: Train batch 97800: loss: 0.01 grad norm: 0.68 time: 0.121
2025-12-08 10:40:01,560: Train batch 98000: loss: 0.01 grad norm: 1.88 time: 0.145
2025-12-08 10:40:01,561: Running test after training batch: 98000
2025-12-08 10:40:11,744: Val batch 98000: PER (avg): 0.1016 CTC Loss (avg): 22.6888 time: 10.183
2025-12-08 10:40:11,744: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:40:11,744: t15.2023.08.13 val PER: 0.0717
2025-12-08 10:40:11,744: t15.2023.08.18 val PER: 0.0595
2025-12-08 10:40:11,745: t15.2023.08.20 val PER: 0.0469
2025-12-08 10:40:11,745: t15.2023.08.25 val PER: 0.0828
2025-12-08 10:40:11,745: t15.2023.08.27 val PER: 0.1431
2025-12-08 10:40:11,745: t15.2023.09.01 val PER: 0.0398
2025-12-08 10:40:11,745: t15.2023.09.03 val PER: 0.1116
2025-12-08 10:40:11,745: t15.2023.09.24 val PER: 0.0740
2025-12-08 10:40:11,745: t15.2023.09.29 val PER: 0.1027
2025-12-08 10:40:11,745: t15.2023.10.01 val PER: 0.1380
2025-12-08 10:40:11,745: t15.2023.10.06 val PER: 0.0614
2025-12-08 10:40:11,745: t15.2023.10.08 val PER: 0.1651
2025-12-08 10:40:11,745: t15.2023.10.13 val PER: 0.1427
2025-12-08 10:40:11,745: t15.2023.10.15 val PER: 0.1015
2025-12-08 10:40:11,745: t15.2023.10.20 val PER: 0.1510
2025-12-08 10:40:11,745: t15.2023.10.22 val PER: 0.0924
2025-12-08 10:40:11,745: t15.2023.11.03 val PER: 0.1377
2025-12-08 10:40:11,745: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:40:11,745: t15.2023.11.17 val PER: 0.0218
2025-12-08 10:40:11,745: t15.2023.11.19 val PER: 0.0240
2025-12-08 10:40:11,745: t15.2023.11.26 val PER: 0.0536
2025-12-08 10:40:11,746: t15.2023.12.03 val PER: 0.0515
2025-12-08 10:40:11,746: t15.2023.12.08 val PER: 0.0386
2025-12-08 10:40:11,746: t15.2023.12.10 val PER: 0.0381
2025-12-08 10:40:11,746: t15.2023.12.17 val PER: 0.0676
2025-12-08 10:40:11,746: t15.2023.12.29 val PER: 0.0638
2025-12-08 10:40:11,746: t15.2024.02.25 val PER: 0.0646
2025-12-08 10:40:11,746: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:40:11,746: t15.2024.03.08 val PER: 0.1536
2025-12-08 10:40:11,746: t15.2024.03.15 val PER: 0.1582
2025-12-08 10:40:11,746: t15.2024.03.17 val PER: 0.0900
2025-12-08 10:40:11,746: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:40:11,746: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:40:11,746: t15.2024.05.10 val PER: 0.1129
2025-12-08 10:40:11,746: t15.2024.06.14 val PER: 0.1183
2025-12-08 10:40:11,746: t15.2024.07.19 val PER: 0.1562
2025-12-08 10:40:11,746: t15.2024.07.21 val PER: 0.0531
2025-12-08 10:40:11,746: t15.2024.07.28 val PER: 0.0794
2025-12-08 10:40:11,746: t15.2025.01.10 val PER: 0.2493
2025-12-08 10:40:11,746: t15.2025.01.12 val PER: 0.0870
2025-12-08 10:40:11,746: t15.2025.03.14 val PER: 0.3033
2025-12-08 10:40:11,747: t15.2025.03.16 val PER: 0.1401
2025-12-08 10:40:11,747: t15.2025.03.30 val PER: 0.2287
2025-12-08 10:40:11,747: t15.2025.04.13 val PER: 0.1883
2025-12-08 10:40:38,914: Train batch 98200: loss: 0.01 grad norm: 1.56 time: 0.126
2025-12-08 10:41:06,752: Train batch 98400: loss: 0.05 grad norm: 4.26 time: 0.127
2025-12-08 10:41:34,406: Train batch 98600: loss: 0.01 grad norm: 0.41 time: 0.156
2025-12-08 10:42:01,868: Train batch 98800: loss: 0.02 grad norm: 5.31 time: 0.128
2025-12-08 10:42:29,550: Train batch 99000: loss: 0.00 grad norm: 0.55 time: 0.139
2025-12-08 10:42:57,463: Train batch 99200: loss: 0.02 grad norm: 0.78 time: 0.123
2025-12-08 10:43:24,534: Train batch 99400: loss: 0.01 grad norm: 0.53 time: 0.118
2025-12-08 10:43:51,778: Train batch 99600: loss: 0.01 grad norm: 0.25 time: 0.118
2025-12-08 10:44:19,311: Train batch 99800: loss: 0.01 grad norm: 0.26 time: 0.086
2025-12-08 10:44:48,235: Train batch 100000: loss: 0.01 grad norm: 0.73 time: 0.146
2025-12-08 10:44:48,235: Running test after training batch: 100000
2025-12-08 10:44:58,491: Val batch 100000: PER (avg): 0.1010 CTC Loss (avg): 22.5128 time: 10.256
2025-12-08 10:44:58,491: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:44:58,491: t15.2023.08.13 val PER: 0.0676
2025-12-08 10:44:58,491: t15.2023.08.18 val PER: 0.0587
2025-12-08 10:44:58,491: t15.2023.08.20 val PER: 0.0461
2025-12-08 10:44:58,491: t15.2023.08.25 val PER: 0.0783
2025-12-08 10:44:58,492: t15.2023.08.27 val PER: 0.1383
2025-12-08 10:44:58,492: t15.2023.09.01 val PER: 0.0390
2025-12-08 10:44:58,492: t15.2023.09.03 val PER: 0.1116
2025-12-08 10:44:58,492: t15.2023.09.24 val PER: 0.0728
2025-12-08 10:44:58,492: t15.2023.09.29 val PER: 0.1021
2025-12-08 10:44:58,492: t15.2023.10.01 val PER: 0.1413
2025-12-08 10:44:58,492: t15.2023.10.06 val PER: 0.0592
2025-12-08 10:44:58,492: t15.2023.10.08 val PER: 0.1651
2025-12-08 10:44:58,492: t15.2023.10.13 val PER: 0.1420
2025-12-08 10:44:58,492: t15.2023.10.15 val PER: 0.1009
2025-12-08 10:44:58,492: t15.2023.10.20 val PER: 0.1477
2025-12-08 10:44:58,492: t15.2023.10.22 val PER: 0.0913
2025-12-08 10:44:58,492: t15.2023.11.03 val PER: 0.1391
2025-12-08 10:44:58,492: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:44:58,492: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:44:58,492: t15.2023.11.19 val PER: 0.0259
2025-12-08 10:44:58,492: t15.2023.11.26 val PER: 0.0522
2025-12-08 10:44:58,492: t15.2023.12.03 val PER: 0.0515
2025-12-08 10:44:58,492: t15.2023.12.08 val PER: 0.0366
2025-12-08 10:44:58,493: t15.2023.12.10 val PER: 0.0394
2025-12-08 10:44:58,493: t15.2023.12.17 val PER: 0.0738
2025-12-08 10:44:58,493: t15.2023.12.29 val PER: 0.0604
2025-12-08 10:44:58,493: t15.2024.02.25 val PER: 0.0646
2025-12-08 10:44:58,493: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:44:58,493: t15.2024.03.08 val PER: 0.1536
2025-12-08 10:44:58,493: t15.2024.03.15 val PER: 0.1576
2025-12-08 10:44:58,493: t15.2024.03.17 val PER: 0.0823
2025-12-08 10:44:58,493: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:44:58,493: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:44:58,493: t15.2024.05.10 val PER: 0.1114
2025-12-08 10:44:58,493: t15.2024.06.14 val PER: 0.1215
2025-12-08 10:44:58,493: t15.2024.07.19 val PER: 0.1562
2025-12-08 10:44:58,493: t15.2024.07.21 val PER: 0.0538
2025-12-08 10:44:58,493: t15.2024.07.28 val PER: 0.0757
2025-12-08 10:44:58,493: t15.2025.01.10 val PER: 0.2438
2025-12-08 10:44:58,493: t15.2025.01.12 val PER: 0.0893
2025-12-08 10:44:58,493: t15.2025.03.14 val PER: 0.2973
2025-12-08 10:44:58,493: t15.2025.03.16 val PER: 0.1505
2025-12-08 10:44:58,493: t15.2025.03.30 val PER: 0.2322
2025-12-08 10:44:58,494: t15.2025.04.13 val PER: 0.1912
2025-12-08 10:44:58,494: New best test loss 22.6913 --> 22.5128
2025-12-08 10:44:58,494: Checkpointing model
2025-12-08 10:44:59,497: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 10:45:26,662: Train batch 100200: loss: 0.01 grad norm: 2.22 time: 0.144
2025-12-08 10:45:54,142: Train batch 100400: loss: 0.02 grad norm: 9.65 time: 0.088
2025-12-08 10:46:21,295: Train batch 100600: loss: 0.01 grad norm: 0.61 time: 0.088
2025-12-08 10:46:49,466: Train batch 100800: loss: 0.01 grad norm: 0.41 time: 0.132
2025-12-08 10:47:16,804: Train batch 101000: loss: 0.01 grad norm: 0.56 time: 0.177
2025-12-08 10:47:44,418: Train batch 101200: loss: 0.08 grad norm: 4.07 time: 0.108
2025-12-08 10:48:11,954: Train batch 101400: loss: 0.00 grad norm: 0.22 time: 0.135
2025-12-08 10:48:39,763: Train batch 101600: loss: 0.00 grad norm: 0.36 time: 0.124
2025-12-08 10:49:07,814: Train batch 101800: loss: 0.01 grad norm: 0.86 time: 0.140
2025-12-08 10:49:34,389: Train batch 102000: loss: 0.03 grad norm: 2.14 time: 0.108
2025-12-08 10:49:34,389: Running test after training batch: 102000
2025-12-08 10:49:44,774: Val batch 102000: PER (avg): 0.1010 CTC Loss (avg): 22.5337 time: 10.385
2025-12-08 10:49:44,774: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:49:44,774: t15.2023.08.13 val PER: 0.0748
2025-12-08 10:49:44,774: t15.2023.08.18 val PER: 0.0578
2025-12-08 10:49:44,775: t15.2023.08.20 val PER: 0.0469
2025-12-08 10:49:44,775: t15.2023.08.25 val PER: 0.0798
2025-12-08 10:49:44,775: t15.2023.08.27 val PER: 0.1463
2025-12-08 10:49:44,775: t15.2023.09.01 val PER: 0.0390
2025-12-08 10:49:44,775: t15.2023.09.03 val PER: 0.1116
2025-12-08 10:49:44,775: t15.2023.09.24 val PER: 0.0740
2025-12-08 10:49:44,775: t15.2023.09.29 val PER: 0.1008
2025-12-08 10:49:44,775: t15.2023.10.01 val PER: 0.1407
2025-12-08 10:49:44,775: t15.2023.10.06 val PER: 0.0581
2025-12-08 10:49:44,776: t15.2023.10.08 val PER: 0.1637
2025-12-08 10:49:44,776: t15.2023.10.13 val PER: 0.1443
2025-12-08 10:49:44,776: t15.2023.10.15 val PER: 0.1009
2025-12-08 10:49:44,776: t15.2023.10.20 val PER: 0.1477
2025-12-08 10:49:44,776: t15.2023.10.22 val PER: 0.0924
2025-12-08 10:49:44,776: t15.2023.11.03 val PER: 0.1364
2025-12-08 10:49:44,776: t15.2023.11.04 val PER: 0.0102
2025-12-08 10:49:44,776: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:49:44,776: t15.2023.11.19 val PER: 0.0259
2025-12-08 10:49:44,776: t15.2023.11.26 val PER: 0.0500
2025-12-08 10:49:44,776: t15.2023.12.03 val PER: 0.0473
2025-12-08 10:49:44,776: t15.2023.12.08 val PER: 0.0366
2025-12-08 10:49:44,776: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:49:44,777: t15.2023.12.17 val PER: 0.0728
2025-12-08 10:49:44,777: t15.2023.12.29 val PER: 0.0631
2025-12-08 10:49:44,777: t15.2024.02.25 val PER: 0.0674
2025-12-08 10:49:44,777: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:49:44,777: t15.2024.03.08 val PER: 0.1536
2025-12-08 10:49:44,777: t15.2024.03.15 val PER: 0.1601
2025-12-08 10:49:44,777: t15.2024.03.17 val PER: 0.0844
2025-12-08 10:49:44,777: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:49:44,777: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:49:44,777: t15.2024.05.10 val PER: 0.1129
2025-12-08 10:49:44,777: t15.2024.06.14 val PER: 0.1167
2025-12-08 10:49:44,777: t15.2024.07.19 val PER: 0.1536
2025-12-08 10:49:44,777: t15.2024.07.21 val PER: 0.0531
2025-12-08 10:49:44,777: t15.2024.07.28 val PER: 0.0765
2025-12-08 10:49:44,777: t15.2025.01.10 val PER: 0.2466
2025-12-08 10:49:44,777: t15.2025.01.12 val PER: 0.0908
2025-12-08 10:49:44,777: t15.2025.03.14 val PER: 0.2899
2025-12-08 10:49:44,777: t15.2025.03.16 val PER: 0.1440
2025-12-08 10:49:44,777: t15.2025.03.30 val PER: 0.2345
2025-12-08 10:49:44,778: t15.2025.04.13 val PER: 0.1883
2025-12-08 10:50:12,100: Train batch 102200: loss: 0.01 grad norm: 0.89 time: 0.147
2025-12-08 10:50:39,410: Train batch 102400: loss: 0.01 grad norm: 1.46 time: 0.110
2025-12-08 10:51:07,545: Train batch 102600: loss: 0.01 grad norm: 1.13 time: 0.095
2025-12-08 10:51:35,259: Train batch 102800: loss: 0.02 grad norm: 1.01 time: 0.160
2025-12-08 10:52:03,422: Train batch 103000: loss: 0.01 grad norm: 0.91 time: 0.111
2025-12-08 10:52:31,274: Train batch 103200: loss: 0.03 grad norm: 3.35 time: 0.082
2025-12-08 10:52:58,378: Train batch 103400: loss: 0.01 grad norm: 1.04 time: 0.098
2025-12-08 10:53:26,643: Train batch 103600: loss: 0.01 grad norm: 0.59 time: 0.169
2025-12-08 10:53:53,958: Train batch 103800: loss: 0.00 grad norm: 0.16 time: 0.149
2025-12-08 10:54:22,118: Train batch 104000: loss: 0.01 grad norm: 0.41 time: 0.097
2025-12-08 10:54:22,118: Running test after training batch: 104000
2025-12-08 10:54:32,546: Val batch 104000: PER (avg): 0.1011 CTC Loss (avg): 22.5084 time: 10.427
2025-12-08 10:54:32,546: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:54:32,546: t15.2023.08.13 val PER: 0.0696
2025-12-08 10:54:32,546: t15.2023.08.18 val PER: 0.0578
2025-12-08 10:54:32,546: t15.2023.08.20 val PER: 0.0461
2025-12-08 10:54:32,546: t15.2023.08.25 val PER: 0.0783
2025-12-08 10:54:32,546: t15.2023.08.27 val PER: 0.1447
2025-12-08 10:54:32,546: t15.2023.09.01 val PER: 0.0398
2025-12-08 10:54:32,546: t15.2023.09.03 val PER: 0.1116
2025-12-08 10:54:32,546: t15.2023.09.24 val PER: 0.0777
2025-12-08 10:54:32,546: t15.2023.09.29 val PER: 0.0989
2025-12-08 10:54:32,547: t15.2023.10.01 val PER: 0.1380
2025-12-08 10:54:32,547: t15.2023.10.06 val PER: 0.0603
2025-12-08 10:54:32,547: t15.2023.10.08 val PER: 0.1637
2025-12-08 10:54:32,547: t15.2023.10.13 val PER: 0.1443
2025-12-08 10:54:32,547: t15.2023.10.15 val PER: 0.0995
2025-12-08 10:54:32,547: t15.2023.10.20 val PER: 0.1510
2025-12-08 10:54:32,547: t15.2023.10.22 val PER: 0.0913
2025-12-08 10:54:32,547: t15.2023.11.03 val PER: 0.1343
2025-12-08 10:54:32,547: t15.2023.11.04 val PER: 0.0068
2025-12-08 10:54:32,547: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:54:32,547: t15.2023.11.19 val PER: 0.0240
2025-12-08 10:54:32,547: t15.2023.11.26 val PER: 0.0493
2025-12-08 10:54:32,547: t15.2023.12.03 val PER: 0.0515
2025-12-08 10:54:32,547: t15.2023.12.08 val PER: 0.0379
2025-12-08 10:54:32,547: t15.2023.12.10 val PER: 0.0368
2025-12-08 10:54:32,547: t15.2023.12.17 val PER: 0.0717
2025-12-08 10:54:32,547: t15.2023.12.29 val PER: 0.0611
2025-12-08 10:54:32,547: t15.2024.02.25 val PER: 0.0702
2025-12-08 10:54:32,548: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:54:32,548: t15.2024.03.08 val PER: 0.1565
2025-12-08 10:54:32,548: t15.2024.03.15 val PER: 0.1601
2025-12-08 10:54:32,548: t15.2024.03.17 val PER: 0.0830
2025-12-08 10:54:32,548: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:54:32,548: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:54:32,548: t15.2024.05.10 val PER: 0.1129
2025-12-08 10:54:32,548: t15.2024.06.14 val PER: 0.1230
2025-12-08 10:54:32,548: t15.2024.07.19 val PER: 0.1569
2025-12-08 10:54:32,548: t15.2024.07.21 val PER: 0.0545
2025-12-08 10:54:32,548: t15.2024.07.28 val PER: 0.0772
2025-12-08 10:54:32,548: t15.2025.01.10 val PER: 0.2493
2025-12-08 10:54:32,548: t15.2025.01.12 val PER: 0.0931
2025-12-08 10:54:32,548: t15.2025.03.14 val PER: 0.2959
2025-12-08 10:54:32,548: t15.2025.03.16 val PER: 0.1427
2025-12-08 10:54:32,548: t15.2025.03.30 val PER: 0.2310
2025-12-08 10:54:32,548: t15.2025.04.13 val PER: 0.1883
2025-12-08 10:55:00,101: Train batch 104200: loss: 0.00 grad norm: 0.12 time: 0.140
2025-12-08 10:55:27,659: Train batch 104400: loss: 0.01 grad norm: 0.68 time: 0.161
2025-12-08 10:55:55,280: Train batch 104600: loss: 0.02 grad norm: 1.31 time: 0.079
2025-12-08 10:56:22,531: Train batch 104800: loss: 0.02 grad norm: 2.37 time: 0.089
2025-12-08 10:56:50,343: Train batch 105000: loss: 0.01 grad norm: 0.82 time: 0.154
2025-12-08 10:57:17,974: Train batch 105200: loss: 0.06 grad norm: 5.54 time: 0.116
2025-12-08 10:57:46,298: Train batch 105400: loss: 0.01 grad norm: 0.83 time: 0.120
2025-12-08 10:58:13,585: Train batch 105600: loss: 0.01 grad norm: 0.71 time: 0.132
2025-12-08 10:58:40,651: Train batch 105800: loss: 0.03 grad norm: 2.40 time: 0.103
2025-12-08 10:59:08,435: Train batch 106000: loss: 0.01 grad norm: 0.31 time: 0.135
2025-12-08 10:59:08,435: Running test after training batch: 106000
2025-12-08 10:59:18,846: Val batch 106000: PER (avg): 0.1008 CTC Loss (avg): 22.4915 time: 10.410
2025-12-08 10:59:18,846: t15.2023.08.11 val PER: 1.0000
2025-12-08 10:59:18,846: t15.2023.08.13 val PER: 0.0717
2025-12-08 10:59:18,846: t15.2023.08.18 val PER: 0.0578
2025-12-08 10:59:18,846: t15.2023.08.20 val PER: 0.0445
2025-12-08 10:59:18,846: t15.2023.08.25 val PER: 0.0753
2025-12-08 10:59:18,846: t15.2023.08.27 val PER: 0.1399
2025-12-08 10:59:18,846: t15.2023.09.01 val PER: 0.0398
2025-12-08 10:59:18,846: t15.2023.09.03 val PER: 0.1128
2025-12-08 10:59:18,846: t15.2023.09.24 val PER: 0.0789
2025-12-08 10:59:18,846: t15.2023.09.29 val PER: 0.1002
2025-12-08 10:59:18,846: t15.2023.10.01 val PER: 0.1374
2025-12-08 10:59:18,846: t15.2023.10.06 val PER: 0.0603
2025-12-08 10:59:18,847: t15.2023.10.08 val PER: 0.1624
2025-12-08 10:59:18,847: t15.2023.10.13 val PER: 0.1435
2025-12-08 10:59:18,847: t15.2023.10.15 val PER: 0.1015
2025-12-08 10:59:18,847: t15.2023.10.20 val PER: 0.1544
2025-12-08 10:59:18,847: t15.2023.10.22 val PER: 0.0913
2025-12-08 10:59:18,847: t15.2023.11.03 val PER: 0.1364
2025-12-08 10:59:18,847: t15.2023.11.04 val PER: 0.0102
2025-12-08 10:59:18,847: t15.2023.11.17 val PER: 0.0156
2025-12-08 10:59:18,847: t15.2023.11.19 val PER: 0.0259
2025-12-08 10:59:18,847: t15.2023.11.26 val PER: 0.0500
2025-12-08 10:59:18,847: t15.2023.12.03 val PER: 0.0473
2025-12-08 10:59:18,847: t15.2023.12.08 val PER: 0.0406
2025-12-08 10:59:18,847: t15.2023.12.10 val PER: 0.0394
2025-12-08 10:59:18,847: t15.2023.12.17 val PER: 0.0665
2025-12-08 10:59:18,847: t15.2023.12.29 val PER: 0.0597
2025-12-08 10:59:18,847: t15.2024.02.25 val PER: 0.0646
2025-12-08 10:59:18,847: t15.2024.03.03 val PER: 1.0000
2025-12-08 10:59:18,847: t15.2024.03.08 val PER: 0.1593
2025-12-08 10:59:18,847: t15.2024.03.15 val PER: 0.1595
2025-12-08 10:59:18,848: t15.2024.03.17 val PER: 0.0823
2025-12-08 10:59:18,848: t15.2024.04.25 val PER: 1.0000
2025-12-08 10:59:18,848: t15.2024.04.28 val PER: 1.0000
2025-12-08 10:59:18,848: t15.2024.05.10 val PER: 0.1189
2025-12-08 10:59:18,848: t15.2024.06.14 val PER: 0.1215
2025-12-08 10:59:18,848: t15.2024.07.19 val PER: 0.1569
2025-12-08 10:59:18,848: t15.2024.07.21 val PER: 0.0531
2025-12-08 10:59:18,848: t15.2024.07.28 val PER: 0.0772
2025-12-08 10:59:18,848: t15.2025.01.10 val PER: 0.2507
2025-12-08 10:59:18,848: t15.2025.01.12 val PER: 0.0924
2025-12-08 10:59:18,848: t15.2025.03.14 val PER: 0.2944
2025-12-08 10:59:18,848: t15.2025.03.16 val PER: 0.1401
2025-12-08 10:59:18,848: t15.2025.03.30 val PER: 0.2264
2025-12-08 10:59:18,848: t15.2025.04.13 val PER: 0.1854
2025-12-08 10:59:18,848: New best test PER 0.1010 --> 0.1008
2025-12-08 10:59:18,848: Checkpointing model
2025-12-08 10:59:19,913: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_064129/checkpoint/best_checkpoint
2025-12-08 10:59:47,731: Train batch 106200: loss: 0.01 grad norm: 1.00 time: 0.104
2025-12-08 11:00:14,880: Train batch 106400: loss: 0.01 grad norm: 1.75 time: 0.112
2025-12-08 11:00:42,346: Train batch 106600: loss: 0.02 grad norm: 0.96 time: 0.083
2025-12-08 11:01:09,904: Train batch 106800: loss: 0.00 grad norm: 0.39 time: 0.140
2025-12-08 11:01:37,095: Train batch 107000: loss: 0.04 grad norm: 2.59 time: 0.148
2025-12-08 11:02:04,986: Train batch 107200: loss: 0.01 grad norm: 0.25 time: 0.129
2025-12-08 11:02:32,458: Train batch 107400: loss: 0.05 grad norm: 3.59 time: 0.196
2025-12-08 11:02:59,665: Train batch 107600: loss: 0.01 grad norm: 0.29 time: 0.116
2025-12-08 11:03:26,751: Train batch 107800: loss: 0.00 grad norm: 0.26 time: 0.133
2025-12-08 11:03:54,539: Train batch 108000: loss: 0.05 grad norm: 3.01 time: 0.181
2025-12-08 11:03:54,540: Running test after training batch: 108000
2025-12-08 11:04:04,976: Val batch 108000: PER (avg): 0.1009 CTC Loss (avg): 22.4390 time: 10.436
2025-12-08 11:04:04,976: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:04:04,977: t15.2023.08.13 val PER: 0.0728
2025-12-08 11:04:04,977: t15.2023.08.18 val PER: 0.0570
2025-12-08 11:04:04,977: t15.2023.08.20 val PER: 0.0453
2025-12-08 11:04:04,977: t15.2023.08.25 val PER: 0.0753
2025-12-08 11:04:04,977: t15.2023.08.27 val PER: 0.1399
2025-12-08 11:04:04,977: t15.2023.09.01 val PER: 0.0406
2025-12-08 11:04:04,977: t15.2023.09.03 val PER: 0.1093
2025-12-08 11:04:04,977: t15.2023.09.24 val PER: 0.0740
2025-12-08 11:04:04,977: t15.2023.09.29 val PER: 0.1002
2025-12-08 11:04:04,977: t15.2023.10.01 val PER: 0.1380
2025-12-08 11:04:04,977: t15.2023.10.06 val PER: 0.0624
2025-12-08 11:04:04,977: t15.2023.10.08 val PER: 0.1624
2025-12-08 11:04:04,977: t15.2023.10.13 val PER: 0.1451
2025-12-08 11:04:04,977: t15.2023.10.15 val PER: 0.0976
2025-12-08 11:04:04,977: t15.2023.10.20 val PER: 0.1510
2025-12-08 11:04:04,977: t15.2023.10.22 val PER: 0.0891
2025-12-08 11:04:04,977: t15.2023.11.03 val PER: 0.1391
2025-12-08 11:04:04,977: t15.2023.11.04 val PER: 0.0102
2025-12-08 11:04:04,977: t15.2023.11.17 val PER: 0.0156
2025-12-08 11:04:04,978: t15.2023.11.19 val PER: 0.0220
2025-12-08 11:04:04,978: t15.2023.11.26 val PER: 0.0500
2025-12-08 11:04:04,978: t15.2023.12.03 val PER: 0.0494
2025-12-08 11:04:04,978: t15.2023.12.08 val PER: 0.0399
2025-12-08 11:04:04,978: t15.2023.12.10 val PER: 0.0381
2025-12-08 11:04:04,978: t15.2023.12.17 val PER: 0.0676
2025-12-08 11:04:04,978: t15.2023.12.29 val PER: 0.0604
2025-12-08 11:04:04,978: t15.2024.02.25 val PER: 0.0674
2025-12-08 11:04:04,978: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:04:04,978: t15.2024.03.08 val PER: 0.1579
2025-12-08 11:04:04,978: t15.2024.03.15 val PER: 0.1588
2025-12-08 11:04:04,978: t15.2024.03.17 val PER: 0.0858
2025-12-08 11:04:04,978: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:04:04,978: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:04:04,978: t15.2024.05.10 val PER: 0.1174
2025-12-08 11:04:04,978: t15.2024.06.14 val PER: 0.1215
2025-12-08 11:04:04,978: t15.2024.07.19 val PER: 0.1549
2025-12-08 11:04:04,978: t15.2024.07.21 val PER: 0.0566
2025-12-08 11:04:04,978: t15.2024.07.28 val PER: 0.0779
2025-12-08 11:04:04,979: t15.2025.01.10 val PER: 0.2534
2025-12-08 11:04:04,979: t15.2025.01.12 val PER: 0.0916
2025-12-08 11:04:04,979: t15.2025.03.14 val PER: 0.2959
2025-12-08 11:04:04,979: t15.2025.03.16 val PER: 0.1374
2025-12-08 11:04:04,979: t15.2025.03.30 val PER: 0.2264
2025-12-08 11:04:04,979: t15.2025.04.13 val PER: 0.1869
2025-12-08 11:04:32,994: Train batch 108200: loss: 0.01 grad norm: 1.17 time: 0.110
2025-12-08 11:05:01,611: Train batch 108400: loss: 0.02 grad norm: 1.48 time: 0.146
2025-12-08 11:05:30,250: Train batch 108600: loss: 0.03 grad norm: 3.20 time: 0.142
2025-12-08 11:05:58,532: Train batch 108800: loss: 0.02 grad norm: 2.02 time: 0.140
2025-12-08 11:06:26,426: Train batch 109000: loss: 0.04 grad norm: 2.37 time: 0.098
2025-12-08 11:06:54,857: Train batch 109200: loss: 0.00 grad norm: 0.10 time: 0.121
2025-12-08 11:07:22,900: Train batch 109400: loss: 0.02 grad norm: 2.58 time: 0.132
2025-12-08 11:07:51,032: Train batch 109600: loss: 0.02 grad norm: 1.60 time: 0.139
2025-12-08 11:08:19,885: Train batch 109800: loss: 0.01 grad norm: 0.59 time: 0.150
2025-12-08 11:08:48,658: Train batch 110000: loss: 0.01 grad norm: 0.68 time: 0.082
2025-12-08 11:08:48,658: Running test after training batch: 110000
2025-12-08 11:08:59,136: Val batch 110000: PER (avg): 0.1017 CTC Loss (avg): 22.5209 time: 10.478
2025-12-08 11:08:59,137: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:08:59,137: t15.2023.08.13 val PER: 0.0728
2025-12-08 11:08:59,137: t15.2023.08.18 val PER: 0.0562
2025-12-08 11:08:59,137: t15.2023.08.20 val PER: 0.0445
2025-12-08 11:08:59,137: t15.2023.08.25 val PER: 0.0768
2025-12-08 11:08:59,137: t15.2023.08.27 val PER: 0.1383
2025-12-08 11:08:59,137: t15.2023.09.01 val PER: 0.0406
2025-12-08 11:08:59,137: t15.2023.09.03 val PER: 0.1128
2025-12-08 11:08:59,137: t15.2023.09.24 val PER: 0.0740
2025-12-08 11:08:59,137: t15.2023.09.29 val PER: 0.1002
2025-12-08 11:08:59,137: t15.2023.10.01 val PER: 0.1380
2025-12-08 11:08:59,137: t15.2023.10.06 val PER: 0.0646
2025-12-08 11:08:59,137: t15.2023.10.08 val PER: 0.1664
2025-12-08 11:08:59,137: t15.2023.10.13 val PER: 0.1490
2025-12-08 11:08:59,137: t15.2023.10.15 val PER: 0.0989
2025-12-08 11:08:59,137: t15.2023.10.20 val PER: 0.1510
2025-12-08 11:08:59,137: t15.2023.10.22 val PER: 0.0902
2025-12-08 11:08:59,138: t15.2023.11.03 val PER: 0.1370
2025-12-08 11:08:59,138: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:08:59,138: t15.2023.11.17 val PER: 0.0156
2025-12-08 11:08:59,138: t15.2023.11.19 val PER: 0.0220
2025-12-08 11:08:59,138: t15.2023.11.26 val PER: 0.0493
2025-12-08 11:08:59,138: t15.2023.12.03 val PER: 0.0483
2025-12-08 11:08:59,138: t15.2023.12.08 val PER: 0.0406
2025-12-08 11:08:59,138: t15.2023.12.10 val PER: 0.0368
2025-12-08 11:08:59,138: t15.2023.12.17 val PER: 0.0717
2025-12-08 11:08:59,138: t15.2023.12.29 val PER: 0.0638
2025-12-08 11:08:59,138: t15.2024.02.25 val PER: 0.0660
2025-12-08 11:08:59,138: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:08:59,138: t15.2024.03.08 val PER: 0.1593
2025-12-08 11:08:59,138: t15.2024.03.15 val PER: 0.1595
2025-12-08 11:08:59,138: t15.2024.03.17 val PER: 0.0865
2025-12-08 11:08:59,138: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:08:59,138: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:08:59,139: t15.2024.05.10 val PER: 0.1204
2025-12-08 11:08:59,139: t15.2024.06.14 val PER: 0.1215
2025-12-08 11:08:59,139: t15.2024.07.19 val PER: 0.1582
2025-12-08 11:08:59,139: t15.2024.07.21 val PER: 0.0552
2025-12-08 11:08:59,139: t15.2024.07.28 val PER: 0.0801
2025-12-08 11:08:59,139: t15.2025.01.10 val PER: 0.2562
2025-12-08 11:08:59,139: t15.2025.01.12 val PER: 0.0908
2025-12-08 11:08:59,139: t15.2025.03.14 val PER: 0.2959
2025-12-08 11:08:59,139: t15.2025.03.16 val PER: 0.1387
2025-12-08 11:08:59,139: t15.2025.03.30 val PER: 0.2287
2025-12-08 11:08:59,139: t15.2025.04.13 val PER: 0.1840
2025-12-08 11:09:27,361: Train batch 110200: loss: 0.00 grad norm: 0.27 time: 0.137
2025-12-08 11:09:56,053: Train batch 110400: loss: 0.01 grad norm: 0.75 time: 0.101
2025-12-08 11:10:25,075: Train batch 110600: loss: 0.01 grad norm: 0.51 time: 0.127
2025-12-08 11:10:53,335: Train batch 110800: loss: 0.01 grad norm: 2.29 time: 0.168
2025-12-08 11:11:20,777: Train batch 111000: loss: 0.01 grad norm: 0.44 time: 0.087
2025-12-08 11:11:49,149: Train batch 111200: loss: 0.00 grad norm: 0.23 time: 0.131
2025-12-08 11:12:17,426: Train batch 111400: loss: 0.02 grad norm: 1.91 time: 0.083
2025-12-08 11:12:44,805: Train batch 111600: loss: 0.08 grad norm: 2.99 time: 0.165
2025-12-08 11:13:11,987: Train batch 111800: loss: 0.01 grad norm: 0.43 time: 0.093
2025-12-08 11:13:40,451: Train batch 112000: loss: 0.01 grad norm: 2.41 time: 0.154
2025-12-08 11:13:40,452: Running test after training batch: 112000
2025-12-08 11:13:50,648: Val batch 112000: PER (avg): 0.1012 CTC Loss (avg): 22.5131 time: 10.196
2025-12-08 11:13:50,648: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:13:50,648: t15.2023.08.13 val PER: 0.0728
2025-12-08 11:13:50,648: t15.2023.08.18 val PER: 0.0578
2025-12-08 11:13:50,648: t15.2023.08.20 val PER: 0.0461
2025-12-08 11:13:50,649: t15.2023.08.25 val PER: 0.0783
2025-12-08 11:13:50,649: t15.2023.08.27 val PER: 0.1367
2025-12-08 11:13:50,649: t15.2023.09.01 val PER: 0.0406
2025-12-08 11:13:50,649: t15.2023.09.03 val PER: 0.1116
2025-12-08 11:13:50,649: t15.2023.09.24 val PER: 0.0752
2025-12-08 11:13:50,649: t15.2023.09.29 val PER: 0.1015
2025-12-08 11:13:50,649: t15.2023.10.01 val PER: 0.1367
2025-12-08 11:13:50,649: t15.2023.10.06 val PER: 0.0592
2025-12-08 11:13:50,649: t15.2023.10.08 val PER: 0.1637
2025-12-08 11:13:50,649: t15.2023.10.13 val PER: 0.1482
2025-12-08 11:13:50,649: t15.2023.10.15 val PER: 0.1015
2025-12-08 11:13:50,649: t15.2023.10.20 val PER: 0.1510
2025-12-08 11:13:50,649: t15.2023.10.22 val PER: 0.0891
2025-12-08 11:13:50,649: t15.2023.11.03 val PER: 0.1384
2025-12-08 11:13:50,649: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:13:50,649: t15.2023.11.17 val PER: 0.0124
2025-12-08 11:13:50,649: t15.2023.11.19 val PER: 0.0200
2025-12-08 11:13:50,649: t15.2023.11.26 val PER: 0.0493
2025-12-08 11:13:50,649: t15.2023.12.03 val PER: 0.0473
2025-12-08 11:13:50,650: t15.2023.12.08 val PER: 0.0379
2025-12-08 11:13:50,650: t15.2023.12.10 val PER: 0.0355
2025-12-08 11:13:50,650: t15.2023.12.17 val PER: 0.0676
2025-12-08 11:13:50,650: t15.2023.12.29 val PER: 0.0645
2025-12-08 11:13:50,650: t15.2024.02.25 val PER: 0.0660
2025-12-08 11:13:50,650: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:13:50,650: t15.2024.03.08 val PER: 0.1607
2025-12-08 11:13:50,650: t15.2024.03.15 val PER: 0.1626
2025-12-08 11:13:50,650: t15.2024.03.17 val PER: 0.0851
2025-12-08 11:13:50,650: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:13:50,650: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:13:50,650: t15.2024.05.10 val PER: 0.1129
2025-12-08 11:13:50,650: t15.2024.06.14 val PER: 0.1167
2025-12-08 11:13:50,650: t15.2024.07.19 val PER: 0.1556
2025-12-08 11:13:50,650: t15.2024.07.21 val PER: 0.0552
2025-12-08 11:13:50,650: t15.2024.07.28 val PER: 0.0809
2025-12-08 11:13:50,650: t15.2025.01.10 val PER: 0.2576
2025-12-08 11:13:50,650: t15.2025.01.12 val PER: 0.0901
2025-12-08 11:13:50,650: t15.2025.03.14 val PER: 0.3003
2025-12-08 11:13:50,650: t15.2025.03.16 val PER: 0.1374
2025-12-08 11:13:50,651: t15.2025.03.30 val PER: 0.2276
2025-12-08 11:13:50,651: t15.2025.04.13 val PER: 0.1826
2025-12-08 11:14:18,101: Train batch 112200: loss: 0.04 grad norm: 4.14 time: 0.168
2025-12-08 11:14:46,377: Train batch 112400: loss: 0.06 grad norm: 5.08 time: 0.083
2025-12-08 11:15:14,568: Train batch 112600: loss: 0.03 grad norm: 3.07 time: 0.111
2025-12-08 11:15:42,836: Train batch 112800: loss: 0.01 grad norm: 0.40 time: 0.121
2025-12-08 11:16:10,340: Train batch 113000: loss: 0.01 grad norm: 2.07 time: 0.131
2025-12-08 11:16:39,117: Train batch 113200: loss: 0.01 grad norm: 0.57 time: 0.150
2025-12-08 11:17:06,899: Train batch 113400: loss: 0.01 grad norm: 0.66 time: 0.175
2025-12-08 11:17:35,031: Train batch 113600: loss: 0.01 grad norm: 0.62 time: 0.134
2025-12-08 11:18:03,331: Train batch 113800: loss: 0.01 grad norm: 1.20 time: 0.082
2025-12-08 11:18:31,233: Train batch 114000: loss: 0.00 grad norm: 0.34 time: 0.112
2025-12-08 11:18:31,234: Running test after training batch: 114000
2025-12-08 11:18:41,803: Val batch 114000: PER (avg): 0.1009 CTC Loss (avg): 22.4083 time: 10.570
2025-12-08 11:18:41,804: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:18:41,804: t15.2023.08.13 val PER: 0.0696
2025-12-08 11:18:41,804: t15.2023.08.18 val PER: 0.0595
2025-12-08 11:18:41,804: t15.2023.08.20 val PER: 0.0469
2025-12-08 11:18:41,804: t15.2023.08.25 val PER: 0.0753
2025-12-08 11:18:41,804: t15.2023.08.27 val PER: 0.1399
2025-12-08 11:18:41,804: t15.2023.09.01 val PER: 0.0398
2025-12-08 11:18:41,804: t15.2023.09.03 val PER: 0.1105
2025-12-08 11:18:41,804: t15.2023.09.24 val PER: 0.0801
2025-12-08 11:18:41,804: t15.2023.09.29 val PER: 0.1034
2025-12-08 11:18:41,804: t15.2023.10.01 val PER: 0.1341
2025-12-08 11:18:41,804: t15.2023.10.06 val PER: 0.0603
2025-12-08 11:18:41,804: t15.2023.10.08 val PER: 0.1664
2025-12-08 11:18:41,804: t15.2023.10.13 val PER: 0.1458
2025-12-08 11:18:41,804: t15.2023.10.15 val PER: 0.0995
2025-12-08 11:18:41,804: t15.2023.10.20 val PER: 0.1510
2025-12-08 11:18:41,805: t15.2023.10.22 val PER: 0.0880
2025-12-08 11:18:41,805: t15.2023.11.03 val PER: 0.1370
2025-12-08 11:18:41,805: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:18:41,805: t15.2023.11.17 val PER: 0.0140
2025-12-08 11:18:41,805: t15.2023.11.19 val PER: 0.0220
2025-12-08 11:18:41,805: t15.2023.11.26 val PER: 0.0507
2025-12-08 11:18:41,805: t15.2023.12.03 val PER: 0.0494
2025-12-08 11:18:41,805: t15.2023.12.08 val PER: 0.0360
2025-12-08 11:18:41,805: t15.2023.12.10 val PER: 0.0355
2025-12-08 11:18:41,805: t15.2023.12.17 val PER: 0.0676
2025-12-08 11:18:41,805: t15.2023.12.29 val PER: 0.0638
2025-12-08 11:18:41,805: t15.2024.02.25 val PER: 0.0646
2025-12-08 11:18:41,805: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:18:41,805: t15.2024.03.08 val PER: 0.1607
2025-12-08 11:18:41,805: t15.2024.03.15 val PER: 0.1607
2025-12-08 11:18:41,805: t15.2024.03.17 val PER: 0.0837
2025-12-08 11:18:41,805: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:18:41,805: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:18:41,805: t15.2024.05.10 val PER: 0.1114
2025-12-08 11:18:41,806: t15.2024.06.14 val PER: 0.1183
2025-12-08 11:18:41,806: t15.2024.07.19 val PER: 0.1556
2025-12-08 11:18:41,806: t15.2024.07.21 val PER: 0.0531
2025-12-08 11:18:41,806: t15.2024.07.28 val PER: 0.0831
2025-12-08 11:18:41,806: t15.2025.01.10 val PER: 0.2562
2025-12-08 11:18:41,806: t15.2025.01.12 val PER: 0.0901
2025-12-08 11:18:41,806: t15.2025.03.14 val PER: 0.3033
2025-12-08 11:18:41,806: t15.2025.03.16 val PER: 0.1361
2025-12-08 11:18:41,806: t15.2025.03.30 val PER: 0.2287
2025-12-08 11:18:41,806: t15.2025.04.13 val PER: 0.1769
2025-12-08 11:19:09,720: Train batch 114200: loss: 0.00 grad norm: 0.14 time: 0.101
2025-12-08 11:19:38,113: Train batch 114400: loss: 0.01 grad norm: 0.67 time: 0.167
2025-12-08 11:20:06,683: Train batch 114600: loss: 0.00 grad norm: 0.21 time: 0.120
2025-12-08 11:20:34,492: Train batch 114800: loss: 0.06 grad norm: 2.12 time: 0.128
2025-12-08 11:21:02,894: Train batch 115000: loss: 0.01 grad norm: 0.38 time: 0.104
2025-12-08 11:21:31,459: Train batch 115200: loss: 0.03 grad norm: 1.87 time: 0.114
2025-12-08 11:21:59,385: Train batch 115400: loss: 0.01 grad norm: 0.43 time: 0.091
2025-12-08 11:22:27,205: Train batch 115600: loss: 0.01 grad norm: 0.43 time: 0.114
2025-12-08 11:22:55,488: Train batch 115800: loss: 0.01 grad norm: 0.98 time: 0.119
2025-12-08 11:23:23,732: Train batch 116000: loss: 0.07 grad norm: 3.43 time: 0.147
2025-12-08 11:23:23,732: Running test after training batch: 116000
2025-12-08 11:23:33,969: Val batch 116000: PER (avg): 0.1012 CTC Loss (avg): 22.4195 time: 10.236
2025-12-08 11:23:33,969: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:23:33,969: t15.2023.08.13 val PER: 0.0717
2025-12-08 11:23:33,969: t15.2023.08.18 val PER: 0.0578
2025-12-08 11:23:33,969: t15.2023.08.20 val PER: 0.0453
2025-12-08 11:23:33,969: t15.2023.08.25 val PER: 0.0753
2025-12-08 11:23:33,969: t15.2023.08.27 val PER: 0.1383
2025-12-08 11:23:33,969: t15.2023.09.01 val PER: 0.0398
2025-12-08 11:23:33,969: t15.2023.09.03 val PER: 0.1116
2025-12-08 11:23:33,969: t15.2023.09.24 val PER: 0.0801
2025-12-08 11:23:33,969: t15.2023.09.29 val PER: 0.1027
2025-12-08 11:23:33,969: t15.2023.10.01 val PER: 0.1374
2025-12-08 11:23:33,969: t15.2023.10.06 val PER: 0.0624
2025-12-08 11:23:33,969: t15.2023.10.08 val PER: 0.1651
2025-12-08 11:23:33,970: t15.2023.10.13 val PER: 0.1443
2025-12-08 11:23:33,970: t15.2023.10.15 val PER: 0.1002
2025-12-08 11:23:33,970: t15.2023.10.20 val PER: 0.1443
2025-12-08 11:23:33,970: t15.2023.10.22 val PER: 0.0869
2025-12-08 11:23:33,970: t15.2023.11.03 val PER: 0.1377
2025-12-08 11:23:33,970: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:23:33,970: t15.2023.11.17 val PER: 0.0140
2025-12-08 11:23:33,970: t15.2023.11.19 val PER: 0.0240
2025-12-08 11:23:33,970: t15.2023.11.26 val PER: 0.0507
2025-12-08 11:23:33,970: t15.2023.12.03 val PER: 0.0483
2025-12-08 11:23:33,970: t15.2023.12.08 val PER: 0.0373
2025-12-08 11:23:33,970: t15.2023.12.10 val PER: 0.0381
2025-12-08 11:23:33,970: t15.2023.12.17 val PER: 0.0728
2025-12-08 11:23:33,970: t15.2023.12.29 val PER: 0.0638
2025-12-08 11:23:33,970: t15.2024.02.25 val PER: 0.0646
2025-12-08 11:23:33,970: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:23:33,970: t15.2024.03.08 val PER: 0.1579
2025-12-08 11:23:33,970: t15.2024.03.15 val PER: 0.1607
2025-12-08 11:23:33,970: t15.2024.03.17 val PER: 0.0872
2025-12-08 11:23:33,971: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:23:33,971: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:23:33,971: t15.2024.05.10 val PER: 0.1100
2025-12-08 11:23:33,971: t15.2024.06.14 val PER: 0.1167
2025-12-08 11:23:33,971: t15.2024.07.19 val PER: 0.1562
2025-12-08 11:23:33,971: t15.2024.07.21 val PER: 0.0531
2025-12-08 11:23:33,971: t15.2024.07.28 val PER: 0.0809
2025-12-08 11:23:33,971: t15.2025.01.10 val PER: 0.2521
2025-12-08 11:23:33,971: t15.2025.01.12 val PER: 0.0924
2025-12-08 11:23:33,971: t15.2025.03.14 val PER: 0.3033
2025-12-08 11:23:33,971: t15.2025.03.16 val PER: 0.1427
2025-12-08 11:23:33,971: t15.2025.03.30 val PER: 0.2241
2025-12-08 11:23:33,971: t15.2025.04.13 val PER: 0.1783
2025-12-08 11:24:01,560: Train batch 116200: loss: 0.01 grad norm: 6.02 time: 0.212
2025-12-08 11:24:30,052: Train batch 116400: loss: 0.02 grad norm: 2.73 time: 0.118
2025-12-08 11:24:58,555: Train batch 116600: loss: 0.01 grad norm: 0.46 time: 0.141
2025-12-08 11:25:26,941: Train batch 116800: loss: 0.01 grad norm: 0.33 time: 0.157
2025-12-08 11:25:55,533: Train batch 117000: loss: 0.00 grad norm: 0.09 time: 0.097
2025-12-08 11:26:24,649: Train batch 117200: loss: 0.01 grad norm: 0.61 time: 0.099
2025-12-08 11:26:53,005: Train batch 117400: loss: 0.02 grad norm: 1.66 time: 0.118
2025-12-08 11:27:20,974: Train batch 117600: loss: 0.03 grad norm: 3.84 time: 0.127
2025-12-08 11:27:49,141: Train batch 117800: loss: 0.09 grad norm: 7.68 time: 0.120
2025-12-08 11:28:18,117: Train batch 118000: loss: 0.01 grad norm: 0.39 time: 0.172
2025-12-08 11:28:18,118: Running test after training batch: 118000
2025-12-08 11:28:28,524: Val batch 118000: PER (avg): 0.1010 CTC Loss (avg): 22.4527 time: 10.406
2025-12-08 11:28:28,524: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:28:28,524: t15.2023.08.13 val PER: 0.0696
2025-12-08 11:28:28,524: t15.2023.08.18 val PER: 0.0595
2025-12-08 11:28:28,524: t15.2023.08.20 val PER: 0.0469
2025-12-08 11:28:28,524: t15.2023.08.25 val PER: 0.0723
2025-12-08 11:28:28,524: t15.2023.08.27 val PER: 0.1383
2025-12-08 11:28:28,525: t15.2023.09.01 val PER: 0.0390
2025-12-08 11:28:28,525: t15.2023.09.03 val PER: 0.1105
2025-12-08 11:28:28,525: t15.2023.09.24 val PER: 0.0740
2025-12-08 11:28:28,525: t15.2023.09.29 val PER: 0.1040
2025-12-08 11:28:28,525: t15.2023.10.01 val PER: 0.1334
2025-12-08 11:28:28,525: t15.2023.10.06 val PER: 0.0603
2025-12-08 11:28:28,525: t15.2023.10.08 val PER: 0.1637
2025-12-08 11:28:28,525: t15.2023.10.13 val PER: 0.1451
2025-12-08 11:28:28,525: t15.2023.10.15 val PER: 0.0995
2025-12-08 11:28:28,525: t15.2023.10.20 val PER: 0.1577
2025-12-08 11:28:28,525: t15.2023.10.22 val PER: 0.0869
2025-12-08 11:28:28,525: t15.2023.11.03 val PER: 0.1350
2025-12-08 11:28:28,525: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:28:28,525: t15.2023.11.17 val PER: 0.0140
2025-12-08 11:28:28,525: t15.2023.11.19 val PER: 0.0240
2025-12-08 11:28:28,525: t15.2023.11.26 val PER: 0.0507
2025-12-08 11:28:28,525: t15.2023.12.03 val PER: 0.0504
2025-12-08 11:28:28,526: t15.2023.12.08 val PER: 0.0366
2025-12-08 11:28:28,526: t15.2023.12.10 val PER: 0.0394
2025-12-08 11:28:28,526: t15.2023.12.17 val PER: 0.0738
2025-12-08 11:28:28,526: t15.2023.12.29 val PER: 0.0638
2025-12-08 11:28:28,526: t15.2024.02.25 val PER: 0.0632
2025-12-08 11:28:28,526: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:28:28,526: t15.2024.03.08 val PER: 0.1565
2025-12-08 11:28:28,526: t15.2024.03.15 val PER: 0.1614
2025-12-08 11:28:28,526: t15.2024.03.17 val PER: 0.0858
2025-12-08 11:28:28,526: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:28:28,526: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:28:28,526: t15.2024.05.10 val PER: 0.1070
2025-12-08 11:28:28,526: t15.2024.06.14 val PER: 0.1167
2025-12-08 11:28:28,526: t15.2024.07.19 val PER: 0.1569
2025-12-08 11:28:28,526: t15.2024.07.21 val PER: 0.0517
2025-12-08 11:28:28,526: t15.2024.07.28 val PER: 0.0838
2025-12-08 11:28:28,526: t15.2025.01.10 val PER: 0.2479
2025-12-08 11:28:28,527: t15.2025.01.12 val PER: 0.0931
2025-12-08 11:28:28,527: t15.2025.03.14 val PER: 0.3003
2025-12-08 11:28:28,527: t15.2025.03.16 val PER: 0.1466
2025-12-08 11:28:28,527: t15.2025.03.30 val PER: 0.2264
2025-12-08 11:28:28,527: t15.2025.04.13 val PER: 0.1840
2025-12-08 11:28:56,051: Train batch 118200: loss: 0.01 grad norm: 0.85 time: 0.118
2025-12-08 11:29:24,435: Train batch 118400: loss: 0.01 grad norm: 0.57 time: 0.125
2025-12-08 11:29:52,951: Train batch 118600: loss: 0.01 grad norm: 0.88 time: 0.181
2025-12-08 11:30:20,936: Train batch 118800: loss: 0.00 grad norm: 0.22 time: 0.105
2025-12-08 11:30:49,044: Train batch 119000: loss: 0.01 grad norm: 0.73 time: 0.120
2025-12-08 11:31:17,724: Train batch 119200: loss: 0.01 grad norm: 1.62 time: 0.154
2025-12-08 11:31:46,417: Train batch 119400: loss: 0.02 grad norm: 1.73 time: 0.137
2025-12-08 11:32:14,215: Train batch 119600: loss: 0.01 grad norm: 0.27 time: 0.112
2025-12-08 11:32:42,142: Train batch 119800: loss: 0.03 grad norm: 5.15 time: 0.128
2025-12-08 11:33:10,511: Running test after training batch: 119999
2025-12-08 11:33:20,746: Val batch 119999: PER (avg): 0.1009 CTC Loss (avg): 22.3852 time: 10.234
2025-12-08 11:33:20,746: t15.2023.08.11 val PER: 1.0000
2025-12-08 11:33:20,746: t15.2023.08.13 val PER: 0.0696
2025-12-08 11:33:20,746: t15.2023.08.18 val PER: 0.0578
2025-12-08 11:33:20,746: t15.2023.08.20 val PER: 0.0453
2025-12-08 11:33:20,746: t15.2023.08.25 val PER: 0.0738
2025-12-08 11:33:20,746: t15.2023.08.27 val PER: 0.1399
2025-12-08 11:33:20,746: t15.2023.09.01 val PER: 0.0381
2025-12-08 11:33:20,746: t15.2023.09.03 val PER: 0.1116
2025-12-08 11:33:20,747: t15.2023.09.24 val PER: 0.0765
2025-12-08 11:33:20,747: t15.2023.09.29 val PER: 0.1047
2025-12-08 11:33:20,747: t15.2023.10.01 val PER: 0.1347
2025-12-08 11:33:20,747: t15.2023.10.06 val PER: 0.0614
2025-12-08 11:33:20,747: t15.2023.10.08 val PER: 0.1664
2025-12-08 11:33:20,747: t15.2023.10.13 val PER: 0.1451
2025-12-08 11:33:20,747: t15.2023.10.15 val PER: 0.1009
2025-12-08 11:33:20,747: t15.2023.10.20 val PER: 0.1510
2025-12-08 11:33:20,747: t15.2023.10.22 val PER: 0.0880
2025-12-08 11:33:20,747: t15.2023.11.03 val PER: 0.1357
2025-12-08 11:33:20,747: t15.2023.11.04 val PER: 0.0137
2025-12-08 11:33:20,747: t15.2023.11.17 val PER: 0.0140
2025-12-08 11:33:20,747: t15.2023.11.19 val PER: 0.0220
2025-12-08 11:33:20,747: t15.2023.11.26 val PER: 0.0507
2025-12-08 11:33:20,747: t15.2023.12.03 val PER: 0.0515
2025-12-08 11:33:20,747: t15.2023.12.08 val PER: 0.0373
2025-12-08 11:33:20,747: t15.2023.12.10 val PER: 0.0381
2025-12-08 11:33:20,747: t15.2023.12.17 val PER: 0.0717
2025-12-08 11:33:20,748: t15.2023.12.29 val PER: 0.0645
2025-12-08 11:33:20,748: t15.2024.02.25 val PER: 0.0646
2025-12-08 11:33:20,748: t15.2024.03.03 val PER: 1.0000
2025-12-08 11:33:20,748: t15.2024.03.08 val PER: 0.1593
2025-12-08 11:33:20,748: t15.2024.03.15 val PER: 0.1601
2025-12-08 11:33:20,748: t15.2024.03.17 val PER: 0.0851
2025-12-08 11:33:20,748: t15.2024.04.25 val PER: 1.0000
2025-12-08 11:33:20,748: t15.2024.04.28 val PER: 1.0000
2025-12-08 11:33:20,748: t15.2024.05.10 val PER: 0.1070
2025-12-08 11:33:20,748: t15.2024.06.14 val PER: 0.1167
2025-12-08 11:33:20,748: t15.2024.07.19 val PER: 0.1549
2025-12-08 11:33:20,748: t15.2024.07.21 val PER: 0.0510
2025-12-08 11:33:20,748: t15.2024.07.28 val PER: 0.0809
2025-12-08 11:33:20,748: t15.2025.01.10 val PER: 0.2507
2025-12-08 11:33:20,748: t15.2025.01.12 val PER: 0.0924
2025-12-08 11:33:20,748: t15.2025.03.14 val PER: 0.2988
2025-12-08 11:33:20,748: t15.2025.03.16 val PER: 0.1440
2025-12-08 11:33:20,748: t15.2025.03.30 val PER: 0.2264
2025-12-08 11:33:20,748: t15.2025.04.13 val PER: 0.1812
2025-12-08 11:33:20,796: Best avg val PER achieved: 0.10084
2025-12-08 11:33:20,796: Total training time: 291.68 minutes
