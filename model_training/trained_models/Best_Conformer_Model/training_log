22025-12-16 19:31:36,045: Using device: cuda:0
2025-12-16 19:31:36,128: Initialized RNN decoding model
2025-12-16 19:31:36,128: ConformerDecoder(
  (day_activation): Softsign()
  (day_dropout): Dropout(p=0.2, inplace=False)
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (input_proj): Sequential(
    (0): Linear(in_features=4096, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.2, inplace=False)
  )
  (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (conformer): Conformer(
    (conformer_layers): ModuleList(
      (0-5): 6 x ConformerLayer(
        (ffn1): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=768, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=768, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_dropout): Dropout(p=0.2, inplace=False)
        (conv_module): _ConvolutionModule(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (sequential): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
            (1): GLU(dim=1)
            (2): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): SiLU()
            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
            (6): Dropout(p=0.2, inplace=False)
          )
        )
        (ffn2): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=768, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=768, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_dropout): Dropout(p=0.1, inplace=False)
  (out): Linear(in_features=256, out_features=41, bias=True)
)
2025-12-16 19:31:36,131: Model has 20,417,577 parameters
2025-12-16 19:31:36,131: Model has 11,819,520 day-specific parameters | 57.89% of total parameters
2025-12-16 19:31:44,692: Successfully initialized datasets
2025-12-16 19:31:46,063: Train batch 0: loss: 28.53 grad norm: 21.18 time: 0.870
2025-12-16 19:31:46,063: Running test after training batch: 0
2025-12-16 19:31:57,983: Val batch 0: PER (avg): 5.0646 CTC Loss (avg): 24.8933 time: 11.920
2025-12-16 19:31:57,983: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:31:57,983: t15.2023.08.13 val PER: 4.2911
2025-12-16 19:31:57,983: t15.2023.08.18 val PER: 4.5901
2025-12-16 19:31:57,983: t15.2023.08.20 val PER: 4.4091
2025-12-16 19:31:57,983: t15.2023.08.25 val PER: 4.5723
2025-12-16 19:31:57,983: t15.2023.08.27 val PER: 4.2090
2025-12-16 19:31:57,984: t15.2023.09.01 val PER: 4.6705
2025-12-16 19:31:57,984: t15.2023.09.03 val PER: 4.4050
2025-12-16 19:31:57,984: t15.2023.09.24 val PER: 5.3277
2025-12-16 19:31:57,984: t15.2023.09.29 val PER: 5.2272
2025-12-16 19:31:57,984: t15.2023.10.01 val PER: 4.1063
2025-12-16 19:31:57,984: t15.2023.10.06 val PER: 5.2982
2025-12-16 19:31:57,984: t15.2023.10.08 val PER: 3.8539
2025-12-16 19:31:57,984: t15.2023.10.13 val PER: 4.9178
2025-12-16 19:31:57,984: t15.2023.10.15 val PER: 5.1068
2025-12-16 19:31:57,984: t15.2023.10.20 val PER: 5.3087
2025-12-16 19:31:57,984: t15.2023.10.22 val PER: 5.1949
2025-12-16 19:31:57,984: t15.2023.11.03 val PER: 5.7307
2025-12-16 19:31:57,984: t15.2023.11.04 val PER: 6.8464
2025-12-16 19:31:57,984: t15.2023.11.17 val PER: 7.5054
2025-12-16 19:31:57,984: t15.2023.11.19 val PER: 6.0200
2025-12-16 19:31:57,984: t15.2023.11.26 val PER: 5.4862
2025-12-16 19:31:57,984: t15.2023.12.03 val PER: 5.2216
2025-12-16 19:31:57,984: t15.2023.12.08 val PER: 5.6312
2025-12-16 19:31:57,984: t15.2023.12.10 val PER: 6.1275
2025-12-16 19:31:57,984: t15.2023.12.17 val PER: 4.8378
2025-12-16 19:31:57,985: t15.2023.12.29 val PER: 4.8648
2025-12-16 19:31:57,985: t15.2024.02.25 val PER: 5.0449
2025-12-16 19:31:57,985: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:31:57,985: t15.2024.03.08 val PER: 4.6330
2025-12-16 19:31:57,985: t15.2024.03.15 val PER: 4.6629
2025-12-16 19:31:57,985: t15.2024.03.17 val PER: 4.8787
2025-12-16 19:31:57,985: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:31:57,985: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:31:57,985: t15.2024.05.10 val PER: 4.8886
2025-12-16 19:31:57,985: t15.2024.06.14 val PER: 5.6719
2025-12-16 19:31:57,985: t15.2024.07.19 val PER: 3.7119
2025-12-16 19:31:57,985: t15.2024.07.21 val PER: 5.8738
2025-12-16 19:31:57,985: t15.2024.07.28 val PER: 5.8897
2025-12-16 19:31:57,985: t15.2025.01.10 val PER: 3.7920
2025-12-16 19:31:57,985: t15.2025.01.12 val PER: 6.4442
2025-12-16 19:31:57,985: t15.2025.03.14 val PER: 3.4867
2025-12-16 19:31:57,985: t15.2025.03.16 val PER: 6.4660
2025-12-16 19:31:57,985: t15.2025.03.30 val PER: 4.9115
2025-12-16 19:31:57,985: t15.2025.04.13 val PER: 5.3110
2025-12-16 19:31:57,985: New best test PER inf --> 5.0646
2025-12-16 19:31:57,986: Checkpointing model
2025-12-16 19:31:58,145: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:32:14,525: Train batch 200: loss: 21.96 grad norm: 48.22 time: 0.068
2025-12-16 19:32:31,330: Train batch 400: loss: 12.68 grad norm: 79.66 time: 0.075
2025-12-16 19:32:48,744: Train batch 600: loss: 7.14 grad norm: 51.10 time: 0.093
2025-12-16 19:33:06,533: Train batch 800: loss: 4.44 grad norm: 20.55 time: 0.103
2025-12-16 19:33:23,651: Train batch 1000: loss: 3.43 grad norm: 2.61 time: 0.029
2025-12-16 19:33:32,943: Train batch 1200: loss: 3.36 grad norm: 0.59 time: 0.036
2025-12-16 19:33:43,299: Train batch 1400: loss: 3.23 grad norm: 0.85 time: 0.051
2025-12-16 19:33:52,937: Train batch 1600: loss: 3.10 grad norm: 1.19 time: 0.055
2025-12-16 19:34:02,783: Train batch 1800: loss: 3.05 grad norm: 1.04 time: 0.050
2025-12-16 19:34:13,421: Train batch 2000: loss: 3.02 grad norm: 1.09 time: 0.059
2025-12-16 19:34:13,421: Running test after training batch: 2000
2025-12-16 19:34:22,498: Val batch 2000: PER (avg): 0.7697 CTC Loss (avg): 3.3348 time: 9.076
2025-12-16 19:34:22,498: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:34:22,498: t15.2023.08.13 val PER: 0.7734
2025-12-16 19:34:22,498: t15.2023.08.18 val PER: 0.7754
2025-12-16 19:34:22,498: t15.2023.08.20 val PER: 0.7506
2025-12-16 19:34:22,498: t15.2023.08.25 val PER: 0.7666
2025-12-16 19:34:22,498: t15.2023.08.27 val PER: 0.8006
2025-12-16 19:34:22,499: t15.2023.09.01 val PER: 0.7670
2025-12-16 19:34:22,499: t15.2023.09.03 val PER: 0.7601
2025-12-16 19:34:22,499: t15.2023.09.24 val PER: 0.7646
2025-12-16 19:34:22,499: t15.2023.09.29 val PER: 0.7856
2025-12-16 19:34:22,499: t15.2023.10.01 val PER: 0.7781
2025-12-16 19:34:22,499: t15.2023.10.06 val PER: 0.7417
2025-12-16 19:34:22,499: t15.2023.10.08 val PER: 0.8106
2025-12-16 19:34:22,499: t15.2023.10.13 val PER: 0.8099
2025-12-16 19:34:22,499: t15.2023.10.15 val PER: 0.7693
2025-12-16 19:34:22,499: t15.2023.10.20 val PER: 0.7886
2025-12-16 19:34:22,499: t15.2023.10.22 val PER: 0.7528
2025-12-16 19:34:22,499: t15.2023.11.03 val PER: 0.7897
2025-12-16 19:34:22,499: t15.2023.11.04 val PER: 0.7543
2025-12-16 19:34:22,499: t15.2023.11.17 val PER: 0.7714
2025-12-16 19:34:22,499: t15.2023.11.19 val PER: 0.7285
2025-12-16 19:34:22,499: t15.2023.11.26 val PER: 0.7819
2025-12-16 19:34:22,499: t15.2023.12.03 val PER: 0.7826
2025-12-16 19:34:22,499: t15.2023.12.08 val PER: 0.7643
2025-12-16 19:34:22,499: t15.2023.12.10 val PER: 0.7661
2025-12-16 19:34:22,500: t15.2023.12.17 val PER: 0.7588
2025-12-16 19:34:22,500: t15.2023.12.29 val PER: 0.7776
2025-12-16 19:34:22,500: t15.2024.02.25 val PER: 0.7851
2025-12-16 19:34:22,500: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:34:22,500: t15.2024.03.08 val PER: 0.7568
2025-12-16 19:34:22,500: t15.2024.03.15 val PER: 0.7817
2025-12-16 19:34:22,500: t15.2024.03.17 val PER: 0.7636
2025-12-16 19:34:22,500: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:34:22,500: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:34:22,500: t15.2024.05.10 val PER: 0.7415
2025-12-16 19:34:22,500: t15.2024.06.14 val PER: 0.7587
2025-12-16 19:34:22,500: t15.2024.07.19 val PER: 0.7594
2025-12-16 19:34:22,500: t15.2024.07.21 val PER: 0.7469
2025-12-16 19:34:22,500: t15.2024.07.28 val PER: 0.7816
2025-12-16 19:34:22,500: t15.2025.01.10 val PER: 0.7700
2025-12-16 19:34:22,500: t15.2025.01.12 val PER: 0.7483
2025-12-16 19:34:22,500: t15.2025.03.14 val PER: 0.7973
2025-12-16 19:34:22,500: t15.2025.03.16 val PER: 0.7526
2025-12-16 19:34:22,500: t15.2025.03.30 val PER: 0.7552
2025-12-16 19:34:22,501: t15.2025.04.13 val PER: 0.7489
2025-12-16 19:34:22,501: New best test PER 5.0646 --> 0.7697
2025-12-16 19:34:22,501: Checkpointing model
2025-12-16 19:34:22,979: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:34:35,356: Train batch 2200: loss: 3.05 grad norm: 1.06 time: 0.046
2025-12-16 19:34:50,036: Train batch 2400: loss: 3.01 grad norm: 1.51 time: 0.073
2025-12-16 19:35:07,615: Train batch 2600: loss: 2.82 grad norm: 1.42 time: 0.073
2025-12-16 19:35:24,702: Train batch 2800: loss: 2.84 grad norm: 2.23 time: 0.078
2025-12-16 19:35:42,510: Train batch 3000: loss: 2.74 grad norm: 2.17 time: 0.089
2025-12-16 19:35:59,612: Train batch 3200: loss: 2.72 grad norm: 2.38 time: 0.103
2025-12-16 19:36:14,992: Train batch 3400: loss: 2.71 grad norm: 2.62 time: 0.050
2025-12-16 19:36:32,387: Train batch 3600: loss: 2.65 grad norm: 2.53 time: 0.107
2025-12-16 19:36:49,720: Train batch 3800: loss: 2.44 grad norm: 2.32 time: 0.066
2025-12-16 19:37:07,351: Train batch 4000: loss: 2.36 grad norm: 2.66 time: 0.068
2025-12-16 19:37:07,351: Running test after training batch: 4000
2025-12-16 19:37:16,594: Val batch 4000: PER (avg): 0.5708 CTC Loss (avg): 2.4320 time: 9.243
2025-12-16 19:37:16,594: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:37:16,594: t15.2023.08.13 val PER: 0.5291
2025-12-16 19:37:16,594: t15.2023.08.18 val PER: 0.5365
2025-12-16 19:37:16,594: t15.2023.08.20 val PER: 0.5052
2025-12-16 19:37:16,594: t15.2023.08.25 val PER: 0.5226
2025-12-16 19:37:16,594: t15.2023.08.27 val PER: 0.5675
2025-12-16 19:37:16,594: t15.2023.09.01 val PER: 0.4976
2025-12-16 19:37:16,594: t15.2023.09.03 val PER: 0.5487
2025-12-16 19:37:16,594: t15.2023.09.24 val PER: 0.5680
2025-12-16 19:37:16,595: t15.2023.09.29 val PER: 0.5673
2025-12-16 19:37:16,595: t15.2023.10.01 val PER: 0.5951
2025-12-16 19:37:16,595: t15.2023.10.06 val PER: 0.5501
2025-12-16 19:37:16,595: t15.2023.10.08 val PER: 0.6022
2025-12-16 19:37:16,595: t15.2023.10.13 val PER: 0.6610
2025-12-16 19:37:16,595: t15.2023.10.15 val PER: 0.5933
2025-12-16 19:37:16,595: t15.2023.10.20 val PER: 0.5604
2025-12-16 19:37:16,595: t15.2023.10.22 val PER: 0.5367
2025-12-16 19:37:16,595: t15.2023.11.03 val PER: 0.5651
2025-12-16 19:37:16,595: t15.2023.11.04 val PER: 0.4505
2025-12-16 19:37:16,595: t15.2023.11.17 val PER: 0.5008
2025-12-16 19:37:16,595: t15.2023.11.19 val PER: 0.4810
2025-12-16 19:37:16,595: t15.2023.11.26 val PER: 0.6232
2025-12-16 19:37:16,595: t15.2023.12.03 val PER: 0.5809
2025-12-16 19:37:16,595: t15.2023.12.08 val PER: 0.5752
2025-12-16 19:37:16,595: t15.2023.12.10 val PER: 0.5598
2025-12-16 19:37:16,595: t15.2023.12.17 val PER: 0.5520
2025-12-16 19:37:16,595: t15.2023.12.29 val PER: 0.5957
2025-12-16 19:37:16,595: t15.2024.02.25 val PER: 0.5407
2025-12-16 19:37:16,596: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:37:16,596: t15.2024.03.08 val PER: 0.5974
2025-12-16 19:37:16,596: t15.2024.03.15 val PER: 0.6079
2025-12-16 19:37:16,596: t15.2024.03.17 val PER: 0.5711
2025-12-16 19:37:16,596: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:37:16,596: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:37:16,596: t15.2024.05.10 val PER: 0.5706
2025-12-16 19:37:16,596: t15.2024.06.14 val PER: 0.5568
2025-12-16 19:37:16,596: t15.2024.07.19 val PER: 0.6025
2025-12-16 19:37:16,596: t15.2024.07.21 val PER: 0.5455
2025-12-16 19:37:16,596: t15.2024.07.28 val PER: 0.5735
2025-12-16 19:37:16,596: t15.2025.01.10 val PER: 0.6185
2025-12-16 19:37:16,596: t15.2025.01.12 val PER: 0.5774
2025-12-16 19:37:16,596: t15.2025.03.14 val PER: 0.6317
2025-12-16 19:37:16,596: t15.2025.03.16 val PER: 0.5877
2025-12-16 19:37:16,596: t15.2025.03.30 val PER: 0.6023
2025-12-16 19:37:16,596: t15.2025.04.13 val PER: 0.5777
2025-12-16 19:37:16,596: New best test PER 0.7697 --> 0.5708
2025-12-16 19:37:16,596: Checkpointing model
2025-12-16 19:37:17,065: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:37:34,610: Train batch 4200: loss: 2.29 grad norm: 3.79 time: 0.092
2025-12-16 19:37:52,875: Train batch 4400: loss: 2.35 grad norm: 3.03 time: 0.070
2025-12-16 19:38:10,544: Train batch 4600: loss: 2.16 grad norm: 2.91 time: 0.069
2025-12-16 19:38:27,850: Train batch 4800: loss: 2.10 grad norm: 3.32 time: 0.059
2025-12-16 19:38:45,017: Train batch 5000: loss: 2.26 grad norm: 3.06 time: 0.079
2025-12-16 19:39:01,378: Train batch 5200: loss: 1.80 grad norm: 2.82 time: 0.078
2025-12-16 19:39:16,515: Train batch 5400: loss: 1.99 grad norm: 3.41 time: 0.052
2025-12-16 19:39:33,416: Train batch 5600: loss: 1.78 grad norm: 3.70 time: 0.081
2025-12-16 19:39:50,885: Train batch 5800: loss: 1.87 grad norm: 3.22 time: 0.088
2025-12-16 19:40:08,183: Train batch 6000: loss: 1.56 grad norm: 2.94 time: 0.056
2025-12-16 19:40:08,183: Running test after training batch: 6000
2025-12-16 19:40:17,660: Val batch 6000: PER (avg): 0.3534 CTC Loss (avg): 1.3998 time: 9.477
2025-12-16 19:40:17,661: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:40:17,661: t15.2023.08.13 val PER: 0.3243
2025-12-16 19:40:17,661: t15.2023.08.18 val PER: 0.3177
2025-12-16 19:40:17,661: t15.2023.08.20 val PER: 0.2963
2025-12-16 19:40:17,661: t15.2023.08.25 val PER: 0.2922
2025-12-16 19:40:17,661: t15.2023.08.27 val PER: 0.3955
2025-12-16 19:40:17,661: t15.2023.09.01 val PER: 0.2711
2025-12-16 19:40:17,661: t15.2023.09.03 val PER: 0.3622
2025-12-16 19:40:17,661: t15.2023.09.24 val PER: 0.2852
2025-12-16 19:40:17,661: t15.2023.09.29 val PER: 0.3357
2025-12-16 19:40:17,661: t15.2023.10.01 val PER: 0.3600
2025-12-16 19:40:17,661: t15.2023.10.06 val PER: 0.3111
2025-12-16 19:40:17,661: t15.2023.10.08 val PER: 0.3924
2025-12-16 19:40:17,661: t15.2023.10.13 val PER: 0.4375
2025-12-16 19:40:17,661: t15.2023.10.15 val PER: 0.3322
2025-12-16 19:40:17,661: t15.2023.10.20 val PER: 0.4161
2025-12-16 19:40:17,661: t15.2023.10.22 val PER: 0.3218
2025-12-16 19:40:17,662: t15.2023.11.03 val PER: 0.3569
2025-12-16 19:40:17,662: t15.2023.11.04 val PER: 0.1570
2025-12-16 19:40:17,662: t15.2023.11.17 val PER: 0.2722
2025-12-16 19:40:17,662: t15.2023.11.19 val PER: 0.2395
2025-12-16 19:40:17,662: t15.2023.11.26 val PER: 0.3957
2025-12-16 19:40:17,662: t15.2023.12.03 val PER: 0.3372
2025-12-16 19:40:17,662: t15.2023.12.08 val PER: 0.3449
2025-12-16 19:40:17,662: t15.2023.12.10 val PER: 0.3127
2025-12-16 19:40:17,662: t15.2023.12.17 val PER: 0.3815
2025-12-16 19:40:17,662: t15.2023.12.29 val PER: 0.3548
2025-12-16 19:40:17,662: t15.2024.02.25 val PER: 0.2809
2025-12-16 19:40:17,662: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:40:17,662: t15.2024.03.08 val PER: 0.4054
2025-12-16 19:40:17,662: t15.2024.03.15 val PER: 0.3865
2025-12-16 19:40:17,662: t15.2024.03.17 val PER: 0.3473
2025-12-16 19:40:17,662: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:40:17,662: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:40:17,662: t15.2024.05.10 val PER: 0.3447
2025-12-16 19:40:17,662: t15.2024.06.14 val PER: 0.3549
2025-12-16 19:40:17,662: t15.2024.07.19 val PER: 0.4272
2025-12-16 19:40:17,663: t15.2024.07.21 val PER: 0.2738
2025-12-16 19:40:17,663: t15.2024.07.28 val PER: 0.3213
2025-12-16 19:40:17,663: t15.2025.01.10 val PER: 0.4890
2025-12-16 19:40:17,663: t15.2025.01.12 val PER: 0.3480
2025-12-16 19:40:17,663: t15.2025.03.14 val PER: 0.5518
2025-12-16 19:40:17,663: t15.2025.03.16 val PER: 0.4071
2025-12-16 19:40:17,663: t15.2025.03.30 val PER: 0.4874
2025-12-16 19:40:17,663: t15.2025.04.13 val PER: 0.4165
2025-12-16 19:40:17,663: New best test PER 0.5708 --> 0.3534
2025-12-16 19:40:17,663: Checkpointing model
2025-12-16 19:40:18,118: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:40:34,062: Train batch 6200: loss: 1.71 grad norm: 4.80 time: 0.050
2025-12-16 19:40:49,933: Train batch 6400: loss: 1.53 grad norm: 2.86 time: 0.091
2025-12-16 19:41:06,930: Train batch 6600: loss: 1.61 grad norm: 5.70 time: 0.089
2025-12-16 19:41:24,869: Train batch 6800: loss: 1.60 grad norm: 4.24 time: 0.080
2025-12-16 19:41:42,565: Train batch 7000: loss: 1.73 grad norm: 7.63 time: 0.084
2025-12-16 19:41:59,341: Train batch 7200: loss: 1.73 grad norm: 4.33 time: 0.075
2025-12-16 19:42:15,446: Train batch 7400: loss: 1.64 grad norm: 2.80 time: 0.085
2025-12-16 19:42:33,172: Train batch 7600: loss: 1.58 grad norm: 10.93 time: 0.071
2025-12-16 19:42:49,515: Train batch 7800: loss: 1.67 grad norm: 3.39 time: 0.079
2025-12-16 19:43:06,330: Train batch 8000: loss: 1.57 grad norm: 9.51 time: 0.089
2025-12-16 19:43:06,330: Running test after training batch: 8000
2025-12-16 19:43:15,774: Val batch 8000: PER (avg): 0.3047 CTC Loss (avg): 1.0856 time: 9.444
2025-12-16 19:43:15,774: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:43:15,774: t15.2023.08.13 val PER: 0.2682
2025-12-16 19:43:15,774: t15.2023.08.18 val PER: 0.2632
2025-12-16 19:43:15,774: t15.2023.08.20 val PER: 0.2550
2025-12-16 19:43:15,774: t15.2023.08.25 val PER: 0.2545
2025-12-16 19:43:15,774: t15.2023.08.27 val PER: 0.3344
2025-12-16 19:43:15,775: t15.2023.09.01 val PER: 0.2208
2025-12-16 19:43:15,775: t15.2023.09.03 val PER: 0.2838
2025-12-16 19:43:15,775: t15.2023.09.24 val PER: 0.2500
2025-12-16 19:43:15,775: t15.2023.09.29 val PER: 0.2693
2025-12-16 19:43:15,775: t15.2023.10.01 val PER: 0.3283
2025-12-16 19:43:15,775: t15.2023.10.06 val PER: 0.2325
2025-12-16 19:43:15,775: t15.2023.10.08 val PER: 0.3640
2025-12-16 19:43:15,775: t15.2023.10.13 val PER: 0.3747
2025-12-16 19:43:15,775: t15.2023.10.15 val PER: 0.2815
2025-12-16 19:43:15,775: t15.2023.10.20 val PER: 0.3255
2025-12-16 19:43:15,775: t15.2023.10.22 val PER: 0.2550
2025-12-16 19:43:15,775: t15.2023.11.03 val PER: 0.3345
2025-12-16 19:43:15,775: t15.2023.11.04 val PER: 0.0887
2025-12-16 19:43:15,775: t15.2023.11.17 val PER: 0.2302
2025-12-16 19:43:15,775: t15.2023.11.19 val PER: 0.1577
2025-12-16 19:43:15,775: t15.2023.11.26 val PER: 0.3486
2025-12-16 19:43:15,775: t15.2023.12.03 val PER: 0.2773
2025-12-16 19:43:15,775: t15.2023.12.08 val PER: 0.3202
2025-12-16 19:43:15,776: t15.2023.12.10 val PER: 0.2549
2025-12-16 19:43:15,776: t15.2023.12.17 val PER: 0.3212
2025-12-16 19:43:15,776: t15.2023.12.29 val PER: 0.3040
2025-12-16 19:43:15,776: t15.2024.02.25 val PER: 0.2289
2025-12-16 19:43:15,776: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:43:15,776: t15.2024.03.08 val PER: 0.3585
2025-12-16 19:43:15,776: t15.2024.03.15 val PER: 0.3527
2025-12-16 19:43:15,776: t15.2024.03.17 val PER: 0.3159
2025-12-16 19:43:15,776: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:43:15,776: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:43:15,776: t15.2024.05.10 val PER: 0.2883
2025-12-16 19:43:15,776: t15.2024.06.14 val PER: 0.3186
2025-12-16 19:43:15,776: t15.2024.07.19 val PER: 0.3685
2025-12-16 19:43:15,776: t15.2024.07.21 val PER: 0.2400
2025-12-16 19:43:15,776: t15.2024.07.28 val PER: 0.2897
2025-12-16 19:43:15,776: t15.2025.01.10 val PER: 0.4421
2025-12-16 19:43:15,776: t15.2025.01.12 val PER: 0.2995
2025-12-16 19:43:15,776: t15.2025.03.14 val PER: 0.4763
2025-12-16 19:43:15,776: t15.2025.03.16 val PER: 0.3364
2025-12-16 19:43:15,776: t15.2025.03.30 val PER: 0.4437
2025-12-16 19:43:15,777: t15.2025.04.13 val PER: 0.3752
2025-12-16 19:43:15,777: New best test PER 0.3534 --> 0.3047
2025-12-16 19:43:15,777: Checkpointing model
2025-12-16 19:43:16,270: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:43:33,053: Train batch 8200: loss: 1.23 grad norm: 3.57 time: 0.094
2025-12-16 19:43:48,974: Train batch 8400: loss: 1.56 grad norm: 2.73 time: 0.067
2025-12-16 19:44:06,630: Train batch 8600: loss: 1.42 grad norm: 5.19 time: 0.080
2025-12-16 19:44:23,205: Train batch 8800: loss: 1.59 grad norm: 6.51 time: 0.080
2025-12-16 19:44:40,562: Train batch 9000: loss: 1.74 grad norm: 12.37 time: 0.079
2025-12-16 19:44:56,862: Train batch 9200: loss: 1.64 grad norm: 3.23 time: 0.086
2025-12-16 19:45:14,515: Train batch 9400: loss: 1.37 grad norm: 2.88 time: 0.074
2025-12-16 19:45:32,208: Train batch 9600: loss: 1.47 grad norm: 5.66 time: 0.093
2025-12-16 19:45:49,386: Train batch 9800: loss: 1.32 grad norm: 3.41 time: 0.089
2025-12-16 19:46:06,711: Train batch 10000: loss: 1.32 grad norm: 10.17 time: 0.092
2025-12-16 19:46:06,711: Running test after training batch: 10000
2025-12-16 19:46:16,164: Val batch 10000: PER (avg): 0.2609 CTC Loss (avg): 0.9861 time: 9.453
2025-12-16 19:46:16,165: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:46:16,165: t15.2023.08.13 val PER: 0.2235
2025-12-16 19:46:16,165: t15.2023.08.18 val PER: 0.2171
2025-12-16 19:46:16,165: t15.2023.08.20 val PER: 0.1835
2025-12-16 19:46:16,165: t15.2023.08.25 val PER: 0.2063
2025-12-16 19:46:16,165: t15.2023.08.27 val PER: 0.2894
2025-12-16 19:46:16,165: t15.2023.09.01 val PER: 0.1802
2025-12-16 19:46:16,165: t15.2023.09.03 val PER: 0.2292
2025-12-16 19:46:16,165: t15.2023.09.24 val PER: 0.2148
2025-12-16 19:46:16,165: t15.2023.09.29 val PER: 0.2253
2025-12-16 19:46:16,165: t15.2023.10.01 val PER: 0.2814
2025-12-16 19:46:16,165: t15.2023.10.06 val PER: 0.2034
2025-12-16 19:46:16,165: t15.2023.10.08 val PER: 0.3139
2025-12-16 19:46:16,165: t15.2023.10.13 val PER: 0.3468
2025-12-16 19:46:16,165: t15.2023.10.15 val PER: 0.2492
2025-12-16 19:46:16,165: t15.2023.10.20 val PER: 0.3322
2025-12-16 19:46:16,165: t15.2023.10.22 val PER: 0.2060
2025-12-16 19:46:16,166: t15.2023.11.03 val PER: 0.2687
2025-12-16 19:46:16,166: t15.2023.11.04 val PER: 0.0717
2025-12-16 19:46:16,166: t15.2023.11.17 val PER: 0.1711
2025-12-16 19:46:16,166: t15.2023.11.19 val PER: 0.1577
2025-12-16 19:46:16,166: t15.2023.11.26 val PER: 0.2674
2025-12-16 19:46:16,166: t15.2023.12.03 val PER: 0.2332
2025-12-16 19:46:16,166: t15.2023.12.08 val PER: 0.2510
2025-12-16 19:46:16,166: t15.2023.12.10 val PER: 0.2011
2025-12-16 19:46:16,166: t15.2023.12.17 val PER: 0.2796
2025-12-16 19:46:16,166: t15.2023.12.29 val PER: 0.2512
2025-12-16 19:46:16,166: t15.2024.02.25 val PER: 0.2275
2025-12-16 19:46:16,166: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:46:16,166: t15.2024.03.08 val PER: 0.3414
2025-12-16 19:46:16,166: t15.2024.03.15 val PER: 0.3058
2025-12-16 19:46:16,166: t15.2024.03.17 val PER: 0.2622
2025-12-16 19:46:16,166: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:46:16,166: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:46:16,166: t15.2024.05.10 val PER: 0.2675
2025-12-16 19:46:16,166: t15.2024.06.14 val PER: 0.3107
2025-12-16 19:46:16,167: t15.2024.07.19 val PER: 0.3309
2025-12-16 19:46:16,167: t15.2024.07.21 val PER: 0.2062
2025-12-16 19:46:16,167: t15.2024.07.28 val PER: 0.2419
2025-12-16 19:46:16,167: t15.2025.01.10 val PER: 0.4366
2025-12-16 19:46:16,167: t15.2025.01.12 val PER: 0.2710
2025-12-16 19:46:16,167: t15.2025.03.14 val PER: 0.4379
2025-12-16 19:46:16,167: t15.2025.03.16 val PER: 0.2788
2025-12-16 19:46:16,167: t15.2025.03.30 val PER: 0.3828
2025-12-16 19:46:16,167: t15.2025.04.13 val PER: 0.3267
2025-12-16 19:46:16,167: New best test PER 0.3047 --> 0.2609
2025-12-16 19:46:16,167: Checkpointing model
2025-12-16 19:46:16,651: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:46:34,264: Train batch 10200: loss: 1.38 grad norm: 4.65 time: 0.078
2025-12-16 19:46:50,427: Train batch 10400: loss: 1.39 grad norm: 9.12 time: 0.081
2025-12-16 19:47:07,096: Train batch 10600: loss: 1.31 grad norm: 10.81 time: 0.095
2025-12-16 19:47:24,332: Train batch 10800: loss: 1.23 grad norm: 2.73 time: 0.107
2025-12-16 19:47:41,797: Train batch 11000: loss: 1.18 grad norm: 4.60 time: 0.056
2025-12-16 19:47:51,630: Train batch 11200: loss: 1.14 grad norm: 3.75 time: 0.043
2025-12-16 19:48:06,744: Train batch 11400: loss: 1.37 grad norm: 6.44 time: 0.051
2025-12-16 19:48:17,491: Train batch 11600: loss: 1.10 grad norm: 3.84 time: 0.042
2025-12-16 19:48:30,926: Train batch 11800: loss: 1.25 grad norm: 2.72 time: 0.075
2025-12-16 19:48:47,863: Train batch 12000: loss: 1.15 grad norm: 10.40 time: 0.084
2025-12-16 19:48:47,864: Running test after training batch: 12000
2025-12-16 19:48:57,276: Val batch 12000: PER (avg): 0.2480 CTC Loss (avg): 0.8979 time: 9.412
2025-12-16 19:48:57,276: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:48:57,277: t15.2023.08.13 val PER: 0.2495
2025-12-16 19:48:57,277: t15.2023.08.18 val PER: 0.1878
2025-12-16 19:48:57,277: t15.2023.08.20 val PER: 0.1668
2025-12-16 19:48:57,277: t15.2023.08.25 val PER: 0.1913
2025-12-16 19:48:57,277: t15.2023.08.27 val PER: 0.2781
2025-12-16 19:48:57,277: t15.2023.09.01 val PER: 0.1510
2025-12-16 19:48:57,277: t15.2023.09.03 val PER: 0.2304
2025-12-16 19:48:57,277: t15.2023.09.24 val PER: 0.2075
2025-12-16 19:48:57,277: t15.2023.09.29 val PER: 0.2304
2025-12-16 19:48:57,277: t15.2023.10.01 val PER: 0.2708
2025-12-16 19:48:57,277: t15.2023.10.06 val PER: 0.1744
2025-12-16 19:48:57,277: t15.2023.10.08 val PER: 0.3099
2025-12-16 19:48:57,277: t15.2023.10.13 val PER: 0.3018
2025-12-16 19:48:57,277: t15.2023.10.15 val PER: 0.2169
2025-12-16 19:48:57,277: t15.2023.10.20 val PER: 0.2852
2025-12-16 19:48:57,277: t15.2023.10.22 val PER: 0.1993
2025-12-16 19:48:57,277: t15.2023.11.03 val PER: 0.2761
2025-12-16 19:48:57,277: t15.2023.11.04 val PER: 0.0444
2025-12-16 19:48:57,278: t15.2023.11.17 val PER: 0.1446
2025-12-16 19:48:57,278: t15.2023.11.19 val PER: 0.1118
2025-12-16 19:48:57,278: t15.2023.11.26 val PER: 0.2703
2025-12-16 19:48:57,278: t15.2023.12.03 val PER: 0.2164
2025-12-16 19:48:57,278: t15.2023.12.08 val PER: 0.2383
2025-12-16 19:48:57,278: t15.2023.12.10 val PER: 0.1984
2025-12-16 19:48:57,278: t15.2023.12.17 val PER: 0.2734
2025-12-16 19:48:57,278: t15.2023.12.29 val PER: 0.2423
2025-12-16 19:48:57,278: t15.2024.02.25 val PER: 0.2233
2025-12-16 19:48:57,278: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:48:57,278: t15.2024.03.08 val PER: 0.3343
2025-12-16 19:48:57,278: t15.2024.03.15 val PER: 0.3096
2025-12-16 19:48:57,278: t15.2024.03.17 val PER: 0.2601
2025-12-16 19:48:57,278: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:48:57,278: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:48:57,278: t15.2024.05.10 val PER: 0.2541
2025-12-16 19:48:57,278: t15.2024.06.14 val PER: 0.2461
2025-12-16 19:48:57,278: t15.2024.07.19 val PER: 0.3434
2025-12-16 19:48:57,279: t15.2024.07.21 val PER: 0.1848
2025-12-16 19:48:57,279: t15.2024.07.28 val PER: 0.2279
2025-12-16 19:48:57,279: t15.2025.01.10 val PER: 0.4091
2025-12-16 19:48:57,279: t15.2025.01.12 val PER: 0.2256
2025-12-16 19:48:57,279: t15.2025.03.14 val PER: 0.4201
2025-12-16 19:48:57,279: t15.2025.03.16 val PER: 0.2906
2025-12-16 19:48:57,279: t15.2025.03.30 val PER: 0.3759
2025-12-16 19:48:57,279: t15.2025.04.13 val PER: 0.3010
2025-12-16 19:48:57,279: New best test PER 0.2609 --> 0.2480
2025-12-16 19:48:57,279: Checkpointing model
2025-12-16 19:48:57,749: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:49:14,777: Train batch 12200: loss: 1.33 grad norm: 2.27 time: 0.091
2025-12-16 19:49:32,176: Train batch 12400: loss: 1.30 grad norm: 3.43 time: 0.091
2025-12-16 19:49:49,559: Train batch 12600: loss: 1.02 grad norm: 4.13 time: 0.090
2025-12-16 19:50:06,858: Train batch 12800: loss: 1.31 grad norm: 3.13 time: 0.072
2025-12-16 19:50:22,541: Train batch 13000: loss: 1.30 grad norm: 3.19 time: 0.082
2025-12-16 19:50:38,527: Train batch 13200: loss: 1.15 grad norm: 2.73 time: 0.070
2025-12-16 19:50:54,043: Train batch 13400: loss: 1.02 grad norm: 2.01 time: 0.071
2025-12-16 19:51:11,018: Train batch 13600: loss: 1.05 grad norm: 3.97 time: 0.068
2025-12-16 19:51:27,292: Train batch 13800: loss: 1.01 grad norm: 1.89 time: 0.084
2025-12-16 19:51:44,586: Train batch 14000: loss: 1.17 grad norm: 9.89 time: 0.087
2025-12-16 19:51:44,587: Running test after training batch: 14000
2025-12-16 19:51:54,497: Val batch 14000: PER (avg): 0.2263 CTC Loss (avg): 0.8631 time: 9.910
2025-12-16 19:51:54,497: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:51:54,497: t15.2023.08.13 val PER: 0.2069
2025-12-16 19:51:54,497: t15.2023.08.18 val PER: 0.1785
2025-12-16 19:51:54,497: t15.2023.08.20 val PER: 0.1565
2025-12-16 19:51:54,497: t15.2023.08.25 val PER: 0.1717
2025-12-16 19:51:54,497: t15.2023.08.27 val PER: 0.2781
2025-12-16 19:51:54,497: t15.2023.09.01 val PER: 0.1291
2025-12-16 19:51:54,498: t15.2023.09.03 val PER: 0.2245
2025-12-16 19:51:54,498: t15.2023.09.24 val PER: 0.1833
2025-12-16 19:51:54,498: t15.2023.09.29 val PER: 0.2214
2025-12-16 19:51:54,498: t15.2023.10.01 val PER: 0.2550
2025-12-16 19:51:54,498: t15.2023.10.06 val PER: 0.1561
2025-12-16 19:51:54,498: t15.2023.10.08 val PER: 0.2774
2025-12-16 19:51:54,498: t15.2023.10.13 val PER: 0.3157
2025-12-16 19:51:54,498: t15.2023.10.15 val PER: 0.2182
2025-12-16 19:51:54,498: t15.2023.10.20 val PER: 0.2718
2025-12-16 19:51:54,498: t15.2023.10.22 val PER: 0.1915
2025-12-16 19:51:54,498: t15.2023.11.03 val PER: 0.2497
2025-12-16 19:51:54,498: t15.2023.11.04 val PER: 0.0410
2025-12-16 19:51:54,498: t15.2023.11.17 val PER: 0.1058
2025-12-16 19:51:54,498: t15.2023.11.19 val PER: 0.1297
2025-12-16 19:51:54,498: t15.2023.11.26 val PER: 0.2616
2025-12-16 19:51:54,498: t15.2023.12.03 val PER: 0.1786
2025-12-16 19:51:54,498: t15.2023.12.08 val PER: 0.2117
2025-12-16 19:51:54,498: t15.2023.12.10 val PER: 0.1682
2025-12-16 19:51:54,498: t15.2023.12.17 val PER: 0.2453
2025-12-16 19:51:54,498: t15.2023.12.29 val PER: 0.2162
2025-12-16 19:51:54,499: t15.2024.02.25 val PER: 0.1854
2025-12-16 19:51:54,499: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:51:54,499: t15.2024.03.08 val PER: 0.2859
2025-12-16 19:51:54,499: t15.2024.03.15 val PER: 0.2939
2025-12-16 19:51:54,499: t15.2024.03.17 val PER: 0.2106
2025-12-16 19:51:54,499: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:51:54,499: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:51:54,499: t15.2024.05.10 val PER: 0.2556
2025-12-16 19:51:54,499: t15.2024.06.14 val PER: 0.2271
2025-12-16 19:51:54,499: t15.2024.07.19 val PER: 0.2782
2025-12-16 19:51:54,499: t15.2024.07.21 val PER: 0.1503
2025-12-16 19:51:54,499: t15.2024.07.28 val PER: 0.1926
2025-12-16 19:51:54,499: t15.2025.01.10 val PER: 0.3829
2025-12-16 19:51:54,499: t15.2025.01.12 val PER: 0.2232
2025-12-16 19:51:54,499: t15.2025.03.14 val PER: 0.4009
2025-12-16 19:51:54,499: t15.2025.03.16 val PER: 0.2487
2025-12-16 19:51:54,499: t15.2025.03.30 val PER: 0.3552
2025-12-16 19:51:54,499: t15.2025.04.13 val PER: 0.2782
2025-12-16 19:51:54,499: New best test PER 0.2480 --> 0.2263
2025-12-16 19:51:54,500: Checkpointing model
2025-12-16 19:51:54,955: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:52:11,576: Train batch 14200: loss: 1.07 grad norm: 3.25 time: 0.063
2025-12-16 19:52:28,524: Train batch 14400: loss: 1.00 grad norm: 2.12 time: 0.092
2025-12-16 19:52:46,358: Train batch 14600: loss: 1.38 grad norm: 4.11 time: 0.083
2025-12-16 19:53:02,123: Train batch 14800: loss: 1.03 grad norm: 38.46 time: 0.083
2025-12-16 19:53:19,006: Train batch 15000: loss: 1.09 grad norm: 1.74 time: 0.087
2025-12-16 19:53:35,635: Train batch 15200: loss: 1.27 grad norm: 14.67 time: 0.085
2025-12-16 19:53:53,379: Train batch 15400: loss: 1.12 grad norm: 2.91 time: 0.091
2025-12-16 19:54:10,966: Train batch 15600: loss: 1.00 grad norm: 17.97 time: 0.093
2025-12-16 19:54:28,588: Train batch 15800: loss: 1.07 grad norm: 4.67 time: 0.091
2025-12-16 19:54:46,112: Train batch 16000: loss: 0.92 grad norm: 2.78 time: 0.088
2025-12-16 19:54:46,113: Running test after training batch: 16000
2025-12-16 19:54:55,518: Val batch 16000: PER (avg): 0.2135 CTC Loss (avg): 0.8106 time: 9.405
2025-12-16 19:54:55,518: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:54:55,518: t15.2023.08.13 val PER: 0.1726
2025-12-16 19:54:55,518: t15.2023.08.18 val PER: 0.1811
2025-12-16 19:54:55,518: t15.2023.08.20 val PER: 0.1541
2025-12-16 19:54:55,518: t15.2023.08.25 val PER: 0.1687
2025-12-16 19:54:55,518: t15.2023.08.27 val PER: 0.2235
2025-12-16 19:54:55,518: t15.2023.09.01 val PER: 0.1226
2025-12-16 19:54:55,518: t15.2023.09.03 val PER: 0.2007
2025-12-16 19:54:55,518: t15.2023.09.24 val PER: 0.1553
2025-12-16 19:54:55,518: t15.2023.09.29 val PER: 0.2093
2025-12-16 19:54:55,518: t15.2023.10.01 val PER: 0.2431
2025-12-16 19:54:55,518: t15.2023.10.06 val PER: 0.1701
2025-12-16 19:54:55,518: t15.2023.10.08 val PER: 0.2706
2025-12-16 19:54:55,519: t15.2023.10.13 val PER: 0.2948
2025-12-16 19:54:55,519: t15.2023.10.15 val PER: 0.2057
2025-12-16 19:54:55,519: t15.2023.10.20 val PER: 0.2315
2025-12-16 19:54:55,519: t15.2023.10.22 val PER: 0.1693
2025-12-16 19:54:55,519: t15.2023.11.03 val PER: 0.2449
2025-12-16 19:54:55,519: t15.2023.11.04 val PER: 0.0546
2025-12-16 19:54:55,519: t15.2023.11.17 val PER: 0.1135
2025-12-16 19:54:55,519: t15.2023.11.19 val PER: 0.1038
2025-12-16 19:54:55,519: t15.2023.11.26 val PER: 0.2232
2025-12-16 19:54:55,519: t15.2023.12.03 val PER: 0.1723
2025-12-16 19:54:55,519: t15.2023.12.08 val PER: 0.1744
2025-12-16 19:54:55,519: t15.2023.12.10 val PER: 0.1511
2025-12-16 19:54:55,519: t15.2023.12.17 val PER: 0.2328
2025-12-16 19:54:55,519: t15.2023.12.29 val PER: 0.2080
2025-12-16 19:54:55,519: t15.2024.02.25 val PER: 0.1517
2025-12-16 19:54:55,519: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:54:55,519: t15.2024.03.08 val PER: 0.3044
2025-12-16 19:54:55,519: t15.2024.03.15 val PER: 0.2821
2025-12-16 19:54:55,520: t15.2024.03.17 val PER: 0.1939
2025-12-16 19:54:55,520: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:54:55,520: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:54:55,520: t15.2024.05.10 val PER: 0.2244
2025-12-16 19:54:55,520: t15.2024.06.14 val PER: 0.2445
2025-12-16 19:54:55,520: t15.2024.07.19 val PER: 0.2782
2025-12-16 19:54:55,520: t15.2024.07.21 val PER: 0.1476
2025-12-16 19:54:55,520: t15.2024.07.28 val PER: 0.2059
2025-12-16 19:54:55,520: t15.2025.01.10 val PER: 0.3444
2025-12-16 19:54:55,520: t15.2025.01.12 val PER: 0.2040
2025-12-16 19:54:55,520: t15.2025.03.14 val PER: 0.3757
2025-12-16 19:54:55,520: t15.2025.03.16 val PER: 0.2421
2025-12-16 19:54:55,520: t15.2025.03.30 val PER: 0.3540
2025-12-16 19:54:55,520: t15.2025.04.13 val PER: 0.2596
2025-12-16 19:54:55,520: New best test PER 0.2263 --> 0.2135
2025-12-16 19:54:55,520: Checkpointing model
2025-12-16 19:54:56,132: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:55:13,712: Train batch 16200: loss: 0.98 grad norm: 2.26 time: 0.088
2025-12-16 19:55:31,651: Train batch 16400: loss: 0.96 grad norm: 5.62 time: 0.097
2025-12-16 19:55:48,993: Train batch 16600: loss: 1.24 grad norm: 15.04 time: 0.100
2025-12-16 19:56:04,638: Train batch 16800: loss: 1.24 grad norm: 3.80 time: 0.085
2025-12-16 19:56:21,247: Train batch 17000: loss: 1.03 grad norm: 4.03 time: 0.087
2025-12-16 19:56:39,070: Train batch 17200: loss: 0.78 grad norm: 1.75 time: 0.100
2025-12-16 19:56:55,397: Train batch 17400: loss: 1.29 grad norm: 4.20 time: 0.056
2025-12-16 19:57:10,953: Train batch 17600: loss: 1.13 grad norm: 13.02 time: 0.093
2025-12-16 19:57:28,119: Train batch 17800: loss: 1.15 grad norm: 4.94 time: 0.068
2025-12-16 19:57:45,672: Train batch 18000: loss: 1.18 grad norm: 3.34 time: 0.086
2025-12-16 19:57:45,672: Running test after training batch: 18000
2025-12-16 19:57:55,329: Val batch 18000: PER (avg): 0.1987 CTC Loss (avg): 0.7751 time: 9.657
2025-12-16 19:57:55,330: t15.2023.08.11 val PER: 1.0000
2025-12-16 19:57:55,330: t15.2023.08.13 val PER: 0.1736
2025-12-16 19:57:55,330: t15.2023.08.18 val PER: 0.1718
2025-12-16 19:57:55,330: t15.2023.08.20 val PER: 0.1342
2025-12-16 19:57:55,330: t15.2023.08.25 val PER: 0.1295
2025-12-16 19:57:55,330: t15.2023.08.27 val PER: 0.2428
2025-12-16 19:57:55,330: t15.2023.09.01 val PER: 0.1112
2025-12-16 19:57:55,330: t15.2023.09.03 val PER: 0.1936
2025-12-16 19:57:55,330: t15.2023.09.24 val PER: 0.1541
2025-12-16 19:57:55,330: t15.2023.09.29 val PER: 0.1870
2025-12-16 19:57:55,330: t15.2023.10.01 val PER: 0.2477
2025-12-16 19:57:55,330: t15.2023.10.06 val PER: 0.1421
2025-12-16 19:57:55,330: t15.2023.10.08 val PER: 0.2639
2025-12-16 19:57:55,330: t15.2023.10.13 val PER: 0.2653
2025-12-16 19:57:55,330: t15.2023.10.15 val PER: 0.1866
2025-12-16 19:57:55,330: t15.2023.10.20 val PER: 0.2181
2025-12-16 19:57:55,330: t15.2023.10.22 val PER: 0.1581
2025-12-16 19:57:55,331: t15.2023.11.03 val PER: 0.2178
2025-12-16 19:57:55,331: t15.2023.11.04 val PER: 0.0410
2025-12-16 19:57:55,331: t15.2023.11.17 val PER: 0.1073
2025-12-16 19:57:55,331: t15.2023.11.19 val PER: 0.0998
2025-12-16 19:57:55,331: t15.2023.11.26 val PER: 0.2007
2025-12-16 19:57:55,331: t15.2023.12.03 val PER: 0.1523
2025-12-16 19:57:55,331: t15.2023.12.08 val PER: 0.1591
2025-12-16 19:57:55,331: t15.2023.12.10 val PER: 0.1524
2025-12-16 19:57:55,331: t15.2023.12.17 val PER: 0.1996
2025-12-16 19:57:55,331: t15.2023.12.29 val PER: 0.1874
2025-12-16 19:57:55,331: t15.2024.02.25 val PER: 0.1587
2025-12-16 19:57:55,331: t15.2024.03.03 val PER: 1.0000
2025-12-16 19:57:55,331: t15.2024.03.08 val PER: 0.2603
2025-12-16 19:57:55,331: t15.2024.03.15 val PER: 0.2708
2025-12-16 19:57:55,331: t15.2024.03.17 val PER: 0.1848
2025-12-16 19:57:55,331: t15.2024.04.25 val PER: 1.0000
2025-12-16 19:57:55,331: t15.2024.04.28 val PER: 1.0000
2025-12-16 19:57:55,332: t15.2024.05.10 val PER: 0.1828
2025-12-16 19:57:55,332: t15.2024.06.14 val PER: 0.2192
2025-12-16 19:57:55,332: t15.2024.07.19 val PER: 0.2452
2025-12-16 19:57:55,332: t15.2024.07.21 val PER: 0.1400
2025-12-16 19:57:55,332: t15.2024.07.28 val PER: 0.1691
2025-12-16 19:57:55,332: t15.2025.01.10 val PER: 0.3664
2025-12-16 19:57:55,332: t15.2025.01.12 val PER: 0.1978
2025-12-16 19:57:55,332: t15.2025.03.14 val PER: 0.3654
2025-12-16 19:57:55,332: t15.2025.03.16 val PER: 0.2382
2025-12-16 19:57:55,332: t15.2025.03.30 val PER: 0.3391
2025-12-16 19:57:55,332: t15.2025.04.13 val PER: 0.2725
2025-12-16 19:57:55,332: New best test PER 0.2135 --> 0.1987
2025-12-16 19:57:55,332: Checkpointing model
2025-12-16 19:57:55,819: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 19:58:13,477: Train batch 18200: loss: 0.92 grad norm: 2.41 time: 0.086
2025-12-16 19:58:30,195: Train batch 18400: loss: 0.97 grad norm: 2.96 time: 0.098
2025-12-16 19:58:47,611: Train batch 18600: loss: 1.05 grad norm: 9.74 time: 0.089
2025-12-16 19:59:03,975: Train batch 18800: loss: 1.01 grad norm: 3.30 time: 0.081
2025-12-16 19:59:20,081: Train batch 19000: loss: 0.91 grad norm: 2.35 time: 0.089
2025-12-16 19:59:37,292: Train batch 19200: loss: 1.25 grad norm: 6.91 time: 0.091
2025-12-16 19:59:53,437: Train batch 19400: loss: 0.98 grad norm: 9.14 time: 0.081
2025-12-16 20:00:10,240: Train batch 19600: loss: 0.90 grad norm: 3.44 time: 0.067
2025-12-16 20:00:27,646: Train batch 19800: loss: 0.98 grad norm: 4.65 time: 0.086
2025-12-16 20:00:43,826: Train batch 20000: loss: 0.93 grad norm: 2.07 time: 0.093
2025-12-16 20:00:43,827: Running test after training batch: 20000
2025-12-16 20:00:53,206: Val batch 20000: PER (avg): 0.1927 CTC Loss (avg): 0.7770 time: 9.379
2025-12-16 20:00:53,207: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:00:53,207: t15.2023.08.13 val PER: 0.1538
2025-12-16 20:00:53,207: t15.2023.08.18 val PER: 0.1316
2025-12-16 20:00:53,207: t15.2023.08.20 val PER: 0.1438
2025-12-16 20:00:53,207: t15.2023.08.25 val PER: 0.1280
2025-12-16 20:00:53,207: t15.2023.08.27 val PER: 0.2186
2025-12-16 20:00:53,207: t15.2023.09.01 val PER: 0.1023
2025-12-16 20:00:53,207: t15.2023.09.03 val PER: 0.2067
2025-12-16 20:00:53,207: t15.2023.09.24 val PER: 0.1675
2025-12-16 20:00:53,207: t15.2023.09.29 val PER: 0.1870
2025-12-16 20:00:53,207: t15.2023.10.01 val PER: 0.2120
2025-12-16 20:00:53,207: t15.2023.10.06 val PER: 0.1582
2025-12-16 20:00:53,207: t15.2023.10.08 val PER: 0.2558
2025-12-16 20:00:53,207: t15.2023.10.13 val PER: 0.2389
2025-12-16 20:00:53,207: t15.2023.10.15 val PER: 0.1852
2025-12-16 20:00:53,207: t15.2023.10.20 val PER: 0.2181
2025-12-16 20:00:53,207: t15.2023.10.22 val PER: 0.1514
2025-12-16 20:00:53,207: t15.2023.11.03 val PER: 0.2246
2025-12-16 20:00:53,208: t15.2023.11.04 val PER: 0.0410
2025-12-16 20:00:53,208: t15.2023.11.17 val PER: 0.0949
2025-12-16 20:00:53,208: t15.2023.11.19 val PER: 0.0938
2025-12-16 20:00:53,208: t15.2023.11.26 val PER: 0.1884
2025-12-16 20:00:53,208: t15.2023.12.03 val PER: 0.1586
2025-12-16 20:00:53,208: t15.2023.12.08 val PER: 0.1664
2025-12-16 20:00:53,208: t15.2023.12.10 val PER: 0.1327
2025-12-16 20:00:53,208: t15.2023.12.17 val PER: 0.2017
2025-12-16 20:00:53,208: t15.2023.12.29 val PER: 0.1551
2025-12-16 20:00:53,208: t15.2024.02.25 val PER: 0.1278
2025-12-16 20:00:53,208: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:00:53,208: t15.2024.03.08 val PER: 0.2632
2025-12-16 20:00:53,208: t15.2024.03.15 val PER: 0.2714
2025-12-16 20:00:53,208: t15.2024.03.17 val PER: 0.1778
2025-12-16 20:00:53,208: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:00:53,208: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:00:53,208: t15.2024.05.10 val PER: 0.2140
2025-12-16 20:00:53,208: t15.2024.06.14 val PER: 0.2350
2025-12-16 20:00:53,208: t15.2024.07.19 val PER: 0.2531
2025-12-16 20:00:53,209: t15.2024.07.21 val PER: 0.1214
2025-12-16 20:00:53,209: t15.2024.07.28 val PER: 0.1743
2025-12-16 20:00:53,209: t15.2025.01.10 val PER: 0.3623
2025-12-16 20:00:53,209: t15.2025.01.12 val PER: 0.1909
2025-12-16 20:00:53,209: t15.2025.03.14 val PER: 0.3654
2025-12-16 20:00:53,209: t15.2025.03.16 val PER: 0.2186
2025-12-16 20:00:53,209: t15.2025.03.30 val PER: 0.3586
2025-12-16 20:00:53,209: t15.2025.04.13 val PER: 0.2382
2025-12-16 20:00:53,209: New best test PER 0.1987 --> 0.1927
2025-12-16 20:00:53,209: Checkpointing model
2025-12-16 20:00:53,685: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:01:08,205: Train batch 20200: loss: 0.93 grad norm: 2.42 time: 0.056
2025-12-16 20:01:22,385: Train batch 20400: loss: 1.17 grad norm: 12.87 time: 0.043
2025-12-16 20:01:39,657: Train batch 20600: loss: 0.94 grad norm: 3.81 time: 0.092
2025-12-16 20:01:57,172: Train batch 20800: loss: 0.87 grad norm: 4.51 time: 0.090
2025-12-16 20:02:15,128: Train batch 21000: loss: 1.11 grad norm: 2.53 time: 0.090
2025-12-16 20:02:32,232: Train batch 21200: loss: 1.16 grad norm: 2.85 time: 0.075
2025-12-16 20:02:48,990: Train batch 21400: loss: 0.91 grad norm: 1.96 time: 0.088
2025-12-16 20:03:05,828: Train batch 21600: loss: 1.05 grad norm: 2.39 time: 0.072
2025-12-16 20:03:22,897: Train batch 21800: loss: 0.87 grad norm: 10.63 time: 0.081
2025-12-16 20:03:39,416: Train batch 22000: loss: 0.93 grad norm: 18.94 time: 0.083
2025-12-16 20:03:39,416: Running test after training batch: 22000
2025-12-16 20:03:48,748: Val batch 22000: PER (avg): 0.1912 CTC Loss (avg): 0.8124 time: 9.332
2025-12-16 20:03:48,749: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:03:48,749: t15.2023.08.13 val PER: 0.1590
2025-12-16 20:03:48,749: t15.2023.08.18 val PER: 0.1618
2025-12-16 20:03:48,749: t15.2023.08.20 val PER: 0.1422
2025-12-16 20:03:48,749: t15.2023.08.25 val PER: 0.1461
2025-12-16 20:03:48,749: t15.2023.08.27 val PER: 0.2460
2025-12-16 20:03:48,749: t15.2023.09.01 val PER: 0.1039
2025-12-16 20:03:48,749: t15.2023.09.03 val PER: 0.2292
2025-12-16 20:03:48,749: t15.2023.09.24 val PER: 0.1456
2025-12-16 20:03:48,749: t15.2023.09.29 val PER: 0.1806
2025-12-16 20:03:48,749: t15.2023.10.01 val PER: 0.2054
2025-12-16 20:03:48,749: t15.2023.10.06 val PER: 0.1346
2025-12-16 20:03:48,749: t15.2023.10.08 val PER: 0.2517
2025-12-16 20:03:48,749: t15.2023.10.13 val PER: 0.2483
2025-12-16 20:03:48,749: t15.2023.10.15 val PER: 0.1945
2025-12-16 20:03:48,749: t15.2023.10.20 val PER: 0.2550
2025-12-16 20:03:48,749: t15.2023.10.22 val PER: 0.1459
2025-12-16 20:03:48,750: t15.2023.11.03 val PER: 0.2069
2025-12-16 20:03:48,750: t15.2023.11.04 val PER: 0.0512
2025-12-16 20:03:48,750: t15.2023.11.17 val PER: 0.0747
2025-12-16 20:03:48,750: t15.2023.11.19 val PER: 0.0958
2025-12-16 20:03:48,750: t15.2023.11.26 val PER: 0.2000
2025-12-16 20:03:48,750: t15.2023.12.03 val PER: 0.1460
2025-12-16 20:03:48,750: t15.2023.12.08 val PER: 0.1445
2025-12-16 20:03:48,750: t15.2023.12.10 val PER: 0.1537
2025-12-16 20:03:48,750: t15.2023.12.17 val PER: 0.1881
2025-12-16 20:03:48,750: t15.2023.12.29 val PER: 0.1647
2025-12-16 20:03:48,750: t15.2024.02.25 val PER: 0.1489
2025-12-16 20:03:48,750: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:03:48,750: t15.2024.03.08 val PER: 0.2518
2025-12-16 20:03:48,750: t15.2024.03.15 val PER: 0.2508
2025-12-16 20:03:48,750: t15.2024.03.17 val PER: 0.1722
2025-12-16 20:03:48,750: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:03:48,750: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:03:48,750: t15.2024.05.10 val PER: 0.1932
2025-12-16 20:03:48,750: t15.2024.06.14 val PER: 0.2240
2025-12-16 20:03:48,751: t15.2024.07.19 val PER: 0.2531
2025-12-16 20:03:48,751: t15.2024.07.21 val PER: 0.1297
2025-12-16 20:03:48,751: t15.2024.07.28 val PER: 0.1743
2025-12-16 20:03:48,751: t15.2025.01.10 val PER: 0.3306
2025-12-16 20:03:48,751: t15.2025.01.12 val PER: 0.1955
2025-12-16 20:03:48,751: t15.2025.03.14 val PER: 0.3462
2025-12-16 20:03:48,751: t15.2025.03.16 val PER: 0.2199
2025-12-16 20:03:48,751: t15.2025.03.30 val PER: 0.3322
2025-12-16 20:03:48,751: t15.2025.04.13 val PER: 0.2668
2025-12-16 20:03:48,751: New best test PER 0.1927 --> 0.1912
2025-12-16 20:03:48,751: Checkpointing model
2025-12-16 20:03:49,226: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:04:05,810: Train batch 22200: loss: 0.87 grad norm: 2.29 time: 0.050
2025-12-16 20:04:21,766: Train batch 22400: loss: 0.89 grad norm: 2.68 time: 0.089
2025-12-16 20:04:39,423: Train batch 22600: loss: 0.80 grad norm: 7.12 time: 0.081
2025-12-16 20:04:56,987: Train batch 22800: loss: 0.85 grad norm: 5.72 time: 0.087
2025-12-16 20:05:14,847: Train batch 23000: loss: 0.95 grad norm: 2.25 time: 0.090
2025-12-16 20:05:32,784: Train batch 23200: loss: 0.93 grad norm: 3.41 time: 0.106
2025-12-16 20:05:47,400: Train batch 23400: loss: 0.95 grad norm: 5.96 time: 0.062
2025-12-16 20:06:05,051: Train batch 23600: loss: 0.90 grad norm: 2.64 time: 0.090
2025-12-16 20:06:22,511: Train batch 23800: loss: 0.90 grad norm: 3.52 time: 0.083
2025-12-16 20:06:39,641: Train batch 24000: loss: 0.82 grad norm: 2.78 time: 0.062
2025-12-16 20:06:39,641: Running test after training batch: 24000
2025-12-16 20:06:48,932: Val batch 24000: PER (avg): 0.1858 CTC Loss (avg): 0.8111 time: 9.290
2025-12-16 20:06:48,932: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:06:48,932: t15.2023.08.13 val PER: 0.1549
2025-12-16 20:06:48,932: t15.2023.08.18 val PER: 0.1492
2025-12-16 20:06:48,932: t15.2023.08.20 val PER: 0.1199
2025-12-16 20:06:48,932: t15.2023.08.25 val PER: 0.1491
2025-12-16 20:06:48,932: t15.2023.08.27 val PER: 0.2814
2025-12-16 20:06:48,932: t15.2023.09.01 val PER: 0.1096
2025-12-16 20:06:48,932: t15.2023.09.03 val PER: 0.2162
2025-12-16 20:06:48,932: t15.2023.09.24 val PER: 0.1590
2025-12-16 20:06:48,933: t15.2023.09.29 val PER: 0.1729
2025-12-16 20:06:48,933: t15.2023.10.01 val PER: 0.2127
2025-12-16 20:06:48,933: t15.2023.10.06 val PER: 0.1421
2025-12-16 20:06:48,933: t15.2023.10.08 val PER: 0.2639
2025-12-16 20:06:48,933: t15.2023.10.13 val PER: 0.2521
2025-12-16 20:06:48,933: t15.2023.10.15 val PER: 0.1819
2025-12-16 20:06:48,933: t15.2023.10.20 val PER: 0.1913
2025-12-16 20:06:48,933: t15.2023.10.22 val PER: 0.1470
2025-12-16 20:06:48,933: t15.2023.11.03 val PER: 0.1845
2025-12-16 20:06:48,933: t15.2023.11.04 val PER: 0.0444
2025-12-16 20:06:48,933: t15.2023.11.17 val PER: 0.0918
2025-12-16 20:06:48,933: t15.2023.11.19 val PER: 0.1018
2025-12-16 20:06:48,933: t15.2023.11.26 val PER: 0.1775
2025-12-16 20:06:48,933: t15.2023.12.03 val PER: 0.1345
2025-12-16 20:06:48,933: t15.2023.12.08 val PER: 0.1332
2025-12-16 20:06:48,933: t15.2023.12.10 val PER: 0.1235
2025-12-16 20:06:48,933: t15.2023.12.17 val PER: 0.1830
2025-12-16 20:06:48,933: t15.2023.12.29 val PER: 0.1633
2025-12-16 20:06:48,933: t15.2024.02.25 val PER: 0.1166
2025-12-16 20:06:48,934: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:06:48,934: t15.2024.03.08 val PER: 0.2504
2025-12-16 20:06:48,934: t15.2024.03.15 val PER: 0.2502
2025-12-16 20:06:48,934: t15.2024.03.17 val PER: 0.1513
2025-12-16 20:06:48,934: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:06:48,934: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:06:48,934: t15.2024.05.10 val PER: 0.1768
2025-12-16 20:06:48,934: t15.2024.06.14 val PER: 0.2319
2025-12-16 20:06:48,934: t15.2024.07.19 val PER: 0.2617
2025-12-16 20:06:48,934: t15.2024.07.21 val PER: 0.1207
2025-12-16 20:06:48,934: t15.2024.07.28 val PER: 0.1684
2025-12-16 20:06:48,934: t15.2025.01.10 val PER: 0.3402
2025-12-16 20:06:48,934: t15.2025.01.12 val PER: 0.1932
2025-12-16 20:06:48,934: t15.2025.03.14 val PER: 0.3388
2025-12-16 20:06:48,934: t15.2025.03.16 val PER: 0.2212
2025-12-16 20:06:48,934: t15.2025.03.30 val PER: 0.3138
2025-12-16 20:06:48,934: t15.2025.04.13 val PER: 0.2739
2025-12-16 20:06:48,934: New best test PER 0.1912 --> 0.1858
2025-12-16 20:06:48,934: Checkpointing model
2025-12-16 20:06:49,400: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:07:07,253: Train batch 24200: loss: 0.90 grad norm: 3.86 time: 0.082
2025-12-16 20:07:24,097: Train batch 24400: loss: 0.87 grad norm: 1.84 time: 0.072
2025-12-16 20:07:38,560: Train batch 24600: loss: 1.07 grad norm: 2.17 time: 0.091
2025-12-16 20:07:56,735: Train batch 24800: loss: 0.91 grad norm: 2.79 time: 0.087
2025-12-16 20:08:14,714: Train batch 25000: loss: 0.90 grad norm: 5.04 time: 0.067
2025-12-16 20:08:32,327: Train batch 25200: loss: 0.89 grad norm: 1.98 time: 0.075
2025-12-16 20:08:49,561: Train batch 25400: loss: 0.88 grad norm: 7.72 time: 0.081
2025-12-16 20:09:06,672: Train batch 25600: loss: 0.86 grad norm: 4.37 time: 0.080
2025-12-16 20:09:23,995: Train batch 25800: loss: 0.90 grad norm: 2.37 time: 0.065
2025-12-16 20:09:41,811: Train batch 26000: loss: 0.91 grad norm: 2.90 time: 0.081
2025-12-16 20:09:41,812: Running test after training batch: 26000
2025-12-16 20:09:51,291: Val batch 26000: PER (avg): 0.1714 CTC Loss (avg): 0.7272 time: 9.479
2025-12-16 20:09:51,292: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:09:51,292: t15.2023.08.13 val PER: 0.1528
2025-12-16 20:09:51,292: t15.2023.08.18 val PER: 0.1308
2025-12-16 20:09:51,292: t15.2023.08.20 val PER: 0.1247
2025-12-16 20:09:51,292: t15.2023.08.25 val PER: 0.1325
2025-12-16 20:09:51,292: t15.2023.08.27 val PER: 0.2106
2025-12-16 20:09:51,292: t15.2023.09.01 val PER: 0.0925
2025-12-16 20:09:51,292: t15.2023.09.03 val PER: 0.1876
2025-12-16 20:09:51,292: t15.2023.09.24 val PER: 0.1456
2025-12-16 20:09:51,292: t15.2023.09.29 val PER: 0.1500
2025-12-16 20:09:51,292: t15.2023.10.01 val PER: 0.2041
2025-12-16 20:09:51,292: t15.2023.10.06 val PER: 0.1432
2025-12-16 20:09:51,292: t15.2023.10.08 val PER: 0.2436
2025-12-16 20:09:51,292: t15.2023.10.13 val PER: 0.2296
2025-12-16 20:09:51,293: t15.2023.10.15 val PER: 0.1688
2025-12-16 20:09:51,293: t15.2023.10.20 val PER: 0.2114
2025-12-16 20:09:51,293: t15.2023.10.22 val PER: 0.1392
2025-12-16 20:09:51,293: t15.2023.11.03 val PER: 0.1940
2025-12-16 20:09:51,293: t15.2023.11.04 val PER: 0.0239
2025-12-16 20:09:51,293: t15.2023.11.17 val PER: 0.0513
2025-12-16 20:09:51,293: t15.2023.11.19 val PER: 0.0858
2025-12-16 20:09:51,293: t15.2023.11.26 val PER: 0.1565
2025-12-16 20:09:51,293: t15.2023.12.03 val PER: 0.1208
2025-12-16 20:09:51,293: t15.2023.12.08 val PER: 0.1205
2025-12-16 20:09:51,293: t15.2023.12.10 val PER: 0.1183
2025-12-16 20:09:51,293: t15.2023.12.17 val PER: 0.1455
2025-12-16 20:09:51,293: t15.2023.12.29 val PER: 0.1510
2025-12-16 20:09:51,293: t15.2024.02.25 val PER: 0.1096
2025-12-16 20:09:51,293: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:09:51,293: t15.2024.03.08 val PER: 0.2276
2025-12-16 20:09:51,293: t15.2024.03.15 val PER: 0.2295
2025-12-16 20:09:51,293: t15.2024.03.17 val PER: 0.1611
2025-12-16 20:09:51,293: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:09:51,294: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:09:51,294: t15.2024.05.10 val PER: 0.1917
2025-12-16 20:09:51,294: t15.2024.06.14 val PER: 0.1924
2025-12-16 20:09:51,294: t15.2024.07.19 val PER: 0.2498
2025-12-16 20:09:51,294: t15.2024.07.21 val PER: 0.1110
2025-12-16 20:09:51,294: t15.2024.07.28 val PER: 0.1721
2025-12-16 20:09:51,294: t15.2025.01.10 val PER: 0.3140
2025-12-16 20:09:51,294: t15.2025.01.12 val PER: 0.1617
2025-12-16 20:09:51,294: t15.2025.03.14 val PER: 0.3225
2025-12-16 20:09:51,294: t15.2025.03.16 val PER: 0.2068
2025-12-16 20:09:51,294: t15.2025.03.30 val PER: 0.3000
2025-12-16 20:09:51,294: t15.2025.04.13 val PER: 0.2297
2025-12-16 20:09:51,294: New best test PER 0.1858 --> 0.1714
2025-12-16 20:09:51,294: Checkpointing model
2025-12-16 20:09:51,775: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:10:09,671: Train batch 26200: loss: 0.89 grad norm: 1.84 time: 0.110
2025-12-16 20:10:26,925: Train batch 26400: loss: 0.74 grad norm: 8.72 time: 0.060
2025-12-16 20:10:43,624: Train batch 26600: loss: 0.72 grad norm: 1.84 time: 0.078
2025-12-16 20:11:01,081: Train batch 26800: loss: 0.83 grad norm: 1.82 time: 0.043
2025-12-16 20:11:17,089: Train batch 27000: loss: 0.75 grad norm: 15.40 time: 0.063
2025-12-16 20:11:34,690: Train batch 27200: loss: 0.80 grad norm: 2.72 time: 0.088
2025-12-16 20:11:51,181: Train batch 27400: loss: 0.86 grad norm: 1.60 time: 0.079
2025-12-16 20:12:07,817: Train batch 27600: loss: 0.79 grad norm: 1.20 time: 0.103
2025-12-16 20:12:25,827: Train batch 27800: loss: 0.91 grad norm: 4.75 time: 0.086
2025-12-16 20:12:42,367: Train batch 28000: loss: 0.88 grad norm: 1.86 time: 0.063
2025-12-16 20:12:42,368: Running test after training batch: 28000
2025-12-16 20:12:51,732: Val batch 28000: PER (avg): 0.1668 CTC Loss (avg): 0.7241 time: 9.364
2025-12-16 20:12:51,732: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:12:51,732: t15.2023.08.13 val PER: 0.1507
2025-12-16 20:12:51,732: t15.2023.08.18 val PER: 0.1417
2025-12-16 20:12:51,732: t15.2023.08.20 val PER: 0.1366
2025-12-16 20:12:51,732: t15.2023.08.25 val PER: 0.0934
2025-12-16 20:12:51,732: t15.2023.08.27 val PER: 0.2154
2025-12-16 20:12:51,732: t15.2023.09.01 val PER: 0.1006
2025-12-16 20:12:51,732: t15.2023.09.03 val PER: 0.1888
2025-12-16 20:12:51,733: t15.2023.09.24 val PER: 0.1323
2025-12-16 20:12:51,733: t15.2023.09.29 val PER: 0.1519
2025-12-16 20:12:51,733: t15.2023.10.01 val PER: 0.1935
2025-12-16 20:12:51,733: t15.2023.10.06 val PER: 0.1335
2025-12-16 20:12:51,733: t15.2023.10.08 val PER: 0.2585
2025-12-16 20:12:51,733: t15.2023.10.13 val PER: 0.2335
2025-12-16 20:12:51,733: t15.2023.10.15 val PER: 0.1622
2025-12-16 20:12:51,733: t15.2023.10.20 val PER: 0.2248
2025-12-16 20:12:51,733: t15.2023.10.22 val PER: 0.1425
2025-12-16 20:12:51,733: t15.2023.11.03 val PER: 0.1805
2025-12-16 20:12:51,733: t15.2023.11.04 val PER: 0.0171
2025-12-16 20:12:51,733: t15.2023.11.17 val PER: 0.0684
2025-12-16 20:12:51,733: t15.2023.11.19 val PER: 0.0798
2025-12-16 20:12:51,733: t15.2023.11.26 val PER: 0.1486
2025-12-16 20:12:51,733: t15.2023.12.03 val PER: 0.1092
2025-12-16 20:12:51,733: t15.2023.12.08 val PER: 0.1198
2025-12-16 20:12:51,733: t15.2023.12.10 val PER: 0.1156
2025-12-16 20:12:51,733: t15.2023.12.17 val PER: 0.1393
2025-12-16 20:12:51,733: t15.2023.12.29 val PER: 0.1400
2025-12-16 20:12:51,734: t15.2024.02.25 val PER: 0.1208
2025-12-16 20:12:51,734: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:12:51,734: t15.2024.03.08 val PER: 0.2376
2025-12-16 20:12:51,734: t15.2024.03.15 val PER: 0.2308
2025-12-16 20:12:51,734: t15.2024.03.17 val PER: 0.1346
2025-12-16 20:12:51,734: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:12:51,734: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:12:51,734: t15.2024.05.10 val PER: 0.1857
2025-12-16 20:12:51,734: t15.2024.06.14 val PER: 0.1893
2025-12-16 20:12:51,734: t15.2024.07.19 val PER: 0.2386
2025-12-16 20:12:51,734: t15.2024.07.21 val PER: 0.1028
2025-12-16 20:12:51,734: t15.2024.07.28 val PER: 0.1478
2025-12-16 20:12:51,734: t15.2025.01.10 val PER: 0.2851
2025-12-16 20:12:51,734: t15.2025.01.12 val PER: 0.1709
2025-12-16 20:12:51,734: t15.2025.03.14 val PER: 0.3062
2025-12-16 20:12:51,734: t15.2025.03.16 val PER: 0.1976
2025-12-16 20:12:51,734: t15.2025.03.30 val PER: 0.2851
2025-12-16 20:12:51,734: t15.2025.04.13 val PER: 0.2368
2025-12-16 20:12:51,734: New best test PER 0.1714 --> 0.1668
2025-12-16 20:12:51,734: Checkpointing model
2025-12-16 20:12:52,203: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:13:09,303: Train batch 28200: loss: 0.71 grad norm: 1.53 time: 0.105
2025-12-16 20:13:26,637: Train batch 28400: loss: 0.84 grad norm: 15.43 time: 0.089
2025-12-16 20:13:43,971: Train batch 28600: loss: 0.80 grad norm: 1.98 time: 0.091
2025-12-16 20:14:01,739: Train batch 28800: loss: 0.83 grad norm: 2.06 time: 0.102
2025-12-16 20:14:18,887: Train batch 29000: loss: 0.77 grad norm: 1.17 time: 0.085
2025-12-16 20:14:36,551: Train batch 29200: loss: 0.83 grad norm: 1.89 time: 0.055
2025-12-16 20:14:53,722: Train batch 29400: loss: 0.83 grad norm: 2.48 time: 0.090
2025-12-16 20:15:11,403: Train batch 29600: loss: 0.85 grad norm: 2.57 time: 0.083
2025-12-16 20:15:27,818: Train batch 29800: loss: 0.86 grad norm: 1.75 time: 0.078
2025-12-16 20:15:45,118: Train batch 30000: loss: 0.75 grad norm: 2.28 time: 0.075
2025-12-16 20:15:45,118: Running test after training batch: 30000
2025-12-16 20:15:54,445: Val batch 30000: PER (avg): 0.1676 CTC Loss (avg): 0.7391 time: 9.327
2025-12-16 20:15:54,445: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:15:54,446: t15.2023.08.13 val PER: 0.1466
2025-12-16 20:15:54,446: t15.2023.08.18 val PER: 0.1383
2025-12-16 20:15:54,446: t15.2023.08.20 val PER: 0.1033
2025-12-16 20:15:54,446: t15.2023.08.25 val PER: 0.1130
2025-12-16 20:15:54,446: t15.2023.08.27 val PER: 0.2058
2025-12-16 20:15:54,446: t15.2023.09.01 val PER: 0.0893
2025-12-16 20:15:54,446: t15.2023.09.03 val PER: 0.1746
2025-12-16 20:15:54,446: t15.2023.09.24 val PER: 0.1359
2025-12-16 20:15:54,446: t15.2023.09.29 val PER: 0.1429
2025-12-16 20:15:54,446: t15.2023.10.01 val PER: 0.1982
2025-12-16 20:15:54,446: t15.2023.10.06 val PER: 0.1313
2025-12-16 20:15:54,446: t15.2023.10.08 val PER: 0.2097
2025-12-16 20:15:54,446: t15.2023.10.13 val PER: 0.2374
2025-12-16 20:15:54,446: t15.2023.10.15 val PER: 0.1622
2025-12-16 20:15:54,446: t15.2023.10.20 val PER: 0.2819
2025-12-16 20:15:54,446: t15.2023.10.22 val PER: 0.1448
2025-12-16 20:15:54,446: t15.2023.11.03 val PER: 0.1886
2025-12-16 20:15:54,446: t15.2023.11.04 val PER: 0.0307
2025-12-16 20:15:54,446: t15.2023.11.17 val PER: 0.0669
2025-12-16 20:15:54,447: t15.2023.11.19 val PER: 0.0858
2025-12-16 20:15:54,447: t15.2023.11.26 val PER: 0.1478
2025-12-16 20:15:54,447: t15.2023.12.03 val PER: 0.1113
2025-12-16 20:15:54,447: t15.2023.12.08 val PER: 0.1165
2025-12-16 20:15:54,447: t15.2023.12.10 val PER: 0.0867
2025-12-16 20:15:54,447: t15.2023.12.17 val PER: 0.1611
2025-12-16 20:15:54,447: t15.2023.12.29 val PER: 0.1366
2025-12-16 20:15:54,447: t15.2024.02.25 val PER: 0.1306
2025-12-16 20:15:54,447: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:15:54,447: t15.2024.03.08 val PER: 0.2248
2025-12-16 20:15:54,447: t15.2024.03.15 val PER: 0.2439
2025-12-16 20:15:54,447: t15.2024.03.17 val PER: 0.1450
2025-12-16 20:15:54,447: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:15:54,447: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:15:54,447: t15.2024.05.10 val PER: 0.1694
2025-12-16 20:15:54,447: t15.2024.06.14 val PER: 0.2050
2025-12-16 20:15:54,447: t15.2024.07.19 val PER: 0.2426
2025-12-16 20:15:54,447: t15.2024.07.21 val PER: 0.1041
2025-12-16 20:15:54,447: t15.2024.07.28 val PER: 0.1493
2025-12-16 20:15:54,448: t15.2025.01.10 val PER: 0.2948
2025-12-16 20:15:54,448: t15.2025.01.12 val PER: 0.1694
2025-12-16 20:15:54,448: t15.2025.03.14 val PER: 0.3402
2025-12-16 20:15:54,448: t15.2025.03.16 val PER: 0.2029
2025-12-16 20:15:54,448: t15.2025.03.30 val PER: 0.3230
2025-12-16 20:15:54,448: t15.2025.04.13 val PER: 0.2439
2025-12-16 20:16:11,694: Train batch 30200: loss: 1.03 grad norm: 2.44 time: 0.083
2025-12-16 20:16:29,223: Train batch 30400: loss: 0.82 grad norm: 3.07 time: 0.068
2025-12-16 20:16:46,715: Train batch 30600: loss: 0.76 grad norm: 5.51 time: 0.088
2025-12-16 20:17:03,673: Train batch 30800: loss: 0.73 grad norm: 1.20 time: 0.101
2025-12-16 20:17:20,992: Train batch 31000: loss: 0.95 grad norm: 2.41 time: 0.087
2025-12-16 20:17:38,532: Train batch 31200: loss: 0.93 grad norm: 2.42 time: 0.068
2025-12-16 20:17:54,376: Train batch 31400: loss: 0.93 grad norm: 11.49 time: 0.088
2025-12-16 20:18:12,039: Train batch 31600: loss: 0.80 grad norm: 1.25 time: 0.076
2025-12-16 20:18:29,732: Train batch 31800: loss: 0.85 grad norm: 2.15 time: 0.103
2025-12-16 20:18:46,676: Train batch 32000: loss: 0.74 grad norm: 1.90 time: 0.084
2025-12-16 20:18:46,677: Running test after training batch: 32000
2025-12-16 20:18:56,090: Val batch 32000: PER (avg): 0.1638 CTC Loss (avg): 0.7638 time: 9.413
2025-12-16 20:18:56,090: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:18:56,091: t15.2023.08.13 val PER: 0.1289
2025-12-16 20:18:56,091: t15.2023.08.18 val PER: 0.1316
2025-12-16 20:18:56,091: t15.2023.08.20 val PER: 0.1207
2025-12-16 20:18:56,091: t15.2023.08.25 val PER: 0.1099
2025-12-16 20:18:56,091: t15.2023.08.27 val PER: 0.2122
2025-12-16 20:18:56,091: t15.2023.09.01 val PER: 0.0933
2025-12-16 20:18:56,091: t15.2023.09.03 val PER: 0.1615
2025-12-16 20:18:56,091: t15.2023.09.24 val PER: 0.1311
2025-12-16 20:18:56,091: t15.2023.09.29 val PER: 0.1461
2025-12-16 20:18:56,091: t15.2023.10.01 val PER: 0.2061
2025-12-16 20:18:56,091: t15.2023.10.06 val PER: 0.1238
2025-12-16 20:18:56,091: t15.2023.10.08 val PER: 0.2395
2025-12-16 20:18:56,091: t15.2023.10.13 val PER: 0.2242
2025-12-16 20:18:56,091: t15.2023.10.15 val PER: 0.1510
2025-12-16 20:18:56,091: t15.2023.10.20 val PER: 0.1879
2025-12-16 20:18:56,091: t15.2023.10.22 val PER: 0.1492
2025-12-16 20:18:56,091: t15.2023.11.03 val PER: 0.1886
2025-12-16 20:18:56,091: t15.2023.11.04 val PER: 0.0512
2025-12-16 20:18:56,091: t15.2023.11.17 val PER: 0.0731
2025-12-16 20:18:56,092: t15.2023.11.19 val PER: 0.0898
2025-12-16 20:18:56,092: t15.2023.11.26 val PER: 0.1413
2025-12-16 20:18:56,092: t15.2023.12.03 val PER: 0.1040
2025-12-16 20:18:56,092: t15.2023.12.08 val PER: 0.1105
2025-12-16 20:18:56,092: t15.2023.12.10 val PER: 0.0986
2025-12-16 20:18:56,092: t15.2023.12.17 val PER: 0.1466
2025-12-16 20:18:56,092: t15.2023.12.29 val PER: 0.1270
2025-12-16 20:18:56,092: t15.2024.02.25 val PER: 0.1124
2025-12-16 20:18:56,092: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:18:56,092: t15.2024.03.08 val PER: 0.2347
2025-12-16 20:18:56,092: t15.2024.03.15 val PER: 0.2270
2025-12-16 20:18:56,092: t15.2024.03.17 val PER: 0.1423
2025-12-16 20:18:56,092: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:18:56,092: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:18:56,092: t15.2024.05.10 val PER: 0.1694
2025-12-16 20:18:56,092: t15.2024.06.14 val PER: 0.1940
2025-12-16 20:18:56,092: t15.2024.07.19 val PER: 0.2334
2025-12-16 20:18:56,092: t15.2024.07.21 val PER: 0.1083
2025-12-16 20:18:56,092: t15.2024.07.28 val PER: 0.1434
2025-12-16 20:18:56,093: t15.2025.01.10 val PER: 0.2879
2025-12-16 20:18:56,093: t15.2025.01.12 val PER: 0.1555
2025-12-16 20:18:56,093: t15.2025.03.14 val PER: 0.3491
2025-12-16 20:18:56,093: t15.2025.03.16 val PER: 0.1937
2025-12-16 20:18:56,093: t15.2025.03.30 val PER: 0.3126
2025-12-16 20:18:56,093: t15.2025.04.13 val PER: 0.2468
2025-12-16 20:18:56,093: New best test PER 0.1668 --> 0.1638
2025-12-16 20:18:56,093: Checkpointing model
2025-12-16 20:18:56,593: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:19:14,509: Train batch 32200: loss: 0.88 grad norm: 2.05 time: 0.080
2025-12-16 20:19:32,172: Train batch 32400: loss: 0.78 grad norm: 4.94 time: 0.065
2025-12-16 20:19:49,001: Train batch 32600: loss: 0.79 grad norm: 1.88 time: 0.065
2025-12-16 20:20:06,181: Train batch 32800: loss: 0.71 grad norm: 0.84 time: 0.082
2025-12-16 20:20:22,972: Train batch 33000: loss: 0.76 grad norm: 5.74 time: 0.069
2025-12-16 20:20:40,643: Train batch 33200: loss: 0.79 grad norm: 5.46 time: 0.087
2025-12-16 20:20:57,934: Train batch 33400: loss: 0.69 grad norm: 1.06 time: 0.099
2025-12-16 20:21:15,375: Train batch 33600: loss: 0.70 grad norm: 2.56 time: 0.061
2025-12-16 20:21:33,238: Train batch 33800: loss: 0.77 grad norm: 2.31 time: 0.089
2025-12-16 20:21:50,686: Train batch 34000: loss: 0.80 grad norm: 1.57 time: 0.084
2025-12-16 20:21:50,687: Running test after training batch: 34000
2025-12-16 20:21:59,908: Val batch 34000: PER (avg): 0.1548 CTC Loss (avg): 0.7374 time: 9.221
2025-12-16 20:21:59,908: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:21:59,908: t15.2023.08.13 val PER: 0.1383
2025-12-16 20:21:59,909: t15.2023.08.18 val PER: 0.1115
2025-12-16 20:21:59,909: t15.2023.08.20 val PER: 0.0985
2025-12-16 20:21:59,909: t15.2023.08.25 val PER: 0.1130
2025-12-16 20:21:59,909: t15.2023.08.27 val PER: 0.1736
2025-12-16 20:21:59,909: t15.2023.09.01 val PER: 0.0852
2025-12-16 20:21:59,909: t15.2023.09.03 val PER: 0.1520
2025-12-16 20:21:59,909: t15.2023.09.24 val PER: 0.1214
2025-12-16 20:21:59,909: t15.2023.09.29 val PER: 0.1481
2025-12-16 20:21:59,909: t15.2023.10.01 val PER: 0.1797
2025-12-16 20:21:59,909: t15.2023.10.06 val PER: 0.1259
2025-12-16 20:21:59,909: t15.2023.10.08 val PER: 0.2206
2025-12-16 20:21:59,909: t15.2023.10.13 val PER: 0.2242
2025-12-16 20:21:59,909: t15.2023.10.15 val PER: 0.1668
2025-12-16 20:21:59,909: t15.2023.10.20 val PER: 0.2282
2025-12-16 20:21:59,909: t15.2023.10.22 val PER: 0.1236
2025-12-16 20:21:59,909: t15.2023.11.03 val PER: 0.1866
2025-12-16 20:21:59,909: t15.2023.11.04 val PER: 0.0341
2025-12-16 20:21:59,909: t15.2023.11.17 val PER: 0.0575
2025-12-16 20:21:59,910: t15.2023.11.19 val PER: 0.0758
2025-12-16 20:21:59,910: t15.2023.11.26 val PER: 0.1268
2025-12-16 20:21:59,910: t15.2023.12.03 val PER: 0.1008
2025-12-16 20:21:59,910: t15.2023.12.08 val PER: 0.0852
2025-12-16 20:21:59,910: t15.2023.12.10 val PER: 0.1025
2025-12-16 20:21:59,910: t15.2023.12.17 val PER: 0.1622
2025-12-16 20:21:59,910: t15.2023.12.29 val PER: 0.1345
2025-12-16 20:21:59,910: t15.2024.02.25 val PER: 0.1110
2025-12-16 20:21:59,910: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:21:59,910: t15.2024.03.08 val PER: 0.2205
2025-12-16 20:21:59,910: t15.2024.03.15 val PER: 0.2133
2025-12-16 20:21:59,910: t15.2024.03.17 val PER: 0.1437
2025-12-16 20:21:59,910: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:21:59,910: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:21:59,910: t15.2024.05.10 val PER: 0.1501
2025-12-16 20:21:59,910: t15.2024.06.14 val PER: 0.1751
2025-12-16 20:21:59,910: t15.2024.07.19 val PER: 0.2149
2025-12-16 20:21:59,910: t15.2024.07.21 val PER: 0.0917
2025-12-16 20:21:59,911: t15.2024.07.28 val PER: 0.1471
2025-12-16 20:21:59,911: t15.2025.01.10 val PER: 0.2810
2025-12-16 20:21:59,911: t15.2025.01.12 val PER: 0.1324
2025-12-16 20:21:59,911: t15.2025.03.14 val PER: 0.3240
2025-12-16 20:21:59,911: t15.2025.03.16 val PER: 0.1872
2025-12-16 20:21:59,911: t15.2025.03.30 val PER: 0.2966
2025-12-16 20:21:59,911: t15.2025.04.13 val PER: 0.2254
2025-12-16 20:21:59,911: New best test PER 0.1638 --> 0.1548
2025-12-16 20:21:59,911: Checkpointing model
2025-12-16 20:22:00,359: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:22:16,820: Train batch 34200: loss: 0.88 grad norm: 2.33 time: 0.080
2025-12-16 20:22:34,186: Train batch 34400: loss: 0.87 grad norm: 1.65 time: 0.074
2025-12-16 20:22:51,320: Train batch 34600: loss: 0.70 grad norm: 1.67 time: 0.089
2025-12-16 20:23:09,259: Train batch 34800: loss: 0.76 grad norm: 6.11 time: 0.087
2025-12-16 20:23:25,920: Train batch 35000: loss: 0.81 grad norm: 1.20 time: 0.051
2025-12-16 20:23:41,398: Train batch 35200: loss: 0.87 grad norm: 2.70 time: 0.053
2025-12-16 20:23:55,624: Train batch 35400: loss: 0.85 grad norm: 1.81 time: 0.083
2025-12-16 20:24:12,237: Train batch 35600: loss: 0.81 grad norm: 1.38 time: 0.059
2025-12-16 20:24:28,651: Train batch 35800: loss: 0.79 grad norm: 1.63 time: 0.061
2025-12-16 20:24:45,154: Train batch 36000: loss: 0.84 grad norm: 2.43 time: 0.080
2025-12-16 20:24:45,154: Running test after training batch: 36000
2025-12-16 20:24:54,428: Val batch 36000: PER (avg): 0.1537 CTC Loss (avg): 0.7345 time: 9.274
2025-12-16 20:24:54,429: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:24:54,429: t15.2023.08.13 val PER: 0.1268
2025-12-16 20:24:54,429: t15.2023.08.18 val PER: 0.1065
2025-12-16 20:24:54,429: t15.2023.08.20 val PER: 0.0913
2025-12-16 20:24:54,429: t15.2023.08.25 val PER: 0.1295
2025-12-16 20:24:54,429: t15.2023.08.27 val PER: 0.1977
2025-12-16 20:24:54,429: t15.2023.09.01 val PER: 0.0844
2025-12-16 20:24:54,429: t15.2023.09.03 val PER: 0.1675
2025-12-16 20:24:54,429: t15.2023.09.24 val PER: 0.1347
2025-12-16 20:24:54,429: t15.2023.09.29 val PER: 0.1321
2025-12-16 20:24:54,429: t15.2023.10.01 val PER: 0.1856
2025-12-16 20:24:54,429: t15.2023.10.06 val PER: 0.1206
2025-12-16 20:24:54,429: t15.2023.10.08 val PER: 0.2179
2025-12-16 20:24:54,429: t15.2023.10.13 val PER: 0.2087
2025-12-16 20:24:54,430: t15.2023.10.15 val PER: 0.1496
2025-12-16 20:24:54,430: t15.2023.10.20 val PER: 0.2483
2025-12-16 20:24:54,430: t15.2023.10.22 val PER: 0.1269
2025-12-16 20:24:54,430: t15.2023.11.03 val PER: 0.1791
2025-12-16 20:24:54,430: t15.2023.11.04 val PER: 0.0273
2025-12-16 20:24:54,430: t15.2023.11.17 val PER: 0.0700
2025-12-16 20:24:54,430: t15.2023.11.19 val PER: 0.0818
2025-12-16 20:24:54,430: t15.2023.11.26 val PER: 0.1341
2025-12-16 20:24:54,430: t15.2023.12.03 val PER: 0.0935
2025-12-16 20:24:54,430: t15.2023.12.08 val PER: 0.0859
2025-12-16 20:24:54,430: t15.2023.12.10 val PER: 0.1130
2025-12-16 20:24:54,430: t15.2023.12.17 val PER: 0.1341
2025-12-16 20:24:54,430: t15.2023.12.29 val PER: 0.1332
2025-12-16 20:24:54,430: t15.2024.02.25 val PER: 0.0871
2025-12-16 20:24:54,430: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:24:54,430: t15.2024.03.08 val PER: 0.2205
2025-12-16 20:24:54,430: t15.2024.03.15 val PER: 0.2126
2025-12-16 20:24:54,430: t15.2024.03.17 val PER: 0.1374
2025-12-16 20:24:54,431: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:24:54,431: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:24:54,431: t15.2024.05.10 val PER: 0.1530
2025-12-16 20:24:54,431: t15.2024.06.14 val PER: 0.1956
2025-12-16 20:24:54,431: t15.2024.07.19 val PER: 0.2221
2025-12-16 20:24:54,431: t15.2024.07.21 val PER: 0.0945
2025-12-16 20:24:54,431: t15.2024.07.28 val PER: 0.1441
2025-12-16 20:24:54,431: t15.2025.01.10 val PER: 0.2865
2025-12-16 20:24:54,431: t15.2025.01.12 val PER: 0.1455
2025-12-16 20:24:54,431: t15.2025.03.14 val PER: 0.3210
2025-12-16 20:24:54,431: t15.2025.03.16 val PER: 0.2068
2025-12-16 20:24:54,431: t15.2025.03.30 val PER: 0.2770
2025-12-16 20:24:54,431: t15.2025.04.13 val PER: 0.2183
2025-12-16 20:24:54,431: New best test PER 0.1548 --> 0.1537
2025-12-16 20:24:54,431: Checkpointing model
2025-12-16 20:24:54,922: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:25:11,587: Train batch 36200: loss: 0.82 grad norm: 1.77 time: 0.061
2025-12-16 20:25:28,406: Train batch 36400: loss: 0.89 grad norm: 3.84 time: 0.064
2025-12-16 20:25:45,224: Train batch 36600: loss: 0.79 grad norm: 1.80 time: 0.074
2025-12-16 20:26:03,130: Train batch 36800: loss: 0.83 grad norm: 2.72 time: 0.091
2025-12-16 20:26:20,951: Train batch 37000: loss: 0.75 grad norm: 1.17 time: 0.082
2025-12-16 20:26:38,926: Train batch 37200: loss: 0.87 grad norm: 2.82 time: 0.087
2025-12-16 20:26:56,221: Train batch 37400: loss: 0.74 grad norm: 2.54 time: 0.078
2025-12-16 20:27:14,177: Train batch 37600: loss: 0.76 grad norm: 5.40 time: 0.093
2025-12-16 20:27:31,855: Train batch 37800: loss: 0.78 grad norm: 1.90 time: 0.074
2025-12-16 20:27:49,501: Train batch 38000: loss: 0.68 grad norm: 1.22 time: 0.053
2025-12-16 20:27:49,502: Running test after training batch: 38000
2025-12-16 20:27:58,945: Val batch 38000: PER (avg): 0.1531 CTC Loss (avg): 0.7456 time: 9.443
2025-12-16 20:27:58,946: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:27:58,946: t15.2023.08.13 val PER: 0.1258
2025-12-16 20:27:58,946: t15.2023.08.18 val PER: 0.1106
2025-12-16 20:27:58,946: t15.2023.08.20 val PER: 0.1001
2025-12-16 20:27:58,946: t15.2023.08.25 val PER: 0.1024
2025-12-16 20:27:58,946: t15.2023.08.27 val PER: 0.1720
2025-12-16 20:27:58,946: t15.2023.09.01 val PER: 0.0755
2025-12-16 20:27:58,946: t15.2023.09.03 val PER: 0.1781
2025-12-16 20:27:58,946: t15.2023.09.24 val PER: 0.1408
2025-12-16 20:27:58,946: t15.2023.09.29 val PER: 0.1417
2025-12-16 20:27:58,946: t15.2023.10.01 val PER: 0.1797
2025-12-16 20:27:58,946: t15.2023.10.06 val PER: 0.1346
2025-12-16 20:27:58,946: t15.2023.10.08 val PER: 0.2260
2025-12-16 20:27:58,946: t15.2023.10.13 val PER: 0.2095
2025-12-16 20:27:58,946: t15.2023.10.15 val PER: 0.1450
2025-12-16 20:27:58,946: t15.2023.10.20 val PER: 0.2148
2025-12-16 20:27:58,946: t15.2023.10.22 val PER: 0.1381
2025-12-16 20:27:58,947: t15.2023.11.03 val PER: 0.1879
2025-12-16 20:27:58,947: t15.2023.11.04 val PER: 0.0341
2025-12-16 20:27:58,947: t15.2023.11.17 val PER: 0.0607
2025-12-16 20:27:58,947: t15.2023.11.19 val PER: 0.0679
2025-12-16 20:27:58,947: t15.2023.11.26 val PER: 0.1319
2025-12-16 20:27:58,947: t15.2023.12.03 val PER: 0.1187
2025-12-16 20:27:58,947: t15.2023.12.08 val PER: 0.0885
2025-12-16 20:27:58,947: t15.2023.12.10 val PER: 0.0828
2025-12-16 20:27:58,947: t15.2023.12.17 val PER: 0.1362
2025-12-16 20:27:58,947: t15.2023.12.29 val PER: 0.1304
2025-12-16 20:27:58,947: t15.2024.02.25 val PER: 0.1096
2025-12-16 20:27:58,947: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:27:58,947: t15.2024.03.08 val PER: 0.2162
2025-12-16 20:27:58,947: t15.2024.03.15 val PER: 0.1907
2025-12-16 20:27:58,947: t15.2024.03.17 val PER: 0.1374
2025-12-16 20:27:58,947: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:27:58,947: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:27:58,948: t15.2024.05.10 val PER: 0.1516
2025-12-16 20:27:58,948: t15.2024.06.14 val PER: 0.1830
2025-12-16 20:27:58,948: t15.2024.07.19 val PER: 0.2063
2025-12-16 20:27:58,948: t15.2024.07.21 val PER: 0.0952
2025-12-16 20:27:58,948: t15.2024.07.28 val PER: 0.1485
2025-12-16 20:27:58,948: t15.2025.01.10 val PER: 0.2824
2025-12-16 20:27:58,948: t15.2025.01.12 val PER: 0.1470
2025-12-16 20:27:58,948: t15.2025.03.14 val PER: 0.3225
2025-12-16 20:27:58,948: t15.2025.03.16 val PER: 0.1911
2025-12-16 20:27:58,948: t15.2025.03.30 val PER: 0.3080
2025-12-16 20:27:58,948: t15.2025.04.13 val PER: 0.2225
2025-12-16 20:27:58,948: New best test PER 0.1537 --> 0.1531
2025-12-16 20:27:58,948: Checkpointing model
2025-12-16 20:27:59,420: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:28:15,766: Train batch 38200: loss: 0.72 grad norm: 1.38 time: 0.088
2025-12-16 20:28:33,949: Train batch 38400: loss: 0.88 grad norm: 2.00 time: 0.090
2025-12-16 20:28:49,623: Train batch 38600: loss: 0.72 grad norm: 1.15 time: 0.070
2025-12-16 20:29:07,130: Train batch 38800: loss: 0.80 grad norm: 1.89 time: 0.073
2025-12-16 20:29:21,482: Train batch 39000: loss: 0.70 grad norm: 1.51 time: 0.087
2025-12-16 20:29:37,323: Train batch 39200: loss: 0.81 grad norm: 1.70 time: 0.088
2025-12-16 20:29:54,472: Train batch 39400: loss: 0.67 grad norm: 1.96 time: 0.084
2025-12-16 20:30:11,758: Train batch 39600: loss: 0.76 grad norm: 1.70 time: 0.086
2025-12-16 20:30:29,303: Train batch 39800: loss: 0.69 grad norm: 0.93 time: 0.072
2025-12-16 20:30:46,749: Train batch 40000: loss: 0.72 grad norm: 1.23 time: 0.090
2025-12-16 20:30:46,750: Running test after training batch: 40000
2025-12-16 20:30:56,144: Val batch 40000: PER (avg): 0.1589 CTC Loss (avg): 0.7752 time: 9.395
2025-12-16 20:30:56,145: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:30:56,145: t15.2023.08.13 val PER: 0.1279
2025-12-16 20:30:56,145: t15.2023.08.18 val PER: 0.0981
2025-12-16 20:30:56,145: t15.2023.08.20 val PER: 0.0953
2025-12-16 20:30:56,145: t15.2023.08.25 val PER: 0.1265
2025-12-16 20:30:56,145: t15.2023.08.27 val PER: 0.1945
2025-12-16 20:30:56,145: t15.2023.09.01 val PER: 0.0836
2025-12-16 20:30:56,145: t15.2023.09.03 val PER: 0.1805
2025-12-16 20:30:56,145: t15.2023.09.24 val PER: 0.1371
2025-12-16 20:30:56,145: t15.2023.09.29 val PER: 0.1595
2025-12-16 20:30:56,145: t15.2023.10.01 val PER: 0.1935
2025-12-16 20:30:56,145: t15.2023.10.06 val PER: 0.1292
2025-12-16 20:30:56,145: t15.2023.10.08 val PER: 0.2300
2025-12-16 20:30:56,145: t15.2023.10.13 val PER: 0.2258
2025-12-16 20:30:56,145: t15.2023.10.15 val PER: 0.1608
2025-12-16 20:30:56,145: t15.2023.10.20 val PER: 0.2450
2025-12-16 20:30:56,145: t15.2023.10.22 val PER: 0.1425
2025-12-16 20:30:56,146: t15.2023.11.03 val PER: 0.1967
2025-12-16 20:30:56,146: t15.2023.11.04 val PER: 0.0444
2025-12-16 20:30:56,146: t15.2023.11.17 val PER: 0.0762
2025-12-16 20:30:56,146: t15.2023.11.19 val PER: 0.0778
2025-12-16 20:30:56,146: t15.2023.11.26 val PER: 0.1457
2025-12-16 20:30:56,146: t15.2023.12.03 val PER: 0.1134
2025-12-16 20:30:56,146: t15.2023.12.08 val PER: 0.1025
2025-12-16 20:30:56,146: t15.2023.12.10 val PER: 0.1078
2025-12-16 20:30:56,146: t15.2023.12.17 val PER: 0.1351
2025-12-16 20:30:56,146: t15.2023.12.29 val PER: 0.1407
2025-12-16 20:30:56,146: t15.2024.02.25 val PER: 0.1081
2025-12-16 20:30:56,146: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:30:56,146: t15.2024.03.08 val PER: 0.2134
2025-12-16 20:30:56,146: t15.2024.03.15 val PER: 0.2039
2025-12-16 20:30:56,146: t15.2024.03.17 val PER: 0.1262
2025-12-16 20:30:56,146: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:30:56,146: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:30:56,146: t15.2024.05.10 val PER: 0.1634
2025-12-16 20:30:56,147: t15.2024.06.14 val PER: 0.1814
2025-12-16 20:30:56,147: t15.2024.07.19 val PER: 0.1997
2025-12-16 20:30:56,147: t15.2024.07.21 val PER: 0.0952
2025-12-16 20:30:56,147: t15.2024.07.28 val PER: 0.1404
2025-12-16 20:30:56,147: t15.2025.01.10 val PER: 0.2769
2025-12-16 20:30:56,147: t15.2025.01.12 val PER: 0.1655
2025-12-16 20:30:56,147: t15.2025.03.14 val PER: 0.3388
2025-12-16 20:30:56,147: t15.2025.03.16 val PER: 0.1963
2025-12-16 20:30:56,147: t15.2025.03.30 val PER: 0.3046
2025-12-16 20:30:56,147: t15.2025.04.13 val PER: 0.2211
2025-12-16 20:31:13,512: Train batch 40200: loss: 0.78 grad norm: 1.17 time: 0.097
2025-12-16 20:31:30,614: Train batch 40400: loss: 0.78 grad norm: 1.68 time: 0.087
2025-12-16 20:31:48,047: Train batch 40600: loss: 0.75 grad norm: 1.90 time: 0.083
2025-12-16 20:32:05,115: Train batch 40800: loss: 0.77 grad norm: 1.39 time: 0.083
2025-12-16 20:32:21,238: Train batch 41000: loss: 0.91 grad norm: 1.32 time: 0.088
2025-12-16 20:32:37,844: Train batch 41200: loss: 0.68 grad norm: 4.06 time: 0.081
2025-12-16 20:32:55,166: Train batch 41400: loss: 0.78 grad norm: 1.56 time: 0.062
2025-12-16 20:33:10,646: Train batch 41600: loss: 0.69 grad norm: 7.35 time: 0.084
2025-12-16 20:33:28,532: Train batch 41800: loss: 0.79 grad norm: 2.35 time: 0.087
2025-12-16 20:33:46,772: Train batch 42000: loss: 0.72 grad norm: 1.18 time: 0.088
2025-12-16 20:33:46,772: Running test after training batch: 42000
2025-12-16 20:33:56,156: Val batch 42000: PER (avg): 0.1471 CTC Loss (avg): 0.7360 time: 9.384
2025-12-16 20:33:56,157: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:33:56,157: t15.2023.08.13 val PER: 0.1279
2025-12-16 20:33:56,157: t15.2023.08.18 val PER: 0.0989
2025-12-16 20:33:56,157: t15.2023.08.20 val PER: 0.1017
2025-12-16 20:33:56,157: t15.2023.08.25 val PER: 0.1009
2025-12-16 20:33:56,157: t15.2023.08.27 val PER: 0.1688
2025-12-16 20:33:56,157: t15.2023.09.01 val PER: 0.0804
2025-12-16 20:33:56,157: t15.2023.09.03 val PER: 0.1568
2025-12-16 20:33:56,157: t15.2023.09.24 val PER: 0.1250
2025-12-16 20:33:56,157: t15.2023.09.29 val PER: 0.1385
2025-12-16 20:33:56,157: t15.2023.10.01 val PER: 0.1764
2025-12-16 20:33:56,157: t15.2023.10.06 val PER: 0.1270
2025-12-16 20:33:56,157: t15.2023.10.08 val PER: 0.2327
2025-12-16 20:33:56,157: t15.2023.10.13 val PER: 0.2110
2025-12-16 20:33:56,157: t15.2023.10.15 val PER: 0.1503
2025-12-16 20:33:56,157: t15.2023.10.20 val PER: 0.2248
2025-12-16 20:33:56,157: t15.2023.10.22 val PER: 0.1214
2025-12-16 20:33:56,157: t15.2023.11.03 val PER: 0.1791
2025-12-16 20:33:56,157: t15.2023.11.04 val PER: 0.0341
2025-12-16 20:33:56,158: t15.2023.11.17 val PER: 0.0404
2025-12-16 20:33:56,158: t15.2023.11.19 val PER: 0.0778
2025-12-16 20:33:56,158: t15.2023.11.26 val PER: 0.1181
2025-12-16 20:33:56,158: t15.2023.12.03 val PER: 0.0924
2025-12-16 20:33:56,158: t15.2023.12.08 val PER: 0.0919
2025-12-16 20:33:56,158: t15.2023.12.10 val PER: 0.0880
2025-12-16 20:33:56,158: t15.2023.12.17 val PER: 0.1289
2025-12-16 20:33:56,158: t15.2023.12.29 val PER: 0.1242
2025-12-16 20:33:56,158: t15.2024.02.25 val PER: 0.1067
2025-12-16 20:33:56,158: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:33:56,158: t15.2024.03.08 val PER: 0.2091
2025-12-16 20:33:56,158: t15.2024.03.15 val PER: 0.2045
2025-12-16 20:33:56,158: t15.2024.03.17 val PER: 0.1241
2025-12-16 20:33:56,158: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:33:56,158: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:33:56,158: t15.2024.05.10 val PER: 0.1486
2025-12-16 20:33:56,158: t15.2024.06.14 val PER: 0.1845
2025-12-16 20:33:56,158: t15.2024.07.19 val PER: 0.1918
2025-12-16 20:33:56,158: t15.2024.07.21 val PER: 0.0938
2025-12-16 20:33:56,159: t15.2024.07.28 val PER: 0.1235
2025-12-16 20:33:56,159: t15.2025.01.10 val PER: 0.2631
2025-12-16 20:33:56,159: t15.2025.01.12 val PER: 0.1424
2025-12-16 20:33:56,159: t15.2025.03.14 val PER: 0.2929
2025-12-16 20:33:56,159: t15.2025.03.16 val PER: 0.2068
2025-12-16 20:33:56,159: t15.2025.03.30 val PER: 0.2828
2025-12-16 20:33:56,159: t15.2025.04.13 val PER: 0.2068
2025-12-16 20:33:56,159: New best test PER 0.1531 --> 0.1471
2025-12-16 20:33:56,159: Checkpointing model
2025-12-16 20:33:56,631: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:34:14,394: Train batch 42200: loss: 0.69 grad norm: 1.11 time: 0.085
2025-12-16 20:34:32,436: Train batch 42400: loss: 0.83 grad norm: 1.56 time: 0.078
2025-12-16 20:34:48,656: Train batch 42600: loss: 0.70 grad norm: 2.09 time: 0.058
2025-12-16 20:35:05,297: Train batch 42800: loss: 0.64 grad norm: 0.78 time: 0.089
2025-12-16 20:35:23,266: Train batch 43000: loss: 0.68 grad norm: 0.80 time: 0.082
2025-12-16 20:35:40,526: Train batch 43200: loss: 0.80 grad norm: 2.44 time: 0.068
2025-12-16 20:35:57,593: Train batch 43400: loss: 0.74 grad norm: 1.55 time: 0.094
2025-12-16 20:36:14,814: Train batch 43600: loss: 0.68 grad norm: 4.95 time: 0.085
2025-12-16 20:36:31,697: Train batch 43800: loss: 0.75 grad norm: 1.69 time: 0.088
2025-12-16 20:36:49,862: Train batch 44000: loss: 0.69 grad norm: 1.41 time: 0.090
2025-12-16 20:36:49,862: Running test after training batch: 44000
2025-12-16 20:36:59,292: Val batch 44000: PER (avg): 0.1484 CTC Loss (avg): 0.7756 time: 9.430
2025-12-16 20:36:59,293: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:36:59,293: t15.2023.08.13 val PER: 0.1227
2025-12-16 20:36:59,293: t15.2023.08.18 val PER: 0.1090
2025-12-16 20:36:59,293: t15.2023.08.20 val PER: 0.0755
2025-12-16 20:36:59,293: t15.2023.08.25 val PER: 0.1205
2025-12-16 20:36:59,293: t15.2023.08.27 val PER: 0.1817
2025-12-16 20:36:59,293: t15.2023.09.01 val PER: 0.0852
2025-12-16 20:36:59,293: t15.2023.09.03 val PER: 0.1568
2025-12-16 20:36:59,293: t15.2023.09.24 val PER: 0.1214
2025-12-16 20:36:59,293: t15.2023.09.29 val PER: 0.1315
2025-12-16 20:36:59,293: t15.2023.10.01 val PER: 0.1711
2025-12-16 20:36:59,293: t15.2023.10.06 val PER: 0.1324
2025-12-16 20:36:59,293: t15.2023.10.08 val PER: 0.2273
2025-12-16 20:36:59,293: t15.2023.10.13 val PER: 0.2164
2025-12-16 20:36:59,293: t15.2023.10.15 val PER: 0.1411
2025-12-16 20:36:59,293: t15.2023.10.20 val PER: 0.2349
2025-12-16 20:36:59,293: t15.2023.10.22 val PER: 0.1158
2025-12-16 20:36:59,293: t15.2023.11.03 val PER: 0.1805
2025-12-16 20:36:59,294: t15.2023.11.04 val PER: 0.0375
2025-12-16 20:36:59,294: t15.2023.11.17 val PER: 0.0591
2025-12-16 20:36:59,294: t15.2023.11.19 val PER: 0.0798
2025-12-16 20:36:59,294: t15.2023.11.26 val PER: 0.1174
2025-12-16 20:36:59,294: t15.2023.12.03 val PER: 0.1176
2025-12-16 20:36:59,294: t15.2023.12.08 val PER: 0.0772
2025-12-16 20:36:59,294: t15.2023.12.10 val PER: 0.1156
2025-12-16 20:36:59,294: t15.2023.12.17 val PER: 0.1279
2025-12-16 20:36:59,294: t15.2023.12.29 val PER: 0.1167
2025-12-16 20:36:59,294: t15.2024.02.25 val PER: 0.0927
2025-12-16 20:36:59,294: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:36:59,294: t15.2024.03.08 val PER: 0.2333
2025-12-16 20:36:59,294: t15.2024.03.15 val PER: 0.1964
2025-12-16 20:36:59,294: t15.2024.03.17 val PER: 0.1248
2025-12-16 20:36:59,294: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:36:59,294: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:36:59,294: t15.2024.05.10 val PER: 0.1412
2025-12-16 20:36:59,294: t15.2024.06.14 val PER: 0.1688
2025-12-16 20:36:59,294: t15.2024.07.19 val PER: 0.1984
2025-12-16 20:36:59,295: t15.2024.07.21 val PER: 0.1000
2025-12-16 20:36:59,295: t15.2024.07.28 val PER: 0.1346
2025-12-16 20:36:59,295: t15.2025.01.10 val PER: 0.3099
2025-12-16 20:36:59,295: t15.2025.01.12 val PER: 0.1409
2025-12-16 20:36:59,295: t15.2025.03.14 val PER: 0.2944
2025-12-16 20:36:59,295: t15.2025.03.16 val PER: 0.2147
2025-12-16 20:36:59,295: t15.2025.03.30 val PER: 0.2805
2025-12-16 20:36:59,295: t15.2025.04.13 val PER: 0.2183
2025-12-16 20:37:17,002: Train batch 44200: loss: 0.74 grad norm: 0.97 time: 0.087
2025-12-16 20:37:34,031: Train batch 44400: loss: 0.75 grad norm: 1.32 time: 0.080
2025-12-16 20:37:49,782: Train batch 44600: loss: 0.66 grad norm: 1.26 time: 0.074
2025-12-16 20:38:05,450: Train batch 44800: loss: 0.71 grad norm: 1.06 time: 0.075
2025-12-16 20:38:20,017: Train batch 45000: loss: 0.70 grad norm: 1.33 time: 0.051
2025-12-16 20:38:34,797: Train batch 45200: loss: 0.72 grad norm: 2.56 time: 0.082
2025-12-16 20:38:51,646: Train batch 45400: loss: 0.72 grad norm: 1.54 time: 0.072
2025-12-16 20:39:08,242: Train batch 45600: loss: 0.78 grad norm: 0.81 time: 0.088
2025-12-16 20:39:25,518: Train batch 45800: loss: 0.73 grad norm: 2.31 time: 0.068
2025-12-16 20:39:42,978: Train batch 46000: loss: 0.87 grad norm: 1.93 time: 0.084
2025-12-16 20:39:42,978: Running test after training batch: 46000
2025-12-16 20:39:52,462: Val batch 46000: PER (avg): 0.1399 CTC Loss (avg): 0.7268 time: 9.483
2025-12-16 20:39:52,462: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:39:52,462: t15.2023.08.13 val PER: 0.1091
2025-12-16 20:39:52,462: t15.2023.08.18 val PER: 0.1056
2025-12-16 20:39:52,462: t15.2023.08.20 val PER: 0.0794
2025-12-16 20:39:52,462: t15.2023.08.25 val PER: 0.0994
2025-12-16 20:39:52,462: t15.2023.08.27 val PER: 0.1752
2025-12-16 20:39:52,462: t15.2023.09.01 val PER: 0.0763
2025-12-16 20:39:52,462: t15.2023.09.03 val PER: 0.1318
2025-12-16 20:39:52,463: t15.2023.09.24 val PER: 0.1056
2025-12-16 20:39:52,463: t15.2023.09.29 val PER: 0.1493
2025-12-16 20:39:52,463: t15.2023.10.01 val PER: 0.1598
2025-12-16 20:39:52,463: t15.2023.10.06 val PER: 0.1066
2025-12-16 20:39:52,463: t15.2023.10.08 val PER: 0.2165
2025-12-16 20:39:52,463: t15.2023.10.13 val PER: 0.2110
2025-12-16 20:39:52,463: t15.2023.10.15 val PER: 0.1397
2025-12-16 20:39:52,463: t15.2023.10.20 val PER: 0.2282
2025-12-16 20:39:52,463: t15.2023.10.22 val PER: 0.1236
2025-12-16 20:39:52,463: t15.2023.11.03 val PER: 0.1669
2025-12-16 20:39:52,463: t15.2023.11.04 val PER: 0.0307
2025-12-16 20:39:52,463: t15.2023.11.17 val PER: 0.0420
2025-12-16 20:39:52,463: t15.2023.11.19 val PER: 0.0599
2025-12-16 20:39:52,463: t15.2023.11.26 val PER: 0.1043
2025-12-16 20:39:52,463: t15.2023.12.03 val PER: 0.0903
2025-12-16 20:39:52,463: t15.2023.12.08 val PER: 0.0772
2025-12-16 20:39:52,463: t15.2023.12.10 val PER: 0.0762
2025-12-16 20:39:52,463: t15.2023.12.17 val PER: 0.1383
2025-12-16 20:39:52,463: t15.2023.12.29 val PER: 0.1194
2025-12-16 20:39:52,464: t15.2024.02.25 val PER: 0.0955
2025-12-16 20:39:52,464: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:39:52,464: t15.2024.03.08 val PER: 0.2077
2025-12-16 20:39:52,464: t15.2024.03.15 val PER: 0.1932
2025-12-16 20:39:52,464: t15.2024.03.17 val PER: 0.1088
2025-12-16 20:39:52,464: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:39:52,464: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:39:52,464: t15.2024.05.10 val PER: 0.1456
2025-12-16 20:39:52,464: t15.2024.06.14 val PER: 0.1688
2025-12-16 20:39:52,464: t15.2024.07.19 val PER: 0.1852
2025-12-16 20:39:52,464: t15.2024.07.21 val PER: 0.0910
2025-12-16 20:39:52,464: t15.2024.07.28 val PER: 0.1338
2025-12-16 20:39:52,464: t15.2025.01.10 val PER: 0.3058
2025-12-16 20:39:52,464: t15.2025.01.12 val PER: 0.1278
2025-12-16 20:39:52,464: t15.2025.03.14 val PER: 0.2870
2025-12-16 20:39:52,464: t15.2025.03.16 val PER: 0.1872
2025-12-16 20:39:52,464: t15.2025.03.30 val PER: 0.2644
2025-12-16 20:39:52,464: t15.2025.04.13 val PER: 0.1983
2025-12-16 20:39:52,464: New best test PER 0.1471 --> 0.1399
2025-12-16 20:39:52,464: Checkpointing model
2025-12-16 20:39:52,967: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:40:09,197: Train batch 46200: loss: 0.73 grad norm: 2.23 time: 0.088
2025-12-16 20:40:26,187: Train batch 46400: loss: 0.77 grad norm: 0.99 time: 0.086
2025-12-16 20:40:43,081: Train batch 46600: loss: 0.69 grad norm: 0.99 time: 0.072
2025-12-16 20:41:00,930: Train batch 46800: loss: 0.86 grad norm: 1.71 time: 0.081
2025-12-16 20:41:18,325: Train batch 47000: loss: 0.77 grad norm: 1.63 time: 0.083
2025-12-16 20:41:35,771: Train batch 47200: loss: 0.76 grad norm: 0.88 time: 0.086
2025-12-16 20:41:52,065: Train batch 47400: loss: 0.71 grad norm: 9.42 time: 0.084
2025-12-16 20:42:06,644: Train batch 47600: loss: 0.81 grad norm: 1.83 time: 0.087
2025-12-16 20:42:23,908: Train batch 47800: loss: 0.78 grad norm: 1.54 time: 0.079
2025-12-16 20:42:41,074: Train batch 48000: loss: 0.80 grad norm: 1.55 time: 0.091
2025-12-16 20:42:41,074: Running test after training batch: 48000
2025-12-16 20:42:50,430: Val batch 48000: PER (avg): 0.1400 CTC Loss (avg): 0.7352 time: 9.356
2025-12-16 20:42:50,430: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:42:50,431: t15.2023.08.13 val PER: 0.1227
2025-12-16 20:42:50,431: t15.2023.08.18 val PER: 0.1115
2025-12-16 20:42:50,431: t15.2023.08.20 val PER: 0.0802
2025-12-16 20:42:50,431: t15.2023.08.25 val PER: 0.1084
2025-12-16 20:42:50,431: t15.2023.08.27 val PER: 0.1656
2025-12-16 20:42:50,431: t15.2023.09.01 val PER: 0.0739
2025-12-16 20:42:50,431: t15.2023.09.03 val PER: 0.1413
2025-12-16 20:42:50,431: t15.2023.09.24 val PER: 0.1153
2025-12-16 20:42:50,431: t15.2023.09.29 val PER: 0.1442
2025-12-16 20:42:50,431: t15.2023.10.01 val PER: 0.1697
2025-12-16 20:42:50,431: t15.2023.10.06 val PER: 0.1109
2025-12-16 20:42:50,431: t15.2023.10.08 val PER: 0.2165
2025-12-16 20:42:50,431: t15.2023.10.13 val PER: 0.2017
2025-12-16 20:42:50,431: t15.2023.10.15 val PER: 0.1496
2025-12-16 20:42:50,431: t15.2023.10.20 val PER: 0.2148
2025-12-16 20:42:50,431: t15.2023.10.22 val PER: 0.1292
2025-12-16 20:42:50,431: t15.2023.11.03 val PER: 0.1771
2025-12-16 20:42:50,431: t15.2023.11.04 val PER: 0.0205
2025-12-16 20:42:50,432: t15.2023.11.17 val PER: 0.0482
2025-12-16 20:42:50,432: t15.2023.11.19 val PER: 0.0699
2025-12-16 20:42:50,432: t15.2023.11.26 val PER: 0.1094
2025-12-16 20:42:50,432: t15.2023.12.03 val PER: 0.0851
2025-12-16 20:42:50,432: t15.2023.12.08 val PER: 0.0779
2025-12-16 20:42:50,432: t15.2023.12.10 val PER: 0.0854
2025-12-16 20:42:50,432: t15.2023.12.17 val PER: 0.1351
2025-12-16 20:42:50,432: t15.2023.12.29 val PER: 0.1064
2025-12-16 20:42:50,432: t15.2024.02.25 val PER: 0.0843
2025-12-16 20:42:50,432: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:42:50,432: t15.2024.03.08 val PER: 0.1878
2025-12-16 20:42:50,432: t15.2024.03.15 val PER: 0.1951
2025-12-16 20:42:50,432: t15.2024.03.17 val PER: 0.0990
2025-12-16 20:42:50,432: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:42:50,432: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:42:50,432: t15.2024.05.10 val PER: 0.1516
2025-12-16 20:42:50,432: t15.2024.06.14 val PER: 0.1593
2025-12-16 20:42:50,432: t15.2024.07.19 val PER: 0.1826
2025-12-16 20:42:50,432: t15.2024.07.21 val PER: 0.0966
2025-12-16 20:42:50,432: t15.2024.07.28 val PER: 0.1331
2025-12-16 20:42:50,433: t15.2025.01.10 val PER: 0.2562
2025-12-16 20:42:50,433: t15.2025.01.12 val PER: 0.1124
2025-12-16 20:42:50,433: t15.2025.03.14 val PER: 0.3047
2025-12-16 20:42:50,433: t15.2025.03.16 val PER: 0.1990
2025-12-16 20:42:50,433: t15.2025.03.30 val PER: 0.2713
2025-12-16 20:42:50,433: t15.2025.04.13 val PER: 0.2040
2025-12-16 20:43:07,312: Train batch 48200: loss: 0.71 grad norm: 0.95 time: 0.084
2025-12-16 20:43:22,709: Train batch 48400: loss: 0.70 grad norm: 0.67 time: 0.103
2025-12-16 20:43:40,214: Train batch 48600: loss: 0.73 grad norm: 1.38 time: 0.076
2025-12-16 20:43:57,983: Train batch 48800: loss: 0.66 grad norm: 0.74 time: 0.034
2025-12-16 20:44:15,201: Train batch 49000: loss: 0.70 grad norm: 1.48 time: 0.101
2025-12-16 20:44:32,052: Train batch 49200: loss: 0.69 grad norm: 1.37 time: 0.066
2025-12-16 20:44:49,392: Train batch 49400: loss: 0.72 grad norm: 2.01 time: 0.073
2025-12-16 20:45:06,774: Train batch 49600: loss: 0.77 grad norm: 1.07 time: 0.046
2025-12-16 20:45:23,864: Train batch 49800: loss: 0.71 grad norm: 6.71 time: 0.076
2025-12-16 20:45:39,977: Train batch 50000: loss: 0.66 grad norm: 0.91 time: 0.088
2025-12-16 20:45:39,978: Running test after training batch: 50000
2025-12-16 20:45:49,384: Val batch 50000: PER (avg): 0.1413 CTC Loss (avg): 0.7583 time: 9.406
2025-12-16 20:45:49,384: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:45:49,384: t15.2023.08.13 val PER: 0.1112
2025-12-16 20:45:49,384: t15.2023.08.18 val PER: 0.1006
2025-12-16 20:45:49,384: t15.2023.08.20 val PER: 0.0818
2025-12-16 20:45:49,384: t15.2023.08.25 val PER: 0.0964
2025-12-16 20:45:49,384: t15.2023.08.27 val PER: 0.1656
2025-12-16 20:45:49,384: t15.2023.09.01 val PER: 0.0649
2025-12-16 20:45:49,385: t15.2023.09.03 val PER: 0.1330
2025-12-16 20:45:49,385: t15.2023.09.24 val PER: 0.1189
2025-12-16 20:45:49,385: t15.2023.09.29 val PER: 0.1429
2025-12-16 20:45:49,385: t15.2023.10.01 val PER: 0.1790
2025-12-16 20:45:49,385: t15.2023.10.06 val PER: 0.1066
2025-12-16 20:45:49,385: t15.2023.10.08 val PER: 0.2111
2025-12-16 20:45:49,385: t15.2023.10.13 val PER: 0.2095
2025-12-16 20:45:49,385: t15.2023.10.15 val PER: 0.1569
2025-12-16 20:45:49,385: t15.2023.10.20 val PER: 0.2215
2025-12-16 20:45:49,385: t15.2023.10.22 val PER: 0.1158
2025-12-16 20:45:49,385: t15.2023.11.03 val PER: 0.1601
2025-12-16 20:45:49,385: t15.2023.11.04 val PER: 0.0205
2025-12-16 20:45:49,385: t15.2023.11.17 val PER: 0.0498
2025-12-16 20:45:49,385: t15.2023.11.19 val PER: 0.0699
2025-12-16 20:45:49,385: t15.2023.11.26 val PER: 0.1572
2025-12-16 20:45:49,385: t15.2023.12.03 val PER: 0.0861
2025-12-16 20:45:49,385: t15.2023.12.08 val PER: 0.0759
2025-12-16 20:45:49,385: t15.2023.12.10 val PER: 0.0880
2025-12-16 20:45:49,386: t15.2023.12.17 val PER: 0.1393
2025-12-16 20:45:49,386: t15.2023.12.29 val PER: 0.1174
2025-12-16 20:45:49,386: t15.2024.02.25 val PER: 0.0941
2025-12-16 20:45:49,386: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:45:49,386: t15.2024.03.08 val PER: 0.2148
2025-12-16 20:45:49,386: t15.2024.03.15 val PER: 0.1901
2025-12-16 20:45:49,386: t15.2024.03.17 val PER: 0.1067
2025-12-16 20:45:49,386: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:45:49,386: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:45:49,386: t15.2024.05.10 val PER: 0.1441
2025-12-16 20:45:49,386: t15.2024.06.14 val PER: 0.1719
2025-12-16 20:45:49,386: t15.2024.07.19 val PER: 0.1846
2025-12-16 20:45:49,386: t15.2024.07.21 val PER: 0.0869
2025-12-16 20:45:49,386: t15.2024.07.28 val PER: 0.1184
2025-12-16 20:45:49,386: t15.2025.01.10 val PER: 0.2741
2025-12-16 20:45:49,386: t15.2025.01.12 val PER: 0.1278
2025-12-16 20:45:49,386: t15.2025.03.14 val PER: 0.2914
2025-12-16 20:45:49,386: t15.2025.03.16 val PER: 0.2016
2025-12-16 20:45:49,386: t15.2025.03.30 val PER: 0.2690
2025-12-16 20:45:49,386: t15.2025.04.13 val PER: 0.2026
2025-12-16 20:46:06,249: Train batch 50200: loss: 0.69 grad norm: 0.77 time: 0.068
2025-12-16 20:46:22,279: Train batch 50400: loss: 0.80 grad norm: 1.19 time: 0.071
2025-12-16 20:46:39,222: Train batch 50600: loss: 0.71 grad norm: 1.70 time: 0.083
2025-12-16 20:46:56,357: Train batch 50800: loss: 0.79 grad norm: 0.76 time: 0.085
2025-12-16 20:47:12,550: Train batch 51000: loss: 0.72 grad norm: 1.26 time: 0.054
2025-12-16 20:47:26,893: Train batch 51200: loss: 0.76 grad norm: 1.50 time: 0.085
2025-12-16 20:47:44,052: Train batch 51400: loss: 0.76 grad norm: 1.02 time: 0.087
2025-12-16 20:48:00,879: Train batch 51600: loss: 0.74 grad norm: 1.21 time: 0.081
2025-12-16 20:48:17,197: Train batch 51800: loss: 0.61 grad norm: 0.81 time: 0.072
2025-12-16 20:48:33,936: Train batch 52000: loss: 0.73 grad norm: 1.22 time: 0.086
2025-12-16 20:48:33,937: Running test after training batch: 52000
2025-12-16 20:48:43,248: Val batch 52000: PER (avg): 0.1348 CTC Loss (avg): 0.7359 time: 9.311
2025-12-16 20:48:43,248: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:48:43,248: t15.2023.08.13 val PER: 0.1029
2025-12-16 20:48:43,248: t15.2023.08.18 val PER: 0.1098
2025-12-16 20:48:43,248: t15.2023.08.20 val PER: 0.0747
2025-12-16 20:48:43,248: t15.2023.08.25 val PER: 0.0949
2025-12-16 20:48:43,248: t15.2023.08.27 val PER: 0.1511
2025-12-16 20:48:43,248: t15.2023.09.01 val PER: 0.0584
2025-12-16 20:48:43,248: t15.2023.09.03 val PER: 0.1128
2025-12-16 20:48:43,248: t15.2023.09.24 val PER: 0.1226
2025-12-16 20:48:43,248: t15.2023.09.29 val PER: 0.1366
2025-12-16 20:48:43,249: t15.2023.10.01 val PER: 0.1618
2025-12-16 20:48:43,249: t15.2023.10.06 val PER: 0.0980
2025-12-16 20:48:43,249: t15.2023.10.08 val PER: 0.2179
2025-12-16 20:48:43,249: t15.2023.10.13 val PER: 0.1908
2025-12-16 20:48:43,249: t15.2023.10.15 val PER: 0.1391
2025-12-16 20:48:43,249: t15.2023.10.20 val PER: 0.2349
2025-12-16 20:48:43,249: t15.2023.10.22 val PER: 0.1180
2025-12-16 20:48:43,249: t15.2023.11.03 val PER: 0.1635
2025-12-16 20:48:43,249: t15.2023.11.04 val PER: 0.0307
2025-12-16 20:48:43,249: t15.2023.11.17 val PER: 0.0731
2025-12-16 20:48:43,249: t15.2023.11.19 val PER: 0.0619
2025-12-16 20:48:43,249: t15.2023.11.26 val PER: 0.1471
2025-12-16 20:48:43,249: t15.2023.12.03 val PER: 0.0809
2025-12-16 20:48:43,249: t15.2023.12.08 val PER: 0.0632
2025-12-16 20:48:43,249: t15.2023.12.10 val PER: 0.0907
2025-12-16 20:48:43,249: t15.2023.12.17 val PER: 0.1383
2025-12-16 20:48:43,249: t15.2023.12.29 val PER: 0.1002
2025-12-16 20:48:43,249: t15.2024.02.25 val PER: 0.0913
2025-12-16 20:48:43,249: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:48:43,250: t15.2024.03.08 val PER: 0.2077
2025-12-16 20:48:43,250: t15.2024.03.15 val PER: 0.1826
2025-12-16 20:48:43,250: t15.2024.03.17 val PER: 0.0927
2025-12-16 20:48:43,250: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:48:43,250: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:48:43,250: t15.2024.05.10 val PER: 0.1159
2025-12-16 20:48:43,250: t15.2024.06.14 val PER: 0.1420
2025-12-16 20:48:43,250: t15.2024.07.19 val PER: 0.1721
2025-12-16 20:48:43,250: t15.2024.07.21 val PER: 0.0869
2025-12-16 20:48:43,250: t15.2024.07.28 val PER: 0.1301
2025-12-16 20:48:43,250: t15.2025.01.10 val PER: 0.2672
2025-12-16 20:48:43,250: t15.2025.01.12 val PER: 0.1255
2025-12-16 20:48:43,250: t15.2025.03.14 val PER: 0.2840
2025-12-16 20:48:43,250: t15.2025.03.16 val PER: 0.1819
2025-12-16 20:48:43,250: t15.2025.03.30 val PER: 0.2747
2025-12-16 20:48:43,250: t15.2025.04.13 val PER: 0.2011
2025-12-16 20:48:43,250: New best test PER 0.1399 --> 0.1348
2025-12-16 20:48:43,250: Checkpointing model
2025-12-16 20:48:43,716: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:48:59,699: Train batch 52200: loss: 0.66 grad norm: 0.76 time: 0.073
2025-12-16 20:49:17,050: Train batch 52400: loss: 0.71 grad norm: 1.05 time: 0.105
2025-12-16 20:49:33,388: Train batch 52600: loss: 0.66 grad norm: 1.98 time: 0.073
2025-12-16 20:49:51,115: Train batch 52800: loss: 0.59 grad norm: 2.55 time: 0.077
2025-12-16 20:50:07,159: Train batch 53000: loss: 0.81 grad norm: 2.19 time: 0.072
2025-12-16 20:50:23,677: Train batch 53200: loss: 0.66 grad norm: 13.70 time: 0.070
2025-12-16 20:50:40,226: Train batch 53400: loss: 0.92 grad norm: 1.67 time: 0.091
2025-12-16 20:50:57,827: Train batch 53600: loss: 0.68 grad norm: 1.16 time: 0.089
2025-12-16 20:51:13,551: Train batch 53800: loss: 0.73 grad norm: 0.74 time: 0.088
2025-12-16 20:51:29,472: Train batch 54000: loss: 0.76 grad norm: 11.27 time: 0.051
2025-12-16 20:51:29,473: Running test after training batch: 54000
2025-12-16 20:51:38,876: Val batch 54000: PER (avg): 0.1389 CTC Loss (avg): 0.8065 time: 9.403
2025-12-16 20:51:38,876: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:51:38,876: t15.2023.08.13 val PER: 0.1227
2025-12-16 20:51:38,876: t15.2023.08.18 val PER: 0.1006
2025-12-16 20:51:38,876: t15.2023.08.20 val PER: 0.0794
2025-12-16 20:51:38,876: t15.2023.08.25 val PER: 0.0889
2025-12-16 20:51:38,876: t15.2023.08.27 val PER: 0.1431
2025-12-16 20:51:38,876: t15.2023.09.01 val PER: 0.0698
2025-12-16 20:51:38,876: t15.2023.09.03 val PER: 0.1318
2025-12-16 20:51:38,876: t15.2023.09.24 val PER: 0.1104
2025-12-16 20:51:38,876: t15.2023.09.29 val PER: 0.1449
2025-12-16 20:51:38,876: t15.2023.10.01 val PER: 0.1711
2025-12-16 20:51:38,876: t15.2023.10.06 val PER: 0.1001
2025-12-16 20:51:38,876: t15.2023.10.08 val PER: 0.2233
2025-12-16 20:51:38,877: t15.2023.10.13 val PER: 0.1994
2025-12-16 20:51:38,877: t15.2023.10.15 val PER: 0.1516
2025-12-16 20:51:38,877: t15.2023.10.20 val PER: 0.2215
2025-12-16 20:51:38,877: t15.2023.10.22 val PER: 0.1214
2025-12-16 20:51:38,877: t15.2023.11.03 val PER: 0.1811
2025-12-16 20:51:38,877: t15.2023.11.04 val PER: 0.0375
2025-12-16 20:51:38,877: t15.2023.11.17 val PER: 0.0420
2025-12-16 20:51:38,877: t15.2023.11.19 val PER: 0.0798
2025-12-16 20:51:38,877: t15.2023.11.26 val PER: 0.1196
2025-12-16 20:51:38,877: t15.2023.12.03 val PER: 0.0746
2025-12-16 20:51:38,877: t15.2023.12.08 val PER: 0.0712
2025-12-16 20:51:38,877: t15.2023.12.10 val PER: 0.0683
2025-12-16 20:51:38,877: t15.2023.12.17 val PER: 0.1341
2025-12-16 20:51:38,877: t15.2023.12.29 val PER: 0.1016
2025-12-16 20:51:38,877: t15.2024.02.25 val PER: 0.1025
2025-12-16 20:51:38,877: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:51:38,877: t15.2024.03.08 val PER: 0.2219
2025-12-16 20:51:38,877: t15.2024.03.15 val PER: 0.1939
2025-12-16 20:51:38,877: t15.2024.03.17 val PER: 0.0921
2025-12-16 20:51:38,878: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:51:38,878: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:51:38,878: t15.2024.05.10 val PER: 0.1590
2025-12-16 20:51:38,878: t15.2024.06.14 val PER: 0.1719
2025-12-16 20:51:38,878: t15.2024.07.19 val PER: 0.1819
2025-12-16 20:51:38,878: t15.2024.07.21 val PER: 0.0938
2025-12-16 20:51:38,878: t15.2024.07.28 val PER: 0.1250
2025-12-16 20:51:38,878: t15.2025.01.10 val PER: 0.2645
2025-12-16 20:51:38,878: t15.2025.01.12 val PER: 0.1301
2025-12-16 20:51:38,878: t15.2025.03.14 val PER: 0.2929
2025-12-16 20:51:38,878: t15.2025.03.16 val PER: 0.1872
2025-12-16 20:51:38,878: t15.2025.03.30 val PER: 0.2747
2025-12-16 20:51:38,878: t15.2025.04.13 val PER: 0.2026
2025-12-16 20:51:55,331: Train batch 54200: loss: 0.63 grad norm: 1.19 time: 0.074
2025-12-16 20:52:12,841: Train batch 54400: loss: 0.72 grad norm: 1.04 time: 0.072
2025-12-16 20:52:30,340: Train batch 54600: loss: 0.61 grad norm: 1.36 time: 0.071
2025-12-16 20:52:47,451: Train batch 54800: loss: 0.86 grad norm: 1.50 time: 0.064
2025-12-16 20:53:05,444: Train batch 55000: loss: 0.73 grad norm: 0.97 time: 0.077
2025-12-16 20:53:23,310: Train batch 55200: loss: 0.65 grad norm: 10.63 time: 0.069
2025-12-16 20:53:40,868: Train batch 55400: loss: 0.69 grad norm: 1.36 time: 0.066
2025-12-16 20:53:58,662: Train batch 55600: loss: 0.80 grad norm: 1.21 time: 0.084
2025-12-16 20:54:16,434: Train batch 55800: loss: 0.73 grad norm: 1.15 time: 0.081
2025-12-16 20:54:34,143: Train batch 56000: loss: 0.71 grad norm: 3.48 time: 0.087
2025-12-16 20:54:34,144: Running test after training batch: 56000
2025-12-16 20:54:43,616: Val batch 56000: PER (avg): 0.1365 CTC Loss (avg): 0.7894 time: 9.472
2025-12-16 20:54:43,617: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:54:43,617: t15.2023.08.13 val PER: 0.1123
2025-12-16 20:54:43,617: t15.2023.08.18 val PER: 0.1048
2025-12-16 20:54:43,617: t15.2023.08.20 val PER: 0.0826
2025-12-16 20:54:43,617: t15.2023.08.25 val PER: 0.0843
2025-12-16 20:54:43,617: t15.2023.08.27 val PER: 0.1559
2025-12-16 20:54:43,617: t15.2023.09.01 val PER: 0.0633
2025-12-16 20:54:43,617: t15.2023.09.03 val PER: 0.1259
2025-12-16 20:54:43,617: t15.2023.09.24 val PER: 0.1007
2025-12-16 20:54:43,617: t15.2023.09.29 val PER: 0.1327
2025-12-16 20:54:43,617: t15.2023.10.01 val PER: 0.1572
2025-12-16 20:54:43,617: t15.2023.10.06 val PER: 0.1044
2025-12-16 20:54:43,617: t15.2023.10.08 val PER: 0.2043
2025-12-16 20:54:43,617: t15.2023.10.13 val PER: 0.1932
2025-12-16 20:54:43,617: t15.2023.10.15 val PER: 0.1338
2025-12-16 20:54:43,617: t15.2023.10.20 val PER: 0.2282
2025-12-16 20:54:43,617: t15.2023.10.22 val PER: 0.1314
2025-12-16 20:54:43,618: t15.2023.11.03 val PER: 0.1818
2025-12-16 20:54:43,618: t15.2023.11.04 val PER: 0.0341
2025-12-16 20:54:43,618: t15.2023.11.17 val PER: 0.0435
2025-12-16 20:54:43,618: t15.2023.11.19 val PER: 0.0679
2025-12-16 20:54:43,618: t15.2023.11.26 val PER: 0.1558
2025-12-16 20:54:43,618: t15.2023.12.03 val PER: 0.0924
2025-12-16 20:54:43,618: t15.2023.12.08 val PER: 0.0832
2025-12-16 20:54:43,618: t15.2023.12.10 val PER: 0.0815
2025-12-16 20:54:43,618: t15.2023.12.17 val PER: 0.1227
2025-12-16 20:54:43,618: t15.2023.12.29 val PER: 0.1091
2025-12-16 20:54:43,618: t15.2024.02.25 val PER: 0.0969
2025-12-16 20:54:43,618: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:54:43,618: t15.2024.03.08 val PER: 0.2119
2025-12-16 20:54:43,618: t15.2024.03.15 val PER: 0.1914
2025-12-16 20:54:43,618: t15.2024.03.17 val PER: 0.0893
2025-12-16 20:54:43,618: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:54:43,618: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:54:43,618: t15.2024.05.10 val PER: 0.1486
2025-12-16 20:54:43,618: t15.2024.06.14 val PER: 0.1372
2025-12-16 20:54:43,619: t15.2024.07.19 val PER: 0.1892
2025-12-16 20:54:43,619: t15.2024.07.21 val PER: 0.0979
2025-12-16 20:54:43,619: t15.2024.07.28 val PER: 0.1169
2025-12-16 20:54:43,619: t15.2025.01.10 val PER: 0.2576
2025-12-16 20:54:43,619: t15.2025.01.12 val PER: 0.1139
2025-12-16 20:54:43,619: t15.2025.03.14 val PER: 0.2929
2025-12-16 20:54:43,619: t15.2025.03.16 val PER: 0.1937
2025-12-16 20:54:43,619: t15.2025.03.30 val PER: 0.2460
2025-12-16 20:54:43,619: t15.2025.04.13 val PER: 0.1883
2025-12-16 20:55:00,210: Train batch 56200: loss: 0.69 grad norm: 1.28 time: 0.082
2025-12-16 20:55:18,024: Train batch 56400: loss: 0.69 grad norm: 1.17 time: 0.087
2025-12-16 20:55:33,661: Train batch 56600: loss: 0.73 grad norm: 3.32 time: 0.055
2025-12-16 20:55:49,123: Train batch 56800: loss: 0.66 grad norm: 1.66 time: 0.082
2025-12-16 20:56:06,769: Train batch 57000: loss: 0.73 grad norm: 1.94 time: 0.089
2025-12-16 20:56:22,491: Train batch 57200: loss: 0.81 grad norm: 1.20 time: 0.063
2025-12-16 20:56:39,664: Train batch 57400: loss: 0.68 grad norm: 2.87 time: 0.098
2025-12-16 20:56:55,380: Train batch 57600: loss: 0.71 grad norm: 1.13 time: 0.088
2025-12-16 20:57:12,558: Train batch 57800: loss: 0.75 grad norm: 1.25 time: 0.071
2025-12-16 20:57:30,191: Train batch 58000: loss: 0.75 grad norm: 1.20 time: 0.076
2025-12-16 20:57:30,192: Running test after training batch: 58000
2025-12-16 20:57:39,632: Val batch 58000: PER (avg): 0.1347 CTC Loss (avg): 0.7716 time: 9.440
2025-12-16 20:57:39,632: t15.2023.08.11 val PER: 1.0000
2025-12-16 20:57:39,632: t15.2023.08.13 val PER: 0.1029
2025-12-16 20:57:39,632: t15.2023.08.18 val PER: 0.0855
2025-12-16 20:57:39,632: t15.2023.08.20 val PER: 0.0723
2025-12-16 20:57:39,632: t15.2023.08.25 val PER: 0.0994
2025-12-16 20:57:39,632: t15.2023.08.27 val PER: 0.1479
2025-12-16 20:57:39,632: t15.2023.09.01 val PER: 0.0633
2025-12-16 20:57:39,632: t15.2023.09.03 val PER: 0.1449
2025-12-16 20:57:39,632: t15.2023.09.24 val PER: 0.1032
2025-12-16 20:57:39,632: t15.2023.09.29 val PER: 0.1289
2025-12-16 20:57:39,632: t15.2023.10.01 val PER: 0.1744
2025-12-16 20:57:39,633: t15.2023.10.06 val PER: 0.1033
2025-12-16 20:57:39,633: t15.2023.10.08 val PER: 0.2070
2025-12-16 20:57:39,633: t15.2023.10.13 val PER: 0.2009
2025-12-16 20:57:39,633: t15.2023.10.15 val PER: 0.1411
2025-12-16 20:57:39,633: t15.2023.10.20 val PER: 0.2349
2025-12-16 20:57:39,633: t15.2023.10.22 val PER: 0.1325
2025-12-16 20:57:39,633: t15.2023.11.03 val PER: 0.1791
2025-12-16 20:57:39,633: t15.2023.11.04 val PER: 0.0410
2025-12-16 20:57:39,633: t15.2023.11.17 val PER: 0.0295
2025-12-16 20:57:39,633: t15.2023.11.19 val PER: 0.0758
2025-12-16 20:57:39,633: t15.2023.11.26 val PER: 0.1203
2025-12-16 20:57:39,633: t15.2023.12.03 val PER: 0.0830
2025-12-16 20:57:39,633: t15.2023.12.08 val PER: 0.0699
2025-12-16 20:57:39,633: t15.2023.12.10 val PER: 0.0788
2025-12-16 20:57:39,633: t15.2023.12.17 val PER: 0.1227
2025-12-16 20:57:39,633: t15.2023.12.29 val PER: 0.1023
2025-12-16 20:57:39,633: t15.2024.02.25 val PER: 0.0843
2025-12-16 20:57:39,633: t15.2024.03.03 val PER: 1.0000
2025-12-16 20:57:39,634: t15.2024.03.08 val PER: 0.1977
2025-12-16 20:57:39,634: t15.2024.03.15 val PER: 0.1720
2025-12-16 20:57:39,634: t15.2024.03.17 val PER: 0.0962
2025-12-16 20:57:39,634: t15.2024.04.25 val PER: 1.0000
2025-12-16 20:57:39,634: t15.2024.04.28 val PER: 1.0000
2025-12-16 20:57:39,634: t15.2024.05.10 val PER: 0.1456
2025-12-16 20:57:39,634: t15.2024.06.14 val PER: 0.1420
2025-12-16 20:57:39,634: t15.2024.07.19 val PER: 0.1885
2025-12-16 20:57:39,634: t15.2024.07.21 val PER: 0.0807
2025-12-16 20:57:39,634: t15.2024.07.28 val PER: 0.1243
2025-12-16 20:57:39,634: t15.2025.01.10 val PER: 0.2810
2025-12-16 20:57:39,634: t15.2025.01.12 val PER: 0.1178
2025-12-16 20:57:39,634: t15.2025.03.14 val PER: 0.3003
2025-12-16 20:57:39,634: t15.2025.03.16 val PER: 0.1846
2025-12-16 20:57:39,634: t15.2025.03.30 val PER: 0.2586
2025-12-16 20:57:39,634: t15.2025.04.13 val PER: 0.2240
2025-12-16 20:57:39,634: New best test PER 0.1348 --> 0.1347
2025-12-16 20:57:39,634: Checkpointing model
2025-12-16 20:57:40,089: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 20:57:58,211: Train batch 58200: loss: 0.72 grad norm: 0.57 time: 0.080
2025-12-16 20:58:15,904: Train batch 58400: loss: 0.61 grad norm: 0.55 time: 0.086
2025-12-16 20:58:33,534: Train batch 58600: loss: 0.66 grad norm: 1.42 time: 0.083
2025-12-16 20:58:50,892: Train batch 58800: loss: 0.81 grad norm: 1.43 time: 0.085
2025-12-16 20:59:08,755: Train batch 59000: loss: 0.79 grad norm: 1.32 time: 0.055
2025-12-16 20:59:25,214: Train batch 59200: loss: 0.68 grad norm: 1.62 time: 0.079
2025-12-16 20:59:42,563: Train batch 59400: loss: 0.77 grad norm: 5.23 time: 0.084
2025-12-16 21:00:00,418: Train batch 59600: loss: 0.78 grad norm: 1.68 time: 0.097
2025-12-16 21:00:18,054: Train batch 59800: loss: 0.73 grad norm: 3.07 time: 0.083
2025-12-16 21:00:34,406: Train batch 60000: loss: 0.77 grad norm: 0.18 time: 0.081
2025-12-16 21:00:34,407: Running test after training batch: 60000
2025-12-16 21:00:43,892: Val batch 60000: PER (avg): 0.1369 CTC Loss (avg): 0.8088 time: 9.485
2025-12-16 21:00:43,892: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:00:43,892: t15.2023.08.13 val PER: 0.1091
2025-12-16 21:00:43,892: t15.2023.08.18 val PER: 0.1048
2025-12-16 21:00:43,892: t15.2023.08.20 val PER: 0.0763
2025-12-16 21:00:43,892: t15.2023.08.25 val PER: 0.0934
2025-12-16 21:00:43,892: t15.2023.08.27 val PER: 0.1576
2025-12-16 21:00:43,892: t15.2023.09.01 val PER: 0.0633
2025-12-16 21:00:43,893: t15.2023.09.03 val PER: 0.1568
2025-12-16 21:00:43,893: t15.2023.09.24 val PER: 0.1080
2025-12-16 21:00:43,893: t15.2023.09.29 val PER: 0.1327
2025-12-16 21:00:43,893: t15.2023.10.01 val PER: 0.1704
2025-12-16 21:00:43,893: t15.2023.10.06 val PER: 0.0947
2025-12-16 21:00:43,893: t15.2023.10.08 val PER: 0.2287
2025-12-16 21:00:43,893: t15.2023.10.13 val PER: 0.1971
2025-12-16 21:00:43,893: t15.2023.10.15 val PER: 0.1338
2025-12-16 21:00:43,893: t15.2023.10.20 val PER: 0.2114
2025-12-16 21:00:43,893: t15.2023.10.22 val PER: 0.1214
2025-12-16 21:00:43,893: t15.2023.11.03 val PER: 0.1811
2025-12-16 21:00:43,893: t15.2023.11.04 val PER: 0.0341
2025-12-16 21:00:43,893: t15.2023.11.17 val PER: 0.0607
2025-12-16 21:00:43,893: t15.2023.11.19 val PER: 0.0798
2025-12-16 21:00:43,893: t15.2023.11.26 val PER: 0.1203
2025-12-16 21:00:43,893: t15.2023.12.03 val PER: 0.0903
2025-12-16 21:00:43,893: t15.2023.12.08 val PER: 0.0606
2025-12-16 21:00:43,893: t15.2023.12.10 val PER: 0.0749
2025-12-16 21:00:43,893: t15.2023.12.17 val PER: 0.1299
2025-12-16 21:00:43,894: t15.2023.12.29 val PER: 0.1036
2025-12-16 21:00:43,894: t15.2024.02.25 val PER: 0.0955
2025-12-16 21:00:43,894: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:00:43,894: t15.2024.03.08 val PER: 0.2233
2025-12-16 21:00:43,894: t15.2024.03.15 val PER: 0.1907
2025-12-16 21:00:43,894: t15.2024.03.17 val PER: 0.0962
2025-12-16 21:00:43,894: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:00:43,894: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:00:43,894: t15.2024.05.10 val PER: 0.1308
2025-12-16 21:00:43,894: t15.2024.06.14 val PER: 0.1672
2025-12-16 21:00:43,894: t15.2024.07.19 val PER: 0.1879
2025-12-16 21:00:43,894: t15.2024.07.21 val PER: 0.0834
2025-12-16 21:00:43,894: t15.2024.07.28 val PER: 0.1199
2025-12-16 21:00:43,894: t15.2025.01.10 val PER: 0.2755
2025-12-16 21:00:43,894: t15.2025.01.12 val PER: 0.1278
2025-12-16 21:00:43,894: t15.2025.03.14 val PER: 0.2914
2025-12-16 21:00:43,894: t15.2025.03.16 val PER: 0.1715
2025-12-16 21:00:43,894: t15.2025.03.30 val PER: 0.2586
2025-12-16 21:00:43,894: t15.2025.04.13 val PER: 0.2111
2025-12-16 21:01:01,277: Train batch 60200: loss: 0.65 grad norm: 0.97 time: 0.087
2025-12-16 21:01:18,187: Train batch 60400: loss: 0.77 grad norm: 1.12 time: 0.082
2025-12-16 21:01:35,894: Train batch 60600: loss: 0.71 grad norm: 0.48 time: 0.082
2025-12-16 21:01:53,567: Train batch 60800: loss: 0.77 grad norm: 1.01 time: 0.083
2025-12-16 21:02:10,714: Train batch 61000: loss: 0.66 grad norm: 1.21 time: 0.044
2025-12-16 21:02:26,470: Train batch 61200: loss: 0.71 grad norm: 6.05 time: 0.083
2025-12-16 21:02:43,939: Train batch 61400: loss: 0.72 grad norm: 1.07 time: 0.080
2025-12-16 21:03:01,447: Train batch 61600: loss: 0.73 grad norm: 1.46 time: 0.084
2025-12-16 21:03:19,440: Train batch 61800: loss: 0.73 grad norm: 1.00 time: 0.081
2025-12-16 21:03:36,475: Train batch 62000: loss: 0.69 grad norm: 1.04 time: 0.071
2025-12-16 21:03:36,475: Running test after training batch: 62000
2025-12-16 21:03:45,826: Val batch 62000: PER (avg): 0.1311 CTC Loss (avg): 0.7715 time: 9.351
2025-12-16 21:03:45,826: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:03:45,826: t15.2023.08.13 val PER: 0.1060
2025-12-16 21:03:45,826: t15.2023.08.18 val PER: 0.0989
2025-12-16 21:03:45,826: t15.2023.08.20 val PER: 0.0707
2025-12-16 21:03:45,826: t15.2023.08.25 val PER: 0.0738
2025-12-16 21:03:45,826: t15.2023.08.27 val PER: 0.1350
2025-12-16 21:03:45,826: t15.2023.09.01 val PER: 0.0576
2025-12-16 21:03:45,826: t15.2023.09.03 val PER: 0.1330
2025-12-16 21:03:45,827: t15.2023.09.24 val PER: 0.1068
2025-12-16 21:03:45,827: t15.2023.09.29 val PER: 0.1238
2025-12-16 21:03:45,827: t15.2023.10.01 val PER: 0.1658
2025-12-16 21:03:45,827: t15.2023.10.06 val PER: 0.1055
2025-12-16 21:03:45,827: t15.2023.10.08 val PER: 0.2084
2025-12-16 21:03:45,827: t15.2023.10.13 val PER: 0.1955
2025-12-16 21:03:45,827: t15.2023.10.15 val PER: 0.1358
2025-12-16 21:03:45,827: t15.2023.10.20 val PER: 0.2383
2025-12-16 21:03:45,827: t15.2023.10.22 val PER: 0.1225
2025-12-16 21:03:45,827: t15.2023.11.03 val PER: 0.1621
2025-12-16 21:03:45,827: t15.2023.11.04 val PER: 0.0375
2025-12-16 21:03:45,827: t15.2023.11.17 val PER: 0.0420
2025-12-16 21:03:45,827: t15.2023.11.19 val PER: 0.0758
2025-12-16 21:03:45,827: t15.2023.11.26 val PER: 0.0971
2025-12-16 21:03:45,827: t15.2023.12.03 val PER: 0.0966
2025-12-16 21:03:45,827: t15.2023.12.08 val PER: 0.0632
2025-12-16 21:03:45,827: t15.2023.12.10 val PER: 0.0644
2025-12-16 21:03:45,828: t15.2023.12.17 val PER: 0.1258
2025-12-16 21:03:45,828: t15.2023.12.29 val PER: 0.1098
2025-12-16 21:03:45,828: t15.2024.02.25 val PER: 0.0941
2025-12-16 21:03:45,828: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:03:45,828: t15.2024.03.08 val PER: 0.2020
2025-12-16 21:03:45,828: t15.2024.03.15 val PER: 0.1826
2025-12-16 21:03:45,828: t15.2024.03.17 val PER: 0.0948
2025-12-16 21:03:45,828: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:03:45,828: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:03:45,828: t15.2024.05.10 val PER: 0.1263
2025-12-16 21:03:45,828: t15.2024.06.14 val PER: 0.1404
2025-12-16 21:03:45,828: t15.2024.07.19 val PER: 0.1813
2025-12-16 21:03:45,828: t15.2024.07.21 val PER: 0.0945
2025-12-16 21:03:45,828: t15.2024.07.28 val PER: 0.1228
2025-12-16 21:03:45,828: t15.2025.01.10 val PER: 0.2617
2025-12-16 21:03:45,828: t15.2025.01.12 val PER: 0.1178
2025-12-16 21:03:45,828: t15.2025.03.14 val PER: 0.2751
2025-12-16 21:03:45,828: t15.2025.03.16 val PER: 0.1623
2025-12-16 21:03:45,829: t15.2025.03.30 val PER: 0.2586
2025-12-16 21:03:45,829: t15.2025.04.13 val PER: 0.2040
2025-12-16 21:03:45,829: New best test PER 0.1347 --> 0.1311
2025-12-16 21:03:45,829: Checkpointing model
2025-12-16 21:03:46,315: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:04:02,992: Train batch 62200: loss: 0.76 grad norm: 2.22 time: 0.071
2025-12-16 21:04:20,447: Train batch 62400: loss: 0.66 grad norm: 0.41 time: 0.088
2025-12-16 21:04:37,703: Train batch 62600: loss: 0.61 grad norm: 1.82 time: 0.099
2025-12-16 21:04:55,630: Train batch 62800: loss: 0.70 grad norm: 6.00 time: 0.047
2025-12-16 21:05:12,935: Train batch 63000: loss: 0.65 grad norm: 1.32 time: 0.090
2025-12-16 21:05:30,285: Train batch 63200: loss: 0.67 grad norm: 0.81 time: 0.080
2025-12-16 21:05:48,272: Train batch 63400: loss: 0.76 grad norm: 1.08 time: 0.100
2025-12-16 21:06:06,017: Train batch 63600: loss: 0.78 grad norm: 1.78 time: 0.080
2025-12-16 21:06:23,471: Train batch 63800: loss: 0.66 grad norm: 1.32 time: 0.055
2025-12-16 21:06:41,233: Train batch 64000: loss: 0.78 grad norm: 1.01 time: 0.091
2025-12-16 21:06:41,234: Running test after training batch: 64000
2025-12-16 21:06:50,713: Val batch 64000: PER (avg): 0.1336 CTC Loss (avg): 0.8079 time: 9.480
2025-12-16 21:06:50,714: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:06:50,714: t15.2023.08.13 val PER: 0.1102
2025-12-16 21:06:50,714: t15.2023.08.18 val PER: 0.0956
2025-12-16 21:06:50,714: t15.2023.08.20 val PER: 0.0763
2025-12-16 21:06:50,714: t15.2023.08.25 val PER: 0.0783
2025-12-16 21:06:50,714: t15.2023.08.27 val PER: 0.1624
2025-12-16 21:06:50,714: t15.2023.09.01 val PER: 0.0584
2025-12-16 21:06:50,714: t15.2023.09.03 val PER: 0.1318
2025-12-16 21:06:50,714: t15.2023.09.24 val PER: 0.0983
2025-12-16 21:06:50,714: t15.2023.09.29 val PER: 0.1449
2025-12-16 21:06:50,714: t15.2023.10.01 val PER: 0.1664
2025-12-16 21:06:50,714: t15.2023.10.06 val PER: 0.0947
2025-12-16 21:06:50,714: t15.2023.10.08 val PER: 0.2179
2025-12-16 21:06:50,714: t15.2023.10.13 val PER: 0.1955
2025-12-16 21:06:50,714: t15.2023.10.15 val PER: 0.1332
2025-12-16 21:06:50,715: t15.2023.10.20 val PER: 0.2114
2025-12-16 21:06:50,715: t15.2023.10.22 val PER: 0.1325
2025-12-16 21:06:50,715: t15.2023.11.03 val PER: 0.1723
2025-12-16 21:06:50,715: t15.2023.11.04 val PER: 0.0239
2025-12-16 21:06:50,715: t15.2023.11.17 val PER: 0.0420
2025-12-16 21:06:50,715: t15.2023.11.19 val PER: 0.0699
2025-12-16 21:06:50,715: t15.2023.11.26 val PER: 0.1022
2025-12-16 21:06:50,715: t15.2023.12.03 val PER: 0.0819
2025-12-16 21:06:50,715: t15.2023.12.08 val PER: 0.0679
2025-12-16 21:06:50,715: t15.2023.12.10 val PER: 0.0723
2025-12-16 21:06:50,715: t15.2023.12.17 val PER: 0.1331
2025-12-16 21:06:50,715: t15.2023.12.29 val PER: 0.1078
2025-12-16 21:06:50,715: t15.2024.02.25 val PER: 0.0983
2025-12-16 21:06:50,715: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:06:50,715: t15.2024.03.08 val PER: 0.2063
2025-12-16 21:06:50,715: t15.2024.03.15 val PER: 0.1926
2025-12-16 21:06:50,715: t15.2024.03.17 val PER: 0.0934
2025-12-16 21:06:50,715: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:06:50,715: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:06:50,716: t15.2024.05.10 val PER: 0.1575
2025-12-16 21:06:50,716: t15.2024.06.14 val PER: 0.1467
2025-12-16 21:06:50,716: t15.2024.07.19 val PER: 0.1826
2025-12-16 21:06:50,716: t15.2024.07.21 val PER: 0.0828
2025-12-16 21:06:50,716: t15.2024.07.28 val PER: 0.1206
2025-12-16 21:06:50,716: t15.2025.01.10 val PER: 0.2645
2025-12-16 21:06:50,716: t15.2025.01.12 val PER: 0.1132
2025-12-16 21:06:50,716: t15.2025.03.14 val PER: 0.2855
2025-12-16 21:06:50,716: t15.2025.03.16 val PER: 0.1741
2025-12-16 21:06:50,716: t15.2025.03.30 val PER: 0.2690
2025-12-16 21:06:50,716: t15.2025.04.13 val PER: 0.1869
2025-12-16 21:07:07,780: Train batch 64200: loss: 0.73 grad norm: 0.48 time: 0.069
2025-12-16 21:07:23,809: Train batch 64400: loss: 0.65 grad norm: 0.74 time: 0.088
2025-12-16 21:07:40,862: Train batch 64600: loss: 0.79 grad norm: 9.55 time: 0.095
2025-12-16 21:07:57,851: Train batch 64800: loss: 0.61 grad norm: 1.53 time: 0.051
2025-12-16 21:08:14,259: Train batch 65000: loss: 0.67 grad norm: 0.53 time: 0.086
2025-12-16 21:08:31,846: Train batch 65200: loss: 0.67 grad norm: 1.36 time: 0.073
2025-12-16 21:08:48,516: Train batch 65400: loss: 0.68 grad norm: 1.31 time: 0.086
2025-12-16 21:09:05,202: Train batch 65600: loss: 0.61 grad norm: 0.39 time: 0.071
2025-12-16 21:09:21,114: Train batch 65800: loss: 0.70 grad norm: 0.30 time: 0.070
2025-12-16 21:09:38,073: Train batch 66000: loss: 0.66 grad norm: 0.44 time: 0.079
2025-12-16 21:09:38,074: Running test after training batch: 66000
2025-12-16 21:09:47,510: Val batch 66000: PER (avg): 0.1304 CTC Loss (avg): 0.7932 time: 9.436
2025-12-16 21:09:47,511: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:09:47,511: t15.2023.08.13 val PER: 0.0998
2025-12-16 21:09:47,511: t15.2023.08.18 val PER: 0.1048
2025-12-16 21:09:47,511: t15.2023.08.20 val PER: 0.0723
2025-12-16 21:09:47,511: t15.2023.08.25 val PER: 0.0753
2025-12-16 21:09:47,511: t15.2023.08.27 val PER: 0.1592
2025-12-16 21:09:47,511: t15.2023.09.01 val PER: 0.0503
2025-12-16 21:09:47,511: t15.2023.09.03 val PER: 0.1330
2025-12-16 21:09:47,511: t15.2023.09.24 val PER: 0.1141
2025-12-16 21:09:47,511: t15.2023.09.29 val PER: 0.1270
2025-12-16 21:09:47,511: t15.2023.10.01 val PER: 0.1605
2025-12-16 21:09:47,511: t15.2023.10.06 val PER: 0.1119
2025-12-16 21:09:47,511: t15.2023.10.08 val PER: 0.1894
2025-12-16 21:09:47,511: t15.2023.10.13 val PER: 0.1924
2025-12-16 21:09:47,511: t15.2023.10.15 val PER: 0.1404
2025-12-16 21:09:47,511: t15.2023.10.20 val PER: 0.2416
2025-12-16 21:09:47,512: t15.2023.10.22 val PER: 0.1147
2025-12-16 21:09:47,512: t15.2023.11.03 val PER: 0.1662
2025-12-16 21:09:47,512: t15.2023.11.04 val PER: 0.0273
2025-12-16 21:09:47,512: t15.2023.11.17 val PER: 0.0311
2025-12-16 21:09:47,512: t15.2023.11.19 val PER: 0.0798
2025-12-16 21:09:47,512: t15.2023.11.26 val PER: 0.1029
2025-12-16 21:09:47,512: t15.2023.12.03 val PER: 0.0840
2025-12-16 21:09:47,512: t15.2023.12.08 val PER: 0.0573
2025-12-16 21:09:47,512: t15.2023.12.10 val PER: 0.0683
2025-12-16 21:09:47,512: t15.2023.12.17 val PER: 0.1258
2025-12-16 21:09:47,512: t15.2023.12.29 val PER: 0.1030
2025-12-16 21:09:47,512: t15.2024.02.25 val PER: 0.0955
2025-12-16 21:09:47,512: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:09:47,512: t15.2024.03.08 val PER: 0.1963
2025-12-16 21:09:47,512: t15.2024.03.15 val PER: 0.1951
2025-12-16 21:09:47,512: t15.2024.03.17 val PER: 0.0948
2025-12-16 21:09:47,512: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:09:47,512: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:09:47,512: t15.2024.05.10 val PER: 0.1412
2025-12-16 21:09:47,513: t15.2024.06.14 val PER: 0.1609
2025-12-16 21:09:47,513: t15.2024.07.19 val PER: 0.1780
2025-12-16 21:09:47,513: t15.2024.07.21 val PER: 0.0855
2025-12-16 21:09:47,513: t15.2024.07.28 val PER: 0.1000
2025-12-16 21:09:47,513: t15.2025.01.10 val PER: 0.2576
2025-12-16 21:09:47,513: t15.2025.01.12 val PER: 0.1247
2025-12-16 21:09:47,513: t15.2025.03.14 val PER: 0.2766
2025-12-16 21:09:47,513: t15.2025.03.16 val PER: 0.1806
2025-12-16 21:09:47,513: t15.2025.03.30 val PER: 0.2517
2025-12-16 21:09:47,513: t15.2025.04.13 val PER: 0.1826
2025-12-16 21:09:47,513: New best test PER 0.1311 --> 0.1304
2025-12-16 21:09:47,513: Checkpointing model
2025-12-16 21:09:48,116: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:10:05,235: Train batch 66200: loss: 0.68 grad norm: 1.20 time: 0.062
2025-12-16 21:10:22,783: Train batch 66400: loss: 0.68 grad norm: 1.54 time: 0.091
2025-12-16 21:10:38,730: Train batch 66600: loss: 0.64 grad norm: 1.26 time: 0.064
2025-12-16 21:10:55,982: Train batch 66800: loss: 0.65 grad norm: 0.66 time: 0.096
2025-12-16 21:11:13,823: Train batch 67000: loss: 0.67 grad norm: 0.84 time: 0.086
2025-12-16 21:11:30,778: Train batch 67200: loss: 0.63 grad norm: 1.46 time: 0.075
2025-12-16 21:11:46,549: Train batch 67400: loss: 0.60 grad norm: 1.32 time: 0.058
2025-12-16 21:12:02,469: Train batch 67600: loss: 0.69 grad norm: 4.72 time: 0.067
2025-12-16 21:12:19,424: Train batch 67800: loss: 0.77 grad norm: 1.65 time: 0.064
2025-12-16 21:12:35,060: Train batch 68000: loss: 0.73 grad norm: 0.60 time: 0.063
2025-12-16 21:12:35,060: Running test after training batch: 68000
2025-12-16 21:12:44,418: Val batch 68000: PER (avg): 0.1312 CTC Loss (avg): 0.8146 time: 9.358
2025-12-16 21:12:44,418: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:12:44,418: t15.2023.08.13 val PER: 0.1040
2025-12-16 21:12:44,418: t15.2023.08.18 val PER: 0.0939
2025-12-16 21:12:44,418: t15.2023.08.20 val PER: 0.0763
2025-12-16 21:12:44,419: t15.2023.08.25 val PER: 0.0843
2025-12-16 21:12:44,419: t15.2023.08.27 val PER: 0.1527
2025-12-16 21:12:44,419: t15.2023.09.01 val PER: 0.0544
2025-12-16 21:12:44,419: t15.2023.09.03 val PER: 0.1105
2025-12-16 21:12:44,419: t15.2023.09.24 val PER: 0.0959
2025-12-16 21:12:44,419: t15.2023.09.29 val PER: 0.1308
2025-12-16 21:12:44,419: t15.2023.10.01 val PER: 0.1519
2025-12-16 21:12:44,419: t15.2023.10.06 val PER: 0.1173
2025-12-16 21:12:44,419: t15.2023.10.08 val PER: 0.2030
2025-12-16 21:12:44,419: t15.2023.10.13 val PER: 0.1971
2025-12-16 21:12:44,419: t15.2023.10.15 val PER: 0.1252
2025-12-16 21:12:44,419: t15.2023.10.20 val PER: 0.2383
2025-12-16 21:12:44,419: t15.2023.10.22 val PER: 0.1214
2025-12-16 21:12:44,419: t15.2023.11.03 val PER: 0.1635
2025-12-16 21:12:44,419: t15.2023.11.04 val PER: 0.0410
2025-12-16 21:12:44,419: t15.2023.11.17 val PER: 0.0358
2025-12-16 21:12:44,419: t15.2023.11.19 val PER: 0.0579
2025-12-16 21:12:44,419: t15.2023.11.26 val PER: 0.1181
2025-12-16 21:12:44,419: t15.2023.12.03 val PER: 0.0788
2025-12-16 21:12:44,420: t15.2023.12.08 val PER: 0.0719
2025-12-16 21:12:44,420: t15.2023.12.10 val PER: 0.0854
2025-12-16 21:12:44,420: t15.2023.12.17 val PER: 0.1247
2025-12-16 21:12:44,420: t15.2023.12.29 val PER: 0.1030
2025-12-16 21:12:44,420: t15.2024.02.25 val PER: 0.1025
2025-12-16 21:12:44,420: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:12:44,420: t15.2024.03.08 val PER: 0.2148
2025-12-16 21:12:44,420: t15.2024.03.15 val PER: 0.1864
2025-12-16 21:12:44,420: t15.2024.03.17 val PER: 0.0941
2025-12-16 21:12:44,420: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:12:44,420: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:12:44,420: t15.2024.05.10 val PER: 0.1293
2025-12-16 21:12:44,420: t15.2024.06.14 val PER: 0.1546
2025-12-16 21:12:44,420: t15.2024.07.19 val PER: 0.1753
2025-12-16 21:12:44,420: t15.2024.07.21 val PER: 0.0786
2025-12-16 21:12:44,420: t15.2024.07.28 val PER: 0.1154
2025-12-16 21:12:44,420: t15.2025.01.10 val PER: 0.2741
2025-12-16 21:12:44,420: t15.2025.01.12 val PER: 0.1024
2025-12-16 21:12:44,420: t15.2025.03.14 val PER: 0.2796
2025-12-16 21:12:44,421: t15.2025.03.16 val PER: 0.2134
2025-12-16 21:12:44,421: t15.2025.03.30 val PER: 0.2621
2025-12-16 21:12:44,421: t15.2025.04.13 val PER: 0.1940
2025-12-16 21:13:02,179: Train batch 68200: loss: 0.71 grad norm: 0.45 time: 0.081
2025-12-16 21:13:20,132: Train batch 68400: loss: 0.64 grad norm: 0.57 time: 0.098
2025-12-16 21:13:36,539: Train batch 68600: loss: 0.72 grad norm: 1.36 time: 0.094
2025-12-16 21:13:52,680: Train batch 68800: loss: 0.71 grad norm: 1.34 time: 0.049
2025-12-16 21:14:09,927: Train batch 69000: loss: 0.66 grad norm: 1.06 time: 0.070
2025-12-16 21:14:26,148: Train batch 69200: loss: 0.65 grad norm: 1.32 time: 0.070
2025-12-16 21:14:43,591: Train batch 69400: loss: 0.62 grad norm: 0.43 time: 0.088
2025-12-16 21:15:01,198: Train batch 69600: loss: 0.73 grad norm: 0.34 time: 0.090
2025-12-16 21:15:18,534: Train batch 69800: loss: 0.62 grad norm: 0.69 time: 0.101
2025-12-16 21:15:36,620: Train batch 70000: loss: 0.73 grad norm: 0.79 time: 0.085
2025-12-16 21:15:36,620: Running test after training batch: 70000
2025-12-16 21:15:45,978: Val batch 70000: PER (avg): 0.1308 CTC Loss (avg): 0.8351 time: 9.357
2025-12-16 21:15:45,978: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:15:45,978: t15.2023.08.13 val PER: 0.1040
2025-12-16 21:15:45,978: t15.2023.08.18 val PER: 0.0972
2025-12-16 21:15:45,978: t15.2023.08.20 val PER: 0.0707
2025-12-16 21:15:45,978: t15.2023.08.25 val PER: 0.0919
2025-12-16 21:15:45,978: t15.2023.08.27 val PER: 0.1559
2025-12-16 21:15:45,978: t15.2023.09.01 val PER: 0.0576
2025-12-16 21:15:45,978: t15.2023.09.03 val PER: 0.1235
2025-12-16 21:15:45,979: t15.2023.09.24 val PER: 0.1201
2025-12-16 21:15:45,979: t15.2023.09.29 val PER: 0.1283
2025-12-16 21:15:45,979: t15.2023.10.01 val PER: 0.1618
2025-12-16 21:15:45,979: t15.2023.10.06 val PER: 0.1033
2025-12-16 21:15:45,979: t15.2023.10.08 val PER: 0.2003
2025-12-16 21:15:45,979: t15.2023.10.13 val PER: 0.1908
2025-12-16 21:15:45,979: t15.2023.10.15 val PER: 0.1351
2025-12-16 21:15:45,979: t15.2023.10.20 val PER: 0.2315
2025-12-16 21:15:45,979: t15.2023.10.22 val PER: 0.1258
2025-12-16 21:15:45,979: t15.2023.11.03 val PER: 0.1676
2025-12-16 21:15:45,979: t15.2023.11.04 val PER: 0.0273
2025-12-16 21:15:45,979: t15.2023.11.17 val PER: 0.0575
2025-12-16 21:15:45,979: t15.2023.11.19 val PER: 0.0719
2025-12-16 21:15:45,979: t15.2023.11.26 val PER: 0.1145
2025-12-16 21:15:45,979: t15.2023.12.03 val PER: 0.0767
2025-12-16 21:15:45,979: t15.2023.12.08 val PER: 0.0686
2025-12-16 21:15:45,979: t15.2023.12.10 val PER: 0.0749
2025-12-16 21:15:45,979: t15.2023.12.17 val PER: 0.1299
2025-12-16 21:15:45,980: t15.2023.12.29 val PER: 0.1105
2025-12-16 21:15:45,980: t15.2024.02.25 val PER: 0.0843
2025-12-16 21:15:45,980: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:15:45,980: t15.2024.03.08 val PER: 0.2119
2025-12-16 21:15:45,980: t15.2024.03.15 val PER: 0.1795
2025-12-16 21:15:45,980: t15.2024.03.17 val PER: 0.0893
2025-12-16 21:15:45,980: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:15:45,980: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:15:45,980: t15.2024.05.10 val PER: 0.1397
2025-12-16 21:15:45,980: t15.2024.06.14 val PER: 0.1530
2025-12-16 21:15:45,980: t15.2024.07.19 val PER: 0.1688
2025-12-16 21:15:45,980: t15.2024.07.21 val PER: 0.0683
2025-12-16 21:15:45,980: t15.2024.07.28 val PER: 0.1103
2025-12-16 21:15:45,980: t15.2025.01.10 val PER: 0.2713
2025-12-16 21:15:45,980: t15.2025.01.12 val PER: 0.1085
2025-12-16 21:15:45,980: t15.2025.03.14 val PER: 0.2737
2025-12-16 21:15:45,980: t15.2025.03.16 val PER: 0.1859
2025-12-16 21:15:45,980: t15.2025.03.30 val PER: 0.2540
2025-12-16 21:15:45,981: t15.2025.04.13 val PER: 0.2068
2025-12-16 21:16:03,629: Train batch 70200: loss: 0.68 grad norm: 1.19 time: 0.079
2025-12-16 21:16:21,324: Train batch 70400: loss: 0.78 grad norm: 0.20 time: 0.119
2025-12-16 21:16:38,934: Train batch 70600: loss: 0.71 grad norm: 0.63 time: 0.078
2025-12-16 21:16:55,277: Train batch 70800: loss: 0.72 grad norm: 0.78 time: 0.025
2025-12-16 21:17:11,770: Train batch 71000: loss: 0.65 grad norm: 0.44 time: 0.085
2025-12-16 21:17:29,272: Train batch 71200: loss: 0.66 grad norm: 1.46 time: 0.085
2025-12-16 21:17:47,029: Train batch 71400: loss: 0.74 grad norm: 0.74 time: 0.070
2025-12-16 21:18:04,621: Train batch 71600: loss: 0.74 grad norm: 1.63 time: 0.068
2025-12-16 21:18:21,421: Train batch 71800: loss: 0.71 grad norm: 1.60 time: 0.084
2025-12-16 21:18:38,824: Train batch 72000: loss: 0.68 grad norm: 0.80 time: 0.067
2025-12-16 21:18:38,824: Running test after training batch: 72000
2025-12-16 21:18:48,262: Val batch 72000: PER (avg): 0.1264 CTC Loss (avg): 0.8289 time: 9.438
2025-12-16 21:18:48,262: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:18:48,262: t15.2023.08.13 val PER: 0.1112
2025-12-16 21:18:48,262: t15.2023.08.18 val PER: 0.0947
2025-12-16 21:18:48,262: t15.2023.08.20 val PER: 0.0675
2025-12-16 21:18:48,262: t15.2023.08.25 val PER: 0.0843
2025-12-16 21:18:48,262: t15.2023.08.27 val PER: 0.1495
2025-12-16 21:18:48,262: t15.2023.09.01 val PER: 0.0528
2025-12-16 21:18:48,262: t15.2023.09.03 val PER: 0.1247
2025-12-16 21:18:48,262: t15.2023.09.24 val PER: 0.1032
2025-12-16 21:18:48,262: t15.2023.09.29 val PER: 0.1200
2025-12-16 21:18:48,262: t15.2023.10.01 val PER: 0.1519
2025-12-16 21:18:48,263: t15.2023.10.06 val PER: 0.1098
2025-12-16 21:18:48,263: t15.2023.10.08 val PER: 0.1827
2025-12-16 21:18:48,263: t15.2023.10.13 val PER: 0.1971
2025-12-16 21:18:48,263: t15.2023.10.15 val PER: 0.1285
2025-12-16 21:18:48,263: t15.2023.10.20 val PER: 0.2081
2025-12-16 21:18:48,263: t15.2023.10.22 val PER: 0.1192
2025-12-16 21:18:48,263: t15.2023.11.03 val PER: 0.1703
2025-12-16 21:18:48,263: t15.2023.11.04 val PER: 0.0478
2025-12-16 21:18:48,263: t15.2023.11.17 val PER: 0.0451
2025-12-16 21:18:48,263: t15.2023.11.19 val PER: 0.0679
2025-12-16 21:18:48,263: t15.2023.11.26 val PER: 0.1043
2025-12-16 21:18:48,263: t15.2023.12.03 val PER: 0.0798
2025-12-16 21:18:48,263: t15.2023.12.08 val PER: 0.0712
2025-12-16 21:18:48,263: t15.2023.12.10 val PER: 0.0670
2025-12-16 21:18:48,263: t15.2023.12.17 val PER: 0.1279
2025-12-16 21:18:48,263: t15.2023.12.29 val PER: 0.0879
2025-12-16 21:18:48,263: t15.2024.02.25 val PER: 0.0843
2025-12-16 21:18:48,263: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:18:48,264: t15.2024.03.08 val PER: 0.2020
2025-12-16 21:18:48,264: t15.2024.03.15 val PER: 0.1689
2025-12-16 21:18:48,264: t15.2024.03.17 val PER: 0.0907
2025-12-16 21:18:48,264: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:18:48,264: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:18:48,264: t15.2024.05.10 val PER: 0.1322
2025-12-16 21:18:48,264: t15.2024.06.14 val PER: 0.1625
2025-12-16 21:18:48,264: t15.2024.07.19 val PER: 0.1681
2025-12-16 21:18:48,264: t15.2024.07.21 val PER: 0.0703
2025-12-16 21:18:48,264: t15.2024.07.28 val PER: 0.0956
2025-12-16 21:18:48,264: t15.2025.01.10 val PER: 0.2617
2025-12-16 21:18:48,264: t15.2025.01.12 val PER: 0.1085
2025-12-16 21:18:48,264: t15.2025.03.14 val PER: 0.2811
2025-12-16 21:18:48,264: t15.2025.03.16 val PER: 0.1832
2025-12-16 21:18:48,264: t15.2025.03.30 val PER: 0.2506
2025-12-16 21:18:48,264: t15.2025.04.13 val PER: 0.1869
2025-12-16 21:18:48,264: New best test PER 0.1304 --> 0.1264
2025-12-16 21:18:48,264: Checkpointing model
2025-12-16 21:18:48,736: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:19:05,368: Train batch 72200: loss: 0.76 grad norm: 0.62 time: 0.079
2025-12-16 21:19:20,745: Train batch 72400: loss: 0.75 grad norm: 1.61 time: 0.052
2025-12-16 21:19:37,300: Train batch 72600: loss: 0.66 grad norm: 0.83 time: 0.094
2025-12-16 21:19:51,619: Train batch 72800: loss: 0.71 grad norm: 0.95 time: 0.073
2025-12-16 21:20:07,837: Train batch 73000: loss: 0.72 grad norm: 2.98 time: 0.091
2025-12-16 21:20:24,257: Train batch 73200: loss: 0.59 grad norm: 0.70 time: 0.087
2025-12-16 21:20:41,656: Train batch 73400: loss: 0.73 grad norm: 1.07 time: 0.085
2025-12-16 21:20:58,815: Train batch 73600: loss: 0.64 grad norm: 0.43 time: 0.062
2025-12-16 21:21:15,620: Train batch 73800: loss: 0.71 grad norm: 0.85 time: 0.082
2025-12-16 21:21:31,674: Train batch 74000: loss: 0.79 grad norm: 0.85 time: 0.042
2025-12-16 21:21:31,675: Running test after training batch: 74000
2025-12-16 21:21:41,003: Val batch 74000: PER (avg): 0.1288 CTC Loss (avg): 0.8469 time: 9.328
2025-12-16 21:21:41,004: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:21:41,004: t15.2023.08.13 val PER: 0.1175
2025-12-16 21:21:41,004: t15.2023.08.18 val PER: 0.0922
2025-12-16 21:21:41,004: t15.2023.08.20 val PER: 0.0643
2025-12-16 21:21:41,004: t15.2023.08.25 val PER: 0.0783
2025-12-16 21:21:41,004: t15.2023.08.27 val PER: 0.1640
2025-12-16 21:21:41,004: t15.2023.09.01 val PER: 0.0584
2025-12-16 21:21:41,004: t15.2023.09.03 val PER: 0.1105
2025-12-16 21:21:41,004: t15.2023.09.24 val PER: 0.1007
2025-12-16 21:21:41,004: t15.2023.09.29 val PER: 0.1334
2025-12-16 21:21:41,004: t15.2023.10.01 val PER: 0.1625
2025-12-16 21:21:41,004: t15.2023.10.06 val PER: 0.1044
2025-12-16 21:21:41,004: t15.2023.10.08 val PER: 0.2057
2025-12-16 21:21:41,004: t15.2023.10.13 val PER: 0.1924
2025-12-16 21:21:41,004: t15.2023.10.15 val PER: 0.1397
2025-12-16 21:21:41,005: t15.2023.10.20 val PER: 0.2148
2025-12-16 21:21:41,005: t15.2023.10.22 val PER: 0.1203
2025-12-16 21:21:41,005: t15.2023.11.03 val PER: 0.1642
2025-12-16 21:21:41,005: t15.2023.11.04 val PER: 0.0307
2025-12-16 21:21:41,005: t15.2023.11.17 val PER: 0.0280
2025-12-16 21:21:41,005: t15.2023.11.19 val PER: 0.0619
2025-12-16 21:21:41,005: t15.2023.11.26 val PER: 0.1196
2025-12-16 21:21:41,005: t15.2023.12.03 val PER: 0.0872
2025-12-16 21:21:41,005: t15.2023.12.08 val PER: 0.0639
2025-12-16 21:21:41,005: t15.2023.12.10 val PER: 0.0749
2025-12-16 21:21:41,005: t15.2023.12.17 val PER: 0.1091
2025-12-16 21:21:41,005: t15.2023.12.29 val PER: 0.0872
2025-12-16 21:21:41,005: t15.2024.02.25 val PER: 0.0913
2025-12-16 21:21:41,005: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:21:41,005: t15.2024.03.08 val PER: 0.2077
2025-12-16 21:21:41,005: t15.2024.03.15 val PER: 0.1726
2025-12-16 21:21:41,005: t15.2024.03.17 val PER: 0.0900
2025-12-16 21:21:41,005: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:21:41,005: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:21:41,006: t15.2024.05.10 val PER: 0.1397
2025-12-16 21:21:41,006: t15.2024.06.14 val PER: 0.1703
2025-12-16 21:21:41,006: t15.2024.07.19 val PER: 0.1786
2025-12-16 21:21:41,006: t15.2024.07.21 val PER: 0.0786
2025-12-16 21:21:41,006: t15.2024.07.28 val PER: 0.1088
2025-12-16 21:21:41,006: t15.2025.01.10 val PER: 0.2658
2025-12-16 21:21:41,006: t15.2025.01.12 val PER: 0.1016
2025-12-16 21:21:41,006: t15.2025.03.14 val PER: 0.2692
2025-12-16 21:21:41,006: t15.2025.03.16 val PER: 0.1793
2025-12-16 21:21:41,006: t15.2025.03.30 val PER: 0.2448
2025-12-16 21:21:41,006: t15.2025.04.13 val PER: 0.2140
2025-12-16 21:21:58,168: Train batch 74200: loss: 0.61 grad norm: 0.88 time: 0.096
2025-12-16 21:22:13,673: Train batch 74400: loss: 0.70 grad norm: 1.21 time: 0.065
2025-12-16 21:22:30,666: Train batch 74600: loss: 0.70 grad norm: 0.68 time: 0.083
2025-12-16 21:22:46,962: Train batch 74800: loss: 0.60 grad norm: 1.27 time: 0.084
2025-12-16 21:23:04,622: Train batch 75000: loss: 0.58 grad norm: 2.25 time: 0.085
2025-12-16 21:23:21,477: Train batch 75200: loss: 0.58 grad norm: 1.71 time: 0.085
2025-12-16 21:23:39,345: Train batch 75400: loss: 0.68 grad norm: 1.40 time: 0.090
2025-12-16 21:23:56,197: Train batch 75600: loss: 0.61 grad norm: 1.15 time: 0.078
2025-12-16 21:24:13,847: Train batch 75800: loss: 0.84 grad norm: 1.32 time: 0.055
2025-12-16 21:24:30,838: Train batch 76000: loss: 0.70 grad norm: 0.83 time: 0.082
2025-12-16 21:24:30,838: Running test after training batch: 76000
2025-12-16 21:24:40,118: Val batch 76000: PER (avg): 0.1258 CTC Loss (avg): 0.8418 time: 9.280
2025-12-16 21:24:40,119: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:24:40,119: t15.2023.08.13 val PER: 0.0946
2025-12-16 21:24:40,120: t15.2023.08.18 val PER: 0.0889
2025-12-16 21:24:40,120: t15.2023.08.20 val PER: 0.0643
2025-12-16 21:24:40,120: t15.2023.08.25 val PER: 0.0723
2025-12-16 21:24:40,120: t15.2023.08.27 val PER: 0.1495
2025-12-16 21:24:40,120: t15.2023.09.01 val PER: 0.0528
2025-12-16 21:24:40,120: t15.2023.09.03 val PER: 0.1235
2025-12-16 21:24:40,120: t15.2023.09.24 val PER: 0.0947
2025-12-16 21:24:40,120: t15.2023.09.29 val PER: 0.1200
2025-12-16 21:24:40,120: t15.2023.10.01 val PER: 0.1658
2025-12-16 21:24:40,121: t15.2023.10.06 val PER: 0.1087
2025-12-16 21:24:40,121: t15.2023.10.08 val PER: 0.1935
2025-12-16 21:24:40,121: t15.2023.10.13 val PER: 0.1877
2025-12-16 21:24:40,121: t15.2023.10.15 val PER: 0.1338
2025-12-16 21:24:40,121: t15.2023.10.20 val PER: 0.2416
2025-12-16 21:24:40,121: t15.2023.10.22 val PER: 0.1225
2025-12-16 21:24:40,121: t15.2023.11.03 val PER: 0.1689
2025-12-16 21:24:40,121: t15.2023.11.04 val PER: 0.0273
2025-12-16 21:24:40,121: t15.2023.11.17 val PER: 0.0529
2025-12-16 21:24:40,121: t15.2023.11.19 val PER: 0.0579
2025-12-16 21:24:40,121: t15.2023.11.26 val PER: 0.1196
2025-12-16 21:24:40,121: t15.2023.12.03 val PER: 0.0830
2025-12-16 21:24:40,121: t15.2023.12.08 val PER: 0.0666
2025-12-16 21:24:40,121: t15.2023.12.10 val PER: 0.0723
2025-12-16 21:24:40,121: t15.2023.12.17 val PER: 0.1258
2025-12-16 21:24:40,121: t15.2023.12.29 val PER: 0.0975
2025-12-16 21:24:40,121: t15.2024.02.25 val PER: 0.0871
2025-12-16 21:24:40,122: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:24:40,122: t15.2024.03.08 val PER: 0.2063
2025-12-16 21:24:40,122: t15.2024.03.15 val PER: 0.1664
2025-12-16 21:24:40,122: t15.2024.03.17 val PER: 0.0858
2025-12-16 21:24:40,122: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:24:40,122: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:24:40,122: t15.2024.05.10 val PER: 0.1352
2025-12-16 21:24:40,122: t15.2024.06.14 val PER: 0.1688
2025-12-16 21:24:40,122: t15.2024.07.19 val PER: 0.1674
2025-12-16 21:24:40,122: t15.2024.07.21 val PER: 0.0697
2025-12-16 21:24:40,122: t15.2024.07.28 val PER: 0.1081
2025-12-16 21:24:40,122: t15.2025.01.10 val PER: 0.2479
2025-12-16 21:24:40,122: t15.2025.01.12 val PER: 0.1047
2025-12-16 21:24:40,122: t15.2025.03.14 val PER: 0.2781
2025-12-16 21:24:40,122: t15.2025.03.16 val PER: 0.1610
2025-12-16 21:24:40,122: t15.2025.03.30 val PER: 0.2322
2025-12-16 21:24:40,122: t15.2025.04.13 val PER: 0.1797
2025-12-16 21:24:40,122: New best test PER 0.1264 --> 0.1258
2025-12-16 21:24:40,122: Checkpointing model
2025-12-16 21:24:40,601: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:24:57,854: Train batch 76200: loss: 0.75 grad norm: 0.57 time: 0.080
2025-12-16 21:25:11,426: Train batch 76400: loss: 0.69 grad norm: 1.14 time: 0.050
2025-12-16 21:25:27,673: Train batch 76600: loss: 0.75 grad norm: 1.66 time: 0.067
2025-12-16 21:25:43,244: Train batch 76800: loss: 0.61 grad norm: 1.08 time: 0.085
2025-12-16 21:26:00,494: Train batch 77000: loss: 0.68 grad norm: 0.48 time: 0.077
2025-12-16 21:26:17,227: Train batch 77200: loss: 0.69 grad norm: 1.11 time: 0.070
2025-12-16 21:26:33,964: Train batch 77400: loss: 0.58 grad norm: 0.53 time: 0.077
2025-12-16 21:26:50,489: Train batch 77600: loss: 0.62 grad norm: 1.17 time: 0.053
2025-12-16 21:27:04,172: Train batch 77800: loss: 0.81 grad norm: 0.08 time: 0.066
2025-12-16 21:27:15,245: Train batch 78000: loss: 0.68 grad norm: 1.59 time: 0.030
2025-12-16 21:27:15,246: Running test after training batch: 78000
2025-12-16 21:27:25,149: Val batch 78000: PER (avg): 0.1256 CTC Loss (avg): 0.8561 time: 9.903
2025-12-16 21:27:25,149: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:27:25,149: t15.2023.08.13 val PER: 0.1071
2025-12-16 21:27:25,149: t15.2023.08.18 val PER: 0.0947
2025-12-16 21:27:25,149: t15.2023.08.20 val PER: 0.0699
2025-12-16 21:27:25,149: t15.2023.08.25 val PER: 0.0633
2025-12-16 21:27:25,149: t15.2023.08.27 val PER: 0.1463
2025-12-16 21:27:25,149: t15.2023.09.01 val PER: 0.0503
2025-12-16 21:27:25,149: t15.2023.09.03 val PER: 0.1200
2025-12-16 21:27:25,149: t15.2023.09.24 val PER: 0.0947
2025-12-16 21:27:25,149: t15.2023.09.29 val PER: 0.1283
2025-12-16 21:27:25,150: t15.2023.10.01 val PER: 0.1645
2025-12-16 21:27:25,150: t15.2023.10.06 val PER: 0.0904
2025-12-16 21:27:25,150: t15.2023.10.08 val PER: 0.1881
2025-12-16 21:27:25,150: t15.2023.10.13 val PER: 0.1808
2025-12-16 21:27:25,150: t15.2023.10.15 val PER: 0.1305
2025-12-16 21:27:25,150: t15.2023.10.20 val PER: 0.2181
2025-12-16 21:27:25,150: t15.2023.10.22 val PER: 0.1114
2025-12-16 21:27:25,150: t15.2023.11.03 val PER: 0.1710
2025-12-16 21:27:25,150: t15.2023.11.04 val PER: 0.0341
2025-12-16 21:27:25,150: t15.2023.11.17 val PER: 0.0389
2025-12-16 21:27:25,150: t15.2023.11.19 val PER: 0.0639
2025-12-16 21:27:25,150: t15.2023.11.26 val PER: 0.1254
2025-12-16 21:27:25,150: t15.2023.12.03 val PER: 0.0735
2025-12-16 21:27:25,150: t15.2023.12.08 val PER: 0.0706
2025-12-16 21:27:25,150: t15.2023.12.10 val PER: 0.0644
2025-12-16 21:27:25,150: t15.2023.12.17 val PER: 0.1237
2025-12-16 21:27:25,150: t15.2023.12.29 val PER: 0.0940
2025-12-16 21:27:25,150: t15.2024.02.25 val PER: 0.0857
2025-12-16 21:27:25,150: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:27:25,151: t15.2024.03.08 val PER: 0.1991
2025-12-16 21:27:25,151: t15.2024.03.15 val PER: 0.1795
2025-12-16 21:27:25,151: t15.2024.03.17 val PER: 0.0921
2025-12-16 21:27:25,151: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:27:25,151: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:27:25,151: t15.2024.05.10 val PER: 0.1337
2025-12-16 21:27:25,151: t15.2024.06.14 val PER: 0.1530
2025-12-16 21:27:25,151: t15.2024.07.19 val PER: 0.1575
2025-12-16 21:27:25,151: t15.2024.07.21 val PER: 0.0697
2025-12-16 21:27:25,151: t15.2024.07.28 val PER: 0.1029
2025-12-16 21:27:25,151: t15.2025.01.10 val PER: 0.2493
2025-12-16 21:27:25,151: t15.2025.01.12 val PER: 0.1162
2025-12-16 21:27:25,151: t15.2025.03.14 val PER: 0.2855
2025-12-16 21:27:25,151: t15.2025.03.16 val PER: 0.1715
2025-12-16 21:27:25,151: t15.2025.03.30 val PER: 0.2299
2025-12-16 21:27:25,151: t15.2025.04.13 val PER: 0.1912
2025-12-16 21:27:25,151: New best test PER 0.1258 --> 0.1256
2025-12-16 21:27:25,151: Checkpointing model
2025-12-16 21:27:25,668: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:27:42,619: Train batch 78200: loss: 0.66 grad norm: 3.15 time: 0.083
2025-12-16 21:28:00,217: Train batch 78400: loss: 0.69 grad norm: 1.38 time: 0.088
2025-12-16 21:28:17,303: Train batch 78600: loss: 0.68 grad norm: 0.13 time: 0.088
2025-12-16 21:28:35,110: Train batch 78800: loss: 0.67 grad norm: 2.38 time: 0.059
2025-12-16 21:28:51,597: Train batch 79000: loss: 0.88 grad norm: 1.07 time: 0.045
2025-12-16 21:29:08,907: Train batch 79200: loss: 0.69 grad norm: 0.65 time: 0.068
2025-12-16 21:29:27,156: Train batch 79400: loss: 0.69 grad norm: 0.67 time: 0.087
2025-12-16 21:29:44,733: Train batch 79600: loss: 0.69 grad norm: 1.37 time: 0.082
2025-12-16 21:30:02,441: Train batch 79800: loss: 0.65 grad norm: 1.57 time: 0.074
2025-12-16 21:30:19,416: Train batch 80000: loss: 0.70 grad norm: 1.48 time: 0.091
2025-12-16 21:30:19,416: Running test after training batch: 80000
2025-12-16 21:30:28,960: Val batch 80000: PER (avg): 0.1243 CTC Loss (avg): 0.8403 time: 9.544
2025-12-16 21:30:28,960: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:30:28,960: t15.2023.08.13 val PER: 0.1008
2025-12-16 21:30:28,961: t15.2023.08.18 val PER: 0.0905
2025-12-16 21:30:28,961: t15.2023.08.20 val PER: 0.0675
2025-12-16 21:30:28,961: t15.2023.08.25 val PER: 0.0753
2025-12-16 21:30:28,961: t15.2023.08.27 val PER: 0.1479
2025-12-16 21:30:28,961: t15.2023.09.01 val PER: 0.0584
2025-12-16 21:30:28,961: t15.2023.09.03 val PER: 0.1259
2025-12-16 21:30:28,961: t15.2023.09.24 val PER: 0.1019
2025-12-16 21:30:28,961: t15.2023.09.29 val PER: 0.1225
2025-12-16 21:30:28,961: t15.2023.10.01 val PER: 0.1460
2025-12-16 21:30:28,961: t15.2023.10.06 val PER: 0.0947
2025-12-16 21:30:28,961: t15.2023.10.08 val PER: 0.1922
2025-12-16 21:30:28,961: t15.2023.10.13 val PER: 0.1877
2025-12-16 21:30:28,961: t15.2023.10.15 val PER: 0.1252
2025-12-16 21:30:28,961: t15.2023.10.20 val PER: 0.2181
2025-12-16 21:30:28,961: t15.2023.10.22 val PER: 0.1281
2025-12-16 21:30:28,961: t15.2023.11.03 val PER: 0.1777
2025-12-16 21:30:28,961: t15.2023.11.04 val PER: 0.0341
2025-12-16 21:30:28,961: t15.2023.11.17 val PER: 0.0327
2025-12-16 21:30:28,961: t15.2023.11.19 val PER: 0.0559
2025-12-16 21:30:28,962: t15.2023.11.26 val PER: 0.0906
2025-12-16 21:30:28,962: t15.2023.12.03 val PER: 0.0746
2025-12-16 21:30:28,962: t15.2023.12.08 val PER: 0.0772
2025-12-16 21:30:28,962: t15.2023.12.10 val PER: 0.0670
2025-12-16 21:30:28,962: t15.2023.12.17 val PER: 0.1195
2025-12-16 21:30:28,962: t15.2023.12.29 val PER: 0.0830
2025-12-16 21:30:28,962: t15.2024.02.25 val PER: 0.0857
2025-12-16 21:30:28,962: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:30:28,962: t15.2024.03.08 val PER: 0.1920
2025-12-16 21:30:28,962: t15.2024.03.15 val PER: 0.1720
2025-12-16 21:30:28,962: t15.2024.03.17 val PER: 0.0907
2025-12-16 21:30:28,962: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:30:28,962: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:30:28,962: t15.2024.05.10 val PER: 0.1263
2025-12-16 21:30:28,962: t15.2024.06.14 val PER: 0.1483
2025-12-16 21:30:28,962: t15.2024.07.19 val PER: 0.1655
2025-12-16 21:30:28,962: t15.2024.07.21 val PER: 0.0793
2025-12-16 21:30:28,962: t15.2024.07.28 val PER: 0.1066
2025-12-16 21:30:28,962: t15.2025.01.10 val PER: 0.2562
2025-12-16 21:30:28,963: t15.2025.01.12 val PER: 0.0939
2025-12-16 21:30:28,963: t15.2025.03.14 val PER: 0.2944
2025-12-16 21:30:28,963: t15.2025.03.16 val PER: 0.1741
2025-12-16 21:30:28,963: t15.2025.03.30 val PER: 0.2379
2025-12-16 21:30:28,963: t15.2025.04.13 val PER: 0.1954
2025-12-16 21:30:28,963: New best test PER 0.1256 --> 0.1243
2025-12-16 21:30:28,963: Checkpointing model
2025-12-16 21:30:29,440: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:30:46,680: Train batch 80200: loss: 0.80 grad norm: 0.48 time: 0.090
2025-12-16 21:31:04,674: Train batch 80400: loss: 0.60 grad norm: 0.82 time: 0.062
2025-12-16 21:31:20,532: Train batch 80600: loss: 0.74 grad norm: 1.50 time: 0.092
2025-12-16 21:31:38,075: Train batch 80800: loss: 0.73 grad norm: 1.42 time: 0.088
2025-12-16 21:31:54,800: Train batch 81000: loss: 0.62 grad norm: 0.44 time: 0.073
2025-12-16 21:32:12,703: Train batch 81200: loss: 0.63 grad norm: 0.61 time: 0.082
2025-12-16 21:32:30,258: Train batch 81400: loss: 0.69 grad norm: 1.35 time: 0.089
2025-12-16 21:32:45,562: Train batch 81600: loss: 0.67 grad norm: 0.94 time: 0.087
2025-12-16 21:33:03,297: Train batch 81800: loss: 0.59 grad norm: 2.23 time: 0.082
2025-12-16 21:33:21,145: Train batch 82000: loss: 0.70 grad norm: 0.69 time: 0.092
2025-12-16 21:33:21,146: Running test after training batch: 82000
2025-12-16 21:33:30,680: Val batch 82000: PER (avg): 0.1233 CTC Loss (avg): 0.8535 time: 9.534
2025-12-16 21:33:30,680: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:33:30,680: t15.2023.08.13 val PER: 0.1091
2025-12-16 21:33:30,680: t15.2023.08.18 val PER: 0.0838
2025-12-16 21:33:30,680: t15.2023.08.20 val PER: 0.0643
2025-12-16 21:33:30,681: t15.2023.08.25 val PER: 0.0813
2025-12-16 21:33:30,681: t15.2023.08.27 val PER: 0.1318
2025-12-16 21:33:30,681: t15.2023.09.01 val PER: 0.0593
2025-12-16 21:33:30,681: t15.2023.09.03 val PER: 0.1306
2025-12-16 21:33:30,681: t15.2023.09.24 val PER: 0.1032
2025-12-16 21:33:30,681: t15.2023.09.29 val PER: 0.1174
2025-12-16 21:33:30,681: t15.2023.10.01 val PER: 0.1513
2025-12-16 21:33:30,681: t15.2023.10.06 val PER: 0.0893
2025-12-16 21:33:30,681: t15.2023.10.08 val PER: 0.1894
2025-12-16 21:33:30,681: t15.2023.10.13 val PER: 0.1862
2025-12-16 21:33:30,681: t15.2023.10.15 val PER: 0.1279
2025-12-16 21:33:30,681: t15.2023.10.20 val PER: 0.2181
2025-12-16 21:33:30,681: t15.2023.10.22 val PER: 0.1281
2025-12-16 21:33:30,681: t15.2023.11.03 val PER: 0.1588
2025-12-16 21:33:30,681: t15.2023.11.04 val PER: 0.0444
2025-12-16 21:33:30,681: t15.2023.11.17 val PER: 0.0389
2025-12-16 21:33:30,681: t15.2023.11.19 val PER: 0.0439
2025-12-16 21:33:30,681: t15.2023.11.26 val PER: 0.1130
2025-12-16 21:33:30,681: t15.2023.12.03 val PER: 0.0872
2025-12-16 21:33:30,682: t15.2023.12.08 val PER: 0.0606
2025-12-16 21:33:30,682: t15.2023.12.10 val PER: 0.0644
2025-12-16 21:33:30,682: t15.2023.12.17 val PER: 0.1175
2025-12-16 21:33:30,682: t15.2023.12.29 val PER: 0.0892
2025-12-16 21:33:30,682: t15.2024.02.25 val PER: 0.0730
2025-12-16 21:33:30,682: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:33:30,682: t15.2024.03.08 val PER: 0.1949
2025-12-16 21:33:30,682: t15.2024.03.15 val PER: 0.1707
2025-12-16 21:33:30,682: t15.2024.03.17 val PER: 0.0941
2025-12-16 21:33:30,682: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:33:30,682: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:33:30,682: t15.2024.05.10 val PER: 0.1263
2025-12-16 21:33:30,682: t15.2024.06.14 val PER: 0.1514
2025-12-16 21:33:30,682: t15.2024.07.19 val PER: 0.1595
2025-12-16 21:33:30,682: t15.2024.07.21 val PER: 0.0690
2025-12-16 21:33:30,682: t15.2024.07.28 val PER: 0.1066
2025-12-16 21:33:30,682: t15.2025.01.10 val PER: 0.2810
2025-12-16 21:33:30,682: t15.2025.01.12 val PER: 0.0939
2025-12-16 21:33:30,682: t15.2025.03.14 val PER: 0.2766
2025-12-16 21:33:30,682: t15.2025.03.16 val PER: 0.1715
2025-12-16 21:33:30,683: t15.2025.03.30 val PER: 0.2333
2025-12-16 21:33:30,683: t15.2025.04.13 val PER: 0.1912
2025-12-16 21:33:30,683: New best test PER 0.1243 --> 0.1233
2025-12-16 21:33:30,683: Checkpointing model
2025-12-16 21:33:31,193: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:33:46,872: Train batch 82200: loss: 0.60 grad norm: 1.14 time: 0.066
2025-12-16 21:34:04,837: Train batch 82400: loss: 0.69 grad norm: 0.54 time: 0.072
2025-12-16 21:34:20,572: Train batch 82600: loss: 0.67 grad norm: 1.20 time: 0.090
2025-12-16 21:34:37,105: Train batch 82800: loss: 0.71 grad norm: 0.63 time: 0.057
2025-12-16 21:34:53,957: Train batch 83000: loss: 0.70 grad norm: 0.96 time: 0.065
2025-12-16 21:35:10,607: Train batch 83200: loss: 0.76 grad norm: 0.60 time: 0.101
2025-12-16 21:35:27,968: Train batch 83400: loss: 0.67 grad norm: 0.75 time: 0.080
2025-12-16 21:35:42,319: Train batch 83600: loss: 0.76 grad norm: 0.95 time: 0.049
2025-12-16 21:35:56,660: Train batch 83800: loss: 0.62 grad norm: 0.30 time: 0.076
2025-12-16 21:36:14,061: Train batch 84000: loss: 0.69 grad norm: 0.89 time: 0.079
2025-12-16 21:36:14,061: Running test after training batch: 84000
2025-12-16 21:36:23,504: Val batch 84000: PER (avg): 0.1233 CTC Loss (avg): 0.9015 time: 9.442
2025-12-16 21:36:23,504: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:36:23,504: t15.2023.08.13 val PER: 0.1008
2025-12-16 21:36:23,504: t15.2023.08.18 val PER: 0.0821
2025-12-16 21:36:23,504: t15.2023.08.20 val PER: 0.0651
2025-12-16 21:36:23,504: t15.2023.08.25 val PER: 0.0798
2025-12-16 21:36:23,504: t15.2023.08.27 val PER: 0.1479
2025-12-16 21:36:23,504: t15.2023.09.01 val PER: 0.0560
2025-12-16 21:36:23,504: t15.2023.09.03 val PER: 0.1200
2025-12-16 21:36:23,504: t15.2023.09.24 val PER: 0.0995
2025-12-16 21:36:23,504: t15.2023.09.29 val PER: 0.1174
2025-12-16 21:36:23,505: t15.2023.10.01 val PER: 0.1532
2025-12-16 21:36:23,505: t15.2023.10.06 val PER: 0.0818
2025-12-16 21:36:23,505: t15.2023.10.08 val PER: 0.1908
2025-12-16 21:36:23,505: t15.2023.10.13 val PER: 0.1784
2025-12-16 21:36:23,505: t15.2023.10.15 val PER: 0.1285
2025-12-16 21:36:23,505: t15.2023.10.20 val PER: 0.2114
2025-12-16 21:36:23,505: t15.2023.10.22 val PER: 0.1258
2025-12-16 21:36:23,505: t15.2023.11.03 val PER: 0.1669
2025-12-16 21:36:23,505: t15.2023.11.04 val PER: 0.0410
2025-12-16 21:36:23,505: t15.2023.11.17 val PER: 0.0295
2025-12-16 21:36:23,505: t15.2023.11.19 val PER: 0.0459
2025-12-16 21:36:23,505: t15.2023.11.26 val PER: 0.1116
2025-12-16 21:36:23,505: t15.2023.12.03 val PER: 0.0819
2025-12-16 21:36:23,505: t15.2023.12.08 val PER: 0.0546
2025-12-16 21:36:23,505: t15.2023.12.10 val PER: 0.0604
2025-12-16 21:36:23,505: t15.2023.12.17 val PER: 0.1175
2025-12-16 21:36:23,505: t15.2023.12.29 val PER: 0.0851
2025-12-16 21:36:23,505: t15.2024.02.25 val PER: 0.0885
2025-12-16 21:36:23,505: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:36:23,506: t15.2024.03.08 val PER: 0.1920
2025-12-16 21:36:23,506: t15.2024.03.15 val PER: 0.1807
2025-12-16 21:36:23,506: t15.2024.03.17 val PER: 0.0781
2025-12-16 21:36:23,506: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:36:23,506: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:36:23,506: t15.2024.05.10 val PER: 0.1248
2025-12-16 21:36:23,506: t15.2024.06.14 val PER: 0.1514
2025-12-16 21:36:23,506: t15.2024.07.19 val PER: 0.1721
2025-12-16 21:36:23,506: t15.2024.07.21 val PER: 0.0738
2025-12-16 21:36:23,506: t15.2024.07.28 val PER: 0.1118
2025-12-16 21:36:23,506: t15.2025.01.10 val PER: 0.2782
2025-12-16 21:36:23,506: t15.2025.01.12 val PER: 0.0978
2025-12-16 21:36:23,506: t15.2025.03.14 val PER: 0.2811
2025-12-16 21:36:23,506: t15.2025.03.16 val PER: 0.1846
2025-12-16 21:36:23,506: t15.2025.03.30 val PER: 0.2460
2025-12-16 21:36:23,506: t15.2025.04.13 val PER: 0.1812
2025-12-16 21:36:40,571: Train batch 84200: loss: 0.75 grad norm: 0.95 time: 0.072
2025-12-16 21:36:58,100: Train batch 84400: loss: 0.66 grad norm: 1.10 time: 0.075
2025-12-16 21:37:15,686: Train batch 84600: loss: 0.67 grad norm: 1.01 time: 0.077
2025-12-16 21:37:33,160: Train batch 84800: loss: 0.69 grad norm: 0.63 time: 0.068
2025-12-16 21:37:49,841: Train batch 85000: loss: 0.73 grad norm: 1.63 time: 0.063
2025-12-16 21:38:03,566: Train batch 85200: loss: 0.62 grad norm: 0.69 time: 0.076
2025-12-16 21:38:18,523: Train batch 85400: loss: 0.68 grad norm: 0.84 time: 0.055
2025-12-16 21:38:33,995: Train batch 85600: loss: 0.65 grad norm: 0.79 time: 0.051
2025-12-16 21:38:50,124: Train batch 85800: loss: 0.76 grad norm: 1.60 time: 0.081
2025-12-16 21:39:06,656: Train batch 86000: loss: 0.58 grad norm: 0.82 time: 0.100
2025-12-16 21:39:06,657: Running test after training batch: 86000
2025-12-16 21:39:16,430: Val batch 86000: PER (avg): 0.1197 CTC Loss (avg): 0.8468 time: 9.773
2025-12-16 21:39:16,431: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:39:16,431: t15.2023.08.13 val PER: 0.0915
2025-12-16 21:39:16,431: t15.2023.08.18 val PER: 0.0746
2025-12-16 21:39:16,431: t15.2023.08.20 val PER: 0.0667
2025-12-16 21:39:16,431: t15.2023.08.25 val PER: 0.0949
2025-12-16 21:39:16,431: t15.2023.08.27 val PER: 0.1350
2025-12-16 21:39:16,431: t15.2023.09.01 val PER: 0.0479
2025-12-16 21:39:16,431: t15.2023.09.03 val PER: 0.1128
2025-12-16 21:39:16,431: t15.2023.09.24 val PER: 0.1032
2025-12-16 21:39:16,431: t15.2023.09.29 val PER: 0.1130
2025-12-16 21:39:16,431: t15.2023.10.01 val PER: 0.1552
2025-12-16 21:39:16,431: t15.2023.10.06 val PER: 0.1055
2025-12-16 21:39:16,431: t15.2023.10.08 val PER: 0.1773
2025-12-16 21:39:16,431: t15.2023.10.13 val PER: 0.1769
2025-12-16 21:39:16,432: t15.2023.10.15 val PER: 0.1365
2025-12-16 21:39:16,432: t15.2023.10.20 val PER: 0.2047
2025-12-16 21:39:16,432: t15.2023.10.22 val PER: 0.1225
2025-12-16 21:39:16,432: t15.2023.11.03 val PER: 0.1662
2025-12-16 21:39:16,432: t15.2023.11.04 val PER: 0.0410
2025-12-16 21:39:16,432: t15.2023.11.17 val PER: 0.0171
2025-12-16 21:39:16,432: t15.2023.11.19 val PER: 0.0539
2025-12-16 21:39:16,432: t15.2023.11.26 val PER: 0.0928
2025-12-16 21:39:16,432: t15.2023.12.03 val PER: 0.0809
2025-12-16 21:39:16,432: t15.2023.12.08 val PER: 0.0519
2025-12-16 21:39:16,432: t15.2023.12.10 val PER: 0.0565
2025-12-16 21:39:16,432: t15.2023.12.17 val PER: 0.1133
2025-12-16 21:39:16,432: t15.2023.12.29 val PER: 0.0892
2025-12-16 21:39:16,432: t15.2024.02.25 val PER: 0.0801
2025-12-16 21:39:16,432: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:39:16,432: t15.2024.03.08 val PER: 0.1849
2025-12-16 21:39:16,432: t15.2024.03.15 val PER: 0.1720
2025-12-16 21:39:16,432: t15.2024.03.17 val PER: 0.0830
2025-12-16 21:39:16,433: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:39:16,433: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:39:16,433: t15.2024.05.10 val PER: 0.1278
2025-12-16 21:39:16,433: t15.2024.06.14 val PER: 0.1309
2025-12-16 21:39:16,433: t15.2024.07.19 val PER: 0.1615
2025-12-16 21:39:16,433: t15.2024.07.21 val PER: 0.0752
2025-12-16 21:39:16,433: t15.2024.07.28 val PER: 0.0985
2025-12-16 21:39:16,433: t15.2025.01.10 val PER: 0.2658
2025-12-16 21:39:16,433: t15.2025.01.12 val PER: 0.0978
2025-12-16 21:39:16,433: t15.2025.03.14 val PER: 0.2604
2025-12-16 21:39:16,433: t15.2025.03.16 val PER: 0.1623
2025-12-16 21:39:16,433: t15.2025.03.30 val PER: 0.2402
2025-12-16 21:39:16,433: t15.2025.04.13 val PER: 0.1912
2025-12-16 21:39:16,433: New best test PER 0.1233 --> 0.1197
2025-12-16 21:39:16,433: Checkpointing model
2025-12-16 21:39:16,942: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:39:28,744: Train batch 86200: loss: 0.72 grad norm: 0.38 time: 0.087
2025-12-16 21:39:45,753: Train batch 86400: loss: 0.61 grad norm: 0.09 time: 0.089
2025-12-16 21:40:02,114: Train batch 86600: loss: 0.67 grad norm: 0.27 time: 0.088
2025-12-16 21:40:17,229: Train batch 86800: loss: 0.66 grad norm: 0.48 time: 0.091
2025-12-16 21:40:32,698: Train batch 87000: loss: 0.85 grad norm: 1.04 time: 0.055
2025-12-16 21:40:47,387: Train batch 87200: loss: 0.64 grad norm: 0.65 time: 0.073
2025-12-16 21:40:59,817: Train batch 87400: loss: 0.65 grad norm: 1.03 time: 0.047
2025-12-16 21:41:11,003: Train batch 87600: loss: 0.62 grad norm: 1.09 time: 0.063
2025-12-16 21:41:21,049: Train batch 87800: loss: 0.66 grad norm: 0.64 time: 0.066
2025-12-16 21:41:30,150: Train batch 88000: loss: 0.65 grad norm: 1.20 time: 0.059
2025-12-16 21:41:30,151: Running test after training batch: 88000
2025-12-16 21:41:39,938: Val batch 88000: PER (avg): 0.1196 CTC Loss (avg): 0.8693 time: 9.787
2025-12-16 21:41:39,938: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:41:39,938: t15.2023.08.13 val PER: 0.1019
2025-12-16 21:41:39,938: t15.2023.08.18 val PER: 0.0796
2025-12-16 21:41:39,938: t15.2023.08.20 val PER: 0.0612
2025-12-16 21:41:39,938: t15.2023.08.25 val PER: 0.0648
2025-12-16 21:41:39,938: t15.2023.08.27 val PER: 0.1415
2025-12-16 21:41:39,938: t15.2023.09.01 val PER: 0.0609
2025-12-16 21:41:39,938: t15.2023.09.03 val PER: 0.1140
2025-12-16 21:41:39,938: t15.2023.09.24 val PER: 0.0971
2025-12-16 21:41:39,939: t15.2023.09.29 val PER: 0.1251
2025-12-16 21:41:39,939: t15.2023.10.01 val PER: 0.1539
2025-12-16 21:41:39,939: t15.2023.10.06 val PER: 0.0872
2025-12-16 21:41:39,939: t15.2023.10.08 val PER: 0.1922
2025-12-16 21:41:39,939: t15.2023.10.13 val PER: 0.1777
2025-12-16 21:41:39,939: t15.2023.10.15 val PER: 0.1226
2025-12-16 21:41:39,939: t15.2023.10.20 val PER: 0.1812
2025-12-16 21:41:39,939: t15.2023.10.22 val PER: 0.1114
2025-12-16 21:41:39,939: t15.2023.11.03 val PER: 0.1703
2025-12-16 21:41:39,939: t15.2023.11.04 val PER: 0.0410
2025-12-16 21:41:39,939: t15.2023.11.17 val PER: 0.0311
2025-12-16 21:41:39,939: t15.2023.11.19 val PER: 0.0539
2025-12-16 21:41:39,939: t15.2023.11.26 val PER: 0.0826
2025-12-16 21:41:39,939: t15.2023.12.03 val PER: 0.0788
2025-12-16 21:41:39,939: t15.2023.12.08 val PER: 0.0559
2025-12-16 21:41:39,939: t15.2023.12.10 val PER: 0.0618
2025-12-16 21:41:39,939: t15.2023.12.17 val PER: 0.0998
2025-12-16 21:41:39,939: t15.2023.12.29 val PER: 0.0796
2025-12-16 21:41:39,939: t15.2024.02.25 val PER: 0.0801
2025-12-16 21:41:39,940: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:41:39,940: t15.2024.03.08 val PER: 0.1849
2025-12-16 21:41:39,940: t15.2024.03.15 val PER: 0.1782
2025-12-16 21:41:39,940: t15.2024.03.17 val PER: 0.0795
2025-12-16 21:41:39,940: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:41:39,940: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:41:39,940: t15.2024.05.10 val PER: 0.1278
2025-12-16 21:41:39,940: t15.2024.06.14 val PER: 0.1498
2025-12-16 21:41:39,940: t15.2024.07.19 val PER: 0.1674
2025-12-16 21:41:39,940: t15.2024.07.21 val PER: 0.0710
2025-12-16 21:41:39,940: t15.2024.07.28 val PER: 0.0949
2025-12-16 21:41:39,940: t15.2025.01.10 val PER: 0.2755
2025-12-16 21:41:39,940: t15.2025.01.12 val PER: 0.1078
2025-12-16 21:41:39,940: t15.2025.03.14 val PER: 0.2618
2025-12-16 21:41:39,940: t15.2025.03.16 val PER: 0.1531
2025-12-16 21:41:39,940: t15.2025.03.30 val PER: 0.2425
2025-12-16 21:41:39,940: t15.2025.04.13 val PER: 0.1969
2025-12-16 21:41:39,940: New best test PER 0.1197 --> 0.1196
2025-12-16 21:41:39,940: Checkpointing model
2025-12-16 21:41:40,419: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:41:53,345: Train batch 88200: loss: 0.56 grad norm: 0.23 time: 0.084
2025-12-16 21:42:09,485: Train batch 88400: loss: 0.72 grad norm: 0.43 time: 0.074
2025-12-16 21:42:24,625: Train batch 88600: loss: 0.71 grad norm: 0.70 time: 0.073
2025-12-16 21:42:35,563: Train batch 88800: loss: 0.68 grad norm: 0.91 time: 0.064
2025-12-16 21:42:45,152: Train batch 89000: loss: 0.68 grad norm: 0.18 time: 0.048
2025-12-16 21:42:57,761: Train batch 89200: loss: 0.73 grad norm: 22.46 time: 0.055
2025-12-16 21:43:10,798: Train batch 89400: loss: 0.63 grad norm: 0.28 time: 0.081
2025-12-16 21:43:27,190: Train batch 89600: loss: 0.60 grad norm: 1.14 time: 0.085
2025-12-16 21:43:43,576: Train batch 89800: loss: 0.65 grad norm: 0.75 time: 0.087
2025-12-16 21:43:58,788: Train batch 90000: loss: 0.62 grad norm: 0.74 time: 0.082
2025-12-16 21:43:58,788: Running test after training batch: 90000
2025-12-16 21:44:08,408: Val batch 90000: PER (avg): 0.1187 CTC Loss (avg): 0.8489 time: 9.620
2025-12-16 21:44:08,408: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:44:08,408: t15.2023.08.13 val PER: 0.0988
2025-12-16 21:44:08,408: t15.2023.08.18 val PER: 0.0754
2025-12-16 21:44:08,408: t15.2023.08.20 val PER: 0.0667
2025-12-16 21:44:08,408: t15.2023.08.25 val PER: 0.0723
2025-12-16 21:44:08,408: t15.2023.08.27 val PER: 0.1511
2025-12-16 21:44:08,409: t15.2023.09.01 val PER: 0.0568
2025-12-16 21:44:08,409: t15.2023.09.03 val PER: 0.1140
2025-12-16 21:44:08,409: t15.2023.09.24 val PER: 0.1019
2025-12-16 21:44:08,409: t15.2023.09.29 val PER: 0.1244
2025-12-16 21:44:08,409: t15.2023.10.01 val PER: 0.1433
2025-12-16 21:44:08,409: t15.2023.10.06 val PER: 0.0893
2025-12-16 21:44:08,409: t15.2023.10.08 val PER: 0.1894
2025-12-16 21:44:08,409: t15.2023.10.13 val PER: 0.1777
2025-12-16 21:44:08,409: t15.2023.10.15 val PER: 0.1272
2025-12-16 21:44:08,409: t15.2023.10.20 val PER: 0.1980
2025-12-16 21:44:08,409: t15.2023.10.22 val PER: 0.1114
2025-12-16 21:44:08,409: t15.2023.11.03 val PER: 0.1642
2025-12-16 21:44:08,409: t15.2023.11.04 val PER: 0.0341
2025-12-16 21:44:08,409: t15.2023.11.17 val PER: 0.0327
2025-12-16 21:44:08,409: t15.2023.11.19 val PER: 0.0659
2025-12-16 21:44:08,409: t15.2023.11.26 val PER: 0.1043
2025-12-16 21:44:08,409: t15.2023.12.03 val PER: 0.0788
2025-12-16 21:44:08,409: t15.2023.12.08 val PER: 0.0539
2025-12-16 21:44:08,409: t15.2023.12.10 val PER: 0.0631
2025-12-16 21:44:08,410: t15.2023.12.17 val PER: 0.1112
2025-12-16 21:44:08,410: t15.2023.12.29 val PER: 0.0776
2025-12-16 21:44:08,410: t15.2024.02.25 val PER: 0.0843
2025-12-16 21:44:08,410: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:44:08,410: t15.2024.03.08 val PER: 0.1764
2025-12-16 21:44:08,410: t15.2024.03.15 val PER: 0.1714
2025-12-16 21:44:08,410: t15.2024.03.17 val PER: 0.0788
2025-12-16 21:44:08,410: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:44:08,410: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:44:08,410: t15.2024.05.10 val PER: 0.1233
2025-12-16 21:44:08,410: t15.2024.06.14 val PER: 0.1435
2025-12-16 21:44:08,410: t15.2024.07.19 val PER: 0.1635
2025-12-16 21:44:08,410: t15.2024.07.21 val PER: 0.0634
2025-12-16 21:44:08,410: t15.2024.07.28 val PER: 0.0963
2025-12-16 21:44:08,410: t15.2025.01.10 val PER: 0.2617
2025-12-16 21:44:08,410: t15.2025.01.12 val PER: 0.0970
2025-12-16 21:44:08,410: t15.2025.03.14 val PER: 0.2574
2025-12-16 21:44:08,410: t15.2025.03.16 val PER: 0.1662
2025-12-16 21:44:08,410: t15.2025.03.30 val PER: 0.2368
2025-12-16 21:44:08,411: t15.2025.04.13 val PER: 0.1783
2025-12-16 21:44:08,411: New best test PER 0.1196 --> 0.1187
2025-12-16 21:44:08,411: Checkpointing model
2025-12-16 21:44:08,886: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:44:24,049: Train batch 90200: loss: 0.65 grad norm: 0.98 time: 0.075
2025-12-16 21:44:40,434: Train batch 90400: loss: 0.73 grad norm: 0.76 time: 0.078
2025-12-16 21:44:54,091: Train batch 90600: loss: 0.69 grad norm: 0.51 time: 0.067
2025-12-16 21:45:07,565: Train batch 90800: loss: 0.67 grad norm: 0.36 time: 0.075
2025-12-16 21:45:22,776: Train batch 91000: loss: 0.59 grad norm: 0.43 time: 0.078
2025-12-16 21:45:37,586: Train batch 91200: loss: 0.64 grad norm: 0.12 time: 0.073
2025-12-16 21:45:51,731: Train batch 91400: loss: 0.71 grad norm: 0.11 time: 0.035
2025-12-16 21:46:01,761: Train batch 91600: loss: 0.65 grad norm: 1.38 time: 0.052
2025-12-16 21:46:16,102: Train batch 91800: loss: 0.62 grad norm: 1.40 time: 0.088
2025-12-16 21:46:31,652: Train batch 92000: loss: 0.55 grad norm: 0.09 time: 0.071
2025-12-16 21:46:31,653: Running test after training batch: 92000
2025-12-16 21:46:41,147: Val batch 92000: PER (avg): 0.1181 CTC Loss (avg): 0.8712 time: 9.494
2025-12-16 21:46:41,148: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:46:41,148: t15.2023.08.13 val PER: 0.1019
2025-12-16 21:46:41,148: t15.2023.08.18 val PER: 0.0771
2025-12-16 21:46:41,148: t15.2023.08.20 val PER: 0.0643
2025-12-16 21:46:41,148: t15.2023.08.25 val PER: 0.0904
2025-12-16 21:46:41,148: t15.2023.08.27 val PER: 0.1367
2025-12-16 21:46:41,148: t15.2023.09.01 val PER: 0.0536
2025-12-16 21:46:41,148: t15.2023.09.03 val PER: 0.1045
2025-12-16 21:46:41,148: t15.2023.09.24 val PER: 0.0934
2025-12-16 21:46:41,148: t15.2023.09.29 val PER: 0.1238
2025-12-16 21:46:41,149: t15.2023.10.01 val PER: 0.1539
2025-12-16 21:46:41,149: t15.2023.10.06 val PER: 0.0797
2025-12-16 21:46:41,149: t15.2023.10.08 val PER: 0.2003
2025-12-16 21:46:41,149: t15.2023.10.13 val PER: 0.1870
2025-12-16 21:46:41,149: t15.2023.10.15 val PER: 0.1272
2025-12-16 21:46:41,149: t15.2023.10.20 val PER: 0.1946
2025-12-16 21:46:41,149: t15.2023.10.22 val PER: 0.1136
2025-12-16 21:46:41,149: t15.2023.11.03 val PER: 0.1642
2025-12-16 21:46:41,149: t15.2023.11.04 val PER: 0.0375
2025-12-16 21:46:41,149: t15.2023.11.17 val PER: 0.0264
2025-12-16 21:46:41,149: t15.2023.11.19 val PER: 0.0699
2025-12-16 21:46:41,149: t15.2023.11.26 val PER: 0.0899
2025-12-16 21:46:41,149: t15.2023.12.03 val PER: 0.0641
2025-12-16 21:46:41,149: t15.2023.12.08 val PER: 0.0573
2025-12-16 21:46:41,149: t15.2023.12.10 val PER: 0.0447
2025-12-16 21:46:41,149: t15.2023.12.17 val PER: 0.1143
2025-12-16 21:46:41,149: t15.2023.12.29 val PER: 0.0762
2025-12-16 21:46:41,149: t15.2024.02.25 val PER: 0.0787
2025-12-16 21:46:41,149: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:46:41,150: t15.2024.03.08 val PER: 0.1750
2025-12-16 21:46:41,150: t15.2024.03.15 val PER: 0.1657
2025-12-16 21:46:41,150: t15.2024.03.17 val PER: 0.0823
2025-12-16 21:46:41,150: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:46:41,150: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:46:41,150: t15.2024.05.10 val PER: 0.1308
2025-12-16 21:46:41,150: t15.2024.06.14 val PER: 0.1278
2025-12-16 21:46:41,150: t15.2024.07.19 val PER: 0.1714
2025-12-16 21:46:41,150: t15.2024.07.21 val PER: 0.0600
2025-12-16 21:46:41,150: t15.2024.07.28 val PER: 0.0978
2025-12-16 21:46:41,150: t15.2025.01.10 val PER: 0.2521
2025-12-16 21:46:41,150: t15.2025.01.12 val PER: 0.0947
2025-12-16 21:46:41,150: t15.2025.03.14 val PER: 0.2766
2025-12-16 21:46:41,150: t15.2025.03.16 val PER: 0.1571
2025-12-16 21:46:41,150: t15.2025.03.30 val PER: 0.2299
2025-12-16 21:46:41,150: t15.2025.04.13 val PER: 0.2026
2025-12-16 21:46:41,150: New best test PER 0.1187 --> 0.1181
2025-12-16 21:46:41,150: Checkpointing model
2025-12-16 21:46:41,653: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:46:51,863: Train batch 92200: loss: 0.62 grad norm: 1.31 time: 0.030
2025-12-16 21:47:01,715: Train batch 92400: loss: 0.66 grad norm: 1.41 time: 0.077
2025-12-16 21:47:16,182: Train batch 92600: loss: 0.63 grad norm: 0.45 time: 0.044
2025-12-16 21:47:30,630: Train batch 92800: loss: 0.64 grad norm: 0.66 time: 0.075
2025-12-16 21:47:44,666: Train batch 93000: loss: 0.62 grad norm: 0.62 time: 0.076
2025-12-16 21:48:00,397: Train batch 93200: loss: 0.72 grad norm: 0.83 time: 0.068
2025-12-16 21:48:14,385: Train batch 93400: loss: 0.61 grad norm: 1.36 time: 0.047
2025-12-16 21:48:25,481: Train batch 93600: loss: 0.69 grad norm: 0.13 time: 0.067
2025-12-16 21:48:37,366: Train batch 93800: loss: 0.76 grad norm: 0.24 time: 0.036
2025-12-16 21:48:51,810: Train batch 94000: loss: 0.69 grad norm: 0.44 time: 0.076
2025-12-16 21:48:51,811: Running test after training batch: 94000
2025-12-16 21:49:01,548: Val batch 94000: PER (avg): 0.1191 CTC Loss (avg): 0.8911 time: 9.737
2025-12-16 21:49:01,548: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:49:01,548: t15.2023.08.13 val PER: 0.0967
2025-12-16 21:49:01,549: t15.2023.08.18 val PER: 0.0721
2025-12-16 21:49:01,549: t15.2023.08.20 val PER: 0.0635
2025-12-16 21:49:01,549: t15.2023.08.25 val PER: 0.0708
2025-12-16 21:49:01,549: t15.2023.08.27 val PER: 0.1463
2025-12-16 21:49:01,549: t15.2023.09.01 val PER: 0.0503
2025-12-16 21:49:01,549: t15.2023.09.03 val PER: 0.1116
2025-12-16 21:49:01,549: t15.2023.09.24 val PER: 0.0922
2025-12-16 21:49:01,549: t15.2023.09.29 val PER: 0.1219
2025-12-16 21:49:01,549: t15.2023.10.01 val PER: 0.1519
2025-12-16 21:49:01,549: t15.2023.10.06 val PER: 0.0861
2025-12-16 21:49:01,549: t15.2023.10.08 val PER: 0.1908
2025-12-16 21:49:01,549: t15.2023.10.13 val PER: 0.1862
2025-12-16 21:49:01,549: t15.2023.10.15 val PER: 0.1365
2025-12-16 21:49:01,549: t15.2023.10.20 val PER: 0.2081
2025-12-16 21:49:01,549: t15.2023.10.22 val PER: 0.1236
2025-12-16 21:49:01,549: t15.2023.11.03 val PER: 0.1655
2025-12-16 21:49:01,549: t15.2023.11.04 val PER: 0.0512
2025-12-16 21:49:01,549: t15.2023.11.17 val PER: 0.0264
2025-12-16 21:49:01,549: t15.2023.11.19 val PER: 0.0619
2025-12-16 21:49:01,550: t15.2023.11.26 val PER: 0.1029
2025-12-16 21:49:01,550: t15.2023.12.03 val PER: 0.0672
2025-12-16 21:49:01,550: t15.2023.12.08 val PER: 0.0453
2025-12-16 21:49:01,550: t15.2023.12.10 val PER: 0.0591
2025-12-16 21:49:01,550: t15.2023.12.17 val PER: 0.1071
2025-12-16 21:49:01,550: t15.2023.12.29 val PER: 0.0830
2025-12-16 21:49:01,550: t15.2024.02.25 val PER: 0.0955
2025-12-16 21:49:01,550: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:49:01,550: t15.2024.03.08 val PER: 0.1750
2025-12-16 21:49:01,550: t15.2024.03.15 val PER: 0.1782
2025-12-16 21:49:01,550: t15.2024.03.17 val PER: 0.0697
2025-12-16 21:49:01,550: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:49:01,550: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:49:01,550: t15.2024.05.10 val PER: 0.1278
2025-12-16 21:49:01,550: t15.2024.06.14 val PER: 0.1278
2025-12-16 21:49:01,550: t15.2024.07.19 val PER: 0.1635
2025-12-16 21:49:01,550: t15.2024.07.21 val PER: 0.0669
2025-12-16 21:49:01,550: t15.2024.07.28 val PER: 0.0963
2025-12-16 21:49:01,550: t15.2025.01.10 val PER: 0.2658
2025-12-16 21:49:01,550: t15.2025.01.12 val PER: 0.0847
2025-12-16 21:49:01,551: t15.2025.03.14 val PER: 0.2840
2025-12-16 21:49:01,551: t15.2025.03.16 val PER: 0.1741
2025-12-16 21:49:01,551: t15.2025.03.30 val PER: 0.2471
2025-12-16 21:49:01,551: t15.2025.04.13 val PER: 0.1826
2025-12-16 21:49:15,702: Train batch 94200: loss: 0.62 grad norm: 5.06 time: 0.058
2025-12-16 21:49:28,165: Train batch 94400: loss: 0.73 grad norm: 0.20 time: 0.083
2025-12-16 21:49:42,918: Train batch 94600: loss: 0.60 grad norm: 0.08 time: 0.077
2025-12-16 21:49:59,427: Train batch 94800: loss: 0.80 grad norm: 1.82 time: 0.076
2025-12-16 21:50:14,891: Train batch 95000: loss: 0.70 grad norm: 1.13 time: 0.074
2025-12-16 21:50:31,355: Train batch 95200: loss: 0.66 grad norm: 0.37 time: 0.082
2025-12-16 21:50:47,797: Train batch 95400: loss: 0.68 grad norm: 1.18 time: 0.080
2025-12-16 21:51:03,141: Train batch 95600: loss: 0.65 grad norm: 1.03 time: 0.050
2025-12-16 21:51:18,978: Train batch 95800: loss: 0.70 grad norm: 0.53 time: 0.085
2025-12-16 21:51:34,150: Train batch 96000: loss: 0.71 grad norm: 0.68 time: 0.036
2025-12-16 21:51:34,151: Running test after training batch: 96000
2025-12-16 21:51:43,481: Val batch 96000: PER (avg): 0.1171 CTC Loss (avg): 0.8993 time: 9.330
2025-12-16 21:51:43,481: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:51:43,481: t15.2023.08.13 val PER: 0.1029
2025-12-16 21:51:43,481: t15.2023.08.18 val PER: 0.0729
2025-12-16 21:51:43,481: t15.2023.08.20 val PER: 0.0651
2025-12-16 21:51:43,482: t15.2023.08.25 val PER: 0.0708
2025-12-16 21:51:43,482: t15.2023.08.27 val PER: 0.1431
2025-12-16 21:51:43,482: t15.2023.09.01 val PER: 0.0544
2025-12-16 21:51:43,482: t15.2023.09.03 val PER: 0.1057
2025-12-16 21:51:43,482: t15.2023.09.24 val PER: 0.0947
2025-12-16 21:51:43,482: t15.2023.09.29 val PER: 0.1206
2025-12-16 21:51:43,482: t15.2023.10.01 val PER: 0.1539
2025-12-16 21:51:43,482: t15.2023.10.06 val PER: 0.0861
2025-12-16 21:51:43,482: t15.2023.10.08 val PER: 0.1773
2025-12-16 21:51:43,482: t15.2023.10.13 val PER: 0.1792
2025-12-16 21:51:43,482: t15.2023.10.15 val PER: 0.1252
2025-12-16 21:51:43,482: t15.2023.10.20 val PER: 0.2047
2025-12-16 21:51:43,482: t15.2023.10.22 val PER: 0.1069
2025-12-16 21:51:43,482: t15.2023.11.03 val PER: 0.1588
2025-12-16 21:51:43,482: t15.2023.11.04 val PER: 0.0375
2025-12-16 21:51:43,482: t15.2023.11.17 val PER: 0.0187
2025-12-16 21:51:43,482: t15.2023.11.19 val PER: 0.0619
2025-12-16 21:51:43,482: t15.2023.11.26 val PER: 0.0957
2025-12-16 21:51:43,483: t15.2023.12.03 val PER: 0.0620
2025-12-16 21:51:43,483: t15.2023.12.08 val PER: 0.0433
2025-12-16 21:51:43,483: t15.2023.12.10 val PER: 0.0552
2025-12-16 21:51:43,483: t15.2023.12.17 val PER: 0.0977
2025-12-16 21:51:43,483: t15.2023.12.29 val PER: 0.0789
2025-12-16 21:51:43,483: t15.2024.02.25 val PER: 0.0927
2025-12-16 21:51:43,483: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:51:43,483: t15.2024.03.08 val PER: 0.1863
2025-12-16 21:51:43,483: t15.2024.03.15 val PER: 0.1614
2025-12-16 21:51:43,483: t15.2024.03.17 val PER: 0.0746
2025-12-16 21:51:43,483: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:51:43,483: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:51:43,483: t15.2024.05.10 val PER: 0.1278
2025-12-16 21:51:43,483: t15.2024.06.14 val PER: 0.1372
2025-12-16 21:51:43,483: t15.2024.07.19 val PER: 0.1721
2025-12-16 21:51:43,483: t15.2024.07.21 val PER: 0.0648
2025-12-16 21:51:43,483: t15.2024.07.28 val PER: 0.1081
2025-12-16 21:51:43,483: t15.2025.01.10 val PER: 0.2617
2025-12-16 21:51:43,483: t15.2025.01.12 val PER: 0.0924
2025-12-16 21:51:43,483: t15.2025.03.14 val PER: 0.2781
2025-12-16 21:51:43,484: t15.2025.03.16 val PER: 0.1767
2025-12-16 21:51:43,484: t15.2025.03.30 val PER: 0.2391
2025-12-16 21:51:43,484: t15.2025.04.13 val PER: 0.1769
2025-12-16 21:51:43,484: New best test PER 0.1181 --> 0.1171
2025-12-16 21:51:43,484: Checkpointing model
2025-12-16 21:51:43,968: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:52:01,036: Train batch 96200: loss: 0.69 grad norm: 0.80 time: 0.082
2025-12-16 21:52:18,135: Train batch 96400: loss: 0.66 grad norm: 0.22 time: 0.073
2025-12-16 21:52:33,370: Train batch 96600: loss: 0.62 grad norm: 0.66 time: 0.109
2025-12-16 21:52:49,146: Train batch 96800: loss: 0.59 grad norm: 2.92 time: 0.081
2025-12-16 21:53:01,092: Train batch 97000: loss: 0.72 grad norm: 0.55 time: 0.043
2025-12-16 21:53:14,655: Train batch 97200: loss: 0.78 grad norm: 0.55 time: 0.066
2025-12-16 21:53:30,945: Train batch 97400: loss: 0.74 grad norm: 1.24 time: 0.057
2025-12-16 21:53:46,824: Train batch 97600: loss: 0.61 grad norm: 0.27 time: 0.081
2025-12-16 21:54:03,117: Train batch 97800: loss: 0.68 grad norm: 0.43 time: 0.076
2025-12-16 21:54:17,808: Train batch 98000: loss: 0.69 grad norm: 0.11 time: 0.044
2025-12-16 21:54:17,809: Running test after training batch: 98000
2025-12-16 21:54:27,325: Val batch 98000: PER (avg): 0.1181 CTC Loss (avg): 0.9078 time: 9.516
2025-12-16 21:54:27,325: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:54:27,325: t15.2023.08.13 val PER: 0.0967
2025-12-16 21:54:27,326: t15.2023.08.18 val PER: 0.0838
2025-12-16 21:54:27,326: t15.2023.08.20 val PER: 0.0651
2025-12-16 21:54:27,326: t15.2023.08.25 val PER: 0.0723
2025-12-16 21:54:27,326: t15.2023.08.27 val PER: 0.1334
2025-12-16 21:54:27,326: t15.2023.09.01 val PER: 0.0568
2025-12-16 21:54:27,326: t15.2023.09.03 val PER: 0.1045
2025-12-16 21:54:27,326: t15.2023.09.24 val PER: 0.0971
2025-12-16 21:54:27,326: t15.2023.09.29 val PER: 0.1142
2025-12-16 21:54:27,326: t15.2023.10.01 val PER: 0.1519
2025-12-16 21:54:27,326: t15.2023.10.06 val PER: 0.0786
2025-12-16 21:54:27,326: t15.2023.10.08 val PER: 0.1881
2025-12-16 21:54:27,326: t15.2023.10.13 val PER: 0.1924
2025-12-16 21:54:27,326: t15.2023.10.15 val PER: 0.1266
2025-12-16 21:54:27,326: t15.2023.10.20 val PER: 0.2148
2025-12-16 21:54:27,326: t15.2023.10.22 val PER: 0.1192
2025-12-16 21:54:27,326: t15.2023.11.03 val PER: 0.1594
2025-12-16 21:54:27,326: t15.2023.11.04 val PER: 0.0273
2025-12-16 21:54:27,326: t15.2023.11.17 val PER: 0.0171
2025-12-16 21:54:27,326: t15.2023.11.19 val PER: 0.0739
2025-12-16 21:54:27,327: t15.2023.11.26 val PER: 0.1022
2025-12-16 21:54:27,327: t15.2023.12.03 val PER: 0.0588
2025-12-16 21:54:27,327: t15.2023.12.08 val PER: 0.0506
2025-12-16 21:54:27,327: t15.2023.12.10 val PER: 0.0473
2025-12-16 21:54:27,327: t15.2023.12.17 val PER: 0.1133
2025-12-16 21:54:27,327: t15.2023.12.29 val PER: 0.0865
2025-12-16 21:54:27,327: t15.2024.02.25 val PER: 0.0885
2025-12-16 21:54:27,327: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:54:27,327: t15.2024.03.08 val PER: 0.1807
2025-12-16 21:54:27,327: t15.2024.03.15 val PER: 0.1657
2025-12-16 21:54:27,327: t15.2024.03.17 val PER: 0.0788
2025-12-16 21:54:27,327: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:54:27,327: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:54:27,327: t15.2024.05.10 val PER: 0.1218
2025-12-16 21:54:27,327: t15.2024.06.14 val PER: 0.1325
2025-12-16 21:54:27,327: t15.2024.07.19 val PER: 0.1635
2025-12-16 21:54:27,327: t15.2024.07.21 val PER: 0.0628
2025-12-16 21:54:27,327: t15.2024.07.28 val PER: 0.0978
2025-12-16 21:54:27,327: t15.2025.01.10 val PER: 0.2658
2025-12-16 21:54:27,327: t15.2025.01.12 val PER: 0.0939
2025-12-16 21:54:27,328: t15.2025.03.14 val PER: 0.2825
2025-12-16 21:54:27,328: t15.2025.03.16 val PER: 0.1688
2025-12-16 21:54:27,328: t15.2025.03.30 val PER: 0.2356
2025-12-16 21:54:27,328: t15.2025.04.13 val PER: 0.1869
2025-12-16 21:54:44,567: Train batch 98200: loss: 0.66 grad norm: 0.78 time: 0.078
2025-12-16 21:55:00,683: Train batch 98400: loss: 0.60 grad norm: 0.06 time: 0.079
2025-12-16 21:55:13,425: Train batch 98600: loss: 0.74 grad norm: 1.24 time: 0.055
2025-12-16 21:55:27,335: Train batch 98800: loss: 0.75 grad norm: 1.38 time: 0.075
2025-12-16 21:55:43,856: Train batch 99000: loss: 0.59 grad norm: 0.24 time: 0.084
2025-12-16 21:55:59,271: Train batch 99200: loss: 0.59 grad norm: 0.34 time: 0.097
2025-12-16 21:56:16,730: Train batch 99400: loss: 0.56 grad norm: 0.61 time: 0.085
2025-12-16 21:56:32,552: Train batch 99600: loss: 0.58 grad norm: 0.83 time: 0.077
2025-12-16 21:56:49,594: Train batch 99800: loss: 0.63 grad norm: 0.70 time: 0.080
2025-12-16 21:57:05,222: Train batch 100000: loss: 0.73 grad norm: 2.86 time: 0.082
2025-12-16 21:57:05,222: Running test after training batch: 100000
2025-12-16 21:57:14,676: Val batch 100000: PER (avg): 0.1170 CTC Loss (avg): 0.9240 time: 9.454
2025-12-16 21:57:14,676: t15.2023.08.11 val PER: 1.0000
2025-12-16 21:57:14,676: t15.2023.08.13 val PER: 0.0863
2025-12-16 21:57:14,676: t15.2023.08.18 val PER: 0.0788
2025-12-16 21:57:14,676: t15.2023.08.20 val PER: 0.0635
2025-12-16 21:57:14,676: t15.2023.08.25 val PER: 0.0753
2025-12-16 21:57:14,676: t15.2023.08.27 val PER: 0.1383
2025-12-16 21:57:14,676: t15.2023.09.01 val PER: 0.0519
2025-12-16 21:57:14,677: t15.2023.09.03 val PER: 0.1057
2025-12-16 21:57:14,677: t15.2023.09.24 val PER: 0.1068
2025-12-16 21:57:14,677: t15.2023.09.29 val PER: 0.1206
2025-12-16 21:57:14,677: t15.2023.10.01 val PER: 0.1433
2025-12-16 21:57:14,677: t15.2023.10.06 val PER: 0.0936
2025-12-16 21:57:14,677: t15.2023.10.08 val PER: 0.1989
2025-12-16 21:57:14,677: t15.2023.10.13 val PER: 0.1831
2025-12-16 21:57:14,677: t15.2023.10.15 val PER: 0.1252
2025-12-16 21:57:14,677: t15.2023.10.20 val PER: 0.2013
2025-12-16 21:57:14,677: t15.2023.10.22 val PER: 0.1047
2025-12-16 21:57:14,677: t15.2023.11.03 val PER: 0.1540
2025-12-16 21:57:14,677: t15.2023.11.04 val PER: 0.0273
2025-12-16 21:57:14,677: t15.2023.11.17 val PER: 0.0171
2025-12-16 21:57:14,677: t15.2023.11.19 val PER: 0.0579
2025-12-16 21:57:14,677: t15.2023.11.26 val PER: 0.0768
2025-12-16 21:57:14,677: t15.2023.12.03 val PER: 0.0714
2025-12-16 21:57:14,677: t15.2023.12.08 val PER: 0.0573
2025-12-16 21:57:14,677: t15.2023.12.10 val PER: 0.0618
2025-12-16 21:57:14,677: t15.2023.12.17 val PER: 0.1081
2025-12-16 21:57:14,678: t15.2023.12.29 val PER: 0.0803
2025-12-16 21:57:14,678: t15.2024.02.25 val PER: 0.0955
2025-12-16 21:57:14,678: t15.2024.03.03 val PER: 1.0000
2025-12-16 21:57:14,678: t15.2024.03.08 val PER: 0.1778
2025-12-16 21:57:14,678: t15.2024.03.15 val PER: 0.1588
2025-12-16 21:57:14,678: t15.2024.03.17 val PER: 0.0753
2025-12-16 21:57:14,678: t15.2024.04.25 val PER: 1.0000
2025-12-16 21:57:14,678: t15.2024.04.28 val PER: 1.0000
2025-12-16 21:57:14,678: t15.2024.05.10 val PER: 0.1174
2025-12-16 21:57:14,678: t15.2024.06.14 val PER: 0.1451
2025-12-16 21:57:14,678: t15.2024.07.19 val PER: 0.1727
2025-12-16 21:57:14,678: t15.2024.07.21 val PER: 0.0607
2025-12-16 21:57:14,678: t15.2024.07.28 val PER: 0.1007
2025-12-16 21:57:14,678: t15.2025.01.10 val PER: 0.2658
2025-12-16 21:57:14,678: t15.2025.01.12 val PER: 0.0908
2025-12-16 21:57:14,678: t15.2025.03.14 val PER: 0.2663
2025-12-16 21:57:14,678: t15.2025.03.16 val PER: 0.1728
2025-12-16 21:57:14,678: t15.2025.03.30 val PER: 0.2391
2025-12-16 21:57:14,678: t15.2025.04.13 val PER: 0.2011
2025-12-16 21:57:14,678: New best test PER 0.1171 --> 0.1170
2025-12-16 21:57:14,678: Checkpointing model
2025-12-16 21:57:15,168: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 21:57:32,542: Train batch 100200: loss: 0.73 grad norm: 0.74 time: 0.093
2025-12-16 21:57:47,110: Train batch 100400: loss: 0.67 grad norm: 0.05 time: 0.085
2025-12-16 21:58:03,582: Train batch 100600: loss: 0.72 grad norm: 0.93 time: 0.093
2025-12-16 21:58:20,310: Train batch 100800: loss: 0.67 grad norm: 0.76 time: 0.085
2025-12-16 21:58:36,943: Train batch 101000: loss: 0.72 grad norm: 0.60 time: 0.077
2025-12-16 21:58:54,175: Train batch 101200: loss: 0.74 grad norm: 0.13 time: 0.090
2025-12-16 21:59:09,046: Train batch 101400: loss: 0.62 grad norm: 0.03 time: 0.091
2025-12-16 21:59:26,010: Train batch 101600: loss: 0.73 grad norm: 0.26 time: 0.090
2025-12-16 21:59:42,997: Train batch 101800: loss: 0.67 grad norm: 0.62 time: 0.081
2025-12-16 21:59:57,704: Train batch 102000: loss: 0.61 grad norm: 0.14 time: 0.075
2025-12-16 21:59:57,704: Running test after training batch: 102000
2025-12-16 22:00:07,173: Val batch 102000: PER (avg): 0.1158 CTC Loss (avg): 0.9109 time: 9.469
2025-12-16 22:00:07,174: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:00:07,174: t15.2023.08.13 val PER: 0.0925
2025-12-16 22:00:07,174: t15.2023.08.18 val PER: 0.0654
2025-12-16 22:00:07,174: t15.2023.08.20 val PER: 0.0612
2025-12-16 22:00:07,174: t15.2023.08.25 val PER: 0.0708
2025-12-16 22:00:07,174: t15.2023.08.27 val PER: 0.1318
2025-12-16 22:00:07,174: t15.2023.09.01 val PER: 0.0519
2025-12-16 22:00:07,174: t15.2023.09.03 val PER: 0.1116
2025-12-16 22:00:07,174: t15.2023.09.24 val PER: 0.0922
2025-12-16 22:00:07,174: t15.2023.09.29 val PER: 0.1155
2025-12-16 22:00:07,174: t15.2023.10.01 val PER: 0.1480
2025-12-16 22:00:07,174: t15.2023.10.06 val PER: 0.0883
2025-12-16 22:00:07,174: t15.2023.10.08 val PER: 0.1962
2025-12-16 22:00:07,174: t15.2023.10.13 val PER: 0.1746
2025-12-16 22:00:07,174: t15.2023.10.15 val PER: 0.1332
2025-12-16 22:00:07,174: t15.2023.10.20 val PER: 0.1913
2025-12-16 22:00:07,174: t15.2023.10.22 val PER: 0.1192
2025-12-16 22:00:07,174: t15.2023.11.03 val PER: 0.1574
2025-12-16 22:00:07,175: t15.2023.11.04 val PER: 0.0205
2025-12-16 22:00:07,175: t15.2023.11.17 val PER: 0.0218
2025-12-16 22:00:07,175: t15.2023.11.19 val PER: 0.0479
2025-12-16 22:00:07,175: t15.2023.11.26 val PER: 0.0725
2025-12-16 22:00:07,175: t15.2023.12.03 val PER: 0.0704
2025-12-16 22:00:07,175: t15.2023.12.08 val PER: 0.0519
2025-12-16 22:00:07,175: t15.2023.12.10 val PER: 0.0539
2025-12-16 22:00:07,175: t15.2023.12.17 val PER: 0.1050
2025-12-16 22:00:07,175: t15.2023.12.29 val PER: 0.0762
2025-12-16 22:00:07,175: t15.2024.02.25 val PER: 0.0772
2025-12-16 22:00:07,175: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:00:07,175: t15.2024.03.08 val PER: 0.1906
2025-12-16 22:00:07,175: t15.2024.03.15 val PER: 0.1682
2025-12-16 22:00:07,175: t15.2024.03.17 val PER: 0.0802
2025-12-16 22:00:07,175: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:00:07,175: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:00:07,175: t15.2024.05.10 val PER: 0.1159
2025-12-16 22:00:07,175: t15.2024.06.14 val PER: 0.1404
2025-12-16 22:00:07,175: t15.2024.07.19 val PER: 0.1641
2025-12-16 22:00:07,176: t15.2024.07.21 val PER: 0.0628
2025-12-16 22:00:07,176: t15.2024.07.28 val PER: 0.0978
2025-12-16 22:00:07,176: t15.2025.01.10 val PER: 0.2534
2025-12-16 22:00:07,176: t15.2025.01.12 val PER: 0.0978
2025-12-16 22:00:07,176: t15.2025.03.14 val PER: 0.2914
2025-12-16 22:00:07,176: t15.2025.03.16 val PER: 0.1636
2025-12-16 22:00:07,176: t15.2025.03.30 val PER: 0.2391
2025-12-16 22:00:07,176: t15.2025.04.13 val PER: 0.1954
2025-12-16 22:00:07,176: New best test PER 0.1170 --> 0.1158
2025-12-16 22:00:07,176: Checkpointing model
2025-12-16 22:00:07,653: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 22:00:21,800: Train batch 102200: loss: 0.70 grad norm: 0.09 time: 0.063
2025-12-16 22:00:36,037: Train batch 102400: loss: 0.70 grad norm: 0.59 time: 0.055
2025-12-16 22:00:49,668: Train batch 102600: loss: 0.76 grad norm: 0.90 time: 0.066
2025-12-16 22:01:04,334: Train batch 102800: loss: 0.69 grad norm: 0.23 time: 0.054
2025-12-16 22:01:20,720: Train batch 103000: loss: 0.66 grad norm: 0.63 time: 0.062
2025-12-16 22:01:38,091: Train batch 103200: loss: 0.64 grad norm: 0.77 time: 0.050
2025-12-16 22:01:55,052: Train batch 103400: loss: 0.57 grad norm: 0.79 time: 0.083
2025-12-16 22:02:12,784: Train batch 103600: loss: 0.77 grad norm: 0.19 time: 0.086
2025-12-16 22:02:29,712: Train batch 103800: loss: 0.74 grad norm: 0.23 time: 0.072
2025-12-16 22:02:47,185: Train batch 104000: loss: 0.71 grad norm: 0.31 time: 0.048
2025-12-16 22:02:47,185: Running test after training batch: 104000
2025-12-16 22:02:56,614: Val batch 104000: PER (avg): 0.1156 CTC Loss (avg): 0.9196 time: 9.428
2025-12-16 22:02:56,614: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:02:56,614: t15.2023.08.13 val PER: 0.0873
2025-12-16 22:02:56,614: t15.2023.08.18 val PER: 0.0696
2025-12-16 22:02:56,614: t15.2023.08.20 val PER: 0.0580
2025-12-16 22:02:56,614: t15.2023.08.25 val PER: 0.0783
2025-12-16 22:02:56,614: t15.2023.08.27 val PER: 0.1318
2025-12-16 22:02:56,614: t15.2023.09.01 val PER: 0.0503
2025-12-16 22:02:56,614: t15.2023.09.03 val PER: 0.1069
2025-12-16 22:02:56,614: t15.2023.09.24 val PER: 0.0995
2025-12-16 22:02:56,614: t15.2023.09.29 val PER: 0.1155
2025-12-16 22:02:56,614: t15.2023.10.01 val PER: 0.1420
2025-12-16 22:02:56,614: t15.2023.10.06 val PER: 0.0904
2025-12-16 22:02:56,615: t15.2023.10.08 val PER: 0.1949
2025-12-16 22:02:56,615: t15.2023.10.13 val PER: 0.1691
2025-12-16 22:02:56,615: t15.2023.10.15 val PER: 0.1312
2025-12-16 22:02:56,615: t15.2023.10.20 val PER: 0.1946
2025-12-16 22:02:56,615: t15.2023.10.22 val PER: 0.1180
2025-12-16 22:02:56,615: t15.2023.11.03 val PER: 0.1676
2025-12-16 22:02:56,615: t15.2023.11.04 val PER: 0.0273
2025-12-16 22:02:56,615: t15.2023.11.17 val PER: 0.0202
2025-12-16 22:02:56,615: t15.2023.11.19 val PER: 0.0479
2025-12-16 22:02:56,615: t15.2023.11.26 val PER: 0.0826
2025-12-16 22:02:56,615: t15.2023.12.03 val PER: 0.0662
2025-12-16 22:02:56,615: t15.2023.12.08 val PER: 0.0499
2025-12-16 22:02:56,615: t15.2023.12.10 val PER: 0.0578
2025-12-16 22:02:56,615: t15.2023.12.17 val PER: 0.1143
2025-12-16 22:02:56,615: t15.2023.12.29 val PER: 0.0844
2025-12-16 22:02:56,615: t15.2024.02.25 val PER: 0.0815
2025-12-16 22:02:56,615: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:02:56,615: t15.2024.03.08 val PER: 0.1750
2025-12-16 22:02:56,615: t15.2024.03.15 val PER: 0.1601
2025-12-16 22:02:56,616: t15.2024.03.17 val PER: 0.0718
2025-12-16 22:02:56,616: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:02:56,616: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:02:56,616: t15.2024.05.10 val PER: 0.1293
2025-12-16 22:02:56,616: t15.2024.06.14 val PER: 0.1483
2025-12-16 22:02:56,616: t15.2024.07.19 val PER: 0.1648
2025-12-16 22:02:56,616: t15.2024.07.21 val PER: 0.0683
2025-12-16 22:02:56,616: t15.2024.07.28 val PER: 0.0963
2025-12-16 22:02:56,616: t15.2025.01.10 val PER: 0.2576
2025-12-16 22:02:56,616: t15.2025.01.12 val PER: 0.0885
2025-12-16 22:02:56,616: t15.2025.03.14 val PER: 0.2766
2025-12-16 22:02:56,616: t15.2025.03.16 val PER: 0.1623
2025-12-16 22:02:56,616: t15.2025.03.30 val PER: 0.2345
2025-12-16 22:02:56,616: t15.2025.04.13 val PER: 0.1883
2025-12-16 22:02:56,616: New best test PER 0.1158 --> 0.1156
2025-12-16 22:02:56,616: Checkpointing model
2025-12-16 22:02:57,085: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 22:03:10,500: Train batch 104200: loss: 0.71 grad norm: 0.31 time: 0.066
2025-12-16 22:03:26,029: Train batch 104400: loss: 0.70 grad norm: 4.99 time: 0.064
2025-12-16 22:03:43,207: Train batch 104600: loss: 0.60 grad norm: 0.47 time: 0.089
2025-12-16 22:04:00,945: Train batch 104800: loss: 0.58 grad norm: 0.05 time: 0.082
2025-12-16 22:04:19,218: Train batch 105000: loss: 0.70 grad norm: 0.45 time: 0.087
2025-12-16 22:04:33,856: Train batch 105200: loss: 0.72 grad norm: 0.33 time: 0.024
2025-12-16 22:04:45,056: Train batch 105400: loss: 0.63 grad norm: 1.60 time: 0.045
2025-12-16 22:04:56,855: Train batch 105600: loss: 0.67 grad norm: 0.48 time: 0.077
2025-12-16 22:05:09,682: Train batch 105800: loss: 0.69 grad norm: 2.91 time: 0.080
2025-12-16 22:05:24,846: Train batch 106000: loss: 0.63 grad norm: 0.23 time: 0.049
2025-12-16 22:05:24,847: Running test after training batch: 106000
2025-12-16 22:05:34,341: Val batch 106000: PER (avg): 0.1149 CTC Loss (avg): 0.9440 time: 9.494
2025-12-16 22:05:34,342: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:05:34,342: t15.2023.08.13 val PER: 0.0863
2025-12-16 22:05:34,342: t15.2023.08.18 val PER: 0.0687
2025-12-16 22:05:34,342: t15.2023.08.20 val PER: 0.0580
2025-12-16 22:05:34,342: t15.2023.08.25 val PER: 0.0783
2025-12-16 22:05:34,342: t15.2023.08.27 val PER: 0.1238
2025-12-16 22:05:34,342: t15.2023.09.01 val PER: 0.0536
2025-12-16 22:05:34,342: t15.2023.09.03 val PER: 0.1045
2025-12-16 22:05:34,342: t15.2023.09.24 val PER: 0.1032
2025-12-16 22:05:34,342: t15.2023.09.29 val PER: 0.1117
2025-12-16 22:05:34,342: t15.2023.10.01 val PER: 0.1473
2025-12-16 22:05:34,342: t15.2023.10.06 val PER: 0.0807
2025-12-16 22:05:34,342: t15.2023.10.08 val PER: 0.1894
2025-12-16 22:05:34,342: t15.2023.10.13 val PER: 0.1746
2025-12-16 22:05:34,342: t15.2023.10.15 val PER: 0.1266
2025-12-16 22:05:34,342: t15.2023.10.20 val PER: 0.2081
2025-12-16 22:05:34,342: t15.2023.10.22 val PER: 0.1169
2025-12-16 22:05:34,343: t15.2023.11.03 val PER: 0.1615
2025-12-16 22:05:34,343: t15.2023.11.04 val PER: 0.0307
2025-12-16 22:05:34,343: t15.2023.11.17 val PER: 0.0218
2025-12-16 22:05:34,343: t15.2023.11.19 val PER: 0.0639
2025-12-16 22:05:34,343: t15.2023.11.26 val PER: 0.0870
2025-12-16 22:05:34,343: t15.2023.12.03 val PER: 0.0630
2025-12-16 22:05:34,343: t15.2023.12.08 val PER: 0.0506
2025-12-16 22:05:34,343: t15.2023.12.10 val PER: 0.0434
2025-12-16 22:05:34,343: t15.2023.12.17 val PER: 0.1040
2025-12-16 22:05:34,343: t15.2023.12.29 val PER: 0.0844
2025-12-16 22:05:34,343: t15.2024.02.25 val PER: 0.0885
2025-12-16 22:05:34,343: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:05:34,343: t15.2024.03.08 val PER: 0.1721
2025-12-16 22:05:34,343: t15.2024.03.15 val PER: 0.1714
2025-12-16 22:05:34,343: t15.2024.03.17 val PER: 0.0718
2025-12-16 22:05:34,343: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:05:34,343: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:05:34,343: t15.2024.05.10 val PER: 0.1263
2025-12-16 22:05:34,343: t15.2024.06.14 val PER: 0.1435
2025-12-16 22:05:34,343: t15.2024.07.19 val PER: 0.1549
2025-12-16 22:05:34,344: t15.2024.07.21 val PER: 0.0614
2025-12-16 22:05:34,344: t15.2024.07.28 val PER: 0.0956
2025-12-16 22:05:34,344: t15.2025.01.10 val PER: 0.2479
2025-12-16 22:05:34,344: t15.2025.01.12 val PER: 0.1024
2025-12-16 22:05:34,344: t15.2025.03.14 val PER: 0.2737
2025-12-16 22:05:34,344: t15.2025.03.16 val PER: 0.1597
2025-12-16 22:05:34,344: t15.2025.03.30 val PER: 0.2287
2025-12-16 22:05:34,344: t15.2025.04.13 val PER: 0.1983
2025-12-16 22:05:34,344: New best test PER 0.1156 --> 0.1149
2025-12-16 22:05:34,344: Checkpointing model
2025-12-16 22:05:34,808: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 22:05:50,090: Train batch 106200: loss: 0.63 grad norm: 0.45 time: 0.087
2025-12-16 22:06:06,971: Train batch 106400: loss: 0.59 grad norm: 0.63 time: 0.081
2025-12-16 22:06:24,118: Train batch 106600: loss: 0.70 grad norm: 1.80 time: 0.088
2025-12-16 22:06:41,622: Train batch 106800: loss: 0.70 grad norm: 0.51 time: 0.092
2025-12-16 22:06:58,129: Train batch 107000: loss: 0.73 grad norm: 0.63 time: 0.082
2025-12-16 22:07:16,025: Train batch 107200: loss: 0.70 grad norm: 0.08 time: 0.071
2025-12-16 22:07:33,526: Train batch 107400: loss: 0.70 grad norm: 0.04 time: 0.090
2025-12-16 22:07:51,220: Train batch 107600: loss: 0.66 grad norm: 0.05 time: 0.088
2025-12-16 22:08:09,349: Train batch 107800: loss: 0.70 grad norm: 1.40 time: 0.087
2025-12-16 22:08:27,398: Train batch 108000: loss: 0.73 grad norm: 0.79 time: 0.093
2025-12-16 22:08:27,398: Running test after training batch: 108000
2025-12-16 22:08:37,210: Val batch 108000: PER (avg): 0.1163 CTC Loss (avg): 0.9500 time: 9.811
2025-12-16 22:08:37,210: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:08:37,210: t15.2023.08.13 val PER: 0.0863
2025-12-16 22:08:37,210: t15.2023.08.18 val PER: 0.0780
2025-12-16 22:08:37,210: t15.2023.08.20 val PER: 0.0635
2025-12-16 22:08:37,210: t15.2023.08.25 val PER: 0.0783
2025-12-16 22:08:37,210: t15.2023.08.27 val PER: 0.1399
2025-12-16 22:08:37,211: t15.2023.09.01 val PER: 0.0487
2025-12-16 22:08:37,211: t15.2023.09.03 val PER: 0.0974
2025-12-16 22:08:37,211: t15.2023.09.24 val PER: 0.0983
2025-12-16 22:08:37,211: t15.2023.09.29 val PER: 0.1181
2025-12-16 22:08:37,211: t15.2023.10.01 val PER: 0.1413
2025-12-16 22:08:37,211: t15.2023.10.06 val PER: 0.0861
2025-12-16 22:08:37,211: t15.2023.10.08 val PER: 0.1908
2025-12-16 22:08:37,211: t15.2023.10.13 val PER: 0.1800
2025-12-16 22:08:37,211: t15.2023.10.15 val PER: 0.1318
2025-12-16 22:08:37,211: t15.2023.10.20 val PER: 0.1980
2025-12-16 22:08:37,211: t15.2023.10.22 val PER: 0.1102
2025-12-16 22:08:37,211: t15.2023.11.03 val PER: 0.1554
2025-12-16 22:08:37,211: t15.2023.11.04 val PER: 0.0171
2025-12-16 22:08:37,211: t15.2023.11.17 val PER: 0.0233
2025-12-16 22:08:37,211: t15.2023.11.19 val PER: 0.0459
2025-12-16 22:08:37,211: t15.2023.11.26 val PER: 0.0906
2025-12-16 22:08:37,211: t15.2023.12.03 val PER: 0.0641
2025-12-16 22:08:37,211: t15.2023.12.08 val PER: 0.0579
2025-12-16 22:08:37,211: t15.2023.12.10 val PER: 0.0499
2025-12-16 22:08:37,212: t15.2023.12.17 val PER: 0.1040
2025-12-16 22:08:37,212: t15.2023.12.29 val PER: 0.0830
2025-12-16 22:08:37,212: t15.2024.02.25 val PER: 0.0871
2025-12-16 22:08:37,212: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:08:37,212: t15.2024.03.08 val PER: 0.1935
2025-12-16 22:08:37,212: t15.2024.03.15 val PER: 0.1707
2025-12-16 22:08:37,212: t15.2024.03.17 val PER: 0.0767
2025-12-16 22:08:37,212: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:08:37,212: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:08:37,212: t15.2024.05.10 val PER: 0.1248
2025-12-16 22:08:37,212: t15.2024.06.14 val PER: 0.1435
2025-12-16 22:08:37,212: t15.2024.07.19 val PER: 0.1529
2025-12-16 22:08:37,212: t15.2024.07.21 val PER: 0.0669
2025-12-16 22:08:37,212: t15.2024.07.28 val PER: 0.1044
2025-12-16 22:08:37,212: t15.2025.01.10 val PER: 0.2521
2025-12-16 22:08:37,212: t15.2025.01.12 val PER: 0.1070
2025-12-16 22:08:37,212: t15.2025.03.14 val PER: 0.2618
2025-12-16 22:08:37,212: t15.2025.03.16 val PER: 0.1558
2025-12-16 22:08:37,212: t15.2025.03.30 val PER: 0.2379
2025-12-16 22:08:37,213: t15.2025.04.13 val PER: 0.1854
2025-12-16 22:08:54,460: Train batch 108200: loss: 0.58 grad norm: 0.02 time: 0.082
2025-12-16 22:09:11,013: Train batch 108400: loss: 0.74 grad norm: 0.01 time: 0.079
2025-12-16 22:09:28,380: Train batch 108600: loss: 0.66 grad norm: 0.57 time: 0.072
2025-12-16 22:09:44,872: Train batch 108800: loss: 0.63 grad norm: 0.82 time: 0.085
2025-12-16 22:10:01,022: Train batch 109000: loss: 0.70 grad norm: 0.71 time: 0.090
2025-12-16 22:10:17,898: Train batch 109200: loss: 0.66 grad norm: 1.75 time: 0.088
2025-12-16 22:10:31,944: Train batch 109400: loss: 0.61 grad norm: 0.68 time: 0.084
2025-12-16 22:10:47,421: Train batch 109600: loss: 0.58 grad norm: 0.03 time: 0.068
2025-12-16 22:11:04,315: Train batch 109800: loss: 0.63 grad norm: 0.27 time: 0.096
2025-12-16 22:11:21,460: Train batch 110000: loss: 0.68 grad norm: 0.11 time: 0.082
2025-12-16 22:11:21,461: Running test after training batch: 110000
2025-12-16 22:11:31,283: Val batch 110000: PER (avg): 0.1165 CTC Loss (avg): 0.9457 time: 9.822
2025-12-16 22:11:31,283: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:11:31,283: t15.2023.08.13 val PER: 0.0832
2025-12-16 22:11:31,284: t15.2023.08.18 val PER: 0.0729
2025-12-16 22:11:31,284: t15.2023.08.20 val PER: 0.0691
2025-12-16 22:11:31,284: t15.2023.08.25 val PER: 0.0738
2025-12-16 22:11:31,284: t15.2023.08.27 val PER: 0.1415
2025-12-16 22:11:31,284: t15.2023.09.01 val PER: 0.0503
2025-12-16 22:11:31,284: t15.2023.09.03 val PER: 0.0998
2025-12-16 22:11:31,284: t15.2023.09.24 val PER: 0.0898
2025-12-16 22:11:31,284: t15.2023.09.29 val PER: 0.1142
2025-12-16 22:11:31,284: t15.2023.10.01 val PER: 0.1413
2025-12-16 22:11:31,284: t15.2023.10.06 val PER: 0.0861
2025-12-16 22:11:31,284: t15.2023.10.08 val PER: 0.1976
2025-12-16 22:11:31,284: t15.2023.10.13 val PER: 0.1761
2025-12-16 22:11:31,284: t15.2023.10.15 val PER: 0.1358
2025-12-16 22:11:31,284: t15.2023.10.20 val PER: 0.2081
2025-12-16 22:11:31,284: t15.2023.10.22 val PER: 0.1180
2025-12-16 22:11:31,284: t15.2023.11.03 val PER: 0.1608
2025-12-16 22:11:31,284: t15.2023.11.04 val PER: 0.0239
2025-12-16 22:11:31,284: t15.2023.11.17 val PER: 0.0202
2025-12-16 22:11:31,285: t15.2023.11.19 val PER: 0.0399
2025-12-16 22:11:31,285: t15.2023.11.26 val PER: 0.0971
2025-12-16 22:11:31,285: t15.2023.12.03 val PER: 0.0662
2025-12-16 22:11:31,285: t15.2023.12.08 val PER: 0.0553
2025-12-16 22:11:31,285: t15.2023.12.10 val PER: 0.0473
2025-12-16 22:11:31,285: t15.2023.12.17 val PER: 0.1112
2025-12-16 22:11:31,285: t15.2023.12.29 val PER: 0.0830
2025-12-16 22:11:31,285: t15.2024.02.25 val PER: 0.0857
2025-12-16 22:11:31,285: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:11:31,285: t15.2024.03.08 val PER: 0.1792
2025-12-16 22:11:31,285: t15.2024.03.15 val PER: 0.1664
2025-12-16 22:11:31,285: t15.2024.03.17 val PER: 0.0781
2025-12-16 22:11:31,285: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:11:31,285: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:11:31,285: t15.2024.05.10 val PER: 0.1308
2025-12-16 22:11:31,285: t15.2024.06.14 val PER: 0.1356
2025-12-16 22:11:31,285: t15.2024.07.19 val PER: 0.1668
2025-12-16 22:11:31,285: t15.2024.07.21 val PER: 0.0628
2025-12-16 22:11:31,285: t15.2024.07.28 val PER: 0.0971
2025-12-16 22:11:31,286: t15.2025.01.10 val PER: 0.2658
2025-12-16 22:11:31,286: t15.2025.01.12 val PER: 0.0955
2025-12-16 22:11:31,286: t15.2025.03.14 val PER: 0.2648
2025-12-16 22:11:31,286: t15.2025.03.16 val PER: 0.1584
2025-12-16 22:11:31,286: t15.2025.03.30 val PER: 0.2356
2025-12-16 22:11:31,286: t15.2025.04.13 val PER: 0.1912
2025-12-16 22:11:47,776: Train batch 110200: loss: 0.69 grad norm: 0.61 time: 0.089
2025-12-16 22:12:03,447: Train batch 110400: loss: 0.71 grad norm: 1.09 time: 0.094
2025-12-16 22:12:20,537: Train batch 110600: loss: 0.70 grad norm: 0.79 time: 0.073
2025-12-16 22:12:38,370: Train batch 110800: loss: 0.76 grad norm: 0.84 time: 0.066
2025-12-16 22:12:54,836: Train batch 111000: loss: 0.63 grad norm: 0.10 time: 0.039
2025-12-16 22:13:10,675: Train batch 111200: loss: 0.58 grad norm: 0.99 time: 0.063
2025-12-16 22:13:26,927: Train batch 111400: loss: 0.74 grad norm: 0.78 time: 0.083
2025-12-16 22:13:43,703: Train batch 111600: loss: 0.77 grad norm: 0.49 time: 0.093
2025-12-16 22:14:00,638: Train batch 111800: loss: 0.67 grad norm: 1.17 time: 0.059
2025-12-16 22:14:17,204: Train batch 112000: loss: 0.77 grad norm: 0.21 time: 0.073
2025-12-16 22:14:17,204: Running test after training batch: 112000
2025-12-16 22:14:26,715: Val batch 112000: PER (avg): 0.1165 CTC Loss (avg): 0.9599 time: 9.511
2025-12-16 22:14:26,715: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:14:26,716: t15.2023.08.13 val PER: 0.0967
2025-12-16 22:14:26,716: t15.2023.08.18 val PER: 0.0738
2025-12-16 22:14:26,716: t15.2023.08.20 val PER: 0.0675
2025-12-16 22:14:26,716: t15.2023.08.25 val PER: 0.0768
2025-12-16 22:14:26,716: t15.2023.08.27 val PER: 0.1415
2025-12-16 22:14:26,716: t15.2023.09.01 val PER: 0.0479
2025-12-16 22:14:26,716: t15.2023.09.03 val PER: 0.1164
2025-12-16 22:14:26,716: t15.2023.09.24 val PER: 0.1019
2025-12-16 22:14:26,716: t15.2023.09.29 val PER: 0.1130
2025-12-16 22:14:26,716: t15.2023.10.01 val PER: 0.1433
2025-12-16 22:14:26,716: t15.2023.10.06 val PER: 0.0861
2025-12-16 22:14:26,716: t15.2023.10.08 val PER: 0.1976
2025-12-16 22:14:26,716: t15.2023.10.13 val PER: 0.1746
2025-12-16 22:14:26,716: t15.2023.10.15 val PER: 0.1325
2025-12-16 22:14:26,716: t15.2023.10.20 val PER: 0.1980
2025-12-16 22:14:26,716: t15.2023.10.22 val PER: 0.1080
2025-12-16 22:14:26,716: t15.2023.11.03 val PER: 0.1554
2025-12-16 22:14:26,716: t15.2023.11.04 val PER: 0.0239
2025-12-16 22:14:26,716: t15.2023.11.17 val PER: 0.0264
2025-12-16 22:14:26,717: t15.2023.11.19 val PER: 0.0439
2025-12-16 22:14:26,717: t15.2023.11.26 val PER: 0.1022
2025-12-16 22:14:26,717: t15.2023.12.03 val PER: 0.0620
2025-12-16 22:14:26,717: t15.2023.12.08 val PER: 0.0466
2025-12-16 22:14:26,717: t15.2023.12.10 val PER: 0.0434
2025-12-16 22:14:26,717: t15.2023.12.17 val PER: 0.1081
2025-12-16 22:14:26,717: t15.2023.12.29 val PER: 0.0844
2025-12-16 22:14:26,717: t15.2024.02.25 val PER: 0.0885
2025-12-16 22:14:26,717: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:14:26,717: t15.2024.03.08 val PER: 0.1906
2025-12-16 22:14:26,717: t15.2024.03.15 val PER: 0.1595
2025-12-16 22:14:26,717: t15.2024.03.17 val PER: 0.0879
2025-12-16 22:14:26,717: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:14:26,717: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:14:26,717: t15.2024.05.10 val PER: 0.1233
2025-12-16 22:14:26,717: t15.2024.06.14 val PER: 0.1293
2025-12-16 22:14:26,717: t15.2024.07.19 val PER: 0.1635
2025-12-16 22:14:26,717: t15.2024.07.21 val PER: 0.0648
2025-12-16 22:14:26,717: t15.2024.07.28 val PER: 0.0934
2025-12-16 22:14:26,718: t15.2025.01.10 val PER: 0.2658
2025-12-16 22:14:26,718: t15.2025.01.12 val PER: 0.0985
2025-12-16 22:14:26,718: t15.2025.03.14 val PER: 0.2589
2025-12-16 22:14:26,718: t15.2025.03.16 val PER: 0.1610
2025-12-16 22:14:26,718: t15.2025.03.30 val PER: 0.2299
2025-12-16 22:14:26,718: t15.2025.04.13 val PER: 0.1983
2025-12-16 22:14:40,401: Train batch 112200: loss: 0.75 grad norm: 0.04 time: 0.063
2025-12-16 22:14:56,708: Train batch 112400: loss: 0.70 grad norm: 1.81 time: 0.082
2025-12-16 22:15:10,207: Train batch 112600: loss: 0.66 grad norm: 1.48 time: 0.045
2025-12-16 22:15:19,296: Train batch 112800: loss: 0.68 grad norm: 0.69 time: 0.053
2025-12-16 22:15:35,189: Train batch 113000: loss: 0.60 grad norm: 0.62 time: 0.087
2025-12-16 22:15:50,537: Train batch 113200: loss: 0.61 grad norm: 0.79 time: 0.050
2025-12-16 22:16:05,219: Train batch 113400: loss: 0.65 grad norm: 0.03 time: 0.073
2025-12-16 22:16:19,659: Train batch 113600: loss: 0.65 grad norm: 0.49 time: 0.038
2025-12-16 22:16:31,693: Train batch 113800: loss: 0.69 grad norm: 12.03 time: 0.079
2025-12-16 22:16:47,866: Train batch 114000: loss: 0.62 grad norm: 0.06 time: 0.082
2025-12-16 22:16:47,867: Running test after training batch: 114000
2025-12-16 22:16:57,642: Val batch 114000: PER (avg): 0.1159 CTC Loss (avg): 0.9659 time: 9.776
2025-12-16 22:16:57,643: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:16:57,643: t15.2023.08.13 val PER: 0.0894
2025-12-16 22:16:57,643: t15.2023.08.18 val PER: 0.0780
2025-12-16 22:16:57,643: t15.2023.08.20 val PER: 0.0635
2025-12-16 22:16:57,643: t15.2023.08.25 val PER: 0.0813
2025-12-16 22:16:57,643: t15.2023.08.27 val PER: 0.1463
2025-12-16 22:16:57,643: t15.2023.09.01 val PER: 0.0479
2025-12-16 22:16:57,643: t15.2023.09.03 val PER: 0.1081
2025-12-16 22:16:57,643: t15.2023.09.24 val PER: 0.0934
2025-12-16 22:16:57,643: t15.2023.09.29 val PER: 0.1117
2025-12-16 22:16:57,643: t15.2023.10.01 val PER: 0.1387
2025-12-16 22:16:57,643: t15.2023.10.06 val PER: 0.0829
2025-12-16 22:16:57,643: t15.2023.10.08 val PER: 0.1949
2025-12-16 22:16:57,644: t15.2023.10.13 val PER: 0.1769
2025-12-16 22:16:57,644: t15.2023.10.15 val PER: 0.1345
2025-12-16 22:16:57,644: t15.2023.10.20 val PER: 0.1980
2025-12-16 22:16:57,644: t15.2023.10.22 val PER: 0.1314
2025-12-16 22:16:57,644: t15.2023.11.03 val PER: 0.1588
2025-12-16 22:16:57,644: t15.2023.11.04 val PER: 0.0239
2025-12-16 22:16:57,644: t15.2023.11.17 val PER: 0.0202
2025-12-16 22:16:57,644: t15.2023.11.19 val PER: 0.0539
2025-12-16 22:16:57,644: t15.2023.11.26 val PER: 0.0833
2025-12-16 22:16:57,644: t15.2023.12.03 val PER: 0.0620
2025-12-16 22:16:57,644: t15.2023.12.08 val PER: 0.0453
2025-12-16 22:16:57,644: t15.2023.12.10 val PER: 0.0486
2025-12-16 22:16:57,644: t15.2023.12.17 val PER: 0.1019
2025-12-16 22:16:57,644: t15.2023.12.29 val PER: 0.0803
2025-12-16 22:16:57,644: t15.2024.02.25 val PER: 0.0829
2025-12-16 22:16:57,644: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:16:57,644: t15.2024.03.08 val PER: 0.1920
2025-12-16 22:16:57,644: t15.2024.03.15 val PER: 0.1595
2025-12-16 22:16:57,644: t15.2024.03.17 val PER: 0.0774
2025-12-16 22:16:57,645: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:16:57,645: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:16:57,645: t15.2024.05.10 val PER: 0.1337
2025-12-16 22:16:57,645: t15.2024.06.14 val PER: 0.1325
2025-12-16 22:16:57,645: t15.2024.07.19 val PER: 0.1661
2025-12-16 22:16:57,645: t15.2024.07.21 val PER: 0.0621
2025-12-16 22:16:57,645: t15.2024.07.28 val PER: 0.0978
2025-12-16 22:16:57,645: t15.2025.01.10 val PER: 0.2645
2025-12-16 22:16:57,645: t15.2025.01.12 val PER: 0.1008
2025-12-16 22:16:57,645: t15.2025.03.14 val PER: 0.2633
2025-12-16 22:16:57,645: t15.2025.03.16 val PER: 0.1702
2025-12-16 22:16:57,645: t15.2025.03.30 val PER: 0.2356
2025-12-16 22:16:57,645: t15.2025.04.13 val PER: 0.1883
2025-12-16 22:17:14,590: Train batch 114200: loss: 0.79 grad norm: 0.72 time: 0.081
2025-12-16 22:17:32,615: Train batch 114400: loss: 0.79 grad norm: 0.24 time: 0.088
2025-12-16 22:17:48,250: Train batch 114600: loss: 0.67 grad norm: 1.80 time: 0.071
2025-12-16 22:18:01,254: Train batch 114800: loss: 0.64 grad norm: 1.21 time: 0.065
2025-12-16 22:18:15,606: Train batch 115000: loss: 0.71 grad norm: 0.22 time: 0.083
2025-12-16 22:18:32,327: Train batch 115200: loss: 0.59 grad norm: 0.20 time: 0.079
2025-12-16 22:18:49,440: Train batch 115400: loss: 0.59 grad norm: 0.10 time: 0.089
2025-12-16 22:19:06,178: Train batch 115600: loss: 0.61 grad norm: 9.71 time: 0.074
2025-12-16 22:19:23,322: Train batch 115800: loss: 0.64 grad norm: 0.41 time: 0.080
2025-12-16 22:19:39,939: Train batch 116000: loss: 0.65 grad norm: 0.05 time: 0.086
2025-12-16 22:19:39,940: Running test after training batch: 116000
2025-12-16 22:19:49,489: Val batch 116000: PER (avg): 0.1139 CTC Loss (avg): 0.9738 time: 9.550
2025-12-16 22:19:49,490: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:19:49,490: t15.2023.08.13 val PER: 0.0873
2025-12-16 22:19:49,490: t15.2023.08.18 val PER: 0.0712
2025-12-16 22:19:49,490: t15.2023.08.20 val PER: 0.0627
2025-12-16 22:19:49,490: t15.2023.08.25 val PER: 0.0693
2025-12-16 22:19:49,490: t15.2023.08.27 val PER: 0.1447
2025-12-16 22:19:49,490: t15.2023.09.01 val PER: 0.0471
2025-12-16 22:19:49,490: t15.2023.09.03 val PER: 0.1081
2025-12-16 22:19:49,490: t15.2023.09.24 val PER: 0.0971
2025-12-16 22:19:49,490: t15.2023.09.29 val PER: 0.1117
2025-12-16 22:19:49,490: t15.2023.10.01 val PER: 0.1387
2025-12-16 22:19:49,490: t15.2023.10.06 val PER: 0.0850
2025-12-16 22:19:49,490: t15.2023.10.08 val PER: 0.1881
2025-12-16 22:19:49,490: t15.2023.10.13 val PER: 0.1885
2025-12-16 22:19:49,490: t15.2023.10.15 val PER: 0.1233
2025-12-16 22:19:49,491: t15.2023.10.20 val PER: 0.2148
2025-12-16 22:19:49,491: t15.2023.10.22 val PER: 0.1147
2025-12-16 22:19:49,491: t15.2023.11.03 val PER: 0.1615
2025-12-16 22:19:49,491: t15.2023.11.04 val PER: 0.0273
2025-12-16 22:19:49,491: t15.2023.11.17 val PER: 0.0202
2025-12-16 22:19:49,491: t15.2023.11.19 val PER: 0.0479
2025-12-16 22:19:49,491: t15.2023.11.26 val PER: 0.0891
2025-12-16 22:19:49,491: t15.2023.12.03 val PER: 0.0620
2025-12-16 22:19:49,491: t15.2023.12.08 val PER: 0.0439
2025-12-16 22:19:49,491: t15.2023.12.10 val PER: 0.0473
2025-12-16 22:19:49,491: t15.2023.12.17 val PER: 0.1019
2025-12-16 22:19:49,491: t15.2023.12.29 val PER: 0.0830
2025-12-16 22:19:49,491: t15.2024.02.25 val PER: 0.0787
2025-12-16 22:19:49,491: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:19:49,491: t15.2024.03.08 val PER: 0.1792
2025-12-16 22:19:49,491: t15.2024.03.15 val PER: 0.1545
2025-12-16 22:19:49,491: t15.2024.03.17 val PER: 0.0739
2025-12-16 22:19:49,491: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:19:49,491: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:19:49,492: t15.2024.05.10 val PER: 0.1159
2025-12-16 22:19:49,492: t15.2024.06.14 val PER: 0.1230
2025-12-16 22:19:49,492: t15.2024.07.19 val PER: 0.1628
2025-12-16 22:19:49,492: t15.2024.07.21 val PER: 0.0621
2025-12-16 22:19:49,492: t15.2024.07.28 val PER: 0.0978
2025-12-16 22:19:49,492: t15.2025.01.10 val PER: 0.2658
2025-12-16 22:19:49,492: t15.2025.01.12 val PER: 0.0901
2025-12-16 22:19:49,492: t15.2025.03.14 val PER: 0.2648
2025-12-16 22:19:49,492: t15.2025.03.16 val PER: 0.1649
2025-12-16 22:19:49,492: t15.2025.03.30 val PER: 0.2368
2025-12-16 22:19:49,492: t15.2025.04.13 val PER: 0.1926
2025-12-16 22:19:49,492: New best test PER 0.1149 --> 0.1139
2025-12-16 22:19:49,492: Checkpointing model
2025-12-16 22:19:49,986: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251216_193135/checkpoint/best_checkpoint
2025-12-16 22:20:07,729: Train batch 116200: loss: 0.73 grad norm: 0.01 time: 0.087
2025-12-16 22:20:25,369: Train batch 116400: loss: 0.63 grad norm: 0.73 time: 0.071
2025-12-16 22:20:42,074: Train batch 116600: loss: 0.61 grad norm: 0.71 time: 0.071
2025-12-16 22:20:59,873: Train batch 116800: loss: 0.62 grad norm: 0.75 time: 0.074
2025-12-16 22:21:17,422: Train batch 117000: loss: 0.73 grad norm: 2.99 time: 0.081
2025-12-16 22:21:34,296: Train batch 117200: loss: 0.59 grad norm: 0.67 time: 0.106
2025-12-16 22:21:51,450: Train batch 117400: loss: 0.58 grad norm: 1.43 time: 0.053
2025-12-16 22:22:08,688: Train batch 117600: loss: 0.64 grad norm: 1.29 time: 0.087
2025-12-16 22:22:24,806: Train batch 117800: loss: 0.77 grad norm: 0.11 time: 0.082
2025-12-16 22:22:41,383: Train batch 118000: loss: 0.62 grad norm: 0.15 time: 0.085
2025-12-16 22:22:41,384: Running test after training batch: 118000
2025-12-16 22:22:50,770: Val batch 118000: PER (avg): 0.1141 CTC Loss (avg): 0.9519 time: 9.386
2025-12-16 22:22:50,770: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:22:50,770: t15.2023.08.13 val PER: 0.0800
2025-12-16 22:22:50,770: t15.2023.08.18 val PER: 0.0763
2025-12-16 22:22:50,770: t15.2023.08.20 val PER: 0.0612
2025-12-16 22:22:50,770: t15.2023.08.25 val PER: 0.0889
2025-12-16 22:22:50,770: t15.2023.08.27 val PER: 0.1350
2025-12-16 22:22:50,770: t15.2023.09.01 val PER: 0.0495
2025-12-16 22:22:50,770: t15.2023.09.03 val PER: 0.0998
2025-12-16 22:22:50,771: t15.2023.09.24 val PER: 0.0947
2025-12-16 22:22:50,771: t15.2023.09.29 val PER: 0.1136
2025-12-16 22:22:50,771: t15.2023.10.01 val PER: 0.1433
2025-12-16 22:22:50,771: t15.2023.10.06 val PER: 0.0883
2025-12-16 22:22:50,771: t15.2023.10.08 val PER: 0.1881
2025-12-16 22:22:50,771: t15.2023.10.13 val PER: 0.1784
2025-12-16 22:22:50,771: t15.2023.10.15 val PER: 0.1279
2025-12-16 22:22:50,771: t15.2023.10.20 val PER: 0.2081
2025-12-16 22:22:50,771: t15.2023.10.22 val PER: 0.1158
2025-12-16 22:22:50,771: t15.2023.11.03 val PER: 0.1588
2025-12-16 22:22:50,771: t15.2023.11.04 val PER: 0.0239
2025-12-16 22:22:50,771: t15.2023.11.17 val PER: 0.0233
2025-12-16 22:22:50,771: t15.2023.11.19 val PER: 0.0539
2025-12-16 22:22:50,771: t15.2023.11.26 val PER: 0.0761
2025-12-16 22:22:50,771: t15.2023.12.03 val PER: 0.0609
2025-12-16 22:22:50,771: t15.2023.12.08 val PER: 0.0493
2025-12-16 22:22:50,771: t15.2023.12.10 val PER: 0.0447
2025-12-16 22:22:50,771: t15.2023.12.17 val PER: 0.1060
2025-12-16 22:22:50,772: t15.2023.12.29 val PER: 0.0961
2025-12-16 22:22:50,772: t15.2024.02.25 val PER: 0.0857
2025-12-16 22:22:50,772: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:22:50,772: t15.2024.03.08 val PER: 0.1849
2025-12-16 22:22:50,772: t15.2024.03.15 val PER: 0.1601
2025-12-16 22:22:50,772: t15.2024.03.17 val PER: 0.0697
2025-12-16 22:22:50,772: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:22:50,772: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:22:50,772: t15.2024.05.10 val PER: 0.1114
2025-12-16 22:22:50,772: t15.2024.06.14 val PER: 0.1451
2025-12-16 22:22:50,772: t15.2024.07.19 val PER: 0.1523
2025-12-16 22:22:50,772: t15.2024.07.21 val PER: 0.0634
2025-12-16 22:22:50,772: t15.2024.07.28 val PER: 0.0882
2025-12-16 22:22:50,772: t15.2025.01.10 val PER: 0.2507
2025-12-16 22:22:50,772: t15.2025.01.12 val PER: 0.0924
2025-12-16 22:22:50,772: t15.2025.03.14 val PER: 0.2781
2025-12-16 22:22:50,772: t15.2025.03.16 val PER: 0.1675
2025-12-16 22:22:50,772: t15.2025.03.30 val PER: 0.2333
2025-12-16 22:22:50,772: t15.2025.04.13 val PER: 0.1897
2025-12-16 22:23:07,588: Train batch 118200: loss: 0.68 grad norm: 0.28 time: 0.074
2025-12-16 22:23:24,131: Train batch 118400: loss: 0.64 grad norm: 0.18 time: 0.059
2025-12-16 22:23:41,021: Train batch 118600: loss: 0.72 grad norm: 0.28 time: 0.050
2025-12-16 22:23:57,068: Train batch 118800: loss: 0.79 grad norm: 0.31 time: 0.083
2025-12-16 22:24:13,638: Train batch 119000: loss: 0.55 grad norm: 0.16 time: 0.071
2025-12-16 22:24:29,633: Train batch 119200: loss: 0.67 grad norm: 0.13 time: 0.072
2025-12-16 22:24:46,276: Train batch 119400: loss: 0.61 grad norm: 0.05 time: 0.095
2025-12-16 22:25:01,859: Train batch 119600: loss: 0.61 grad norm: 1.64 time: 0.087
2025-12-16 22:25:18,823: Train batch 119800: loss: 0.64 grad norm: 1.38 time: 0.051
2025-12-16 22:25:35,496: Running test after training batch: 119999
2025-12-16 22:25:44,922: Val batch 119999: PER (avg): 0.1156 CTC Loss (avg): 0.9702 time: 9.425
2025-12-16 22:25:44,922: t15.2023.08.11 val PER: 1.0000
2025-12-16 22:25:44,922: t15.2023.08.13 val PER: 0.0904
2025-12-16 22:25:44,922: t15.2023.08.18 val PER: 0.0746
2025-12-16 22:25:44,922: t15.2023.08.20 val PER: 0.0620
2025-12-16 22:25:44,922: t15.2023.08.25 val PER: 0.0753
2025-12-16 22:25:44,922: t15.2023.08.27 val PER: 0.1350
2025-12-16 22:25:44,922: t15.2023.09.01 val PER: 0.0446
2025-12-16 22:25:44,922: t15.2023.09.03 val PER: 0.1093
2025-12-16 22:25:44,922: t15.2023.09.24 val PER: 0.0862
2025-12-16 22:25:44,922: t15.2023.09.29 val PER: 0.1232
2025-12-16 22:25:44,922: t15.2023.10.01 val PER: 0.1446
2025-12-16 22:25:44,922: t15.2023.10.06 val PER: 0.0807
2025-12-16 22:25:44,922: t15.2023.10.08 val PER: 0.1813
2025-12-16 22:25:44,922: t15.2023.10.13 val PER: 0.1831
2025-12-16 22:25:44,923: t15.2023.10.15 val PER: 0.1239
2025-12-16 22:25:44,923: t15.2023.10.20 val PER: 0.2081
2025-12-16 22:25:44,923: t15.2023.10.22 val PER: 0.1158
2025-12-16 22:25:44,923: t15.2023.11.03 val PER: 0.1601
2025-12-16 22:25:44,923: t15.2023.11.04 val PER: 0.0273
2025-12-16 22:25:44,923: t15.2023.11.17 val PER: 0.0187
2025-12-16 22:25:44,923: t15.2023.11.19 val PER: 0.0599
2025-12-16 22:25:44,923: t15.2023.11.26 val PER: 0.0812
2025-12-16 22:25:44,923: t15.2023.12.03 val PER: 0.0641
2025-12-16 22:25:44,923: t15.2023.12.08 val PER: 0.0553
2025-12-16 22:25:44,923: t15.2023.12.10 val PER: 0.0526
2025-12-16 22:25:44,923: t15.2023.12.17 val PER: 0.1071
2025-12-16 22:25:44,923: t15.2023.12.29 val PER: 0.0796
2025-12-16 22:25:44,923: t15.2024.02.25 val PER: 0.0857
2025-12-16 22:25:44,923: t15.2024.03.03 val PER: 1.0000
2025-12-16 22:25:44,923: t15.2024.03.08 val PER: 0.1821
2025-12-16 22:25:44,923: t15.2024.03.15 val PER: 0.1701
2025-12-16 22:25:44,923: t15.2024.03.17 val PER: 0.0774
2025-12-16 22:25:44,923: t15.2024.04.25 val PER: 1.0000
2025-12-16 22:25:44,924: t15.2024.04.28 val PER: 1.0000
2025-12-16 22:25:44,924: t15.2024.05.10 val PER: 0.1308
2025-12-16 22:25:44,924: t15.2024.06.14 val PER: 0.1435
2025-12-16 22:25:44,924: t15.2024.07.19 val PER: 0.1562
2025-12-16 22:25:44,924: t15.2024.07.21 val PER: 0.0572
2025-12-16 22:25:44,924: t15.2024.07.28 val PER: 0.0963
2025-12-16 22:25:44,924: t15.2025.01.10 val PER: 0.2645
2025-12-16 22:25:44,924: t15.2025.01.12 val PER: 0.0978
2025-12-16 22:25:44,924: t15.2025.03.14 val PER: 0.2722
2025-12-16 22:25:44,924: t15.2025.03.16 val PER: 0.1728
2025-12-16 22:25:44,924: t15.2025.03.30 val PER: 0.2287
2025-12-16 22:25:44,924: t15.2025.04.13 val PER: 0.1854
2025-12-16 22:25:44,979: Best avg val PER achieved: 0.11394
2025-12-16 22:25:44,979: Total training time: 174.00 minutes
