2025-12-10 22:39:15,608: Using device: cuda:0
2025-12-10 22:39:15,610: Initializing RNN with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-10 22:39:16,311: Using torch.compile
2025-12-10 22:39:16,979: Initialized RNN decoding model
2025-12-10 22:39:16,979: OptimizedModule(
  (_orig_mod): UNetEnhancedModel(
    (unet): NeuralUNet(
      (inc): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
      (down1): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down2): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down3): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (down4): Down(
        (maxpool_conv): Sequential(
          (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
          (1): DoubleConv(
            (double_conv): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): ReLU(inplace=True)
              (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (5): ReLU(inplace=True)
            )
          )
        )
      )
      (up1): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up2): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up3): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (up4): Up(
        (up): Upsample(scale_factor=2.0, mode='bilinear')
        (conv): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
      (outc): OutConv(
        (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (decoder): GRUDecoder(
      (day_layer_activation): Softsign()
      (day_weights): ParameterList(
          (0): Parameter containing: [torch.float32 of size 512x512]
          (1): Parameter containing: [torch.float32 of size 512x512]
          (2): Parameter containing: [torch.float32 of size 512x512]
          (3): Parameter containing: [torch.float32 of size 512x512]
          (4): Parameter containing: [torch.float32 of size 512x512]
          (5): Parameter containing: [torch.float32 of size 512x512]
          (6): Parameter containing: [torch.float32 of size 512x512]
          (7): Parameter containing: [torch.float32 of size 512x512]
          (8): Parameter containing: [torch.float32 of size 512x512]
          (9): Parameter containing: [torch.float32 of size 512x512]
          (10): Parameter containing: [torch.float32 of size 512x512]
          (11): Parameter containing: [torch.float32 of size 512x512]
          (12): Parameter containing: [torch.float32 of size 512x512]
          (13): Parameter containing: [torch.float32 of size 512x512]
          (14): Parameter containing: [torch.float32 of size 512x512]
          (15): Parameter containing: [torch.float32 of size 512x512]
          (16): Parameter containing: [torch.float32 of size 512x512]
          (17): Parameter containing: [torch.float32 of size 512x512]
          (18): Parameter containing: [torch.float32 of size 512x512]
          (19): Parameter containing: [torch.float32 of size 512x512]
          (20): Parameter containing: [torch.float32 of size 512x512]
          (21): Parameter containing: [torch.float32 of size 512x512]
          (22): Parameter containing: [torch.float32 of size 512x512]
          (23): Parameter containing: [torch.float32 of size 512x512]
          (24): Parameter containing: [torch.float32 of size 512x512]
          (25): Parameter containing: [torch.float32 of size 512x512]
          (26): Parameter containing: [torch.float32 of size 512x512]
          (27): Parameter containing: [torch.float32 of size 512x512]
          (28): Parameter containing: [torch.float32 of size 512x512]
          (29): Parameter containing: [torch.float32 of size 512x512]
          (30): Parameter containing: [torch.float32 of size 512x512]
          (31): Parameter containing: [torch.float32 of size 512x512]
          (32): Parameter containing: [torch.float32 of size 512x512]
          (33): Parameter containing: [torch.float32 of size 512x512]
          (34): Parameter containing: [torch.float32 of size 512x512]
          (35): Parameter containing: [torch.float32 of size 512x512]
          (36): Parameter containing: [torch.float32 of size 512x512]
          (37): Parameter containing: [torch.float32 of size 512x512]
          (38): Parameter containing: [torch.float32 of size 512x512]
          (39): Parameter containing: [torch.float32 of size 512x512]
          (40): Parameter containing: [torch.float32 of size 512x512]
          (41): Parameter containing: [torch.float32 of size 512x512]
          (42): Parameter containing: [torch.float32 of size 512x512]
          (43): Parameter containing: [torch.float32 of size 512x512]
          (44): Parameter containing: [torch.float32 of size 512x512]
      )
      (day_biases): ParameterList(
          (0): Parameter containing: [torch.float32 of size 1x512]
          (1): Parameter containing: [torch.float32 of size 1x512]
          (2): Parameter containing: [torch.float32 of size 1x512]
          (3): Parameter containing: [torch.float32 of size 1x512]
          (4): Parameter containing: [torch.float32 of size 1x512]
          (5): Parameter containing: [torch.float32 of size 1x512]
          (6): Parameter containing: [torch.float32 of size 1x512]
          (7): Parameter containing: [torch.float32 of size 1x512]
          (8): Parameter containing: [torch.float32 of size 1x512]
          (9): Parameter containing: [torch.float32 of size 1x512]
          (10): Parameter containing: [torch.float32 of size 1x512]
          (11): Parameter containing: [torch.float32 of size 1x512]
          (12): Parameter containing: [torch.float32 of size 1x512]
          (13): Parameter containing: [torch.float32 of size 1x512]
          (14): Parameter containing: [torch.float32 of size 1x512]
          (15): Parameter containing: [torch.float32 of size 1x512]
          (16): Parameter containing: [torch.float32 of size 1x512]
          (17): Parameter containing: [torch.float32 of size 1x512]
          (18): Parameter containing: [torch.float32 of size 1x512]
          (19): Parameter containing: [torch.float32 of size 1x512]
          (20): Parameter containing: [torch.float32 of size 1x512]
          (21): Parameter containing: [torch.float32 of size 1x512]
          (22): Parameter containing: [torch.float32 of size 1x512]
          (23): Parameter containing: [torch.float32 of size 1x512]
          (24): Parameter containing: [torch.float32 of size 1x512]
          (25): Parameter containing: [torch.float32 of size 1x512]
          (26): Parameter containing: [torch.float32 of size 1x512]
          (27): Parameter containing: [torch.float32 of size 1x512]
          (28): Parameter containing: [torch.float32 of size 1x512]
          (29): Parameter containing: [torch.float32 of size 1x512]
          (30): Parameter containing: [torch.float32 of size 1x512]
          (31): Parameter containing: [torch.float32 of size 1x512]
          (32): Parameter containing: [torch.float32 of size 1x512]
          (33): Parameter containing: [torch.float32 of size 1x512]
          (34): Parameter containing: [torch.float32 of size 1x512]
          (35): Parameter containing: [torch.float32 of size 1x512]
          (36): Parameter containing: [torch.float32 of size 1x512]
          (37): Parameter containing: [torch.float32 of size 1x512]
          (38): Parameter containing: [torch.float32 of size 1x512]
          (39): Parameter containing: [torch.float32 of size 1x512]
          (40): Parameter containing: [torch.float32 of size 1x512]
          (41): Parameter containing: [torch.float32 of size 1x512]
          (42): Parameter containing: [torch.float32 of size 1x512]
          (43): Parameter containing: [torch.float32 of size 1x512]
          (44): Parameter containing: [torch.float32 of size 1x512]
      )
      (day_layer_dropout): Dropout(p=0.4, inplace=False)
      (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
      (out): Linear(in_features=768, out_features=41, bias=True)
    )
  )
)
2025-12-10 22:39:16,982: Model has 61,577,002 parameters
2025-12-10 22:39:16,982: Model has 11,819,520 day-specific parameters | 19.19% of total parameters
2025-12-10 22:39:25,455: Successfully initialized datasets
2025-12-10 22:39:45,617: Train batch 0: loss: 656.07 grad norm: 264.74 time: 19.538
2025-12-10 22:39:45,617: Running test after training batch: 0
2025-12-10 22:43:04,208: Val batch 0: PER (avg): 1.1535 CTC Loss (avg): 726.9686 time: 198.591
2025-12-10 22:43:04,208: t15.2023.08.11 val PER: 1.0000
2025-12-10 22:43:04,208: t15.2023.08.13 val PER: 1.0135
2025-12-10 22:43:04,208: t15.2023.08.18 val PER: 1.0796
2025-12-10 22:43:04,209: t15.2023.08.20 val PER: 1.0691
2025-12-10 22:43:04,209: t15.2023.08.25 val PER: 1.1054
2025-12-10 22:43:04,209: t15.2023.08.27 val PER: 0.9791
2025-12-10 22:43:04,209: t15.2023.09.01 val PER: 1.1437
2025-12-10 22:43:04,209: t15.2023.09.03 val PER: 1.1045
2025-12-10 22:43:04,209: t15.2023.09.24 val PER: 1.2257
2025-12-10 22:43:04,209: t15.2023.09.29 val PER: 1.1749
2025-12-10 22:43:04,209: t15.2023.10.01 val PER: 1.0211
2025-12-10 22:43:04,209: t15.2023.10.06 val PER: 1.2573
2025-12-10 22:43:04,209: t15.2023.10.08 val PER: 0.9797
2025-12-10 22:43:04,209: t15.2023.10.13 val PER: 1.1412
2025-12-10 22:43:04,209: t15.2023.10.15 val PER: 1.1569
2025-12-10 22:43:04,209: t15.2023.10.20 val PER: 1.2383
2025-12-10 22:43:04,209: t15.2023.10.22 val PER: 1.0935
2025-12-10 22:43:04,209: t15.2023.11.03 val PER: 1.2402
2025-12-10 22:43:04,209: t15.2023.11.04 val PER: 1.3686
2025-12-10 22:43:04,209: t15.2023.11.17 val PER: 1.5272
2025-12-10 22:43:04,209: t15.2023.11.19 val PER: 1.3613
2025-12-10 22:43:04,210: t15.2023.11.26 val PER: 1.2123
2025-12-10 22:43:04,210: t15.2023.12.03 val PER: 1.1807
2025-12-10 22:43:04,210: t15.2023.12.08 val PER: 1.2383
2025-12-10 22:43:04,210: t15.2023.12.10 val PER: 1.2707
2025-12-10 22:43:04,210: t15.2023.12.17 val PER: 1.0447
2025-12-10 22:43:04,210: t15.2023.12.29 val PER: 1.1112
2025-12-10 22:43:04,210: t15.2024.02.25 val PER: 1.2191
2025-12-10 22:43:04,210: t15.2024.03.03 val PER: 1.0000
2025-12-10 22:43:04,210: t15.2024.03.08 val PER: 1.1664
2025-12-10 22:43:04,210: t15.2024.03.15 val PER: 1.1026
2025-12-10 22:43:04,210: t15.2024.03.17 val PER: 1.1269
2025-12-10 22:43:04,210: t15.2024.04.25 val PER: 1.0000
2025-12-10 22:43:04,210: t15.2024.04.28 val PER: 1.0000
2025-12-10 22:43:04,210: t15.2024.05.10 val PER: 1.1337
2025-12-10 22:43:04,210: t15.2024.06.14 val PER: 1.2413
2025-12-10 22:43:04,210: t15.2024.07.19 val PER: 0.9499
2025-12-10 22:43:04,210: t15.2024.07.21 val PER: 1.2117
2025-12-10 22:43:04,210: t15.2024.07.28 val PER: 1.2500
2025-12-10 22:43:04,210: t15.2025.01.10 val PER: 0.9752
2025-12-10 22:43:04,211: t15.2025.01.12 val PER: 1.4234
2025-12-10 22:43:04,211: t15.2025.03.14 val PER: 0.9453
2025-12-10 22:43:04,211: t15.2025.03.16 val PER: 1.2435
2025-12-10 22:43:04,211: t15.2025.03.30 val PER: 1.1379
2025-12-10 22:43:04,211: t15.2025.04.13 val PER: 1.1355
2025-12-10 22:43:04,211: New best test PER inf --> 1.1535
2025-12-10 22:43:04,211: Checkpointing model
2025-12-10 22:43:05,211: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 22:52:58,367: Train batch 200: loss: 81.18 grad norm: 72.06 time: 0.277
2025-12-10 22:54:24,072: Train batch 400: loss: 74.53 grad norm: 32.55 time: 0.272
2025-12-10 22:55:27,343: Train batch 600: loss: 69.12 grad norm: 57.24 time: 0.238
2025-12-10 22:56:30,153: Train batch 800: loss: 64.35 grad norm: 50.43 time: 0.312
2025-12-10 22:57:34,300: Train batch 1000: loss: 53.98 grad norm: 61.70 time: 0.207
2025-12-10 22:58:37,468: Train batch 1200: loss: 69.19 grad norm: 81.44 time: 0.381
2025-12-10 22:59:38,212: Train batch 1400: loss: 59.06 grad norm: 79.48 time: 0.258
2025-12-10 23:00:39,653: Train batch 1600: loss: 46.17 grad norm: 67.19 time: 0.280
2025-12-10 23:01:42,000: Train batch 1800: loss: 62.49 grad norm: 80.53 time: 0.334
2025-12-10 23:02:43,875: Train batch 2000: loss: 63.80 grad norm: 84.66 time: 0.388
2025-12-10 23:02:43,876: Running test after training batch: 2000
2025-12-10 23:03:09,081: Val batch 2000: PER (avg): 0.5722 CTC Loss (avg): 68.2779 time: 25.205
2025-12-10 23:03:09,082: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:03:09,082: t15.2023.08.13 val PER: 0.5551
2025-12-10 23:03:09,082: t15.2023.08.18 val PER: 0.5583
2025-12-10 23:03:09,082: t15.2023.08.20 val PER: 0.5481
2025-12-10 23:03:09,082: t15.2023.08.25 val PER: 0.5301
2025-12-10 23:03:09,082: t15.2023.08.27 val PER: 0.5772
2025-12-10 23:03:09,082: t15.2023.09.01 val PER: 0.5081
2025-12-10 23:03:09,082: t15.2023.09.03 val PER: 0.5428
2025-12-10 23:03:09,082: t15.2023.09.24 val PER: 0.5182
2025-12-10 23:03:09,082: t15.2023.09.29 val PER: 0.5412
2025-12-10 23:03:09,082: t15.2023.10.01 val PER: 0.5978
2025-12-10 23:03:09,082: t15.2023.10.06 val PER: 0.5490
2025-12-10 23:03:09,082: t15.2023.10.08 val PER: 0.5995
2025-12-10 23:03:09,082: t15.2023.10.13 val PER: 0.5912
2025-12-10 23:03:09,082: t15.2023.10.15 val PER: 0.5432
2025-12-10 23:03:09,082: t15.2023.10.20 val PER: 0.5638
2025-12-10 23:03:09,082: t15.2023.10.22 val PER: 0.5457
2025-12-10 23:03:09,082: t15.2023.11.03 val PER: 0.5550
2025-12-10 23:03:09,083: t15.2023.11.04 val PER: 0.3652
2025-12-10 23:03:09,083: t15.2023.11.17 val PER: 0.3919
2025-12-10 23:03:09,083: t15.2023.11.19 val PER: 0.4152
2025-12-10 23:03:09,083: t15.2023.11.26 val PER: 0.5739
2025-12-10 23:03:09,083: t15.2023.12.03 val PER: 0.5410
2025-12-10 23:03:09,083: t15.2023.12.08 val PER: 0.5459
2025-12-10 23:03:09,083: t15.2023.12.10 val PER: 0.5138
2025-12-10 23:03:09,083: t15.2023.12.17 val PER: 0.6185
2025-12-10 23:03:09,083: t15.2023.12.29 val PER: 0.6067
2025-12-10 23:03:09,083: t15.2024.02.25 val PER: 0.5506
2025-12-10 23:03:09,083: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:03:09,083: t15.2024.03.08 val PER: 0.6046
2025-12-10 23:03:09,083: t15.2024.03.15 val PER: 0.6041
2025-12-10 23:03:09,083: t15.2024.03.17 val PER: 0.5802
2025-12-10 23:03:09,083: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:03:09,083: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:03:09,083: t15.2024.05.10 val PER: 0.6241
2025-12-10 23:03:09,083: t15.2024.06.14 val PER: 0.5489
2025-12-10 23:03:09,084: t15.2024.07.19 val PER: 0.7139
2025-12-10 23:03:09,084: t15.2024.07.21 val PER: 0.5131
2025-12-10 23:03:09,084: t15.2024.07.28 val PER: 0.5360
2025-12-10 23:03:09,084: t15.2025.01.10 val PER: 0.6997
2025-12-10 23:03:09,084: t15.2025.01.12 val PER: 0.6120
2025-12-10 23:03:09,084: t15.2025.03.14 val PER: 0.7234
2025-12-10 23:03:09,084: t15.2025.03.16 val PER: 0.6021
2025-12-10 23:03:09,084: t15.2025.03.30 val PER: 0.7034
2025-12-10 23:03:09,084: t15.2025.04.13 val PER: 0.6234
2025-12-10 23:03:09,084: New best test PER 1.1535 --> 0.5722
2025-12-10 23:03:09,084: Checkpointing model
2025-12-10 23:03:10,218: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 23:04:12,208: Train batch 2200: loss: 39.43 grad norm: 66.27 time: 0.280
2025-12-10 23:05:14,008: Train batch 2400: loss: 53.63 grad norm: 84.61 time: 0.339
2025-12-10 23:06:15,591: Train batch 2600: loss: 47.16 grad norm: 83.98 time: 0.389
2025-12-10 23:07:18,458: Train batch 2800: loss: 42.26 grad norm: 80.54 time: 0.336
2025-12-10 23:08:20,342: Train batch 3000: loss: 33.02 grad norm: 63.80 time: 0.330
2025-12-10 23:09:22,812: Train batch 3200: loss: 40.16 grad norm: 74.76 time: 0.283
2025-12-10 23:10:26,895: Train batch 3400: loss: 39.55 grad norm: 87.72 time: 0.288
2025-12-10 23:11:28,420: Train batch 3600: loss: 34.01 grad norm: 79.37 time: 0.246
2025-12-10 23:12:29,570: Train batch 3800: loss: 43.40 grad norm: 83.81 time: 0.259
2025-12-10 23:13:32,578: Train batch 4000: loss: 30.63 grad norm: 72.56 time: 0.349
2025-12-10 23:13:32,578: Running test after training batch: 4000
2025-12-10 23:13:58,168: Val batch 4000: PER (avg): 0.5355 CTC Loss (avg): 64.1912 time: 25.589
2025-12-10 23:13:58,168: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:13:58,168: t15.2023.08.13 val PER: 0.5125
2025-12-10 23:13:58,168: t15.2023.08.18 val PER: 0.5029
2025-12-10 23:13:58,168: t15.2023.08.20 val PER: 0.4790
2025-12-10 23:13:58,169: t15.2023.08.25 val PER: 0.5060
2025-12-10 23:13:58,169: t15.2023.08.27 val PER: 0.5386
2025-12-10 23:13:58,169: t15.2023.09.01 val PER: 0.4537
2025-12-10 23:13:58,169: t15.2023.09.03 val PER: 0.5202
2025-12-10 23:13:58,169: t15.2023.09.24 val PER: 0.4600
2025-12-10 23:13:58,169: t15.2023.09.29 val PER: 0.4588
2025-12-10 23:13:58,169: t15.2023.10.01 val PER: 0.5575
2025-12-10 23:13:58,169: t15.2023.10.06 val PER: 0.5070
2025-12-10 23:13:58,169: t15.2023.10.08 val PER: 0.5562
2025-12-10 23:13:58,169: t15.2023.10.13 val PER: 0.5376
2025-12-10 23:13:58,169: t15.2023.10.15 val PER: 0.5049
2025-12-10 23:13:58,169: t15.2023.10.20 val PER: 0.4732
2025-12-10 23:13:58,169: t15.2023.10.22 val PER: 0.5045
2025-12-10 23:13:58,170: t15.2023.11.03 val PER: 0.5360
2025-12-10 23:13:58,170: t15.2023.11.04 val PER: 0.3720
2025-12-10 23:13:58,170: t15.2023.11.17 val PER: 0.3655
2025-12-10 23:13:58,170: t15.2023.11.19 val PER: 0.3952
2025-12-10 23:13:58,170: t15.2023.11.26 val PER: 0.5543
2025-12-10 23:13:58,170: t15.2023.12.03 val PER: 0.5179
2025-12-10 23:13:58,170: t15.2023.12.08 val PER: 0.5060
2025-12-10 23:13:58,170: t15.2023.12.10 val PER: 0.4836
2025-12-10 23:13:58,170: t15.2023.12.17 val PER: 0.5842
2025-12-10 23:13:58,170: t15.2023.12.29 val PER: 0.5758
2025-12-10 23:13:58,170: t15.2024.02.25 val PER: 0.5309
2025-12-10 23:13:58,170: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:13:58,170: t15.2024.03.08 val PER: 0.5846
2025-12-10 23:13:58,171: t15.2024.03.15 val PER: 0.5779
2025-12-10 23:13:58,171: t15.2024.03.17 val PER: 0.5474
2025-12-10 23:13:58,171: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:13:58,171: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:13:58,171: t15.2024.05.10 val PER: 0.5988
2025-12-10 23:13:58,171: t15.2024.06.14 val PER: 0.5379
2025-12-10 23:13:58,171: t15.2024.07.19 val PER: 0.6638
2025-12-10 23:13:58,171: t15.2024.07.21 val PER: 0.4834
2025-12-10 23:13:58,171: t15.2024.07.28 val PER: 0.5294
2025-12-10 23:13:58,171: t15.2025.01.10 val PER: 0.6653
2025-12-10 23:13:58,171: t15.2025.01.12 val PER: 0.5727
2025-12-10 23:13:58,171: t15.2025.03.14 val PER: 0.6598
2025-12-10 23:13:58,171: t15.2025.03.16 val PER: 0.5733
2025-12-10 23:13:58,172: t15.2025.03.30 val PER: 0.6609
2025-12-10 23:13:58,172: t15.2025.04.13 val PER: 0.6391
2025-12-10 23:13:58,172: New best test PER 0.5722 --> 0.5355
2025-12-10 23:13:58,172: Checkpointing model
2025-12-10 23:13:59,403: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 23:15:01,801: Train batch 4200: loss: 42.08 grad norm: 86.96 time: 0.271
2025-12-10 23:16:03,919: Train batch 4400: loss: 31.79 grad norm: 85.17 time: 0.248
2025-12-10 23:17:07,791: Train batch 4600: loss: 40.75 grad norm: 104.14 time: 0.362
2025-12-10 23:18:11,538: Train batch 4800: loss: 40.49 grad norm: 107.40 time: 0.292
2025-12-10 23:19:14,482: Train batch 5000: loss: 32.99 grad norm: 115.43 time: 0.245
2025-12-10 23:20:17,576: Train batch 5200: loss: 28.84 grad norm: 101.74 time: 0.310
2025-12-10 23:21:21,356: Train batch 5400: loss: 31.72 grad norm: 112.62 time: 0.292
2025-12-10 23:22:24,280: Train batch 5600: loss: 32.71 grad norm: 101.54 time: 0.346
2025-12-10 23:23:26,890: Train batch 5800: loss: 34.63 grad norm: 101.09 time: 0.305
2025-12-10 23:24:31,203: Train batch 6000: loss: 31.36 grad norm: 86.53 time: 0.327
2025-12-10 23:24:31,204: Running test after training batch: 6000
2025-12-10 23:24:56,669: Val batch 6000: PER (avg): 0.5124 CTC Loss (avg): 61.2328 time: 25.465
2025-12-10 23:24:56,669: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:24:56,669: t15.2023.08.13 val PER: 0.5010
2025-12-10 23:24:56,669: t15.2023.08.18 val PER: 0.4837
2025-12-10 23:24:56,669: t15.2023.08.20 val PER: 0.4472
2025-12-10 23:24:56,669: t15.2023.08.25 val PER: 0.4654
2025-12-10 23:24:56,669: t15.2023.08.27 val PER: 0.5257
2025-12-10 23:24:56,670: t15.2023.09.01 val PER: 0.4610
2025-12-10 23:24:56,670: t15.2023.09.03 val PER: 0.4786
2025-12-10 23:24:56,670: t15.2023.09.24 val PER: 0.4187
2025-12-10 23:24:56,670: t15.2023.09.29 val PER: 0.4601
2025-12-10 23:24:56,670: t15.2023.10.01 val PER: 0.5178
2025-12-10 23:24:56,670: t15.2023.10.06 val PER: 0.4726
2025-12-10 23:24:56,670: t15.2023.10.08 val PER: 0.5318
2025-12-10 23:24:56,670: t15.2023.10.13 val PER: 0.5035
2025-12-10 23:24:56,670: t15.2023.10.15 val PER: 0.4838
2025-12-10 23:24:56,670: t15.2023.10.20 val PER: 0.4631
2025-12-10 23:24:56,670: t15.2023.10.22 val PER: 0.4755
2025-12-10 23:24:56,670: t15.2023.11.03 val PER: 0.5278
2025-12-10 23:24:56,670: t15.2023.11.04 val PER: 0.3208
2025-12-10 23:24:56,670: t15.2023.11.17 val PER: 0.3235
2025-12-10 23:24:56,670: t15.2023.11.19 val PER: 0.3353
2025-12-10 23:24:56,670: t15.2023.11.26 val PER: 0.5094
2025-12-10 23:24:56,670: t15.2023.12.03 val PER: 0.4118
2025-12-10 23:24:56,670: t15.2023.12.08 val PER: 0.4907
2025-12-10 23:24:56,670: t15.2023.12.10 val PER: 0.4599
2025-12-10 23:24:56,671: t15.2023.12.17 val PER: 0.5447
2025-12-10 23:24:56,671: t15.2023.12.29 val PER: 0.5429
2025-12-10 23:24:56,671: t15.2024.02.25 val PER: 0.5393
2025-12-10 23:24:56,671: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:24:56,671: t15.2024.03.08 val PER: 0.5320
2025-12-10 23:24:56,671: t15.2024.03.15 val PER: 0.5416
2025-12-10 23:24:56,671: t15.2024.03.17 val PER: 0.5209
2025-12-10 23:24:56,671: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:24:56,671: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:24:56,671: t15.2024.05.10 val PER: 0.5914
2025-12-10 23:24:56,671: t15.2024.06.14 val PER: 0.5331
2025-12-10 23:24:56,671: t15.2024.07.19 val PER: 0.6407
2025-12-10 23:24:56,671: t15.2024.07.21 val PER: 0.4538
2025-12-10 23:24:56,671: t15.2024.07.28 val PER: 0.5279
2025-12-10 23:24:56,671: t15.2025.01.10 val PER: 0.6185
2025-12-10 23:24:56,671: t15.2025.01.12 val PER: 0.6089
2025-12-10 23:24:56,671: t15.2025.03.14 val PER: 0.6716
2025-12-10 23:24:56,671: t15.2025.03.16 val PER: 0.5707
2025-12-10 23:24:56,671: t15.2025.03.30 val PER: 0.6793
2025-12-10 23:24:56,672: t15.2025.04.13 val PER: 0.6106
2025-12-10 23:24:56,672: New best test PER 0.5355 --> 0.5124
2025-12-10 23:24:56,672: Checkpointing model
2025-12-10 23:24:58,020: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 23:26:01,997: Train batch 6200: loss: 37.47 grad norm: 157.47 time: 0.420
2025-12-10 23:27:06,457: Train batch 6400: loss: 30.24 grad norm: 81.58 time: 0.307
2025-12-10 23:28:10,715: Train batch 6600: loss: 33.07 grad norm: 86.97 time: 0.361
2025-12-10 23:29:12,592: Train batch 6800: loss: 33.93 grad norm: 138.49 time: 0.252
2025-12-10 23:30:15,336: Train batch 7000: loss: 27.21 grad norm: 126.74 time: 0.265
2025-12-10 23:31:18,874: Train batch 7200: loss: 37.61 grad norm: 125.73 time: 0.289
2025-12-10 23:32:21,199: Train batch 7400: loss: 29.05 grad norm: 85.34 time: 0.286
2025-12-10 23:33:24,185: Train batch 7600: loss: 25.25 grad norm: 101.78 time: 0.333
2025-12-10 23:34:27,651: Train batch 7800: loss: 19.54 grad norm: 66.99 time: 0.306
2025-12-10 23:35:29,901: Train batch 8000: loss: 40.22 grad norm: 135.66 time: 0.429
2025-12-10 23:35:29,901: Running test after training batch: 8000
2025-12-10 23:35:55,165: Val batch 8000: PER (avg): 0.4908 CTC Loss (avg): 58.9977 time: 25.264
2025-12-10 23:35:55,165: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:35:55,165: t15.2023.08.13 val PER: 0.4823
2025-12-10 23:35:55,165: t15.2023.08.18 val PER: 0.4644
2025-12-10 23:35:55,165: t15.2023.08.20 val PER: 0.4257
2025-12-10 23:35:55,166: t15.2023.08.25 val PER: 0.4849
2025-12-10 23:35:55,166: t15.2023.08.27 val PER: 0.5289
2025-12-10 23:35:55,166: t15.2023.09.01 val PER: 0.4091
2025-12-10 23:35:55,166: t15.2023.09.03 val PER: 0.4276
2025-12-10 23:35:55,166: t15.2023.09.24 val PER: 0.4296
2025-12-10 23:35:55,166: t15.2023.09.29 val PER: 0.4391
2025-12-10 23:35:55,166: t15.2023.10.01 val PER: 0.5079
2025-12-10 23:35:55,166: t15.2023.10.06 val PER: 0.4392
2025-12-10 23:35:55,166: t15.2023.10.08 val PER: 0.4993
2025-12-10 23:35:55,166: t15.2023.10.13 val PER: 0.4787
2025-12-10 23:35:55,166: t15.2023.10.15 val PER: 0.4483
2025-12-10 23:35:55,166: t15.2023.10.20 val PER: 0.4765
2025-12-10 23:35:55,166: t15.2023.10.22 val PER: 0.4655
2025-12-10 23:35:55,166: t15.2023.11.03 val PER: 0.4817
2025-12-10 23:35:55,166: t15.2023.11.04 val PER: 0.2765
2025-12-10 23:35:55,166: t15.2023.11.17 val PER: 0.2846
2025-12-10 23:35:55,166: t15.2023.11.19 val PER: 0.2934
2025-12-10 23:35:55,166: t15.2023.11.26 val PER: 0.5159
2025-12-10 23:35:55,167: t15.2023.12.03 val PER: 0.4370
2025-12-10 23:35:55,167: t15.2023.12.08 val PER: 0.4494
2025-12-10 23:35:55,167: t15.2023.12.10 val PER: 0.4218
2025-12-10 23:35:55,167: t15.2023.12.17 val PER: 0.5613
2025-12-10 23:35:55,167: t15.2023.12.29 val PER: 0.5443
2025-12-10 23:35:55,167: t15.2024.02.25 val PER: 0.4719
2025-12-10 23:35:55,167: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:35:55,167: t15.2024.03.08 val PER: 0.5462
2025-12-10 23:35:55,167: t15.2024.03.15 val PER: 0.5253
2025-12-10 23:35:55,167: t15.2024.03.17 val PER: 0.4944
2025-12-10 23:35:55,167: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:35:55,167: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:35:55,167: t15.2024.05.10 val PER: 0.5453
2025-12-10 23:35:55,167: t15.2024.06.14 val PER: 0.5158
2025-12-10 23:35:55,167: t15.2024.07.19 val PER: 0.6473
2025-12-10 23:35:55,167: t15.2024.07.21 val PER: 0.4379
2025-12-10 23:35:55,167: t15.2024.07.28 val PER: 0.4676
2025-12-10 23:35:55,167: t15.2025.01.10 val PER: 0.6267
2025-12-10 23:35:55,168: t15.2025.01.12 val PER: 0.5543
2025-12-10 23:35:55,168: t15.2025.03.14 val PER: 0.6731
2025-12-10 23:35:55,168: t15.2025.03.16 val PER: 0.5406
2025-12-10 23:35:55,168: t15.2025.03.30 val PER: 0.6345
2025-12-10 23:35:55,168: t15.2025.04.13 val PER: 0.5863
2025-12-10 23:35:55,168: New best test PER 0.5124 --> 0.4908
2025-12-10 23:35:55,168: Checkpointing model
2025-12-10 23:35:56,439: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 23:37:01,302: Train batch 8200: loss: 35.15 grad norm: 82.31 time: 0.343
2025-12-10 23:38:05,057: Train batch 8400: loss: 32.12 grad norm: 114.53 time: 0.294
2025-12-10 23:39:07,431: Train batch 8600: loss: 20.04 grad norm: 111.26 time: 0.280
2025-12-10 23:40:11,488: Train batch 8800: loss: 24.82 grad norm: 154.62 time: 0.275
2025-12-10 23:41:13,464: Train batch 9000: loss: 27.56 grad norm: 96.62 time: 0.289
2025-12-10 23:42:17,172: Train batch 9200: loss: 22.12 grad norm: 155.53 time: 0.319
2025-12-10 23:43:22,290: Train batch 9400: loss: 30.39 grad norm: 132.21 time: 0.334
2025-12-10 23:44:25,996: Train batch 9600: loss: 30.92 grad norm: 88.27 time: 0.329
2025-12-10 23:45:29,566: Train batch 9800: loss: 24.17 grad norm: 93.85 time: 0.256
2025-12-10 23:46:33,285: Train batch 10000: loss: 18.97 grad norm: 86.68 time: 0.273
2025-12-10 23:46:33,285: Running test after training batch: 10000
2025-12-10 23:46:58,680: Val batch 10000: PER (avg): 0.4812 CTC Loss (avg): 56.8155 time: 25.395
2025-12-10 23:46:58,681: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:46:58,681: t15.2023.08.13 val PER: 0.4657
2025-12-10 23:46:58,681: t15.2023.08.18 val PER: 0.4635
2025-12-10 23:46:58,681: t15.2023.08.20 val PER: 0.4178
2025-12-10 23:46:58,681: t15.2023.08.25 val PER: 0.4367
2025-12-10 23:46:58,681: t15.2023.08.27 val PER: 0.5161
2025-12-10 23:46:58,681: t15.2023.09.01 val PER: 0.4115
2025-12-10 23:46:58,681: t15.2023.09.03 val PER: 0.4489
2025-12-10 23:46:58,681: t15.2023.09.24 val PER: 0.4308
2025-12-10 23:46:58,681: t15.2023.09.29 val PER: 0.4065
2025-12-10 23:46:58,681: t15.2023.10.01 val PER: 0.4908
2025-12-10 23:46:58,681: t15.2023.10.06 val PER: 0.4166
2025-12-10 23:46:58,681: t15.2023.10.08 val PER: 0.5291
2025-12-10 23:46:58,682: t15.2023.10.13 val PER: 0.5074
2025-12-10 23:46:58,682: t15.2023.10.15 val PER: 0.4357
2025-12-10 23:46:58,682: t15.2023.10.20 val PER: 0.4564
2025-12-10 23:46:58,682: t15.2023.10.22 val PER: 0.4432
2025-12-10 23:46:58,682: t15.2023.11.03 val PER: 0.4898
2025-12-10 23:46:58,682: t15.2023.11.04 val PER: 0.3003
2025-12-10 23:46:58,682: t15.2023.11.17 val PER: 0.2877
2025-12-10 23:46:58,682: t15.2023.11.19 val PER: 0.3174
2025-12-10 23:46:58,682: t15.2023.11.26 val PER: 0.4877
2025-12-10 23:46:58,682: t15.2023.12.03 val PER: 0.4233
2025-12-10 23:46:58,682: t15.2023.12.08 val PER: 0.4354
2025-12-10 23:46:58,682: t15.2023.12.10 val PER: 0.3968
2025-12-10 23:46:58,682: t15.2023.12.17 val PER: 0.5395
2025-12-10 23:46:58,682: t15.2023.12.29 val PER: 0.5148
2025-12-10 23:46:58,682: t15.2024.02.25 val PER: 0.4649
2025-12-10 23:46:58,682: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:46:58,683: t15.2024.03.08 val PER: 0.5363
2025-12-10 23:46:58,683: t15.2024.03.15 val PER: 0.4984
2025-12-10 23:46:58,683: t15.2024.03.17 val PER: 0.4979
2025-12-10 23:46:58,683: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:46:58,683: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:46:58,683: t15.2024.05.10 val PER: 0.5171
2025-12-10 23:46:58,683: t15.2024.06.14 val PER: 0.4858
2025-12-10 23:46:58,683: t15.2024.07.19 val PER: 0.6374
2025-12-10 23:46:58,683: t15.2024.07.21 val PER: 0.4193
2025-12-10 23:46:58,683: t15.2024.07.28 val PER: 0.4493
2025-12-10 23:46:58,683: t15.2025.01.10 val PER: 0.6240
2025-12-10 23:46:58,683: t15.2025.01.12 val PER: 0.5427
2025-12-10 23:46:58,683: t15.2025.03.14 val PER: 0.6657
2025-12-10 23:46:58,683: t15.2025.03.16 val PER: 0.5602
2025-12-10 23:46:58,683: t15.2025.03.30 val PER: 0.6345
2025-12-10 23:46:58,683: t15.2025.04.13 val PER: 0.6091
2025-12-10 23:46:58,683: New best test PER 0.4908 --> 0.4812
2025-12-10 23:46:58,683: Checkpointing model
2025-12-10 23:46:59,885: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-10 23:48:04,545: Train batch 10200: loss: 23.40 grad norm: 85.26 time: 0.298
2025-12-10 23:49:08,129: Train batch 10400: loss: 22.27 grad norm: 158.52 time: 0.358
2025-12-10 23:50:10,833: Train batch 10600: loss: 14.65 grad norm: 53.84 time: 0.265
2025-12-10 23:51:15,513: Train batch 10800: loss: 32.19 grad norm: 177.07 time: 0.429
2025-12-10 23:52:17,903: Train batch 11000: loss: 37.90 grad norm: 100.59 time: 0.300
2025-12-10 23:53:20,745: Train batch 11200: loss: 31.59 grad norm: 99.56 time: 0.240
2025-12-10 23:54:24,034: Train batch 11400: loss: 29.55 grad norm: 107.95 time: 0.236
2025-12-10 23:55:28,156: Train batch 11600: loss: 24.75 grad norm: 79.25 time: 0.277
2025-12-10 23:56:31,942: Train batch 11800: loss: 20.64 grad norm: 67.48 time: 0.407
2025-12-10 23:57:34,737: Train batch 12000: loss: 33.91 grad norm: 168.93 time: 0.356
2025-12-10 23:57:34,737: Running test after training batch: 12000
2025-12-10 23:58:00,132: Val batch 12000: PER (avg): 0.4913 CTC Loss (avg): 58.3136 time: 25.395
2025-12-10 23:58:00,132: t15.2023.08.11 val PER: 1.0000
2025-12-10 23:58:00,132: t15.2023.08.13 val PER: 0.4854
2025-12-10 23:58:00,132: t15.2023.08.18 val PER: 0.4778
2025-12-10 23:58:00,133: t15.2023.08.20 val PER: 0.4464
2025-12-10 23:58:00,133: t15.2023.08.25 val PER: 0.4759
2025-12-10 23:58:00,133: t15.2023.08.27 val PER: 0.5129
2025-12-10 23:58:00,133: t15.2023.09.01 val PER: 0.4440
2025-12-10 23:58:00,133: t15.2023.09.03 val PER: 0.4489
2025-12-10 23:58:00,133: t15.2023.09.24 val PER: 0.4260
2025-12-10 23:58:00,133: t15.2023.09.29 val PER: 0.4218
2025-12-10 23:58:00,133: t15.2023.10.01 val PER: 0.5231
2025-12-10 23:58:00,133: t15.2023.10.06 val PER: 0.4026
2025-12-10 23:58:00,133: t15.2023.10.08 val PER: 0.5196
2025-12-10 23:58:00,133: t15.2023.10.13 val PER: 0.4825
2025-12-10 23:58:00,133: t15.2023.10.15 val PER: 0.4647
2025-12-10 23:58:00,133: t15.2023.10.20 val PER: 0.5000
2025-12-10 23:58:00,133: t15.2023.10.22 val PER: 0.4410
2025-12-10 23:58:00,133: t15.2023.11.03 val PER: 0.4783
2025-12-10 23:58:00,133: t15.2023.11.04 val PER: 0.2901
2025-12-10 23:58:00,133: t15.2023.11.17 val PER: 0.2768
2025-12-10 23:58:00,133: t15.2023.11.19 val PER: 0.3114
2025-12-10 23:58:00,134: t15.2023.11.26 val PER: 0.4993
2025-12-10 23:58:00,134: t15.2023.12.03 val PER: 0.4254
2025-12-10 23:58:00,134: t15.2023.12.08 val PER: 0.4567
2025-12-10 23:58:00,134: t15.2023.12.10 val PER: 0.4455
2025-12-10 23:58:00,134: t15.2023.12.17 val PER: 0.5468
2025-12-10 23:58:00,134: t15.2023.12.29 val PER: 0.5422
2025-12-10 23:58:00,134: t15.2024.02.25 val PER: 0.4902
2025-12-10 23:58:00,134: t15.2024.03.03 val PER: 1.0000
2025-12-10 23:58:00,134: t15.2024.03.08 val PER: 0.5107
2025-12-10 23:58:00,134: t15.2024.03.15 val PER: 0.5047
2025-12-10 23:58:00,134: t15.2024.03.17 val PER: 0.4728
2025-12-10 23:58:00,134: t15.2024.04.25 val PER: 1.0000
2025-12-10 23:58:00,134: t15.2024.04.28 val PER: 1.0000
2025-12-10 23:58:00,134: t15.2024.05.10 val PER: 0.5498
2025-12-10 23:58:00,134: t15.2024.06.14 val PER: 0.5205
2025-12-10 23:58:00,134: t15.2024.07.19 val PER: 0.6460
2025-12-10 23:58:00,134: t15.2024.07.21 val PER: 0.4124
2025-12-10 23:58:00,134: t15.2024.07.28 val PER: 0.4772
2025-12-10 23:58:00,134: t15.2025.01.10 val PER: 0.6198
2025-12-10 23:58:00,134: t15.2025.01.12 val PER: 0.5720
2025-12-10 23:58:00,135: t15.2025.03.14 val PER: 0.6701
2025-12-10 23:58:00,135: t15.2025.03.16 val PER: 0.5524
2025-12-10 23:58:00,135: t15.2025.03.30 val PER: 0.6563
2025-12-10 23:58:00,135: t15.2025.04.13 val PER: 0.6077
2025-12-10 23:59:03,090: Train batch 12200: loss: 27.50 grad norm: 106.57 time: 0.285
2025-12-11 00:00:06,028: Train batch 12400: loss: 25.49 grad norm: 100.31 time: 0.252
2025-12-11 00:01:10,442: Train batch 12600: loss: 23.78 grad norm: 100.60 time: 0.317
2025-12-11 00:02:12,896: Train batch 12800: loss: 33.21 grad norm: 281.86 time: 0.425
2025-12-11 00:03:15,557: Train batch 13000: loss: 28.04 grad norm: 82.61 time: 0.310
2025-12-11 00:04:19,410: Train batch 13200: loss: 31.91 grad norm: 150.10 time: 0.455
2025-12-11 00:05:22,161: Train batch 13400: loss: 26.08 grad norm: 144.13 time: 0.323
2025-12-11 00:06:25,314: Train batch 13600: loss: 27.03 grad norm: 144.86 time: 0.252
2025-12-11 00:07:28,714: Train batch 13800: loss: 26.37 grad norm: 168.22 time: 0.249
2025-12-11 00:08:32,310: Train batch 14000: loss: 40.00 grad norm: 373.24 time: 0.240
2025-12-11 00:08:32,310: Running test after training batch: 14000
2025-12-11 00:08:57,681: Val batch 14000: PER (avg): 0.4951 CTC Loss (avg): 60.0828 time: 25.370
2025-12-11 00:08:57,681: t15.2023.08.11 val PER: 1.0000
2025-12-11 00:08:57,681: t15.2023.08.13 val PER: 0.5031
2025-12-11 00:08:57,681: t15.2023.08.18 val PER: 0.4526
2025-12-11 00:08:57,681: t15.2023.08.20 val PER: 0.4559
2025-12-11 00:08:57,681: t15.2023.08.25 val PER: 0.5015
2025-12-11 00:08:57,681: t15.2023.08.27 val PER: 0.5161
2025-12-11 00:08:57,681: t15.2023.09.01 val PER: 0.4383
2025-12-11 00:08:57,681: t15.2023.09.03 val PER: 0.4454
2025-12-11 00:08:57,681: t15.2023.09.24 val PER: 0.4223
2025-12-11 00:08:57,681: t15.2023.09.29 val PER: 0.4480
2025-12-11 00:08:57,681: t15.2023.10.01 val PER: 0.5172
2025-12-11 00:08:57,682: t15.2023.10.06 val PER: 0.4715
2025-12-11 00:08:57,682: t15.2023.10.08 val PER: 0.5237
2025-12-11 00:08:57,682: t15.2023.10.13 val PER: 0.4779
2025-12-11 00:08:57,682: t15.2023.10.15 val PER: 0.4628
2025-12-11 00:08:57,682: t15.2023.10.20 val PER: 0.4866
2025-12-11 00:08:57,682: t15.2023.10.22 val PER: 0.4488
2025-12-11 00:08:57,682: t15.2023.11.03 val PER: 0.4905
2025-12-11 00:08:57,682: t15.2023.11.04 val PER: 0.2799
2025-12-11 00:08:57,682: t15.2023.11.17 val PER: 0.3017
2025-12-11 00:08:57,682: t15.2023.11.19 val PER: 0.3453
2025-12-11 00:08:57,682: t15.2023.11.26 val PER: 0.5181
2025-12-11 00:08:57,682: t15.2023.12.03 val PER: 0.4254
2025-12-11 00:08:57,682: t15.2023.12.08 val PER: 0.4587
2025-12-11 00:08:57,682: t15.2023.12.10 val PER: 0.4244
2025-12-11 00:08:57,682: t15.2023.12.17 val PER: 0.5613
2025-12-11 00:08:57,682: t15.2023.12.29 val PER: 0.5360
2025-12-11 00:08:57,682: t15.2024.02.25 val PER: 0.4846
2025-12-11 00:08:57,682: t15.2024.03.03 val PER: 1.0000
2025-12-11 00:08:57,683: t15.2024.03.08 val PER: 0.5405
2025-12-11 00:08:57,683: t15.2024.03.15 val PER: 0.5091
2025-12-11 00:08:57,683: t15.2024.03.17 val PER: 0.5028
2025-12-11 00:08:57,683: t15.2024.04.25 val PER: 1.0000
2025-12-11 00:08:57,683: t15.2024.04.28 val PER: 1.0000
2025-12-11 00:08:57,683: t15.2024.05.10 val PER: 0.5349
2025-12-11 00:08:57,683: t15.2024.06.14 val PER: 0.5189
2025-12-11 00:08:57,683: t15.2024.07.19 val PER: 0.6348
2025-12-11 00:08:57,683: t15.2024.07.21 val PER: 0.4248
2025-12-11 00:08:57,683: t15.2024.07.28 val PER: 0.4904
2025-12-11 00:08:57,683: t15.2025.01.10 val PER: 0.6322
2025-12-11 00:08:57,683: t15.2025.01.12 val PER: 0.5296
2025-12-11 00:08:57,683: t15.2025.03.14 val PER: 0.6790
2025-12-11 00:08:57,683: t15.2025.03.16 val PER: 0.5445
2025-12-11 00:08:57,683: t15.2025.03.30 val PER: 0.6241
2025-12-11 00:08:57,683: t15.2025.04.13 val PER: 0.5949
2025-12-11 00:10:01,191: Train batch 14200: loss: 26.82 grad norm: 214.31 time: 0.318
2025-12-11 00:11:04,179: Train batch 14400: loss: 22.22 grad norm: 111.63 time: 0.275
2025-12-11 00:12:08,259: Train batch 14600: loss: 30.32 grad norm: 207.66 time: 0.368
2025-12-11 00:13:11,100: Train batch 14800: loss: 35.60 grad norm: 348.99 time: 0.438
2025-12-11 00:14:13,961: Train batch 15000: loss: 40.64 grad norm: 187.51 time: 0.407
2025-12-11 00:15:14,550: Train batch 15200: loss: 32.56 grad norm: 112.73 time: 0.443
2025-12-11 00:16:17,382: Train batch 15400: loss: 21.25 grad norm: 235.56 time: 0.322
2025-12-11 00:17:19,137: Train batch 15600: loss: 25.55 grad norm: 102.27 time: 0.339
2025-12-11 00:18:21,974: Train batch 15800: loss: 28.42 grad norm: 201.14 time: 0.275
2025-12-11 00:19:25,175: Train batch 16000: loss: 26.44 grad norm: 105.00 time: 0.397
2025-12-11 00:19:25,175: Running test after training batch: 16000
2025-12-11 00:19:50,458: Val batch 16000: PER (avg): 0.4892 CTC Loss (avg): 58.3621 time: 25.283
2025-12-11 00:19:50,459: t15.2023.08.11 val PER: 1.0000
2025-12-11 00:19:50,459: t15.2023.08.13 val PER: 0.4969
2025-12-11 00:19:50,459: t15.2023.08.18 val PER: 0.4392
2025-12-11 00:19:50,459: t15.2023.08.20 val PER: 0.4384
2025-12-11 00:19:50,459: t15.2023.08.25 val PER: 0.4608
2025-12-11 00:19:50,459: t15.2023.08.27 val PER: 0.5129
2025-12-11 00:19:50,459: t15.2023.09.01 val PER: 0.4326
2025-12-11 00:19:50,459: t15.2023.09.03 val PER: 0.4216
2025-12-11 00:19:50,459: t15.2023.09.24 val PER: 0.4150
2025-12-11 00:19:50,459: t15.2023.09.29 val PER: 0.4448
2025-12-11 00:19:50,459: t15.2023.10.01 val PER: 0.5264
2025-12-11 00:19:50,459: t15.2023.10.06 val PER: 0.3929
2025-12-11 00:19:50,459: t15.2023.10.08 val PER: 0.5183
2025-12-11 00:19:50,459: t15.2023.10.13 val PER: 0.4942
2025-12-11 00:19:50,459: t15.2023.10.15 val PER: 0.4548
2025-12-11 00:19:50,460: t15.2023.10.20 val PER: 0.4530
2025-12-11 00:19:50,460: t15.2023.10.22 val PER: 0.4844
2025-12-11 00:19:50,460: t15.2023.11.03 val PER: 0.4844
2025-12-11 00:19:50,460: t15.2023.11.04 val PER: 0.3003
2025-12-11 00:19:50,460: t15.2023.11.17 val PER: 0.3328
2025-12-11 00:19:50,460: t15.2023.11.19 val PER: 0.3653
2025-12-11 00:19:50,460: t15.2023.11.26 val PER: 0.5167
2025-12-11 00:19:50,460: t15.2023.12.03 val PER: 0.4370
2025-12-11 00:19:50,460: t15.2023.12.08 val PER: 0.4461
2025-12-11 00:19:50,460: t15.2023.12.10 val PER: 0.4271
2025-12-11 00:19:50,460: t15.2023.12.17 val PER: 0.5509
2025-12-11 00:19:50,460: t15.2023.12.29 val PER: 0.5045
2025-12-11 00:19:50,460: t15.2024.02.25 val PER: 0.4789
2025-12-11 00:19:50,460: t15.2024.03.03 val PER: 1.0000
2025-12-11 00:19:50,460: t15.2024.03.08 val PER: 0.5562
2025-12-11 00:19:50,460: t15.2024.03.15 val PER: 0.5172
2025-12-11 00:19:50,460: t15.2024.03.17 val PER: 0.4895
2025-12-11 00:19:50,460: t15.2024.04.25 val PER: 1.0000
2025-12-11 00:19:50,461: t15.2024.04.28 val PER: 1.0000
2025-12-11 00:19:50,461: t15.2024.05.10 val PER: 0.5453
2025-12-11 00:19:50,461: t15.2024.06.14 val PER: 0.5063
2025-12-11 00:19:50,461: t15.2024.07.19 val PER: 0.6539
2025-12-11 00:19:50,461: t15.2024.07.21 val PER: 0.3883
2025-12-11 00:19:50,461: t15.2024.07.28 val PER: 0.4596
2025-12-11 00:19:50,461: t15.2025.01.10 val PER: 0.6364
2025-12-11 00:19:50,461: t15.2025.01.12 val PER: 0.5258
2025-12-11 00:19:50,461: t15.2025.03.14 val PER: 0.6849
2025-12-11 00:19:50,461: t15.2025.03.16 val PER: 0.5353
2025-12-11 00:19:50,461: t15.2025.03.30 val PER: 0.6402
2025-12-11 00:19:50,461: t15.2025.04.13 val PER: 0.5849
2025-12-11 00:20:52,442: Train batch 16200: loss: 29.70 grad norm: 197.80 time: 0.268
2025-12-11 00:21:54,349: Train batch 16400: loss: 16.17 grad norm: 242.92 time: 0.254
2025-12-11 00:22:56,990: Train batch 16600: loss: 34.45 grad norm: 187.42 time: 0.262
2025-12-11 00:23:59,822: Train batch 16800: loss: 33.04 grad norm: 287.49 time: 0.368
2025-12-11 00:25:01,703: Train batch 17000: loss: 27.04 grad norm: 123.88 time: 0.315
2025-12-11 00:26:04,524: Train batch 17200: loss: 32.99 grad norm: 76.06 time: 0.399
2025-12-11 00:27:07,853: Train batch 17400: loss: 28.29 grad norm: 107.59 time: 0.332
2025-12-11 00:28:11,213: Train batch 17600: loss: 35.03 grad norm: 166.88 time: 0.437
2025-12-11 00:29:14,676: Train batch 17800: loss: 24.81 grad norm: 139.91 time: 0.274
2025-12-11 00:30:18,413: Train batch 18000: loss: 24.25 grad norm: 90.27 time: 0.381
2025-12-11 00:30:18,414: Running test after training batch: 18000
2025-12-11 00:30:43,849: Val batch 18000: PER (avg): 0.4725 CTC Loss (avg): 57.8490 time: 25.435
2025-12-11 00:30:43,849: t15.2023.08.11 val PER: 1.0000
2025-12-11 00:30:43,849: t15.2023.08.13 val PER: 0.4834
2025-12-11 00:30:43,849: t15.2023.08.18 val PER: 0.4417
2025-12-11 00:30:43,850: t15.2023.08.20 val PER: 0.4329
2025-12-11 00:30:43,850: t15.2023.08.25 val PER: 0.4578
2025-12-11 00:30:43,850: t15.2023.08.27 val PER: 0.4823
2025-12-11 00:30:43,850: t15.2023.09.01 val PER: 0.4188
2025-12-11 00:30:43,850: t15.2023.09.03 val PER: 0.3990
2025-12-11 00:30:43,850: t15.2023.09.24 val PER: 0.4053
2025-12-11 00:30:43,850: t15.2023.09.29 val PER: 0.4154
2025-12-11 00:30:43,850: t15.2023.10.01 val PER: 0.4954
2025-12-11 00:30:43,850: t15.2023.10.06 val PER: 0.4015
2025-12-11 00:30:43,850: t15.2023.10.08 val PER: 0.5223
2025-12-11 00:30:43,850: t15.2023.10.13 val PER: 0.4670
2025-12-11 00:30:43,850: t15.2023.10.15 val PER: 0.4423
2025-12-11 00:30:43,850: t15.2023.10.20 val PER: 0.4497
2025-12-11 00:30:43,850: t15.2023.10.22 val PER: 0.4365
2025-12-11 00:30:43,850: t15.2023.11.03 val PER: 0.4552
2025-12-11 00:30:43,850: t15.2023.11.04 val PER: 0.3481
2025-12-11 00:30:43,850: t15.2023.11.17 val PER: 0.2504
2025-12-11 00:30:43,850: t15.2023.11.19 val PER: 0.3393
2025-12-11 00:30:43,851: t15.2023.11.26 val PER: 0.4739
2025-12-11 00:30:43,851: t15.2023.12.03 val PER: 0.4044
2025-12-11 00:30:43,851: t15.2023.12.08 val PER: 0.4421
2025-12-11 00:30:43,851: t15.2023.12.10 val PER: 0.4100
2025-12-11 00:30:43,851: t15.2023.12.17 val PER: 0.5385
2025-12-11 00:30:43,851: t15.2023.12.29 val PER: 0.5024
2025-12-11 00:30:43,851: t15.2024.02.25 val PER: 0.4522
2025-12-11 00:30:43,851: t15.2024.03.03 val PER: 1.0000
2025-12-11 00:30:43,851: t15.2024.03.08 val PER: 0.5263
2025-12-11 00:30:43,851: t15.2024.03.15 val PER: 0.4891
2025-12-11 00:30:43,851: t15.2024.03.17 val PER: 0.4686
2025-12-11 00:30:43,851: t15.2024.04.25 val PER: 1.0000
2025-12-11 00:30:43,851: t15.2024.04.28 val PER: 1.0000
2025-12-11 00:30:43,851: t15.2024.05.10 val PER: 0.5379
2025-12-11 00:30:43,851: t15.2024.06.14 val PER: 0.4842
2025-12-11 00:30:43,851: t15.2024.07.19 val PER: 0.6289
2025-12-11 00:30:43,851: t15.2024.07.21 val PER: 0.3738
2025-12-11 00:30:43,851: t15.2024.07.28 val PER: 0.4691
2025-12-11 00:30:43,851: t15.2025.01.10 val PER: 0.6419
2025-12-11 00:30:43,852: t15.2025.01.12 val PER: 0.5227
2025-12-11 00:30:43,852: t15.2025.03.14 val PER: 0.6627
2025-12-11 00:30:43,852: t15.2025.03.16 val PER: 0.5458
2025-12-11 00:30:43,852: t15.2025.03.30 val PER: 0.6149
2025-12-11 00:30:43,852: t15.2025.04.13 val PER: 0.5820
2025-12-11 00:30:43,852: New best test PER 0.4812 --> 0.4725
2025-12-11 00:30:43,852: Checkpointing model
2025-12-11 00:30:45,164: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 00:31:47,780: Train batch 18200: loss: 32.44 grad norm: 91.13 time: 0.270
2025-12-11 00:32:51,179: Train batch 18400: loss: 18.26 grad norm: 73.91 time: 0.324
2025-12-11 00:33:54,399: Train batch 18600: loss: 32.15 grad norm: 163.93 time: 0.330
2025-12-11 00:34:57,764: Train batch 18800: loss: 30.52 grad norm: 167.56 time: 0.316
2025-12-11 00:36:00,965: Train batch 19000: loss: 34.53 grad norm: 214.07 time: 0.311
2025-12-11 00:37:04,061: Train batch 19200: loss: 33.25 grad norm: 99.80 time: 0.329
2025-12-11 00:38:07,115: Train batch 19400: loss: 24.92 grad norm: 245.18 time: 0.300
2025-12-11 00:39:11,259: Train batch 19600: loss: 29.09 grad norm: 73.64 time: 0.362
2025-12-11 00:40:13,974: Train batch 19800: loss: 13.21 grad norm: 70.00 time: 0.301
2025-12-11 00:41:16,006: Train batch 20000: loss: 24.45 grad norm: 84.09 time: 0.284
2025-12-11 00:41:16,006: Running test after training batch: 20000
2025-12-11 00:41:41,249: Val batch 20000: PER (avg): 0.5000 CTC Loss (avg): 62.1673 time: 25.242
2025-12-11 00:41:41,249: t15.2023.08.11 val PER: 1.0000
2025-12-11 00:41:41,249: t15.2023.08.13 val PER: 0.5021
2025-12-11 00:41:41,249: t15.2023.08.18 val PER: 0.4677
2025-12-11 00:41:41,249: t15.2023.08.20 val PER: 0.4615
2025-12-11 00:41:41,249: t15.2023.08.25 val PER: 0.4834
2025-12-11 00:41:41,249: t15.2023.08.27 val PER: 0.5161
2025-12-11 00:41:41,249: t15.2023.09.01 val PER: 0.4237
2025-12-11 00:41:41,249: t15.2023.09.03 val PER: 0.4394
2025-12-11 00:41:41,249: t15.2023.09.24 val PER: 0.4308
2025-12-11 00:41:41,249: t15.2023.09.29 val PER: 0.4454
2025-12-11 00:41:41,250: t15.2023.10.01 val PER: 0.5363
2025-12-11 00:41:41,250: t15.2023.10.06 val PER: 0.4381
2025-12-11 00:41:41,250: t15.2023.10.08 val PER: 0.5237
2025-12-11 00:41:41,250: t15.2023.10.13 val PER: 0.4663
2025-12-11 00:41:41,250: t15.2023.10.15 val PER: 0.4845
2025-12-11 00:41:41,250: t15.2023.10.20 val PER: 0.4765
2025-12-11 00:41:41,250: t15.2023.10.22 val PER: 0.4232
2025-12-11 00:41:41,250: t15.2023.11.03 val PER: 0.4607
2025-12-11 00:41:41,250: t15.2023.11.04 val PER: 0.3038
2025-12-11 00:41:41,250: t15.2023.11.17 val PER: 0.3375
2025-12-11 00:41:41,250: t15.2023.11.19 val PER: 0.3912
2025-12-11 00:41:41,250: t15.2023.11.26 val PER: 0.4957
2025-12-11 00:41:41,250: t15.2023.12.03 val PER: 0.4443
2025-12-11 00:41:41,250: t15.2023.12.08 val PER: 0.4774
2025-12-11 00:41:41,250: t15.2023.12.10 val PER: 0.3955
2025-12-11 00:41:41,250: t15.2023.12.17 val PER: 0.5561
2025-12-11 00:41:41,250: t15.2023.12.29 val PER: 0.5443
2025-12-11 00:41:41,250: t15.2024.02.25 val PER: 0.4831
2025-12-11 00:41:41,250: t15.2024.03.03 val PER: 1.0000
2025-12-11 00:41:41,251: t15.2024.03.08 val PER: 0.5477
2025-12-11 00:41:41,251: t15.2024.03.15 val PER: 0.5241
2025-12-11 00:41:41,251: t15.2024.03.17 val PER: 0.4923
2025-12-11 00:41:41,251: t15.2024.04.25 val PER: 1.0000
2025-12-11 00:41:41,251: t15.2024.04.28 val PER: 1.0000
2025-12-11 00:41:41,251: t15.2024.05.10 val PER: 0.5944
2025-12-11 00:41:41,251: t15.2024.06.14 val PER: 0.4984
2025-12-11 00:41:41,251: t15.2024.07.19 val PER: 0.6750
2025-12-11 00:41:41,251: t15.2024.07.21 val PER: 0.4028
2025-12-11 00:41:41,251: t15.2024.07.28 val PER: 0.4904
2025-12-11 00:41:41,251: t15.2025.01.10 val PER: 0.6653
2025-12-11 00:41:41,251: t15.2025.01.12 val PER: 0.5743
2025-12-11 00:41:41,251: t15.2025.03.14 val PER: 0.6820
2025-12-11 00:41:41,251: t15.2025.03.16 val PER: 0.5707
2025-12-11 00:41:41,251: t15.2025.03.30 val PER: 0.6494
2025-12-11 00:41:41,251: t15.2025.04.13 val PER: 0.6220
2025-12-11 00:42:43,725: Train batch 20200: loss: 29.87 grad norm: 261.67 time: 0.299
2025-12-11 00:43:47,923: Train batch 20400: loss: 15.87 grad norm: 133.67 time: 0.227
2025-12-11 00:44:50,498: Train batch 20600: loss: 19.36 grad norm: 116.86 time: 0.317
2025-12-11 00:45:54,111: Train batch 20800: loss: 20.24 grad norm: 99.47 time: 0.409
2025-12-11 00:46:57,779: Train batch 21000: loss: 27.66 grad norm: 104.23 time: 0.265
2025-12-11 00:48:01,421: Train batch 21200: loss: 18.57 grad norm: 119.11 time: 0.282
2025-12-11 00:49:05,473: Train batch 21400: loss: 22.87 grad norm: 116.31 time: 0.222
2025-12-11 00:50:09,307: Train batch 21600: loss: 35.48 grad norm: 256.98 time: 0.277
2025-12-11 00:51:12,235: Train batch 21800: loss: 25.88 grad norm: 152.86 time: 0.342
2025-12-11 00:52:15,968: Train batch 22000: loss: 11.76 grad norm: 64.21 time: 0.267
2025-12-11 00:52:15,969: Running test after training batch: 22000
2025-12-11 00:52:41,251: Val batch 22000: PER (avg): 0.4663 CTC Loss (avg): 57.2198 time: 25.283
2025-12-11 00:52:41,252: t15.2023.08.11 val PER: 1.0000
2025-12-11 00:52:41,252: t15.2023.08.13 val PER: 0.4834
2025-12-11 00:52:41,252: t15.2023.08.18 val PER: 0.4241
2025-12-11 00:52:41,252: t15.2023.08.20 val PER: 0.4281
2025-12-11 00:52:41,252: t15.2023.08.25 val PER: 0.4383
2025-12-11 00:52:41,252: t15.2023.08.27 val PER: 0.5145
2025-12-11 00:52:41,252: t15.2023.09.01 val PER: 0.3718
2025-12-11 00:52:41,252: t15.2023.09.03 val PER: 0.4359
2025-12-11 00:52:41,252: t15.2023.09.24 val PER: 0.3786
2025-12-11 00:52:41,252: t15.2023.09.29 val PER: 0.4180
2025-12-11 00:52:41,252: t15.2023.10.01 val PER: 0.4947
2025-12-11 00:52:41,252: t15.2023.10.06 val PER: 0.3800
2025-12-11 00:52:41,252: t15.2023.10.08 val PER: 0.5183
2025-12-11 00:52:41,252: t15.2023.10.13 val PER: 0.4717
2025-12-11 00:52:41,253: t15.2023.10.15 val PER: 0.4443
2025-12-11 00:52:41,253: t15.2023.10.20 val PER: 0.4396
2025-12-11 00:52:41,253: t15.2023.10.22 val PER: 0.4443
2025-12-11 00:52:41,253: t15.2023.11.03 val PER: 0.4681
2025-12-11 00:52:41,253: t15.2023.11.04 val PER: 0.3276
2025-12-11 00:52:41,253: t15.2023.11.17 val PER: 0.2395
2025-12-11 00:52:41,253: t15.2023.11.19 val PER: 0.2934
2025-12-11 00:52:41,253: t15.2023.11.26 val PER: 0.5022
2025-12-11 00:52:41,253: t15.2023.12.03 val PER: 0.3960
2025-12-11 00:52:41,253: t15.2023.12.08 val PER: 0.4321
2025-12-11 00:52:41,253: t15.2023.12.10 val PER: 0.3706
2025-12-11 00:52:41,253: t15.2023.12.17 val PER: 0.5374
2025-12-11 00:52:41,253: t15.2023.12.29 val PER: 0.5003
2025-12-11 00:52:41,253: t15.2024.02.25 val PER: 0.4171
2025-12-11 00:52:41,253: t15.2024.03.03 val PER: 1.0000
2025-12-11 00:52:41,253: t15.2024.03.08 val PER: 0.5306
2025-12-11 00:52:41,253: t15.2024.03.15 val PER: 0.4784
2025-12-11 00:52:41,253: t15.2024.03.17 val PER: 0.4533
2025-12-11 00:52:41,253: t15.2024.04.25 val PER: 1.0000
2025-12-11 00:52:41,254: t15.2024.04.28 val PER: 1.0000
2025-12-11 00:52:41,254: t15.2024.05.10 val PER: 0.5527
2025-12-11 00:52:41,254: t15.2024.06.14 val PER: 0.5032
2025-12-11 00:52:41,254: t15.2024.07.19 val PER: 0.6440
2025-12-11 00:52:41,254: t15.2024.07.21 val PER: 0.3862
2025-12-11 00:52:41,254: t15.2024.07.28 val PER: 0.4176
2025-12-11 00:52:41,254: t15.2025.01.10 val PER: 0.5937
2025-12-11 00:52:41,254: t15.2025.01.12 val PER: 0.5089
2025-12-11 00:52:41,254: t15.2025.03.14 val PER: 0.6420
2025-12-11 00:52:41,254: t15.2025.03.16 val PER: 0.5209
2025-12-11 00:52:41,254: t15.2025.03.30 val PER: 0.6368
2025-12-11 00:52:41,254: t15.2025.04.13 val PER: 0.5663
2025-12-11 00:52:41,254: New best test PER 0.4725 --> 0.4663
2025-12-11 00:52:41,254: Checkpointing model
2025-12-11 00:52:41,944: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 00:53:46,528: Train batch 22200: loss: 9.99 grad norm: 75.12 time: 0.355
2025-12-11 00:54:49,530: Train batch 22400: loss: 25.33 grad norm: 92.23 time: 0.332
2025-12-11 00:55:53,707: Train batch 22600: loss: 27.33 grad norm: 95.22 time: 0.308
2025-12-11 00:56:56,633: Train batch 22800: loss: 16.53 grad norm: 203.22 time: 0.292
2025-12-11 00:57:59,072: Train batch 23000: loss: 14.10 grad norm: 114.67 time: 0.369
2025-12-11 00:59:01,187: Train batch 23200: loss: 21.33 grad norm: 123.46 time: 0.264
2025-12-11 01:00:05,071: Train batch 23400: loss: 15.13 grad norm: 70.72 time: 0.265
2025-12-11 01:01:07,036: Train batch 23600: loss: 21.71 grad norm: 250.89 time: 0.379
2025-12-11 01:02:10,382: Train batch 23800: loss: 17.73 grad norm: 105.40 time: 0.240
2025-12-11 01:03:14,371: Train batch 24000: loss: 27.64 grad norm: 87.15 time: 0.301
2025-12-11 01:03:14,371: Running test after training batch: 24000
2025-12-11 01:03:39,641: Val batch 24000: PER (avg): 0.4800 CTC Loss (avg): 59.0571 time: 25.270
2025-12-11 01:03:39,642: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:03:39,642: t15.2023.08.13 val PER: 0.5146
2025-12-11 01:03:39,642: t15.2023.08.18 val PER: 0.4543
2025-12-11 01:03:39,642: t15.2023.08.20 val PER: 0.4313
2025-12-11 01:03:39,642: t15.2023.08.25 val PER: 0.4729
2025-12-11 01:03:39,642: t15.2023.08.27 val PER: 0.5241
2025-12-11 01:03:39,642: t15.2023.09.01 val PER: 0.4261
2025-12-11 01:03:39,642: t15.2023.09.03 val PER: 0.4584
2025-12-11 01:03:39,642: t15.2023.09.24 val PER: 0.4053
2025-12-11 01:03:39,642: t15.2023.09.29 val PER: 0.4608
2025-12-11 01:03:39,642: t15.2023.10.01 val PER: 0.4908
2025-12-11 01:03:39,642: t15.2023.10.06 val PER: 0.3983
2025-12-11 01:03:39,642: t15.2023.10.08 val PER: 0.5277
2025-12-11 01:03:39,642: t15.2023.10.13 val PER: 0.4818
2025-12-11 01:03:39,642: t15.2023.10.15 val PER: 0.4515
2025-12-11 01:03:39,643: t15.2023.10.20 val PER: 0.4899
2025-12-11 01:03:39,643: t15.2023.10.22 val PER: 0.4543
2025-12-11 01:03:39,643: t15.2023.11.03 val PER: 0.4715
2025-12-11 01:03:39,643: t15.2023.11.04 val PER: 0.2935
2025-12-11 01:03:39,643: t15.2023.11.17 val PER: 0.2799
2025-12-11 01:03:39,643: t15.2023.11.19 val PER: 0.3473
2025-12-11 01:03:39,643: t15.2023.11.26 val PER: 0.4891
2025-12-11 01:03:39,643: t15.2023.12.03 val PER: 0.4223
2025-12-11 01:03:39,643: t15.2023.12.08 val PER: 0.4547
2025-12-11 01:03:39,643: t15.2023.12.10 val PER: 0.3666
2025-12-11 01:03:39,643: t15.2023.12.17 val PER: 0.5353
2025-12-11 01:03:39,643: t15.2023.12.29 val PER: 0.5175
2025-12-11 01:03:39,643: t15.2024.02.25 val PER: 0.4466
2025-12-11 01:03:39,643: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:03:39,643: t15.2024.03.08 val PER: 0.5405
2025-12-11 01:03:39,643: t15.2024.03.15 val PER: 0.4747
2025-12-11 01:03:39,643: t15.2024.03.17 val PER: 0.4623
2025-12-11 01:03:39,643: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:03:39,643: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:03:39,644: t15.2024.05.10 val PER: 0.5602
2025-12-11 01:03:39,644: t15.2024.06.14 val PER: 0.4811
2025-12-11 01:03:39,644: t15.2024.07.19 val PER: 0.6546
2025-12-11 01:03:39,644: t15.2024.07.21 val PER: 0.3779
2025-12-11 01:03:39,644: t15.2024.07.28 val PER: 0.4449
2025-12-11 01:03:39,644: t15.2025.01.10 val PER: 0.6088
2025-12-11 01:03:39,644: t15.2025.01.12 val PER: 0.5327
2025-12-11 01:03:39,644: t15.2025.03.14 val PER: 0.6346
2025-12-11 01:03:39,644: t15.2025.03.16 val PER: 0.5445
2025-12-11 01:03:39,644: t15.2025.03.30 val PER: 0.6345
2025-12-11 01:03:39,644: t15.2025.04.13 val PER: 0.5663
2025-12-11 01:04:42,697: Train batch 24200: loss: 27.50 grad norm: 130.10 time: 0.330
2025-12-11 01:05:46,180: Train batch 24400: loss: 25.31 grad norm: 109.92 time: 0.225
2025-12-11 01:06:47,341: Train batch 24600: loss: 23.52 grad norm: 132.53 time: 0.294
2025-12-11 01:07:51,555: Train batch 24800: loss: 21.32 grad norm: 136.94 time: 0.388
2025-12-11 01:08:55,655: Train batch 25000: loss: 28.31 grad norm: 140.15 time: 0.296
2025-12-11 01:09:58,318: Train batch 25200: loss: 8.81 grad norm: 135.32 time: 0.290
2025-12-11 01:11:02,221: Train batch 25400: loss: 22.06 grad norm: 71.25 time: 0.321
2025-12-11 01:12:06,560: Train batch 25600: loss: 27.67 grad norm: 194.29 time: 0.255
2025-12-11 01:13:10,771: Train batch 25800: loss: 15.04 grad norm: 60.35 time: 0.306
2025-12-11 01:14:13,453: Train batch 26000: loss: 15.29 grad norm: 418.39 time: 0.292
2025-12-11 01:14:13,454: Running test after training batch: 26000
2025-12-11 01:14:38,638: Val batch 26000: PER (avg): 0.4720 CTC Loss (avg): 57.7623 time: 25.184
2025-12-11 01:14:38,639: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:14:38,639: t15.2023.08.13 val PER: 0.5073
2025-12-11 01:14:38,639: t15.2023.08.18 val PER: 0.4258
2025-12-11 01:14:38,639: t15.2023.08.20 val PER: 0.4289
2025-12-11 01:14:38,639: t15.2023.08.25 val PER: 0.4623
2025-12-11 01:14:38,639: t15.2023.08.27 val PER: 0.5145
2025-12-11 01:14:38,639: t15.2023.09.01 val PER: 0.4067
2025-12-11 01:14:38,639: t15.2023.09.03 val PER: 0.4252
2025-12-11 01:14:38,639: t15.2023.09.24 val PER: 0.4005
2025-12-11 01:14:38,639: t15.2023.09.29 val PER: 0.4174
2025-12-11 01:14:38,639: t15.2023.10.01 val PER: 0.4947
2025-12-11 01:14:38,639: t15.2023.10.06 val PER: 0.3940
2025-12-11 01:14:38,639: t15.2023.10.08 val PER: 0.5115
2025-12-11 01:14:38,639: t15.2023.10.13 val PER: 0.4577
2025-12-11 01:14:38,639: t15.2023.10.15 val PER: 0.4647
2025-12-11 01:14:38,639: t15.2023.10.20 val PER: 0.4362
2025-12-11 01:14:38,639: t15.2023.10.22 val PER: 0.4733
2025-12-11 01:14:38,640: t15.2023.11.03 val PER: 0.4491
2025-12-11 01:14:38,640: t15.2023.11.04 val PER: 0.2765
2025-12-11 01:14:38,640: t15.2023.11.17 val PER: 0.2753
2025-12-11 01:14:38,640: t15.2023.11.19 val PER: 0.3433
2025-12-11 01:14:38,640: t15.2023.11.26 val PER: 0.4630
2025-12-11 01:14:38,640: t15.2023.12.03 val PER: 0.4202
2025-12-11 01:14:38,640: t15.2023.12.08 val PER: 0.4700
2025-12-11 01:14:38,640: t15.2023.12.10 val PER: 0.3587
2025-12-11 01:14:38,640: t15.2023.12.17 val PER: 0.5478
2025-12-11 01:14:38,640: t15.2023.12.29 val PER: 0.4880
2025-12-11 01:14:38,640: t15.2024.02.25 val PER: 0.4298
2025-12-11 01:14:38,640: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:14:38,640: t15.2024.03.08 val PER: 0.5391
2025-12-11 01:14:38,640: t15.2024.03.15 val PER: 0.4934
2025-12-11 01:14:38,640: t15.2024.03.17 val PER: 0.4749
2025-12-11 01:14:38,640: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:14:38,640: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:14:38,640: t15.2024.05.10 val PER: 0.5156
2025-12-11 01:14:38,640: t15.2024.06.14 val PER: 0.5047
2025-12-11 01:14:38,641: t15.2024.07.19 val PER: 0.6539
2025-12-11 01:14:38,641: t15.2024.07.21 val PER: 0.3717
2025-12-11 01:14:38,641: t15.2024.07.28 val PER: 0.4331
2025-12-11 01:14:38,641: t15.2025.01.10 val PER: 0.6364
2025-12-11 01:14:38,641: t15.2025.01.12 val PER: 0.5104
2025-12-11 01:14:38,641: t15.2025.03.14 val PER: 0.6405
2025-12-11 01:14:38,641: t15.2025.03.16 val PER: 0.5026
2025-12-11 01:14:38,641: t15.2025.03.30 val PER: 0.6425
2025-12-11 01:14:38,641: t15.2025.04.13 val PER: 0.5635
2025-12-11 01:15:41,704: Train batch 26200: loss: 17.94 grad norm: 491.28 time: 0.261
2025-12-11 01:16:46,002: Train batch 26400: loss: 24.46 grad norm: 82.09 time: 0.476
2025-12-11 01:17:49,797: Train batch 26600: loss: 24.31 grad norm: 152.05 time: 0.411
2025-12-11 01:18:52,846: Train batch 26800: loss: 21.20 grad norm: 133.15 time: 0.425
2025-12-11 01:19:56,175: Train batch 27000: loss: 25.84 grad norm: 301.97 time: 0.287
2025-12-11 01:21:00,255: Train batch 27200: loss: 26.95 grad norm: 263.75 time: 0.446
2025-12-11 01:22:02,899: Train batch 27400: loss: 33.08 grad norm: 122.86 time: 0.348
2025-12-11 01:23:06,485: Train batch 27600: loss: 28.98 grad norm: 136.30 time: 0.426
2025-12-11 01:24:08,846: Train batch 27800: loss: 19.40 grad norm: 67.97 time: 0.279
2025-12-11 01:25:12,052: Train batch 28000: loss: 30.07 grad norm: 133.99 time: 0.382
2025-12-11 01:25:12,053: Running test after training batch: 28000
2025-12-11 01:25:37,190: Val batch 28000: PER (avg): 0.4534 CTC Loss (avg): 54.5156 time: 25.137
2025-12-11 01:25:37,190: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:25:37,190: t15.2023.08.13 val PER: 0.4771
2025-12-11 01:25:37,190: t15.2023.08.18 val PER: 0.4082
2025-12-11 01:25:37,190: t15.2023.08.20 val PER: 0.3884
2025-12-11 01:25:37,191: t15.2023.08.25 val PER: 0.4789
2025-12-11 01:25:37,191: t15.2023.08.27 val PER: 0.5161
2025-12-11 01:25:37,191: t15.2023.09.01 val PER: 0.3677
2025-12-11 01:25:37,191: t15.2023.09.03 val PER: 0.4549
2025-12-11 01:25:37,191: t15.2023.09.24 val PER: 0.3786
2025-12-11 01:25:37,191: t15.2023.09.29 val PER: 0.4129
2025-12-11 01:25:37,191: t15.2023.10.01 val PER: 0.4762
2025-12-11 01:25:37,191: t15.2023.10.06 val PER: 0.3875
2025-12-11 01:25:37,191: t15.2023.10.08 val PER: 0.4709
2025-12-11 01:25:37,191: t15.2023.10.13 val PER: 0.4414
2025-12-11 01:25:37,191: t15.2023.10.15 val PER: 0.4450
2025-12-11 01:25:37,191: t15.2023.10.20 val PER: 0.4799
2025-12-11 01:25:37,191: t15.2023.10.22 val PER: 0.4477
2025-12-11 01:25:37,191: t15.2023.11.03 val PER: 0.4600
2025-12-11 01:25:37,191: t15.2023.11.04 val PER: 0.2491
2025-12-11 01:25:37,191: t15.2023.11.17 val PER: 0.2348
2025-12-11 01:25:37,191: t15.2023.11.19 val PER: 0.2874
2025-12-11 01:25:37,191: t15.2023.11.26 val PER: 0.4543
2025-12-11 01:25:37,192: t15.2023.12.03 val PER: 0.3645
2025-12-11 01:25:37,192: t15.2023.12.08 val PER: 0.4128
2025-12-11 01:25:37,192: t15.2023.12.10 val PER: 0.3482
2025-12-11 01:25:37,192: t15.2023.12.17 val PER: 0.5468
2025-12-11 01:25:37,192: t15.2023.12.29 val PER: 0.4907
2025-12-11 01:25:37,192: t15.2024.02.25 val PER: 0.4691
2025-12-11 01:25:37,192: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:25:37,192: t15.2024.03.08 val PER: 0.5149
2025-12-11 01:25:37,192: t15.2024.03.15 val PER: 0.4803
2025-12-11 01:25:37,192: t15.2024.03.17 val PER: 0.4128
2025-12-11 01:25:37,192: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:25:37,192: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:25:37,192: t15.2024.05.10 val PER: 0.5037
2025-12-11 01:25:37,192: t15.2024.06.14 val PER: 0.4464
2025-12-11 01:25:37,192: t15.2024.07.19 val PER: 0.6084
2025-12-11 01:25:37,192: t15.2024.07.21 val PER: 0.3455
2025-12-11 01:25:37,192: t15.2024.07.28 val PER: 0.4294
2025-12-11 01:25:37,192: t15.2025.01.10 val PER: 0.6309
2025-12-11 01:25:37,192: t15.2025.01.12 val PER: 0.4965
2025-12-11 01:25:37,193: t15.2025.03.14 val PER: 0.6346
2025-12-11 01:25:37,193: t15.2025.03.16 val PER: 0.5039
2025-12-11 01:25:37,193: t15.2025.03.30 val PER: 0.6368
2025-12-11 01:25:37,193: t15.2025.04.13 val PER: 0.5378
2025-12-11 01:25:37,193: New best test PER 0.4663 --> 0.4534
2025-12-11 01:25:37,193: Checkpointing model
2025-12-11 01:25:38,394: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 01:26:41,053: Train batch 28200: loss: 31.86 grad norm: 389.41 time: 0.323
2025-12-11 01:27:45,992: Train batch 28400: loss: 25.14 grad norm: 147.29 time: 0.319
2025-12-11 01:28:47,774: Train batch 28600: loss: 29.86 grad norm: 188.60 time: 0.210
2025-12-11 01:29:51,841: Train batch 28800: loss: 27.19 grad norm: 432.84 time: 0.395
2025-12-11 01:30:55,809: Train batch 29000: loss: 22.56 grad norm: 198.15 time: 0.250
2025-12-11 01:31:59,151: Train batch 29200: loss: 29.93 grad norm: 289.54 time: 0.242
2025-12-11 01:33:03,397: Train batch 29400: loss: 30.72 grad norm: 86.88 time: 0.298
2025-12-11 01:34:06,129: Train batch 29600: loss: 24.54 grad norm: 87.03 time: 0.287
2025-12-11 01:35:08,193: Train batch 29800: loss: 18.27 grad norm: 76.88 time: 0.344
2025-12-11 01:36:10,121: Train batch 30000: loss: 20.41 grad norm: 116.76 time: 0.368
2025-12-11 01:36:10,121: Running test after training batch: 30000
2025-12-11 01:36:35,309: Val batch 30000: PER (avg): 0.4712 CTC Loss (avg): 58.8735 time: 25.187
2025-12-11 01:36:35,309: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:36:35,309: t15.2023.08.13 val PER: 0.4761
2025-12-11 01:36:35,309: t15.2023.08.18 val PER: 0.4409
2025-12-11 01:36:35,310: t15.2023.08.20 val PER: 0.4138
2025-12-11 01:36:35,310: t15.2023.08.25 val PER: 0.4940
2025-12-11 01:36:35,310: t15.2023.08.27 val PER: 0.5096
2025-12-11 01:36:35,310: t15.2023.09.01 val PER: 0.3920
2025-12-11 01:36:35,310: t15.2023.09.03 val PER: 0.4181
2025-12-11 01:36:35,310: t15.2023.09.24 val PER: 0.4041
2025-12-11 01:36:35,310: t15.2023.09.29 val PER: 0.4371
2025-12-11 01:36:35,310: t15.2023.10.01 val PER: 0.4954
2025-12-11 01:36:35,310: t15.2023.10.06 val PER: 0.3961
2025-12-11 01:36:35,310: t15.2023.10.08 val PER: 0.5088
2025-12-11 01:36:35,310: t15.2023.10.13 val PER: 0.4492
2025-12-11 01:36:35,310: t15.2023.10.15 val PER: 0.4562
2025-12-11 01:36:35,310: t15.2023.10.20 val PER: 0.4664
2025-12-11 01:36:35,310: t15.2023.10.22 val PER: 0.4209
2025-12-11 01:36:35,310: t15.2023.11.03 val PER: 0.4661
2025-12-11 01:36:35,310: t15.2023.11.04 val PER: 0.3413
2025-12-11 01:36:35,310: t15.2023.11.17 val PER: 0.2955
2025-12-11 01:36:35,310: t15.2023.11.19 val PER: 0.3293
2025-12-11 01:36:35,310: t15.2023.11.26 val PER: 0.4768
2025-12-11 01:36:35,311: t15.2023.12.03 val PER: 0.4013
2025-12-11 01:36:35,311: t15.2023.12.08 val PER: 0.4194
2025-12-11 01:36:35,311: t15.2023.12.10 val PER: 0.3863
2025-12-11 01:36:35,311: t15.2023.12.17 val PER: 0.5624
2025-12-11 01:36:35,311: t15.2023.12.29 val PER: 0.5264
2025-12-11 01:36:35,311: t15.2024.02.25 val PER: 0.4677
2025-12-11 01:36:35,311: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:36:35,311: t15.2024.03.08 val PER: 0.5277
2025-12-11 01:36:35,311: t15.2024.03.15 val PER: 0.4834
2025-12-11 01:36:35,311: t15.2024.03.17 val PER: 0.4582
2025-12-11 01:36:35,311: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:36:35,311: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:36:35,311: t15.2024.05.10 val PER: 0.5290
2025-12-11 01:36:35,311: t15.2024.06.14 val PER: 0.4700
2025-12-11 01:36:35,311: t15.2024.07.19 val PER: 0.6309
2025-12-11 01:36:35,311: t15.2024.07.21 val PER: 0.3724
2025-12-11 01:36:35,311: t15.2024.07.28 val PER: 0.4676
2025-12-11 01:36:35,311: t15.2025.01.10 val PER: 0.6171
2025-12-11 01:36:35,311: t15.2025.01.12 val PER: 0.5150
2025-12-11 01:36:35,312: t15.2025.03.14 val PER: 0.6494
2025-12-11 01:36:35,312: t15.2025.03.16 val PER: 0.5262
2025-12-11 01:36:35,312: t15.2025.03.30 val PER: 0.6241
2025-12-11 01:36:35,312: t15.2025.04.13 val PER: 0.5449
2025-12-11 01:37:37,690: Train batch 30200: loss: 27.47 grad norm: 379.70 time: 0.303
2025-12-11 01:38:41,762: Train batch 30400: loss: 27.36 grad norm: 123.65 time: 0.397
2025-12-11 01:39:43,619: Train batch 30600: loss: 18.41 grad norm: 83.50 time: 0.326
2025-12-11 01:40:46,308: Train batch 30800: loss: 22.28 grad norm: 410.48 time: 0.353
2025-12-11 01:41:50,298: Train batch 31000: loss: 18.94 grad norm: 61.64 time: 0.251
2025-12-11 01:42:55,160: Train batch 31200: loss: 18.72 grad norm: 797.56 time: 0.455
2025-12-11 01:43:58,075: Train batch 31400: loss: 24.22 grad norm: 253.64 time: 0.318
2025-12-11 01:45:01,617: Train batch 31600: loss: 25.20 grad norm: 150.52 time: 0.349
2025-12-11 01:46:04,523: Train batch 31800: loss: 31.93 grad norm: 242.59 time: 0.447
2025-12-11 01:47:08,222: Train batch 32000: loss: 28.58 grad norm: 144.32 time: 0.450
2025-12-11 01:47:08,222: Running test after training batch: 32000
2025-12-11 01:47:33,503: Val batch 32000: PER (avg): 0.4471 CTC Loss (avg): 55.0483 time: 25.281
2025-12-11 01:47:33,504: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:47:33,504: t15.2023.08.13 val PER: 0.4574
2025-12-11 01:47:33,504: t15.2023.08.18 val PER: 0.4166
2025-12-11 01:47:33,504: t15.2023.08.20 val PER: 0.3956
2025-12-11 01:47:33,504: t15.2023.08.25 val PER: 0.4744
2025-12-11 01:47:33,504: t15.2023.08.27 val PER: 0.4936
2025-12-11 01:47:33,504: t15.2023.09.01 val PER: 0.3523
2025-12-11 01:47:33,504: t15.2023.09.03 val PER: 0.4109
2025-12-11 01:47:33,504: t15.2023.09.24 val PER: 0.3617
2025-12-11 01:47:33,504: t15.2023.09.29 val PER: 0.3925
2025-12-11 01:47:33,504: t15.2023.10.01 val PER: 0.4676
2025-12-11 01:47:33,505: t15.2023.10.06 val PER: 0.3638
2025-12-11 01:47:33,505: t15.2023.10.08 val PER: 0.5020
2025-12-11 01:47:33,505: t15.2023.10.13 val PER: 0.4414
2025-12-11 01:47:33,505: t15.2023.10.15 val PER: 0.4179
2025-12-11 01:47:33,505: t15.2023.10.20 val PER: 0.4295
2025-12-11 01:47:33,505: t15.2023.10.22 val PER: 0.4254
2025-12-11 01:47:33,505: t15.2023.11.03 val PER: 0.4464
2025-12-11 01:47:33,505: t15.2023.11.04 val PER: 0.2048
2025-12-11 01:47:33,505: t15.2023.11.17 val PER: 0.2784
2025-12-11 01:47:33,505: t15.2023.11.19 val PER: 0.3214
2025-12-11 01:47:33,505: t15.2023.11.26 val PER: 0.4587
2025-12-11 01:47:33,505: t15.2023.12.03 val PER: 0.3908
2025-12-11 01:47:33,505: t15.2023.12.08 val PER: 0.3995
2025-12-11 01:47:33,505: t15.2023.12.10 val PER: 0.3561
2025-12-11 01:47:33,505: t15.2023.12.17 val PER: 0.5135
2025-12-11 01:47:33,505: t15.2023.12.29 val PER: 0.5024
2025-12-11 01:47:33,505: t15.2024.02.25 val PER: 0.4312
2025-12-11 01:47:33,505: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:47:33,505: t15.2024.03.08 val PER: 0.4936
2025-12-11 01:47:33,506: t15.2024.03.15 val PER: 0.4709
2025-12-11 01:47:33,506: t15.2024.03.17 val PER: 0.4149
2025-12-11 01:47:33,506: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:47:33,506: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:47:33,506: t15.2024.05.10 val PER: 0.4963
2025-12-11 01:47:33,506: t15.2024.06.14 val PER: 0.4811
2025-12-11 01:47:33,506: t15.2024.07.19 val PER: 0.5999
2025-12-11 01:47:33,506: t15.2024.07.21 val PER: 0.3462
2025-12-11 01:47:33,506: t15.2024.07.28 val PER: 0.4279
2025-12-11 01:47:33,506: t15.2025.01.10 val PER: 0.6185
2025-12-11 01:47:33,506: t15.2025.01.12 val PER: 0.5089
2025-12-11 01:47:33,506: t15.2025.03.14 val PER: 0.6287
2025-12-11 01:47:33,506: t15.2025.03.16 val PER: 0.4974
2025-12-11 01:47:33,506: t15.2025.03.30 val PER: 0.6402
2025-12-11 01:47:33,506: t15.2025.04.13 val PER: 0.5121
2025-12-11 01:47:33,506: New best test PER 0.4534 --> 0.4471
2025-12-11 01:47:33,506: Checkpointing model
2025-12-11 01:47:34,775: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 01:48:38,209: Train batch 32200: loss: 33.21 grad norm: 386.85 time: 0.259
2025-12-11 01:49:39,553: Train batch 32400: loss: 31.73 grad norm: 283.88 time: 0.277
2025-12-11 01:50:41,508: Train batch 32600: loss: 14.62 grad norm: 61.00 time: 0.362
2025-12-11 01:51:45,080: Train batch 32800: loss: 34.91 grad norm: 285.36 time: 0.366
2025-12-11 01:52:47,021: Train batch 33000: loss: 18.66 grad norm: 65.31 time: 0.274
2025-12-11 01:53:49,866: Train batch 33200: loss: 16.33 grad norm: 121.23 time: 0.317
2025-12-11 01:54:52,539: Train batch 33400: loss: 22.06 grad norm: 475.45 time: 0.349
2025-12-11 01:55:54,908: Train batch 33600: loss: 20.97 grad norm: 258.35 time: 0.345
2025-12-11 01:56:57,718: Train batch 33800: loss: 19.01 grad norm: 199.63 time: 0.311
2025-12-11 01:58:00,880: Train batch 34000: loss: 22.73 grad norm: 218.30 time: 0.201
2025-12-11 01:58:00,880: Running test after training batch: 34000
2025-12-11 01:58:26,111: Val batch 34000: PER (avg): 0.4586 CTC Loss (avg): 56.3120 time: 25.230
2025-12-11 01:58:26,111: t15.2023.08.11 val PER: 1.0000
2025-12-11 01:58:26,111: t15.2023.08.13 val PER: 0.4678
2025-12-11 01:58:26,111: t15.2023.08.18 val PER: 0.4401
2025-12-11 01:58:26,111: t15.2023.08.20 val PER: 0.4218
2025-12-11 01:58:26,111: t15.2023.08.25 val PER: 0.4578
2025-12-11 01:58:26,111: t15.2023.08.27 val PER: 0.4984
2025-12-11 01:58:26,111: t15.2023.09.01 val PER: 0.3425
2025-12-11 01:58:26,112: t15.2023.09.03 val PER: 0.4133
2025-12-11 01:58:26,112: t15.2023.09.24 val PER: 0.3908
2025-12-11 01:58:26,112: t15.2023.09.29 val PER: 0.4078
2025-12-11 01:58:26,112: t15.2023.10.01 val PER: 0.4690
2025-12-11 01:58:26,112: t15.2023.10.06 val PER: 0.3811
2025-12-11 01:58:26,112: t15.2023.10.08 val PER: 0.5074
2025-12-11 01:58:26,112: t15.2023.10.13 val PER: 0.4608
2025-12-11 01:58:26,112: t15.2023.10.15 val PER: 0.4667
2025-12-11 01:58:26,112: t15.2023.10.20 val PER: 0.5268
2025-12-11 01:58:26,112: t15.2023.10.22 val PER: 0.4633
2025-12-11 01:58:26,112: t15.2023.11.03 val PER: 0.4518
2025-12-11 01:58:26,112: t15.2023.11.04 val PER: 0.2150
2025-12-11 01:58:26,112: t15.2023.11.17 val PER: 0.2659
2025-12-11 01:58:26,112: t15.2023.11.19 val PER: 0.3293
2025-12-11 01:58:26,112: t15.2023.11.26 val PER: 0.4543
2025-12-11 01:58:26,112: t15.2023.12.03 val PER: 0.3782
2025-12-11 01:58:26,112: t15.2023.12.08 val PER: 0.4161
2025-12-11 01:58:26,112: t15.2023.12.10 val PER: 0.3890
2025-12-11 01:58:26,113: t15.2023.12.17 val PER: 0.5052
2025-12-11 01:58:26,113: t15.2023.12.29 val PER: 0.5154
2025-12-11 01:58:26,113: t15.2024.02.25 val PER: 0.4382
2025-12-11 01:58:26,113: t15.2024.03.03 val PER: 1.0000
2025-12-11 01:58:26,113: t15.2024.03.08 val PER: 0.5263
2025-12-11 01:58:26,113: t15.2024.03.15 val PER: 0.4572
2025-12-11 01:58:26,113: t15.2024.03.17 val PER: 0.4372
2025-12-11 01:58:26,113: t15.2024.04.25 val PER: 1.0000
2025-12-11 01:58:26,113: t15.2024.04.28 val PER: 1.0000
2025-12-11 01:58:26,113: t15.2024.05.10 val PER: 0.5022
2025-12-11 01:58:26,113: t15.2024.06.14 val PER: 0.4732
2025-12-11 01:58:26,113: t15.2024.07.19 val PER: 0.6078
2025-12-11 01:58:26,113: t15.2024.07.21 val PER: 0.3745
2025-12-11 01:58:26,113: t15.2024.07.28 val PER: 0.4397
2025-12-11 01:58:26,113: t15.2025.01.10 val PER: 0.6185
2025-12-11 01:58:26,113: t15.2025.01.12 val PER: 0.5212
2025-12-11 01:58:26,113: t15.2025.03.14 val PER: 0.6272
2025-12-11 01:58:26,113: t15.2025.03.16 val PER: 0.5262
2025-12-11 01:58:26,113: t15.2025.03.30 val PER: 0.6126
2025-12-11 01:58:26,114: t15.2025.04.13 val PER: 0.5464
2025-12-11 01:59:29,041: Train batch 34200: loss: 17.59 grad norm: 131.03 time: 0.323
2025-12-11 02:00:32,286: Train batch 34400: loss: 24.86 grad norm: 353.02 time: 0.308
2025-12-11 02:01:36,226: Train batch 34600: loss: 15.23 grad norm: 348.75 time: 0.280
2025-12-11 02:02:40,166: Train batch 34800: loss: 14.15 grad norm: 63.56 time: 0.283
2025-12-11 02:03:41,803: Train batch 35000: loss: 23.98 grad norm: 83.17 time: 0.295
2025-12-11 02:04:43,761: Train batch 35200: loss: 18.92 grad norm: 73.82 time: 0.296
2025-12-11 02:05:44,611: Train batch 35400: loss: 18.48 grad norm: 143.17 time: 0.275
2025-12-11 02:06:47,361: Train batch 35600: loss: 22.10 grad norm: 125.91 time: 0.348
2025-12-11 02:07:50,748: Train batch 35800: loss: 15.01 grad norm: 53.21 time: 0.325
2025-12-11 02:08:53,732: Train batch 36000: loss: 21.95 grad norm: 322.96 time: 0.260
2025-12-11 02:08:53,732: Running test after training batch: 36000
2025-12-11 02:09:18,997: Val batch 36000: PER (avg): 0.4566 CTC Loss (avg): 57.1848 time: 25.265
2025-12-11 02:09:18,997: t15.2023.08.11 val PER: 1.0000
2025-12-11 02:09:18,997: t15.2023.08.13 val PER: 0.4886
2025-12-11 02:09:18,997: t15.2023.08.18 val PER: 0.4241
2025-12-11 02:09:18,997: t15.2023.08.20 val PER: 0.4416
2025-12-11 02:09:18,998: t15.2023.08.25 val PER: 0.4518
2025-12-11 02:09:18,998: t15.2023.08.27 val PER: 0.4968
2025-12-11 02:09:18,998: t15.2023.09.01 val PER: 0.3612
2025-12-11 02:09:18,998: t15.2023.09.03 val PER: 0.3967
2025-12-11 02:09:18,998: t15.2023.09.24 val PER: 0.3689
2025-12-11 02:09:18,998: t15.2023.09.29 val PER: 0.3899
2025-12-11 02:09:18,998: t15.2023.10.01 val PER: 0.4663
2025-12-11 02:09:18,998: t15.2023.10.06 val PER: 0.3789
2025-12-11 02:09:18,998: t15.2023.10.08 val PER: 0.5034
2025-12-11 02:09:18,998: t15.2023.10.13 val PER: 0.4616
2025-12-11 02:09:18,998: t15.2023.10.15 val PER: 0.4463
2025-12-11 02:09:18,998: t15.2023.10.20 val PER: 0.4698
2025-12-11 02:09:18,998: t15.2023.10.22 val PER: 0.4287
2025-12-11 02:09:18,998: t15.2023.11.03 val PER: 0.4559
2025-12-11 02:09:18,998: t15.2023.11.04 val PER: 0.2253
2025-12-11 02:09:18,998: t15.2023.11.17 val PER: 0.2519
2025-12-11 02:09:18,998: t15.2023.11.19 val PER: 0.3114
2025-12-11 02:09:18,998: t15.2023.11.26 val PER: 0.4500
2025-12-11 02:09:18,999: t15.2023.12.03 val PER: 0.3918
2025-12-11 02:09:18,999: t15.2023.12.08 val PER: 0.4141
2025-12-11 02:09:18,999: t15.2023.12.10 val PER: 0.3627
2025-12-11 02:09:18,999: t15.2023.12.17 val PER: 0.4990
2025-12-11 02:09:18,999: t15.2023.12.29 val PER: 0.5093
2025-12-11 02:09:18,999: t15.2024.02.25 val PER: 0.4242
2025-12-11 02:09:18,999: t15.2024.03.03 val PER: 1.0000
2025-12-11 02:09:18,999: t15.2024.03.08 val PER: 0.5220
2025-12-11 02:09:18,999: t15.2024.03.15 val PER: 0.4828
2025-12-11 02:09:18,999: t15.2024.03.17 val PER: 0.4268
2025-12-11 02:09:18,999: t15.2024.04.25 val PER: 1.0000
2025-12-11 02:09:18,999: t15.2024.04.28 val PER: 1.0000
2025-12-11 02:09:18,999: t15.2024.05.10 val PER: 0.5052
2025-12-11 02:09:18,999: t15.2024.06.14 val PER: 0.4858
2025-12-11 02:09:18,999: t15.2024.07.19 val PER: 0.6302
2025-12-11 02:09:18,999: t15.2024.07.21 val PER: 0.3807
2025-12-11 02:09:18,999: t15.2024.07.28 val PER: 0.4265
2025-12-11 02:09:18,999: t15.2025.01.10 val PER: 0.6143
2025-12-11 02:09:18,999: t15.2025.01.12 val PER: 0.5158
2025-12-11 02:09:19,000: t15.2025.03.14 val PER: 0.6612
2025-12-11 02:09:19,000: t15.2025.03.16 val PER: 0.5052
2025-12-11 02:09:19,000: t15.2025.03.30 val PER: 0.6092
2025-12-11 02:09:19,000: t15.2025.04.13 val PER: 0.5692
2025-12-11 02:10:21,865: Train batch 36200: loss: 21.06 grad norm: 182.92 time: 0.259
2025-12-11 02:11:24,155: Train batch 36400: loss: 19.45 grad norm: 366.04 time: 0.243
2025-12-11 02:12:27,454: Train batch 36600: loss: 20.33 grad norm: 241.89 time: 0.240
2025-12-11 02:13:30,621: Train batch 36800: loss: 22.97 grad norm: 72.80 time: 0.271
2025-12-11 02:14:34,085: Train batch 37000: loss: 11.86 grad norm: 70.18 time: 0.331
2025-12-11 02:15:37,604: Train batch 37200: loss: 21.60 grad norm: 287.27 time: 0.294
2025-12-11 02:16:39,617: Train batch 37400: loss: 31.45 grad norm: 121.54 time: 0.229
2025-12-11 02:17:43,071: Train batch 37600: loss: 16.91 grad norm: 92.41 time: 0.264
2025-12-11 02:18:46,852: Train batch 37800: loss: 15.70 grad norm: 118.34 time: 0.339
2025-12-11 02:19:50,309: Train batch 38000: loss: 20.46 grad norm: 243.58 time: 0.256
2025-12-11 02:19:50,310: Running test after training batch: 38000
2025-12-11 02:20:15,598: Val batch 38000: PER (avg): 0.4565 CTC Loss (avg): 57.9502 time: 25.288
2025-12-11 02:20:15,598: t15.2023.08.11 val PER: 1.0000
2025-12-11 02:20:15,599: t15.2023.08.13 val PER: 0.4647
2025-12-11 02:20:15,599: t15.2023.08.18 val PER: 0.4216
2025-12-11 02:20:15,599: t15.2023.08.20 val PER: 0.4043
2025-12-11 02:20:15,599: t15.2023.08.25 val PER: 0.4473
2025-12-11 02:20:15,599: t15.2023.08.27 val PER: 0.4550
2025-12-11 02:20:15,599: t15.2023.09.01 val PER: 0.3669
2025-12-11 02:20:15,599: t15.2023.09.03 val PER: 0.4097
2025-12-11 02:20:15,599: t15.2023.09.24 val PER: 0.3932
2025-12-11 02:20:15,599: t15.2023.09.29 val PER: 0.3854
2025-12-11 02:20:15,599: t15.2023.10.01 val PER: 0.4723
2025-12-11 02:20:15,599: t15.2023.10.06 val PER: 0.3950
2025-12-11 02:20:15,599: t15.2023.10.08 val PER: 0.5156
2025-12-11 02:20:15,599: t15.2023.10.13 val PER: 0.4407
2025-12-11 02:20:15,599: t15.2023.10.15 val PER: 0.4364
2025-12-11 02:20:15,599: t15.2023.10.20 val PER: 0.4765
2025-12-11 02:20:15,600: t15.2023.10.22 val PER: 0.4053
2025-12-11 02:20:15,600: t15.2023.11.03 val PER: 0.4790
2025-12-11 02:20:15,600: t15.2023.11.04 val PER: 0.2048
2025-12-11 02:20:15,600: t15.2023.11.17 val PER: 0.2737
2025-12-11 02:20:15,600: t15.2023.11.19 val PER: 0.3234
2025-12-11 02:20:15,600: t15.2023.11.26 val PER: 0.4399
2025-12-11 02:20:15,600: t15.2023.12.03 val PER: 0.3571
2025-12-11 02:20:15,600: t15.2023.12.08 val PER: 0.3955
2025-12-11 02:20:15,600: t15.2023.12.10 val PER: 0.3968
2025-12-11 02:20:15,600: t15.2023.12.17 val PER: 0.5281
2025-12-11 02:20:15,600: t15.2023.12.29 val PER: 0.5038
2025-12-11 02:20:15,600: t15.2024.02.25 val PER: 0.4466
2025-12-11 02:20:15,600: t15.2024.03.03 val PER: 1.0000
2025-12-11 02:20:15,600: t15.2024.03.08 val PER: 0.5391
2025-12-11 02:20:15,600: t15.2024.03.15 val PER: 0.4765
2025-12-11 02:20:15,600: t15.2024.03.17 val PER: 0.4470
2025-12-11 02:20:15,600: t15.2024.04.25 val PER: 1.0000
2025-12-11 02:20:15,600: t15.2024.04.28 val PER: 1.0000
2025-12-11 02:20:15,600: t15.2024.05.10 val PER: 0.4918
2025-12-11 02:20:15,601: t15.2024.06.14 val PER: 0.4874
2025-12-11 02:20:15,601: t15.2024.07.19 val PER: 0.6249
2025-12-11 02:20:15,601: t15.2024.07.21 val PER: 0.4055
2025-12-11 02:20:15,601: t15.2024.07.28 val PER: 0.4309
2025-12-11 02:20:15,601: t15.2025.01.10 val PER: 0.6143
2025-12-11 02:20:15,601: t15.2025.01.12 val PER: 0.5212
2025-12-11 02:20:15,601: t15.2025.03.14 val PER: 0.6612
2025-12-11 02:20:15,601: t15.2025.03.16 val PER: 0.4961
2025-12-11 02:20:15,601: t15.2025.03.30 val PER: 0.6253
2025-12-11 02:20:15,601: t15.2025.04.13 val PER: 0.5549
2025-12-11 02:21:19,502: Train batch 38200: loss: 16.02 grad norm: 148.31 time: 0.432
2025-12-11 02:22:21,779: Train batch 38400: loss: 27.96 grad norm: 109.76 time: 0.360
2025-12-11 02:23:23,778: Train batch 38600: loss: 14.35 grad norm: 79.03 time: 0.318
2025-12-11 02:24:28,833: Train batch 38800: loss: 22.61 grad norm: 281.19 time: 0.311
2025-12-11 02:25:31,735: Train batch 39000: loss: 6.07 grad norm: 104.07 time: 0.311
2025-12-11 02:26:35,450: Train batch 39200: loss: 11.21 grad norm: 71.45 time: 0.351
2025-12-11 02:27:39,137: Train batch 39400: loss: 18.42 grad norm: 212.47 time: 0.280
2025-12-11 02:28:41,277: Train batch 39600: loss: 18.07 grad norm: 238.42 time: 0.346
2025-12-11 02:29:44,891: Train batch 39800: loss: 18.19 grad norm: 96.61 time: 0.262
2025-12-11 02:30:46,823: Train batch 40000: loss: 17.03 grad norm: 92.51 time: 0.218
2025-12-11 02:30:46,824: Running test after training batch: 40000
2025-12-11 02:31:12,070: Val batch 40000: PER (avg): 0.4574 CTC Loss (avg): 57.3234 time: 25.246
2025-12-11 02:31:12,071: t15.2023.08.11 val PER: 1.0000
2025-12-11 02:31:12,071: t15.2023.08.13 val PER: 0.4688
2025-12-11 02:31:12,071: t15.2023.08.18 val PER: 0.4359
2025-12-11 02:31:12,071: t15.2023.08.20 val PER: 0.4178
2025-12-11 02:31:12,071: t15.2023.08.25 val PER: 0.4669
2025-12-11 02:31:12,071: t15.2023.08.27 val PER: 0.4936
2025-12-11 02:31:12,071: t15.2023.09.01 val PER: 0.3904
2025-12-11 02:31:12,071: t15.2023.09.03 val PER: 0.4074
2025-12-11 02:31:12,071: t15.2023.09.24 val PER: 0.3786
2025-12-11 02:31:12,071: t15.2023.09.29 val PER: 0.4167
2025-12-11 02:31:12,071: t15.2023.10.01 val PER: 0.4716
2025-12-11 02:31:12,071: t15.2023.10.06 val PER: 0.3800
2025-12-11 02:31:12,071: t15.2023.10.08 val PER: 0.5169
2025-12-11 02:31:12,071: t15.2023.10.13 val PER: 0.4391
2025-12-11 02:31:12,071: t15.2023.10.15 val PER: 0.4489
2025-12-11 02:31:12,071: t15.2023.10.20 val PER: 0.4799
2025-12-11 02:31:12,071: t15.2023.10.22 val PER: 0.4343
2025-12-11 02:31:12,071: t15.2023.11.03 val PER: 0.4634
2025-12-11 02:31:12,072: t15.2023.11.04 val PER: 0.2218
2025-12-11 02:31:12,072: t15.2023.11.17 val PER: 0.2504
2025-12-11 02:31:12,072: t15.2023.11.19 val PER: 0.2894
2025-12-11 02:31:12,072: t15.2023.11.26 val PER: 0.4384
2025-12-11 02:31:12,072: t15.2023.12.03 val PER: 0.3603
2025-12-11 02:31:12,072: t15.2023.12.08 val PER: 0.4121
2025-12-11 02:31:12,072: t15.2023.12.10 val PER: 0.3627
2025-12-11 02:31:12,072: t15.2023.12.17 val PER: 0.5094
2025-12-11 02:31:12,072: t15.2023.12.29 val PER: 0.4990
2025-12-11 02:31:12,072: t15.2024.02.25 val PER: 0.4551
2025-12-11 02:31:12,072: t15.2024.03.03 val PER: 1.0000
2025-12-11 02:31:12,072: t15.2024.03.08 val PER: 0.5149
2025-12-11 02:31:12,072: t15.2024.03.15 val PER: 0.4747
2025-12-11 02:31:12,072: t15.2024.03.17 val PER: 0.4498
2025-12-11 02:31:12,072: t15.2024.04.25 val PER: 1.0000
2025-12-11 02:31:12,072: t15.2024.04.28 val PER: 1.0000
2025-12-11 02:31:12,072: t15.2024.05.10 val PER: 0.5438
2025-12-11 02:31:12,072: t15.2024.06.14 val PER: 0.4795
2025-12-11 02:31:12,072: t15.2024.07.19 val PER: 0.6276
2025-12-11 02:31:12,073: t15.2024.07.21 val PER: 0.3710
2025-12-11 02:31:12,073: t15.2024.07.28 val PER: 0.4426
2025-12-11 02:31:12,073: t15.2025.01.10 val PER: 0.6295
2025-12-11 02:31:12,073: t15.2025.01.12 val PER: 0.4865
2025-12-11 02:31:12,073: t15.2025.03.14 val PER: 0.6331
2025-12-11 02:31:12,073: t15.2025.03.16 val PER: 0.4817
2025-12-11 02:31:12,073: t15.2025.03.30 val PER: 0.6471
2025-12-11 02:31:12,073: t15.2025.04.13 val PER: 0.5521
2025-12-11 02:32:15,129: Train batch 40200: loss: 25.80 grad norm: 127.83 time: 0.293
2025-12-11 02:33:19,355: Train batch 40400: loss: 23.58 grad norm: 566.95 time: 0.245
2025-12-11 02:34:22,987: Train batch 40600: loss: 21.39 grad norm: 85.96 time: 0.327
2025-12-11 02:35:26,463: Train batch 40800: loss: 15.03 grad norm: 156.48 time: 0.247
2025-12-11 02:36:29,512: Train batch 41000: loss: 12.48 grad norm: 108.02 time: 0.270
2025-12-11 02:37:33,405: Train batch 41200: loss: 14.31 grad norm: 159.83 time: 0.367
2025-12-11 02:38:35,652: Train batch 41400: loss: 18.13 grad norm: 122.96 time: 0.323
2025-12-11 02:39:39,204: Train batch 41600: loss: 14.69 grad norm: 63.81 time: 0.316
2025-12-11 02:40:39,827: Train batch 41800: loss: 16.61 grad norm: 160.06 time: 0.299
2025-12-11 02:41:40,016: Train batch 42000: loss: 18.84 grad norm: 146.34 time: 0.306
2025-12-11 02:41:40,016: Running test after training batch: 42000
2025-12-11 02:42:05,261: Val batch 42000: PER (avg): 0.4427 CTC Loss (avg): 55.4098 time: 25.244
2025-12-11 02:42:05,261: t15.2023.08.11 val PER: 1.0000
2025-12-11 02:42:05,261: t15.2023.08.13 val PER: 0.4563
2025-12-11 02:42:05,261: t15.2023.08.18 val PER: 0.4132
2025-12-11 02:42:05,261: t15.2023.08.20 val PER: 0.3956
2025-12-11 02:42:05,261: t15.2023.08.25 val PER: 0.4277
2025-12-11 02:42:05,261: t15.2023.08.27 val PER: 0.4839
2025-12-11 02:42:05,261: t15.2023.09.01 val PER: 0.3393
2025-12-11 02:42:05,261: t15.2023.09.03 val PER: 0.3967
2025-12-11 02:42:05,262: t15.2023.09.24 val PER: 0.3811
2025-12-11 02:42:05,262: t15.2023.09.29 val PER: 0.4071
2025-12-11 02:42:05,262: t15.2023.10.01 val PER: 0.4452
2025-12-11 02:42:05,262: t15.2023.10.06 val PER: 0.3445
2025-12-11 02:42:05,262: t15.2023.10.08 val PER: 0.4953
2025-12-11 02:42:05,262: t15.2023.10.13 val PER: 0.4306
2025-12-11 02:42:05,262: t15.2023.10.15 val PER: 0.4232
2025-12-11 02:42:05,262: t15.2023.10.20 val PER: 0.4564
2025-12-11 02:42:05,262: t15.2023.10.22 val PER: 0.4098
2025-12-11 02:42:05,262: t15.2023.11.03 val PER: 0.4593
2025-12-11 02:42:05,262: t15.2023.11.04 val PER: 0.1672
2025-12-11 02:42:05,262: t15.2023.11.17 val PER: 0.2348
2025-12-11 02:42:05,262: t15.2023.11.19 val PER: 0.3194
2025-12-11 02:42:05,262: t15.2023.11.26 val PER: 0.4138
2025-12-11 02:42:05,262: t15.2023.12.03 val PER: 0.3718
2025-12-11 02:42:05,262: t15.2023.12.08 val PER: 0.3675
2025-12-11 02:42:05,263: t15.2023.12.10 val PER: 0.3706
2025-12-11 02:42:05,263: t15.2023.12.17 val PER: 0.5031
2025-12-11 02:42:05,263: t15.2023.12.29 val PER: 0.4969
2025-12-11 02:42:05,263: t15.2024.02.25 val PER: 0.4213
2025-12-11 02:42:05,263: t15.2024.03.03 val PER: 1.0000
2025-12-11 02:42:05,263: t15.2024.03.08 val PER: 0.5249
2025-12-11 02:42:05,263: t15.2024.03.15 val PER: 0.4578
2025-12-11 02:42:05,263: t15.2024.03.17 val PER: 0.4456
2025-12-11 02:42:05,263: t15.2024.04.25 val PER: 1.0000
2025-12-11 02:42:05,263: t15.2024.04.28 val PER: 1.0000
2025-12-11 02:42:05,263: t15.2024.05.10 val PER: 0.5037
2025-12-11 02:42:05,263: t15.2024.06.14 val PER: 0.4842
2025-12-11 02:42:05,263: t15.2024.07.19 val PER: 0.5953
2025-12-11 02:42:05,263: t15.2024.07.21 val PER: 0.3683
2025-12-11 02:42:05,263: t15.2024.07.28 val PER: 0.4147
2025-12-11 02:42:05,263: t15.2025.01.10 val PER: 0.6047
2025-12-11 02:42:05,263: t15.2025.01.12 val PER: 0.4981
2025-12-11 02:42:05,264: t15.2025.03.14 val PER: 0.6553
2025-12-11 02:42:05,264: t15.2025.03.16 val PER: 0.5092
2025-12-11 02:42:05,264: t15.2025.03.30 val PER: 0.6391
2025-12-11 02:42:05,264: t15.2025.04.13 val PER: 0.5435
2025-12-11 02:42:05,264: New best test PER 0.4471 --> 0.4427
2025-12-11 02:42:05,264: Checkpointing model
2025-12-11 02:42:06,504: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 02:43:09,635: Train batch 42200: loss: 10.78 grad norm: 48.62 time: 0.279
2025-12-11 02:44:12,421: Train batch 42400: loss: 18.65 grad norm: 494.42 time: 0.267
2025-12-11 02:45:15,460: Train batch 42600: loss: 20.26 grad norm: 335.34 time: 0.385
2025-12-11 02:46:18,504: Train batch 42800: loss: 7.21 grad norm: 53.89 time: 0.246
2025-12-11 02:47:21,018: Train batch 43000: loss: 12.62 grad norm: 198.00 time: 0.281
2025-12-11 02:48:21,867: Train batch 43200: loss: 14.18 grad norm: 149.19 time: 0.374
2025-12-11 02:49:24,934: Train batch 43400: loss: 23.61 grad norm: 333.44 time: 0.234
2025-12-11 02:50:28,727: Train batch 43600: loss: 22.24 grad norm: 121.49 time: 0.335
2025-12-11 02:51:31,786: Train batch 43800: loss: 13.18 grad norm: 59.49 time: 0.297
2025-12-11 02:52:34,266: Train batch 44000: loss: 12.74 grad norm: 289.73 time: 0.272
2025-12-11 02:52:34,267: Running test after training batch: 44000
2025-12-11 02:52:59,558: Val batch 44000: PER (avg): 0.4494 CTC Loss (avg): 58.1233 time: 25.291
2025-12-11 02:52:59,558: t15.2023.08.11 val PER: 1.0000
2025-12-11 02:52:59,558: t15.2023.08.13 val PER: 0.4511
2025-12-11 02:52:59,558: t15.2023.08.18 val PER: 0.4166
2025-12-11 02:52:59,558: t15.2023.08.20 val PER: 0.4027
2025-12-11 02:52:59,558: t15.2023.08.25 val PER: 0.4729
2025-12-11 02:52:59,558: t15.2023.08.27 val PER: 0.5032
2025-12-11 02:52:59,558: t15.2023.09.01 val PER: 0.3758
2025-12-11 02:52:59,558: t15.2023.09.03 val PER: 0.3955
2025-12-11 02:52:59,558: t15.2023.09.24 val PER: 0.3629
2025-12-11 02:52:59,558: t15.2023.09.29 val PER: 0.3867
2025-12-11 02:52:59,558: t15.2023.10.01 val PER: 0.4531
2025-12-11 02:52:59,559: t15.2023.10.06 val PER: 0.3606
2025-12-11 02:52:59,559: t15.2023.10.08 val PER: 0.4953
2025-12-11 02:52:59,559: t15.2023.10.13 val PER: 0.4407
2025-12-11 02:52:59,559: t15.2023.10.15 val PER: 0.4291
2025-12-11 02:52:59,559: t15.2023.10.20 val PER: 0.4631
2025-12-11 02:52:59,559: t15.2023.10.22 val PER: 0.4131
2025-12-11 02:52:59,559: t15.2023.11.03 val PER: 0.4552
2025-12-11 02:52:59,559: t15.2023.11.04 val PER: 0.2287
2025-12-11 02:52:59,559: t15.2023.11.17 val PER: 0.2551
2025-12-11 02:52:59,559: t15.2023.11.19 val PER: 0.3234
2025-12-11 02:52:59,559: t15.2023.11.26 val PER: 0.4406
2025-12-11 02:52:59,559: t15.2023.12.03 val PER: 0.4128
2025-12-11 02:52:59,559: t15.2023.12.08 val PER: 0.3915
2025-12-11 02:52:59,559: t15.2023.12.10 val PER: 0.3495
2025-12-11 02:52:59,559: t15.2023.12.17 val PER: 0.5301
2025-12-11 02:52:59,559: t15.2023.12.29 val PER: 0.4990
2025-12-11 02:52:59,559: t15.2024.02.25 val PER: 0.4326
2025-12-11 02:52:59,559: t15.2024.03.03 val PER: 1.0000
2025-12-11 02:52:59,559: t15.2024.03.08 val PER: 0.5164
2025-12-11 02:52:59,560: t15.2024.03.15 val PER: 0.4809
2025-12-11 02:52:59,560: t15.2024.03.17 val PER: 0.4310
2025-12-11 02:52:59,560: t15.2024.04.25 val PER: 1.0000
2025-12-11 02:52:59,560: t15.2024.04.28 val PER: 1.0000
2025-12-11 02:52:59,560: t15.2024.05.10 val PER: 0.5141
2025-12-11 02:52:59,560: t15.2024.06.14 val PER: 0.4574
2025-12-11 02:52:59,560: t15.2024.07.19 val PER: 0.6203
2025-12-11 02:52:59,560: t15.2024.07.21 val PER: 0.3759
2025-12-11 02:52:59,560: t15.2024.07.28 val PER: 0.4228
2025-12-11 02:52:59,560: t15.2025.01.10 val PER: 0.6116
2025-12-11 02:52:59,560: t15.2025.01.12 val PER: 0.4834
2025-12-11 02:52:59,560: t15.2025.03.14 val PER: 0.6420
2025-12-11 02:52:59,560: t15.2025.03.16 val PER: 0.4817
2025-12-11 02:52:59,560: t15.2025.03.30 val PER: 0.6379
2025-12-11 02:52:59,560: t15.2025.04.13 val PER: 0.5421
2025-12-11 02:54:03,185: Train batch 44200: loss: 14.62 grad norm: 54.88 time: 0.264
2025-12-11 02:55:06,025: Train batch 44400: loss: 27.34 grad norm: 76.52 time: 0.277
2025-12-11 02:56:09,037: Train batch 44600: loss: 25.79 grad norm: 239.39 time: 0.253
2025-12-11 02:57:12,654: Train batch 44800: loss: 17.77 grad norm: 297.43 time: 0.257
2025-12-11 02:58:14,756: Train batch 45000: loss: 10.62 grad norm: 93.18 time: 0.230
2025-12-11 02:59:18,180: Train batch 45200: loss: 22.96 grad norm: 233.33 time: 0.285
2025-12-11 03:00:21,991: Train batch 45400: loss: 17.30 grad norm: 1769.67 time: 0.413
2025-12-11 03:01:25,768: Train batch 45600: loss: 14.47 grad norm: 66.67 time: 0.246
2025-12-11 03:02:29,610: Train batch 45800: loss: 17.47 grad norm: 170.89 time: 0.340
2025-12-11 03:03:32,656: Train batch 46000: loss: 20.51 grad norm: 188.16 time: 0.301
2025-12-11 03:03:32,656: Running test after training batch: 46000
2025-12-11 03:03:58,014: Val batch 46000: PER (avg): 0.4325 CTC Loss (avg): 53.2021 time: 25.358
2025-12-11 03:03:58,014: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:03:58,014: t15.2023.08.13 val PER: 0.4470
2025-12-11 03:03:58,015: t15.2023.08.18 val PER: 0.4107
2025-12-11 03:03:58,015: t15.2023.08.20 val PER: 0.3685
2025-12-11 03:03:58,015: t15.2023.08.25 val PER: 0.4352
2025-12-11 03:03:58,015: t15.2023.08.27 val PER: 0.4791
2025-12-11 03:03:58,015: t15.2023.09.01 val PER: 0.3653
2025-12-11 03:03:58,015: t15.2023.09.03 val PER: 0.4026
2025-12-11 03:03:58,015: t15.2023.09.24 val PER: 0.3726
2025-12-11 03:03:58,015: t15.2023.09.29 val PER: 0.3689
2025-12-11 03:03:58,015: t15.2023.10.01 val PER: 0.4637
2025-12-11 03:03:58,015: t15.2023.10.06 val PER: 0.3305
2025-12-11 03:03:58,015: t15.2023.10.08 val PER: 0.4763
2025-12-11 03:03:58,015: t15.2023.10.13 val PER: 0.4368
2025-12-11 03:03:58,015: t15.2023.10.15 val PER: 0.4265
2025-12-11 03:03:58,015: t15.2023.10.20 val PER: 0.4497
2025-12-11 03:03:58,015: t15.2023.10.22 val PER: 0.3964
2025-12-11 03:03:58,015: t15.2023.11.03 val PER: 0.4308
2025-12-11 03:03:58,015: t15.2023.11.04 val PER: 0.2628
2025-12-11 03:03:58,015: t15.2023.11.17 val PER: 0.2348
2025-12-11 03:03:58,016: t15.2023.11.19 val PER: 0.2715
2025-12-11 03:03:58,016: t15.2023.11.26 val PER: 0.4174
2025-12-11 03:03:58,016: t15.2023.12.03 val PER: 0.3382
2025-12-11 03:03:58,016: t15.2023.12.08 val PER: 0.3875
2025-12-11 03:03:58,016: t15.2023.12.10 val PER: 0.3351
2025-12-11 03:03:58,016: t15.2023.12.17 val PER: 0.5073
2025-12-11 03:03:58,016: t15.2023.12.29 val PER: 0.4708
2025-12-11 03:03:58,016: t15.2024.02.25 val PER: 0.3961
2025-12-11 03:03:58,016: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:03:58,016: t15.2024.03.08 val PER: 0.5164
2025-12-11 03:03:58,016: t15.2024.03.15 val PER: 0.4422
2025-12-11 03:03:58,016: t15.2024.03.17 val PER: 0.4093
2025-12-11 03:03:58,016: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:03:58,016: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:03:58,016: t15.2024.05.10 val PER: 0.5082
2025-12-11 03:03:58,016: t15.2024.06.14 val PER: 0.4353
2025-12-11 03:03:58,016: t15.2024.07.19 val PER: 0.6131
2025-12-11 03:03:58,016: t15.2024.07.21 val PER: 0.3469
2025-12-11 03:03:58,016: t15.2024.07.28 val PER: 0.4037
2025-12-11 03:03:58,016: t15.2025.01.10 val PER: 0.5923
2025-12-11 03:03:58,017: t15.2025.01.12 val PER: 0.4827
2025-12-11 03:03:58,017: t15.2025.03.14 val PER: 0.6494
2025-12-11 03:03:58,017: t15.2025.03.16 val PER: 0.4751
2025-12-11 03:03:58,017: t15.2025.03.30 val PER: 0.6126
2025-12-11 03:03:58,017: t15.2025.04.13 val PER: 0.5121
2025-12-11 03:03:58,017: New best test PER 0.4427 --> 0.4325
2025-12-11 03:03:58,017: Checkpointing model
2025-12-11 03:03:59,267: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 03:05:02,096: Train batch 46200: loss: 12.46 grad norm: 86.28 time: 0.399
2025-12-11 03:06:06,075: Train batch 46400: loss: 18.89 grad norm: 373.09 time: 0.400
2025-12-11 03:07:09,121: Train batch 46600: loss: 16.66 grad norm: 457.43 time: 0.321
2025-12-11 03:08:12,920: Train batch 46800: loss: 26.57 grad norm: 507.44 time: 0.253
2025-12-11 03:09:16,313: Train batch 47000: loss: 10.07 grad norm: 69.79 time: 0.295
2025-12-11 03:10:18,854: Train batch 47200: loss: 12.26 grad norm: 84.35 time: 0.322
2025-12-11 03:11:21,101: Train batch 47400: loss: 16.29 grad norm: 254.81 time: 0.362
2025-12-11 03:12:23,364: Train batch 47600: loss: 38.85 grad norm: 651.02 time: 0.309
2025-12-11 03:13:27,838: Train batch 47800: loss: 14.78 grad norm: 114.77 time: 0.350
2025-12-11 03:14:31,418: Train batch 48000: loss: 25.26 grad norm: 671.38 time: 0.305
2025-12-11 03:14:31,418: Running test after training batch: 48000
2025-12-11 03:14:56,698: Val batch 48000: PER (avg): 0.4339 CTC Loss (avg): 54.4713 time: 25.280
2025-12-11 03:14:56,699: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:14:56,699: t15.2023.08.13 val PER: 0.4678
2025-12-11 03:14:56,699: t15.2023.08.18 val PER: 0.3923
2025-12-11 03:14:56,699: t15.2023.08.20 val PER: 0.3725
2025-12-11 03:14:56,699: t15.2023.08.25 val PER: 0.4307
2025-12-11 03:14:56,699: t15.2023.08.27 val PER: 0.4662
2025-12-11 03:14:56,699: t15.2023.09.01 val PER: 0.3580
2025-12-11 03:14:56,699: t15.2023.09.03 val PER: 0.3872
2025-12-11 03:14:56,699: t15.2023.09.24 val PER: 0.3544
2025-12-11 03:14:56,699: t15.2023.09.29 val PER: 0.3759
2025-12-11 03:14:56,699: t15.2023.10.01 val PER: 0.4399
2025-12-11 03:14:56,699: t15.2023.10.06 val PER: 0.3628
2025-12-11 03:14:56,700: t15.2023.10.08 val PER: 0.4993
2025-12-11 03:14:56,700: t15.2023.10.13 val PER: 0.4313
2025-12-11 03:14:56,700: t15.2023.10.15 val PER: 0.4061
2025-12-11 03:14:56,700: t15.2023.10.20 val PER: 0.4463
2025-12-11 03:14:56,700: t15.2023.10.22 val PER: 0.3764
2025-12-11 03:14:56,700: t15.2023.11.03 val PER: 0.4172
2025-12-11 03:14:56,700: t15.2023.11.04 val PER: 0.2184
2025-12-11 03:14:56,700: t15.2023.11.17 val PER: 0.2317
2025-12-11 03:14:56,700: t15.2023.11.19 val PER: 0.3114
2025-12-11 03:14:56,700: t15.2023.11.26 val PER: 0.4543
2025-12-11 03:14:56,700: t15.2023.12.03 val PER: 0.3687
2025-12-11 03:14:56,700: t15.2023.12.08 val PER: 0.3762
2025-12-11 03:14:56,700: t15.2023.12.10 val PER: 0.3259
2025-12-11 03:14:56,700: t15.2023.12.17 val PER: 0.4906
2025-12-11 03:14:56,700: t15.2023.12.29 val PER: 0.4626
2025-12-11 03:14:56,700: t15.2024.02.25 val PER: 0.4115
2025-12-11 03:14:56,700: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:14:56,700: t15.2024.03.08 val PER: 0.5192
2025-12-11 03:14:56,700: t15.2024.03.15 val PER: 0.4909
2025-12-11 03:14:56,701: t15.2024.03.17 val PER: 0.4331
2025-12-11 03:14:56,701: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:14:56,701: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:14:56,701: t15.2024.05.10 val PER: 0.5379
2025-12-11 03:14:56,701: t15.2024.06.14 val PER: 0.4543
2025-12-11 03:14:56,701: t15.2024.07.19 val PER: 0.6045
2025-12-11 03:14:56,701: t15.2024.07.21 val PER: 0.3421
2025-12-11 03:14:56,701: t15.2024.07.28 val PER: 0.3882
2025-12-11 03:14:56,701: t15.2025.01.10 val PER: 0.6006
2025-12-11 03:14:56,701: t15.2025.01.12 val PER: 0.4611
2025-12-11 03:14:56,701: t15.2025.03.14 val PER: 0.6420
2025-12-11 03:14:56,701: t15.2025.03.16 val PER: 0.4856
2025-12-11 03:14:56,701: t15.2025.03.30 val PER: 0.6115
2025-12-11 03:14:56,701: t15.2025.04.13 val PER: 0.5492
2025-12-11 03:16:00,220: Train batch 48200: loss: 18.96 grad norm: 168.37 time: 0.305
2025-12-11 03:17:03,222: Train batch 48400: loss: 15.72 grad norm: 94.92 time: 0.273
2025-12-11 03:18:04,249: Train batch 48600: loss: 14.16 grad norm: 179.26 time: 0.449
2025-12-11 03:19:03,997: Train batch 48800: loss: 6.95 grad norm: 263.55 time: 0.284
2025-12-11 03:20:03,042: Train batch 49000: loss: 19.51 grad norm: 217.38 time: 0.288
2025-12-11 03:21:02,255: Train batch 49200: loss: 14.31 grad norm: 118.73 time: 0.245
2025-12-11 03:22:01,414: Train batch 49400: loss: 22.78 grad norm: 111.95 time: 0.326
2025-12-11 03:23:00,520: Train batch 49600: loss: 16.95 grad norm: 66.88 time: 0.372
2025-12-11 03:24:03,444: Train batch 49800: loss: 22.72 grad norm: 92.09 time: 0.253
2025-12-11 03:25:05,854: Train batch 50000: loss: 14.72 grad norm: 574.69 time: 0.256
2025-12-11 03:25:05,855: Running test after training batch: 50000
2025-12-11 03:25:31,328: Val batch 50000: PER (avg): 0.4368 CTC Loss (avg): 53.1562 time: 25.473
2025-12-11 03:25:31,328: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:25:31,328: t15.2023.08.13 val PER: 0.4459
2025-12-11 03:25:31,328: t15.2023.08.18 val PER: 0.4040
2025-12-11 03:25:31,328: t15.2023.08.20 val PER: 0.3916
2025-12-11 03:25:31,329: t15.2023.08.25 val PER: 0.4367
2025-12-11 03:25:31,329: t15.2023.08.27 val PER: 0.4871
2025-12-11 03:25:31,329: t15.2023.09.01 val PER: 0.3580
2025-12-11 03:25:31,329: t15.2023.09.03 val PER: 0.4109
2025-12-11 03:25:31,329: t15.2023.09.24 val PER: 0.3604
2025-12-11 03:25:31,329: t15.2023.09.29 val PER: 0.3867
2025-12-11 03:25:31,329: t15.2023.10.01 val PER: 0.4379
2025-12-11 03:25:31,329: t15.2023.10.06 val PER: 0.3262
2025-12-11 03:25:31,329: t15.2023.10.08 val PER: 0.4926
2025-12-11 03:25:31,329: t15.2023.10.13 val PER: 0.4236
2025-12-11 03:25:31,329: t15.2023.10.15 val PER: 0.4120
2025-12-11 03:25:31,329: t15.2023.10.20 val PER: 0.4933
2025-12-11 03:25:31,329: t15.2023.10.22 val PER: 0.3653
2025-12-11 03:25:31,329: t15.2023.11.03 val PER: 0.4281
2025-12-11 03:25:31,329: t15.2023.11.04 val PER: 0.2321
2025-12-11 03:25:31,329: t15.2023.11.17 val PER: 0.2286
2025-12-11 03:25:31,329: t15.2023.11.19 val PER: 0.3034
2025-12-11 03:25:31,329: t15.2023.11.26 val PER: 0.4094
2025-12-11 03:25:31,330: t15.2023.12.03 val PER: 0.3319
2025-12-11 03:25:31,330: t15.2023.12.08 val PER: 0.3675
2025-12-11 03:25:31,330: t15.2023.12.10 val PER: 0.3745
2025-12-11 03:25:31,330: t15.2023.12.17 val PER: 0.4906
2025-12-11 03:25:31,330: t15.2023.12.29 val PER: 0.4811
2025-12-11 03:25:31,330: t15.2024.02.25 val PER: 0.4579
2025-12-11 03:25:31,330: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:25:31,330: t15.2024.03.08 val PER: 0.5277
2025-12-11 03:25:31,330: t15.2024.03.15 val PER: 0.4590
2025-12-11 03:25:31,330: t15.2024.03.17 val PER: 0.4456
2025-12-11 03:25:31,330: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:25:31,330: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:25:31,330: t15.2024.05.10 val PER: 0.5156
2025-12-11 03:25:31,330: t15.2024.06.14 val PER: 0.4685
2025-12-11 03:25:31,330: t15.2024.07.19 val PER: 0.5939
2025-12-11 03:25:31,330: t15.2024.07.21 val PER: 0.3759
2025-12-11 03:25:31,330: t15.2024.07.28 val PER: 0.4191
2025-12-11 03:25:31,330: t15.2025.01.10 val PER: 0.6061
2025-12-11 03:25:31,330: t15.2025.01.12 val PER: 0.4804
2025-12-11 03:25:31,331: t15.2025.03.14 val PER: 0.6361
2025-12-11 03:25:31,331: t15.2025.03.16 val PER: 0.5144
2025-12-11 03:25:31,331: t15.2025.03.30 val PER: 0.6023
2025-12-11 03:25:31,331: t15.2025.04.13 val PER: 0.5521
2025-12-11 03:26:33,017: Train batch 50200: loss: 11.83 grad norm: 146.67 time: 0.425
2025-12-11 03:27:35,878: Train batch 50400: loss: 20.29 grad norm: 351.73 time: 0.300
2025-12-11 03:28:40,018: Train batch 50600: loss: 19.09 grad norm: 301.45 time: 0.318
2025-12-11 03:29:43,991: Train batch 50800: loss: 20.42 grad norm: 156.69 time: 0.310
2025-12-11 03:30:45,951: Train batch 51000: loss: 13.84 grad norm: 139.79 time: 0.308
2025-12-11 03:31:48,765: Train batch 51200: loss: 18.80 grad norm: 508.39 time: 0.314
2025-12-11 03:32:52,773: Train batch 51400: loss: 16.06 grad norm: 303.64 time: 0.265
2025-12-11 03:33:55,054: Train batch 51600: loss: 18.24 grad norm: 207.24 time: 0.297
2025-12-11 03:34:58,792: Train batch 51800: loss: 18.73 grad norm: 295.95 time: 0.295
2025-12-11 03:35:59,053: Train batch 52000: loss: 23.02 grad norm: 379.38 time: 0.372
2025-12-11 03:35:59,054: Running test after training batch: 52000
2025-12-11 03:36:24,222: Val batch 52000: PER (avg): 0.4570 CTC Loss (avg): 59.6462 time: 25.168
2025-12-11 03:36:24,222: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:36:24,222: t15.2023.08.13 val PER: 0.4678
2025-12-11 03:36:24,222: t15.2023.08.18 val PER: 0.4216
2025-12-11 03:36:24,222: t15.2023.08.20 val PER: 0.4059
2025-12-11 03:36:24,222: t15.2023.08.25 val PER: 0.4654
2025-12-11 03:36:24,222: t15.2023.08.27 val PER: 0.5257
2025-12-11 03:36:24,223: t15.2023.09.01 val PER: 0.3612
2025-12-11 03:36:24,223: t15.2023.09.03 val PER: 0.4204
2025-12-11 03:36:24,223: t15.2023.09.24 val PER: 0.3665
2025-12-11 03:36:24,223: t15.2023.09.29 val PER: 0.3912
2025-12-11 03:36:24,223: t15.2023.10.01 val PER: 0.4729
2025-12-11 03:36:24,223: t15.2023.10.06 val PER: 0.3832
2025-12-11 03:36:24,223: t15.2023.10.08 val PER: 0.5142
2025-12-11 03:36:24,223: t15.2023.10.13 val PER: 0.4430
2025-12-11 03:36:24,223: t15.2023.10.15 val PER: 0.4390
2025-12-11 03:36:24,223: t15.2023.10.20 val PER: 0.4765
2025-12-11 03:36:24,223: t15.2023.10.22 val PER: 0.4098
2025-12-11 03:36:24,223: t15.2023.11.03 val PER: 0.4735
2025-12-11 03:36:24,223: t15.2023.11.04 val PER: 0.2116
2025-12-11 03:36:24,223: t15.2023.11.17 val PER: 0.2395
2025-12-11 03:36:24,223: t15.2023.11.19 val PER: 0.3453
2025-12-11 03:36:24,223: t15.2023.11.26 val PER: 0.4580
2025-12-11 03:36:24,223: t15.2023.12.03 val PER: 0.3666
2025-12-11 03:36:24,223: t15.2023.12.08 val PER: 0.4241
2025-12-11 03:36:24,223: t15.2023.12.10 val PER: 0.3968
2025-12-11 03:36:24,224: t15.2023.12.17 val PER: 0.5229
2025-12-11 03:36:24,224: t15.2023.12.29 val PER: 0.4997
2025-12-11 03:36:24,224: t15.2024.02.25 val PER: 0.4621
2025-12-11 03:36:24,224: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:36:24,224: t15.2024.03.08 val PER: 0.5320
2025-12-11 03:36:24,224: t15.2024.03.15 val PER: 0.4828
2025-12-11 03:36:24,224: t15.2024.03.17 val PER: 0.4400
2025-12-11 03:36:24,224: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:36:24,224: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:36:24,224: t15.2024.05.10 val PER: 0.5319
2025-12-11 03:36:24,224: t15.2024.06.14 val PER: 0.4448
2025-12-11 03:36:24,224: t15.2024.07.19 val PER: 0.6309
2025-12-11 03:36:24,224: t15.2024.07.21 val PER: 0.3703
2025-12-11 03:36:24,224: t15.2024.07.28 val PER: 0.4324
2025-12-11 03:36:24,224: t15.2025.01.10 val PER: 0.6253
2025-12-11 03:36:24,224: t15.2025.01.12 val PER: 0.4773
2025-12-11 03:36:24,224: t15.2025.03.14 val PER: 0.6450
2025-12-11 03:36:24,224: t15.2025.03.16 val PER: 0.5183
2025-12-11 03:36:24,224: t15.2025.03.30 val PER: 0.6460
2025-12-11 03:36:24,225: t15.2025.04.13 val PER: 0.5278
2025-12-11 03:37:23,655: Train batch 52200: loss: 12.86 grad norm: 60.82 time: 0.204
2025-12-11 03:38:26,145: Train batch 52400: loss: 13.50 grad norm: 65.96 time: 0.304
2025-12-11 03:39:30,953: Train batch 52600: loss: 16.34 grad norm: 106.68 time: 0.344
2025-12-11 03:40:34,746: Train batch 52800: loss: 20.40 grad norm: 197.20 time: 0.334
2025-12-11 03:41:38,295: Train batch 53000: loss: 17.04 grad norm: 98.24 time: 0.338
2025-12-11 03:42:41,762: Train batch 53200: loss: 22.01 grad norm: 115.98 time: 0.433
2025-12-11 03:43:45,071: Train batch 53400: loss: 12.77 grad norm: 374.69 time: 0.312
2025-12-11 03:44:48,724: Train batch 53600: loss: 9.52 grad norm: 207.54 time: 0.431
2025-12-11 03:45:52,644: Train batch 53800: loss: 16.42 grad norm: 277.42 time: 0.294
2025-12-11 03:46:53,855: Train batch 54000: loss: 11.38 grad norm: 108.15 time: 0.262
2025-12-11 03:46:53,855: Running test after training batch: 54000
2025-12-11 03:47:19,146: Val batch 54000: PER (avg): 0.4225 CTC Loss (avg): 54.5721 time: 25.291
2025-12-11 03:47:19,147: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:47:19,147: t15.2023.08.13 val PER: 0.4200
2025-12-11 03:47:19,147: t15.2023.08.18 val PER: 0.3847
2025-12-11 03:47:19,147: t15.2023.08.20 val PER: 0.3678
2025-12-11 03:47:19,147: t15.2023.08.25 val PER: 0.4398
2025-12-11 03:47:19,147: t15.2023.08.27 val PER: 0.4807
2025-12-11 03:47:19,147: t15.2023.09.01 val PER: 0.3523
2025-12-11 03:47:19,147: t15.2023.09.03 val PER: 0.3824
2025-12-11 03:47:19,147: t15.2023.09.24 val PER: 0.3507
2025-12-11 03:47:19,147: t15.2023.09.29 val PER: 0.3720
2025-12-11 03:47:19,147: t15.2023.10.01 val PER: 0.4267
2025-12-11 03:47:19,147: t15.2023.10.06 val PER: 0.3563
2025-12-11 03:47:19,147: t15.2023.10.08 val PER: 0.5034
2025-12-11 03:47:19,147: t15.2023.10.13 val PER: 0.4189
2025-12-11 03:47:19,147: t15.2023.10.15 val PER: 0.3909
2025-12-11 03:47:19,147: t15.2023.10.20 val PER: 0.4362
2025-12-11 03:47:19,147: t15.2023.10.22 val PER: 0.3764
2025-12-11 03:47:19,147: t15.2023.11.03 val PER: 0.4233
2025-12-11 03:47:19,148: t15.2023.11.04 val PER: 0.1945
2025-12-11 03:47:19,148: t15.2023.11.17 val PER: 0.2146
2025-12-11 03:47:19,148: t15.2023.11.19 val PER: 0.2774
2025-12-11 03:47:19,148: t15.2023.11.26 val PER: 0.3841
2025-12-11 03:47:19,148: t15.2023.12.03 val PER: 0.3655
2025-12-11 03:47:19,148: t15.2023.12.08 val PER: 0.3589
2025-12-11 03:47:19,148: t15.2023.12.10 val PER: 0.3311
2025-12-11 03:47:19,148: t15.2023.12.17 val PER: 0.4802
2025-12-11 03:47:19,148: t15.2023.12.29 val PER: 0.4770
2025-12-11 03:47:19,148: t15.2024.02.25 val PER: 0.3876
2025-12-11 03:47:19,148: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:47:19,148: t15.2024.03.08 val PER: 0.4922
2025-12-11 03:47:19,148: t15.2024.03.15 val PER: 0.4497
2025-12-11 03:47:19,148: t15.2024.03.17 val PER: 0.4066
2025-12-11 03:47:19,148: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:47:19,148: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:47:19,148: t15.2024.05.10 val PER: 0.5067
2025-12-11 03:47:19,148: t15.2024.06.14 val PER: 0.4543
2025-12-11 03:47:19,148: t15.2024.07.19 val PER: 0.5939
2025-12-11 03:47:19,149: t15.2024.07.21 val PER: 0.3586
2025-12-11 03:47:19,149: t15.2024.07.28 val PER: 0.3890
2025-12-11 03:47:19,149: t15.2025.01.10 val PER: 0.6047
2025-12-11 03:47:19,149: t15.2025.01.12 val PER: 0.4403
2025-12-11 03:47:19,149: t15.2025.03.14 val PER: 0.6391
2025-12-11 03:47:19,149: t15.2025.03.16 val PER: 0.4830
2025-12-11 03:47:19,149: t15.2025.03.30 val PER: 0.6092
2025-12-11 03:47:19,149: t15.2025.04.13 val PER: 0.5178
2025-12-11 03:47:19,149: New best test PER 0.4325 --> 0.4225
2025-12-11 03:47:19,149: Checkpointing model
2025-12-11 03:47:20,323: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 03:48:23,223: Train batch 54200: loss: 21.14 grad norm: 179.37 time: 0.325
2025-12-11 03:49:25,897: Train batch 54400: loss: 13.68 grad norm: 57.38 time: 0.294
2025-12-11 03:50:29,689: Train batch 54600: loss: 13.02 grad norm: 61.14 time: 0.401
2025-12-11 03:51:32,481: Train batch 54800: loss: 8.15 grad norm: 383.83 time: 0.302
2025-12-11 03:52:36,043: Train batch 55000: loss: 30.92 grad norm: 267.68 time: 0.446
2025-12-11 03:53:39,067: Train batch 55200: loss: 11.21 grad norm: 267.09 time: 0.330
2025-12-11 03:54:42,462: Train batch 55400: loss: 20.50 grad norm: 119.40 time: 0.286
2025-12-11 03:55:45,352: Train batch 55600: loss: 15.05 grad norm: 117.10 time: 0.234
2025-12-11 03:56:48,622: Train batch 55800: loss: 22.04 grad norm: 245.23 time: 0.258
2025-12-11 03:57:53,096: Train batch 56000: loss: 15.53 grad norm: 57.53 time: 0.437
2025-12-11 03:57:53,096: Running test after training batch: 56000
2025-12-11 03:58:18,396: Val batch 56000: PER (avg): 0.4394 CTC Loss (avg): 55.5780 time: 25.300
2025-12-11 03:58:18,397: t15.2023.08.11 val PER: 1.0000
2025-12-11 03:58:18,397: t15.2023.08.13 val PER: 0.4137
2025-12-11 03:58:18,397: t15.2023.08.18 val PER: 0.4023
2025-12-11 03:58:18,397: t15.2023.08.20 val PER: 0.3852
2025-12-11 03:58:18,397: t15.2023.08.25 val PER: 0.4518
2025-12-11 03:58:18,397: t15.2023.08.27 val PER: 0.4759
2025-12-11 03:58:18,397: t15.2023.09.01 val PER: 0.3547
2025-12-11 03:58:18,397: t15.2023.09.03 val PER: 0.4038
2025-12-11 03:58:18,397: t15.2023.09.24 val PER: 0.3374
2025-12-11 03:58:18,397: t15.2023.09.29 val PER: 0.3867
2025-12-11 03:58:18,397: t15.2023.10.01 val PER: 0.4452
2025-12-11 03:58:18,397: t15.2023.10.06 val PER: 0.3638
2025-12-11 03:58:18,397: t15.2023.10.08 val PER: 0.4980
2025-12-11 03:58:18,398: t15.2023.10.13 val PER: 0.4205
2025-12-11 03:58:18,398: t15.2023.10.15 val PER: 0.4305
2025-12-11 03:58:18,398: t15.2023.10.20 val PER: 0.4530
2025-12-11 03:58:18,398: t15.2023.10.22 val PER: 0.3820
2025-12-11 03:58:18,398: t15.2023.11.03 val PER: 0.4240
2025-12-11 03:58:18,398: t15.2023.11.04 val PER: 0.2184
2025-12-11 03:58:18,398: t15.2023.11.17 val PER: 0.2426
2025-12-11 03:58:18,398: t15.2023.11.19 val PER: 0.2954
2025-12-11 03:58:18,398: t15.2023.11.26 val PER: 0.4159
2025-12-11 03:58:18,398: t15.2023.12.03 val PER: 0.3445
2025-12-11 03:58:18,398: t15.2023.12.08 val PER: 0.4048
2025-12-11 03:58:18,398: t15.2023.12.10 val PER: 0.3653
2025-12-11 03:58:18,398: t15.2023.12.17 val PER: 0.4802
2025-12-11 03:58:18,398: t15.2023.12.29 val PER: 0.4949
2025-12-11 03:58:18,398: t15.2024.02.25 val PER: 0.4480
2025-12-11 03:58:18,398: t15.2024.03.03 val PER: 1.0000
2025-12-11 03:58:18,398: t15.2024.03.08 val PER: 0.4936
2025-12-11 03:58:18,398: t15.2024.03.15 val PER: 0.4659
2025-12-11 03:58:18,398: t15.2024.03.17 val PER: 0.4351
2025-12-11 03:58:18,399: t15.2024.04.25 val PER: 1.0000
2025-12-11 03:58:18,399: t15.2024.04.28 val PER: 1.0000
2025-12-11 03:58:18,399: t15.2024.05.10 val PER: 0.5260
2025-12-11 03:58:18,399: t15.2024.06.14 val PER: 0.4732
2025-12-11 03:58:18,399: t15.2024.07.19 val PER: 0.6058
2025-12-11 03:58:18,399: t15.2024.07.21 val PER: 0.3683
2025-12-11 03:58:18,399: t15.2024.07.28 val PER: 0.3949
2025-12-11 03:58:18,399: t15.2025.01.10 val PER: 0.6116
2025-12-11 03:58:18,399: t15.2025.01.12 val PER: 0.4865
2025-12-11 03:58:18,399: t15.2025.03.14 val PER: 0.6346
2025-12-11 03:58:18,399: t15.2025.03.16 val PER: 0.5484
2025-12-11 03:58:18,399: t15.2025.03.30 val PER: 0.6402
2025-12-11 03:58:18,399: t15.2025.04.13 val PER: 0.5549
2025-12-11 03:59:20,939: Train batch 56200: loss: 16.41 grad norm: 244.83 time: 0.361
2025-12-11 04:00:24,922: Train batch 56400: loss: 18.43 grad norm: 328.96 time: 0.285
2025-12-11 04:01:28,210: Train batch 56600: loss: 17.82 grad norm: 490.46 time: 0.342
2025-12-11 04:02:31,336: Train batch 56800: loss: 11.06 grad norm: 67.53 time: 0.311
2025-12-11 04:03:34,348: Train batch 57000: loss: 13.47 grad norm: 158.90 time: 0.301
2025-12-11 04:04:36,514: Train batch 57200: loss: 16.82 grad norm: 69.34 time: 0.259
2025-12-11 04:05:39,412: Train batch 57400: loss: 16.74 grad norm: 81.02 time: 0.332
2025-12-11 04:06:42,493: Train batch 57600: loss: 15.50 grad norm: 93.58 time: 0.280
2025-12-11 04:07:46,745: Train batch 57800: loss: 10.34 grad norm: 196.79 time: 0.322
2025-12-11 04:08:49,326: Train batch 58000: loss: 12.69 grad norm: 134.74 time: 0.268
2025-12-11 04:08:49,327: Running test after training batch: 58000
2025-12-11 04:09:14,636: Val batch 58000: PER (avg): 0.4315 CTC Loss (avg): 56.3697 time: 25.309
2025-12-11 04:09:14,636: t15.2023.08.11 val PER: 1.0000
2025-12-11 04:09:14,636: t15.2023.08.13 val PER: 0.4293
2025-12-11 04:09:14,636: t15.2023.08.18 val PER: 0.4007
2025-12-11 04:09:14,637: t15.2023.08.20 val PER: 0.3511
2025-12-11 04:09:14,637: t15.2023.08.25 val PER: 0.4458
2025-12-11 04:09:14,637: t15.2023.08.27 val PER: 0.4968
2025-12-11 04:09:14,637: t15.2023.09.01 val PER: 0.3661
2025-12-11 04:09:14,637: t15.2023.09.03 val PER: 0.4133
2025-12-11 04:09:14,637: t15.2023.09.24 val PER: 0.3495
2025-12-11 04:09:14,637: t15.2023.09.29 val PER: 0.3918
2025-12-11 04:09:14,637: t15.2023.10.01 val PER: 0.4425
2025-12-11 04:09:14,637: t15.2023.10.06 val PER: 0.3767
2025-12-11 04:09:14,637: t15.2023.10.08 val PER: 0.4790
2025-12-11 04:09:14,637: t15.2023.10.13 val PER: 0.4228
2025-12-11 04:09:14,637: t15.2023.10.15 val PER: 0.4212
2025-12-11 04:09:14,637: t15.2023.10.20 val PER: 0.4564
2025-12-11 04:09:14,637: t15.2023.10.22 val PER: 0.3630
2025-12-11 04:09:14,637: t15.2023.11.03 val PER: 0.4098
2025-12-11 04:09:14,637: t15.2023.11.04 val PER: 0.2457
2025-12-11 04:09:14,638: t15.2023.11.17 val PER: 0.2053
2025-12-11 04:09:14,638: t15.2023.11.19 val PER: 0.2974
2025-12-11 04:09:14,638: t15.2023.11.26 val PER: 0.4051
2025-12-11 04:09:14,638: t15.2023.12.03 val PER: 0.3676
2025-12-11 04:09:14,638: t15.2023.12.08 val PER: 0.3788
2025-12-11 04:09:14,638: t15.2023.12.10 val PER: 0.3233
2025-12-11 04:09:14,638: t15.2023.12.17 val PER: 0.4667
2025-12-11 04:09:14,638: t15.2023.12.29 val PER: 0.4784
2025-12-11 04:09:14,638: t15.2024.02.25 val PER: 0.4003
2025-12-11 04:09:14,638: t15.2024.03.03 val PER: 1.0000
2025-12-11 04:09:14,638: t15.2024.03.08 val PER: 0.4964
2025-12-11 04:09:14,638: t15.2024.03.15 val PER: 0.4603
2025-12-11 04:09:14,638: t15.2024.03.17 val PER: 0.4261
2025-12-11 04:09:14,638: t15.2024.04.25 val PER: 1.0000
2025-12-11 04:09:14,638: t15.2024.04.28 val PER: 1.0000
2025-12-11 04:09:14,638: t15.2024.05.10 val PER: 0.5097
2025-12-11 04:09:14,638: t15.2024.06.14 val PER: 0.4543
2025-12-11 04:09:14,639: t15.2024.07.19 val PER: 0.6229
2025-12-11 04:09:14,639: t15.2024.07.21 val PER: 0.3421
2025-12-11 04:09:14,639: t15.2024.07.28 val PER: 0.4000
2025-12-11 04:09:14,639: t15.2025.01.10 val PER: 0.5964
2025-12-11 04:09:14,639: t15.2025.01.12 val PER: 0.4511
2025-12-11 04:09:14,639: t15.2025.03.14 val PER: 0.6509
2025-12-11 04:09:14,639: t15.2025.03.16 val PER: 0.5340
2025-12-11 04:09:14,639: t15.2025.03.30 val PER: 0.6287
2025-12-11 04:09:14,639: t15.2025.04.13 val PER: 0.5164
2025-12-11 04:10:17,589: Train batch 58200: loss: 13.48 grad norm: 261.74 time: 0.322
2025-12-11 04:11:20,697: Train batch 58400: loss: 15.17 grad norm: 388.05 time: 0.247
2025-12-11 04:12:23,477: Train batch 58600: loss: 24.01 grad norm: 76.14 time: 0.295
2025-12-11 04:13:26,981: Train batch 58800: loss: 10.89 grad norm: 339.97 time: 0.430
2025-12-11 04:14:31,158: Train batch 59000: loss: 17.57 grad norm: 219.02 time: 0.364
2025-12-11 04:15:36,172: Train batch 59200: loss: 13.92 grad norm: 69.03 time: 0.274
2025-12-11 04:16:38,272: Train batch 59400: loss: 11.88 grad norm: 82.75 time: 0.301
2025-12-11 04:17:41,583: Train batch 59600: loss: 13.60 grad norm: 1077.51 time: 0.325
2025-12-11 04:18:44,688: Train batch 59800: loss: 13.85 grad norm: 101.76 time: 0.335
2025-12-11 04:19:46,727: Train batch 60000: loss: 10.15 grad norm: 82.26 time: 0.342
2025-12-11 04:19:46,727: Running test after training batch: 60000
2025-12-11 04:20:12,048: Val batch 60000: PER (avg): 0.4397 CTC Loss (avg): 57.9808 time: 25.321
2025-12-11 04:20:12,049: t15.2023.08.11 val PER: 1.0000
2025-12-11 04:20:12,049: t15.2023.08.13 val PER: 0.4636
2025-12-11 04:20:12,049: t15.2023.08.18 val PER: 0.4015
2025-12-11 04:20:12,049: t15.2023.08.20 val PER: 0.3860
2025-12-11 04:20:12,049: t15.2023.08.25 val PER: 0.4578
2025-12-11 04:20:12,049: t15.2023.08.27 val PER: 0.5032
2025-12-11 04:20:12,049: t15.2023.09.01 val PER: 0.3604
2025-12-11 04:20:12,049: t15.2023.09.03 val PER: 0.3931
2025-12-11 04:20:12,049: t15.2023.09.24 val PER: 0.3677
2025-12-11 04:20:12,049: t15.2023.09.29 val PER: 0.4020
2025-12-11 04:20:12,049: t15.2023.10.01 val PER: 0.4577
2025-12-11 04:20:12,049: t15.2023.10.06 val PER: 0.3703
2025-12-11 04:20:12,049: t15.2023.10.08 val PER: 0.4993
2025-12-11 04:20:12,050: t15.2023.10.13 val PER: 0.4352
2025-12-11 04:20:12,050: t15.2023.10.15 val PER: 0.4047
2025-12-11 04:20:12,050: t15.2023.10.20 val PER: 0.4597
2025-12-11 04:20:12,050: t15.2023.10.22 val PER: 0.3964
2025-12-11 04:20:12,050: t15.2023.11.03 val PER: 0.4281
2025-12-11 04:20:12,050: t15.2023.11.04 val PER: 0.2184
2025-12-11 04:20:12,050: t15.2023.11.17 val PER: 0.2675
2025-12-11 04:20:12,050: t15.2023.11.19 val PER: 0.3174
2025-12-11 04:20:12,050: t15.2023.11.26 val PER: 0.4304
2025-12-11 04:20:12,050: t15.2023.12.03 val PER: 0.3655
2025-12-11 04:20:12,050: t15.2023.12.08 val PER: 0.4035
2025-12-11 04:20:12,050: t15.2023.12.10 val PER: 0.3443
2025-12-11 04:20:12,050: t15.2023.12.17 val PER: 0.4782
2025-12-11 04:20:12,050: t15.2023.12.29 val PER: 0.4811
2025-12-11 04:20:12,050: t15.2024.02.25 val PER: 0.3848
2025-12-11 04:20:12,050: t15.2024.03.03 val PER: 1.0000
2025-12-11 04:20:12,051: t15.2024.03.08 val PER: 0.4908
2025-12-11 04:20:12,051: t15.2024.03.15 val PER: 0.4509
2025-12-11 04:20:12,051: t15.2024.03.17 val PER: 0.4296
2025-12-11 04:20:12,051: t15.2024.04.25 val PER: 1.0000
2025-12-11 04:20:12,051: t15.2024.04.28 val PER: 1.0000
2025-12-11 04:20:12,051: t15.2024.05.10 val PER: 0.5230
2025-12-11 04:20:12,051: t15.2024.06.14 val PER: 0.4322
2025-12-11 04:20:12,051: t15.2024.07.19 val PER: 0.6051
2025-12-11 04:20:12,051: t15.2024.07.21 val PER: 0.3572
2025-12-11 04:20:12,051: t15.2024.07.28 val PER: 0.4096
2025-12-11 04:20:12,051: t15.2025.01.10 val PER: 0.6006
2025-12-11 04:20:12,051: t15.2025.01.12 val PER: 0.4557
2025-12-11 04:20:12,051: t15.2025.03.14 val PER: 0.6479
2025-12-11 04:20:12,051: t15.2025.03.16 val PER: 0.5353
2025-12-11 04:20:12,051: t15.2025.03.30 val PER: 0.6391
2025-12-11 04:20:12,051: t15.2025.04.13 val PER: 0.5549
2025-12-11 04:21:14,829: Train batch 60200: loss: 9.82 grad norm: 66.76 time: 0.383
2025-12-11 04:22:19,022: Train batch 60400: loss: 29.28 grad norm: 146.02 time: 0.326
2025-12-11 04:23:23,509: Train batch 60600: loss: 14.12 grad norm: 856.31 time: 0.317
2025-12-11 04:24:26,978: Train batch 60800: loss: 14.58 grad norm: 186.93 time: 0.332
2025-12-11 04:25:29,778: Train batch 61000: loss: 17.74 grad norm: 504.70 time: 0.291
2025-12-11 04:26:33,731: Train batch 61200: loss: 12.89 grad norm: 73.15 time: 0.290
2025-12-11 04:27:37,122: Train batch 61400: loss: 12.33 grad norm: 100.11 time: 0.255
2025-12-11 04:28:41,116: Train batch 61600: loss: 10.01 grad norm: 78.82 time: 0.256
2025-12-11 04:29:44,398: Train batch 61800: loss: 18.47 grad norm: 148.04 time: 0.295
2025-12-11 04:30:48,825: Train batch 62000: loss: 10.59 grad norm: 56.99 time: 0.327
2025-12-11 04:30:48,825: Running test after training batch: 62000
2025-12-11 04:31:14,046: Val batch 62000: PER (avg): 0.4114 CTC Loss (avg): 53.4695 time: 25.221
2025-12-11 04:31:14,046: t15.2023.08.11 val PER: 1.0000
2025-12-11 04:31:14,046: t15.2023.08.13 val PER: 0.4241
2025-12-11 04:31:14,046: t15.2023.08.18 val PER: 0.3915
2025-12-11 04:31:14,046: t15.2023.08.20 val PER: 0.3479
2025-12-11 04:31:14,046: t15.2023.08.25 val PER: 0.4127
2025-12-11 04:31:14,046: t15.2023.08.27 val PER: 0.4598
2025-12-11 04:31:14,047: t15.2023.09.01 val PER: 0.3442
2025-12-11 04:31:14,047: t15.2023.09.03 val PER: 0.3729
2025-12-11 04:31:14,047: t15.2023.09.24 val PER: 0.3374
2025-12-11 04:31:14,047: t15.2023.09.29 val PER: 0.3714
2025-12-11 04:31:14,047: t15.2023.10.01 val PER: 0.4247
2025-12-11 04:31:14,047: t15.2023.10.06 val PER: 0.3262
2025-12-11 04:31:14,047: t15.2023.10.08 val PER: 0.4790
2025-12-11 04:31:14,047: t15.2023.10.13 val PER: 0.4182
2025-12-11 04:31:14,047: t15.2023.10.15 val PER: 0.4054
2025-12-11 04:31:14,047: t15.2023.10.20 val PER: 0.4362
2025-12-11 04:31:14,047: t15.2023.10.22 val PER: 0.3597
2025-12-11 04:31:14,047: t15.2023.11.03 val PER: 0.4050
2025-12-11 04:31:14,047: t15.2023.11.04 val PER: 0.1638
2025-12-11 04:31:14,047: t15.2023.11.17 val PER: 0.1960
2025-12-11 04:31:14,047: t15.2023.11.19 val PER: 0.2635
2025-12-11 04:31:14,047: t15.2023.11.26 val PER: 0.3797
2025-12-11 04:31:14,047: t15.2023.12.03 val PER: 0.3141
2025-12-11 04:31:14,047: t15.2023.12.08 val PER: 0.3475
2025-12-11 04:31:14,047: t15.2023.12.10 val PER: 0.2957
2025-12-11 04:31:14,048: t15.2023.12.17 val PER: 0.4595
2025-12-11 04:31:14,048: t15.2023.12.29 val PER: 0.4399
2025-12-11 04:31:14,048: t15.2024.02.25 val PER: 0.3806
2025-12-11 04:31:14,048: t15.2024.03.03 val PER: 1.0000
2025-12-11 04:31:14,048: t15.2024.03.08 val PER: 0.4879
2025-12-11 04:31:14,048: t15.2024.03.15 val PER: 0.4465
2025-12-11 04:31:14,048: t15.2024.03.17 val PER: 0.4031
2025-12-11 04:31:14,048: t15.2024.04.25 val PER: 1.0000
2025-12-11 04:31:14,048: t15.2024.04.28 val PER: 1.0000
2025-12-11 04:31:14,048: t15.2024.05.10 val PER: 0.4799
2025-12-11 04:31:14,048: t15.2024.06.14 val PER: 0.4464
2025-12-11 04:31:14,048: t15.2024.07.19 val PER: 0.5860
2025-12-11 04:31:14,048: t15.2024.07.21 val PER: 0.3179
2025-12-11 04:31:14,048: t15.2024.07.28 val PER: 0.3809
2025-12-11 04:31:14,048: t15.2025.01.10 val PER: 0.5675
2025-12-11 04:31:14,048: t15.2025.01.12 val PER: 0.4511
2025-12-11 04:31:14,048: t15.2025.03.14 val PER: 0.6391
2025-12-11 04:31:14,048: t15.2025.03.16 val PER: 0.5026
2025-12-11 04:31:14,048: t15.2025.03.30 val PER: 0.6184
2025-12-11 04:31:14,049: t15.2025.04.13 val PER: 0.5193
2025-12-11 04:31:14,049: New best test PER 0.4225 --> 0.4114
2025-12-11 04:31:14,049: Checkpointing model
2025-12-11 04:31:15,226: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 04:32:17,995: Train batch 62200: loss: 7.39 grad norm: 117.49 time: 0.349
2025-12-11 04:33:20,786: Train batch 62400: loss: 4.99 grad norm: 69.78 time: 0.375
2025-12-11 04:34:24,434: Train batch 62600: loss: 8.61 grad norm: 57.23 time: 0.261
2025-12-11 04:35:28,345: Train batch 62800: loss: 19.11 grad norm: 617.02 time: 0.342
2025-12-11 04:36:31,761: Train batch 63000: loss: 16.61 grad norm: 500.67 time: 0.257
2025-12-11 04:37:34,668: Train batch 63200: loss: 14.85 grad norm: 166.07 time: 0.422
2025-12-11 04:38:37,209: Train batch 63400: loss: 8.12 grad norm: 53.27 time: 0.226
2025-12-11 04:39:41,292: Train batch 63600: loss: 14.13 grad norm: 243.04 time: 0.467
2025-12-11 04:40:43,987: Train batch 63800: loss: 14.77 grad norm: 133.49 time: 0.277
2025-12-11 04:41:47,379: Train batch 64000: loss: 10.11 grad norm: 66.54 time: 0.299
2025-12-11 04:41:47,379: Running test after training batch: 64000
2025-12-11 04:42:12,630: Val batch 64000: PER (avg): 0.4177 CTC Loss (avg): 54.8714 time: 25.251
2025-12-11 04:42:12,630: t15.2023.08.11 val PER: 1.0000
2025-12-11 04:42:12,630: t15.2023.08.13 val PER: 0.4366
2025-12-11 04:42:12,630: t15.2023.08.18 val PER: 0.3881
2025-12-11 04:42:12,630: t15.2023.08.20 val PER: 0.3598
2025-12-11 04:42:12,631: t15.2023.08.25 val PER: 0.4096
2025-12-11 04:42:12,631: t15.2023.08.27 val PER: 0.4839
2025-12-11 04:42:12,631: t15.2023.09.01 val PER: 0.3466
2025-12-11 04:42:12,631: t15.2023.09.03 val PER: 0.3646
2025-12-11 04:42:12,631: t15.2023.09.24 val PER: 0.3580
2025-12-11 04:42:12,631: t15.2023.09.29 val PER: 0.3535
2025-12-11 04:42:12,631: t15.2023.10.01 val PER: 0.4419
2025-12-11 04:42:12,631: t15.2023.10.06 val PER: 0.3391
2025-12-11 04:42:12,631: t15.2023.10.08 val PER: 0.4763
2025-12-11 04:42:12,631: t15.2023.10.13 val PER: 0.4197
2025-12-11 04:42:12,631: t15.2023.10.15 val PER: 0.3764
2025-12-11 04:42:12,631: t15.2023.10.20 val PER: 0.4362
2025-12-11 04:42:12,631: t15.2023.10.22 val PER: 0.3875
2025-12-11 04:42:12,631: t15.2023.11.03 val PER: 0.4050
2025-12-11 04:42:12,631: t15.2023.11.04 val PER: 0.1502
2025-12-11 04:42:12,631: t15.2023.11.17 val PER: 0.2177
2025-12-11 04:42:12,631: t15.2023.11.19 val PER: 0.2735
2025-12-11 04:42:12,632: t15.2023.11.26 val PER: 0.3833
2025-12-11 04:42:12,632: t15.2023.12.03 val PER: 0.3393
2025-12-11 04:42:12,632: t15.2023.12.08 val PER: 0.3475
2025-12-11 04:42:12,632: t15.2023.12.10 val PER: 0.3206
2025-12-11 04:42:12,632: t15.2023.12.17 val PER: 0.4771
2025-12-11 04:42:12,632: t15.2023.12.29 val PER: 0.4509
2025-12-11 04:42:12,632: t15.2024.02.25 val PER: 0.3919
2025-12-11 04:42:12,632: t15.2024.03.03 val PER: 1.0000
2025-12-11 04:42:12,632: t15.2024.03.08 val PER: 0.4780
2025-12-11 04:42:12,632: t15.2024.03.15 val PER: 0.4522
2025-12-11 04:42:12,632: t15.2024.03.17 val PER: 0.4205
2025-12-11 04:42:12,632: t15.2024.04.25 val PER: 1.0000
2025-12-11 04:42:12,632: t15.2024.04.28 val PER: 1.0000
2025-12-11 04:42:12,632: t15.2024.05.10 val PER: 0.5156
2025-12-11 04:42:12,632: t15.2024.06.14 val PER: 0.4259
2025-12-11 04:42:12,632: t15.2024.07.19 val PER: 0.5953
2025-12-11 04:42:12,632: t15.2024.07.21 val PER: 0.3386
2025-12-11 04:42:12,632: t15.2024.07.28 val PER: 0.3926
2025-12-11 04:42:12,633: t15.2025.01.10 val PER: 0.5868
2025-12-11 04:42:12,633: t15.2025.01.12 val PER: 0.4550
2025-12-11 04:42:12,633: t15.2025.03.14 val PER: 0.6494
2025-12-11 04:42:12,633: t15.2025.03.16 val PER: 0.4974
2025-12-11 04:42:12,633: t15.2025.03.30 val PER: 0.6092
2025-12-11 04:42:12,633: t15.2025.04.13 val PER: 0.5221
2025-12-11 04:43:16,129: Train batch 64200: loss: 14.45 grad norm: 209.88 time: 0.307
2025-12-11 04:44:18,866: Train batch 64400: loss: 19.23 grad norm: 567.66 time: 0.335
2025-12-11 04:45:21,860: Train batch 64600: loss: 12.18 grad norm: 69.76 time: 0.182
2025-12-11 04:46:24,427: Train batch 64800: loss: 16.87 grad norm: 362.33 time: 0.333
2025-12-11 04:47:26,879: Train batch 65000: loss: 10.39 grad norm: 112.82 time: 0.292
2025-12-11 04:48:29,341: Train batch 65200: loss: 16.92 grad norm: 64.64 time: 0.331
2025-12-11 04:49:33,395: Train batch 65400: loss: 11.00 grad norm: 57.74 time: 0.457
2025-12-11 04:50:35,534: Train batch 65600: loss: 12.37 grad norm: 201.06 time: 0.250
2025-12-11 04:51:38,094: Train batch 65800: loss: 14.64 grad norm: 80.68 time: 0.359
2025-12-11 04:52:41,930: Train batch 66000: loss: 13.58 grad norm: 63.47 time: 0.251
2025-12-11 04:52:41,930: Running test after training batch: 66000
2025-12-11 04:53:07,242: Val batch 66000: PER (avg): 0.4088 CTC Loss (avg): 53.4126 time: 25.312
2025-12-11 04:53:07,243: t15.2023.08.11 val PER: 1.0000
2025-12-11 04:53:07,243: t15.2023.08.13 val PER: 0.4262
2025-12-11 04:53:07,243: t15.2023.08.18 val PER: 0.3797
2025-12-11 04:53:07,243: t15.2023.08.20 val PER: 0.3630
2025-12-11 04:53:07,243: t15.2023.08.25 val PER: 0.4187
2025-12-11 04:53:07,243: t15.2023.08.27 val PER: 0.4598
2025-12-11 04:53:07,243: t15.2023.09.01 val PER: 0.3490
2025-12-11 04:53:07,243: t15.2023.09.03 val PER: 0.3658
2025-12-11 04:53:07,243: t15.2023.09.24 val PER: 0.3252
2025-12-11 04:53:07,244: t15.2023.09.29 val PER: 0.3535
2025-12-11 04:53:07,244: t15.2023.10.01 val PER: 0.4234
2025-12-11 04:53:07,244: t15.2023.10.06 val PER: 0.3186
2025-12-11 04:53:07,244: t15.2023.10.08 val PER: 0.4601
2025-12-11 04:53:07,244: t15.2023.10.13 val PER: 0.4259
2025-12-11 04:53:07,244: t15.2023.10.15 val PER: 0.3777
2025-12-11 04:53:07,244: t15.2023.10.20 val PER: 0.4664
2025-12-11 04:53:07,244: t15.2023.10.22 val PER: 0.3664
2025-12-11 04:53:07,244: t15.2023.11.03 val PER: 0.4159
2025-12-11 04:53:07,244: t15.2023.11.04 val PER: 0.2253
2025-12-11 04:53:07,244: t15.2023.11.17 val PER: 0.1928
2025-12-11 04:53:07,244: t15.2023.11.19 val PER: 0.2854
2025-12-11 04:53:07,244: t15.2023.11.26 val PER: 0.3848
2025-12-11 04:53:07,245: t15.2023.12.03 val PER: 0.3151
2025-12-11 04:53:07,245: t15.2023.12.08 val PER: 0.3362
2025-12-11 04:53:07,245: t15.2023.12.10 val PER: 0.3062
2025-12-11 04:53:07,245: t15.2023.12.17 val PER: 0.4678
2025-12-11 04:53:07,245: t15.2023.12.29 val PER: 0.4516
2025-12-11 04:53:07,245: t15.2024.02.25 val PER: 0.3694
2025-12-11 04:53:07,245: t15.2024.03.03 val PER: 1.0000
2025-12-11 04:53:07,245: t15.2024.03.08 val PER: 0.4908
2025-12-11 04:53:07,245: t15.2024.03.15 val PER: 0.4246
2025-12-11 04:53:07,245: t15.2024.03.17 val PER: 0.3884
2025-12-11 04:53:07,245: t15.2024.04.25 val PER: 1.0000
2025-12-11 04:53:07,245: t15.2024.04.28 val PER: 1.0000
2025-12-11 04:53:07,245: t15.2024.05.10 val PER: 0.4963
2025-12-11 04:53:07,246: t15.2024.06.14 val PER: 0.4369
2025-12-11 04:53:07,246: t15.2024.07.19 val PER: 0.5794
2025-12-11 04:53:07,246: t15.2024.07.21 val PER: 0.3179
2025-12-11 04:53:07,246: t15.2024.07.28 val PER: 0.3735
2025-12-11 04:53:07,246: t15.2025.01.10 val PER: 0.5854
2025-12-11 04:53:07,246: t15.2025.01.12 val PER: 0.4349
2025-12-11 04:53:07,246: t15.2025.03.14 val PER: 0.6376
2025-12-11 04:53:07,246: t15.2025.03.16 val PER: 0.4843
2025-12-11 04:53:07,246: t15.2025.03.30 val PER: 0.6103
2025-12-11 04:53:07,246: t15.2025.04.13 val PER: 0.5150
2025-12-11 04:53:07,246: New best test PER 0.4114 --> 0.4088
2025-12-11 04:53:07,246: Checkpointing model
2025-12-11 04:53:08,420: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 04:54:11,743: Train batch 66200: loss: 11.42 grad norm: 76.03 time: 0.375
2025-12-11 04:55:14,869: Train batch 66400: loss: 10.83 grad norm: 51.38 time: 0.351
2025-12-11 04:56:17,264: Train batch 66600: loss: 9.38 grad norm: 47.38 time: 0.227
2025-12-11 04:57:19,848: Train batch 66800: loss: 16.30 grad norm: 76.82 time: 0.264
2025-12-11 04:58:22,412: Train batch 67000: loss: 8.00 grad norm: 54.02 time: 0.393
2025-12-11 04:59:26,265: Train batch 67200: loss: 11.90 grad norm: 263.08 time: 0.289
2025-12-11 05:00:29,778: Train batch 67400: loss: 10.57 grad norm: 67.30 time: 0.280
2025-12-11 05:01:33,288: Train batch 67600: loss: 9.51 grad norm: 57.90 time: 0.270
2025-12-11 05:02:36,847: Train batch 67800: loss: 12.16 grad norm: 276.50 time: 0.278
2025-12-11 05:03:40,053: Train batch 68000: loss: 10.63 grad norm: 81.30 time: 0.278
2025-12-11 05:03:40,053: Running test after training batch: 68000
2025-12-11 05:04:05,340: Val batch 68000: PER (avg): 0.4127 CTC Loss (avg): 54.8176 time: 25.286
2025-12-11 05:04:05,340: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:04:05,340: t15.2023.08.13 val PER: 0.4168
2025-12-11 05:04:05,340: t15.2023.08.18 val PER: 0.3738
2025-12-11 05:04:05,340: t15.2023.08.20 val PER: 0.3511
2025-12-11 05:04:05,340: t15.2023.08.25 val PER: 0.4262
2025-12-11 05:04:05,340: t15.2023.08.27 val PER: 0.4839
2025-12-11 05:04:05,340: t15.2023.09.01 val PER: 0.3231
2025-12-11 05:04:05,341: t15.2023.09.03 val PER: 0.3717
2025-12-11 05:04:05,341: t15.2023.09.24 val PER: 0.3653
2025-12-11 05:04:05,341: t15.2023.09.29 val PER: 0.3574
2025-12-11 05:04:05,341: t15.2023.10.01 val PER: 0.4373
2025-12-11 05:04:05,341: t15.2023.10.06 val PER: 0.3122
2025-12-11 05:04:05,341: t15.2023.10.08 val PER: 0.4641
2025-12-11 05:04:05,341: t15.2023.10.13 val PER: 0.4166
2025-12-11 05:04:05,341: t15.2023.10.15 val PER: 0.3916
2025-12-11 05:04:05,341: t15.2023.10.20 val PER: 0.4530
2025-12-11 05:04:05,341: t15.2023.10.22 val PER: 0.3719
2025-12-11 05:04:05,341: t15.2023.11.03 val PER: 0.4071
2025-12-11 05:04:05,341: t15.2023.11.04 val PER: 0.1877
2025-12-11 05:04:05,341: t15.2023.11.17 val PER: 0.1820
2025-12-11 05:04:05,341: t15.2023.11.19 val PER: 0.2814
2025-12-11 05:04:05,341: t15.2023.11.26 val PER: 0.3884
2025-12-11 05:04:05,341: t15.2023.12.03 val PER: 0.3109
2025-12-11 05:04:05,341: t15.2023.12.08 val PER: 0.3495
2025-12-11 05:04:05,341: t15.2023.12.10 val PER: 0.3127
2025-12-11 05:04:05,341: t15.2023.12.17 val PER: 0.4657
2025-12-11 05:04:05,342: t15.2023.12.29 val PER: 0.4413
2025-12-11 05:04:05,342: t15.2024.02.25 val PER: 0.3904
2025-12-11 05:04:05,342: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:04:05,342: t15.2024.03.08 val PER: 0.4936
2025-12-11 05:04:05,342: t15.2024.03.15 val PER: 0.4390
2025-12-11 05:04:05,342: t15.2024.03.17 val PER: 0.4045
2025-12-11 05:04:05,342: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:04:05,342: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:04:05,342: t15.2024.05.10 val PER: 0.5082
2025-12-11 05:04:05,342: t15.2024.06.14 val PER: 0.4353
2025-12-11 05:04:05,342: t15.2024.07.19 val PER: 0.5920
2025-12-11 05:04:05,342: t15.2024.07.21 val PER: 0.3379
2025-12-11 05:04:05,342: t15.2024.07.28 val PER: 0.3912
2025-12-11 05:04:05,342: t15.2025.01.10 val PER: 0.5992
2025-12-11 05:04:05,342: t15.2025.01.12 val PER: 0.4280
2025-12-11 05:04:05,342: t15.2025.03.14 val PER: 0.6509
2025-12-11 05:04:05,342: t15.2025.03.16 val PER: 0.4908
2025-12-11 05:04:05,342: t15.2025.03.30 val PER: 0.6080
2025-12-11 05:04:05,343: t15.2025.04.13 val PER: 0.5164
2025-12-11 05:05:08,932: Train batch 68200: loss: 21.82 grad norm: 175.17 time: 0.303
2025-12-11 05:06:12,909: Train batch 68400: loss: 11.55 grad norm: 90.31 time: 0.259
2025-12-11 05:07:16,174: Train batch 68600: loss: 7.33 grad norm: 80.26 time: 0.267
2025-12-11 05:08:18,704: Train batch 68800: loss: 10.74 grad norm: 48.52 time: 0.220
2025-12-11 05:09:21,679: Train batch 69000: loss: 18.23 grad norm: 104.18 time: 0.330
2025-12-11 05:10:24,435: Train batch 69200: loss: 11.00 grad norm: 78.83 time: 0.462
2025-12-11 05:11:27,126: Train batch 69400: loss: 17.45 grad norm: 416.18 time: 0.245
2025-12-11 05:12:30,185: Train batch 69600: loss: 15.21 grad norm: 107.28 time: 0.262
2025-12-11 05:13:33,231: Train batch 69800: loss: 9.29 grad norm: 65.47 time: 0.289
2025-12-11 05:14:36,030: Train batch 70000: loss: 9.27 grad norm: 1803.42 time: 0.267
2025-12-11 05:14:36,030: Running test after training batch: 70000
2025-12-11 05:15:01,413: Val batch 70000: PER (avg): 0.4146 CTC Loss (avg): 55.8426 time: 25.383
2025-12-11 05:15:01,413: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:15:01,413: t15.2023.08.13 val PER: 0.4231
2025-12-11 05:15:01,413: t15.2023.08.18 val PER: 0.3671
2025-12-11 05:15:01,414: t15.2023.08.20 val PER: 0.3550
2025-12-11 05:15:01,414: t15.2023.08.25 val PER: 0.4127
2025-12-11 05:15:01,414: t15.2023.08.27 val PER: 0.4807
2025-12-11 05:15:01,414: t15.2023.09.01 val PER: 0.3539
2025-12-11 05:15:01,414: t15.2023.09.03 val PER: 0.3432
2025-12-11 05:15:01,414: t15.2023.09.24 val PER: 0.3580
2025-12-11 05:15:01,414: t15.2023.09.29 val PER: 0.3529
2025-12-11 05:15:01,414: t15.2023.10.01 val PER: 0.4399
2025-12-11 05:15:01,414: t15.2023.10.06 val PER: 0.3326
2025-12-11 05:15:01,414: t15.2023.10.08 val PER: 0.4763
2025-12-11 05:15:01,414: t15.2023.10.13 val PER: 0.4135
2025-12-11 05:15:01,414: t15.2023.10.15 val PER: 0.3797
2025-12-11 05:15:01,414: t15.2023.10.20 val PER: 0.4899
2025-12-11 05:15:01,414: t15.2023.10.22 val PER: 0.3552
2025-12-11 05:15:01,414: t15.2023.11.03 val PER: 0.4057
2025-12-11 05:15:01,414: t15.2023.11.04 val PER: 0.1980
2025-12-11 05:15:01,414: t15.2023.11.17 val PER: 0.2177
2025-12-11 05:15:01,414: t15.2023.11.19 val PER: 0.2655
2025-12-11 05:15:01,415: t15.2023.11.26 val PER: 0.3964
2025-12-11 05:15:01,415: t15.2023.12.03 val PER: 0.3204
2025-12-11 05:15:01,415: t15.2023.12.08 val PER: 0.3555
2025-12-11 05:15:01,415: t15.2023.12.10 val PER: 0.3285
2025-12-11 05:15:01,415: t15.2023.12.17 val PER: 0.4834
2025-12-11 05:15:01,415: t15.2023.12.29 val PER: 0.4626
2025-12-11 05:15:01,415: t15.2024.02.25 val PER: 0.3834
2025-12-11 05:15:01,415: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:15:01,415: t15.2024.03.08 val PER: 0.5021
2025-12-11 05:15:01,415: t15.2024.03.15 val PER: 0.4390
2025-12-11 05:15:01,415: t15.2024.03.17 val PER: 0.3975
2025-12-11 05:15:01,415: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:15:01,415: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:15:01,415: t15.2024.05.10 val PER: 0.5067
2025-12-11 05:15:01,415: t15.2024.06.14 val PER: 0.4369
2025-12-11 05:15:01,415: t15.2024.07.19 val PER: 0.5926
2025-12-11 05:15:01,415: t15.2024.07.21 val PER: 0.3290
2025-12-11 05:15:01,415: t15.2024.07.28 val PER: 0.3809
2025-12-11 05:15:01,415: t15.2025.01.10 val PER: 0.5799
2025-12-11 05:15:01,415: t15.2025.01.12 val PER: 0.4419
2025-12-11 05:15:01,416: t15.2025.03.14 val PER: 0.6346
2025-12-11 05:15:01,416: t15.2025.03.16 val PER: 0.4856
2025-12-11 05:15:01,416: t15.2025.03.30 val PER: 0.6057
2025-12-11 05:15:01,416: t15.2025.04.13 val PER: 0.5492
2025-12-11 05:16:04,026: Train batch 70200: loss: 12.68 grad norm: 72.53 time: 0.354
2025-12-11 05:17:07,298: Train batch 70400: loss: 11.19 grad norm: 142.61 time: 0.260
2025-12-11 05:18:10,187: Train batch 70600: loss: 13.15 grad norm: 136.99 time: 0.367
2025-12-11 05:19:13,265: Train batch 70800: loss: 10.60 grad norm: 61.11 time: 0.334
2025-12-11 05:20:14,965: Train batch 71000: loss: 7.74 grad norm: 731.64 time: 0.404
2025-12-11 05:21:17,745: Train batch 71200: loss: 9.86 grad norm: 231.15 time: 0.309
2025-12-11 05:22:21,003: Train batch 71400: loss: 13.79 grad norm: 55.98 time: 0.355
2025-12-11 05:23:22,639: Train batch 71600: loss: 12.42 grad norm: 98.46 time: 0.337
2025-12-11 05:24:25,970: Train batch 71800: loss: 16.79 grad norm: 438.19 time: 0.422
2025-12-11 05:25:28,895: Train batch 72000: loss: 12.26 grad norm: 126.28 time: 0.238
2025-12-11 05:25:28,895: Running test after training batch: 72000
2025-12-11 05:25:54,166: Val batch 72000: PER (avg): 0.4011 CTC Loss (avg): 52.2867 time: 25.270
2025-12-11 05:25:54,166: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:25:54,166: t15.2023.08.13 val PER: 0.4168
2025-12-11 05:25:54,166: t15.2023.08.18 val PER: 0.3646
2025-12-11 05:25:54,166: t15.2023.08.20 val PER: 0.3606
2025-12-11 05:25:54,166: t15.2023.08.25 val PER: 0.4202
2025-12-11 05:25:54,166: t15.2023.08.27 val PER: 0.4614
2025-12-11 05:25:54,166: t15.2023.09.01 val PER: 0.3474
2025-12-11 05:25:54,166: t15.2023.09.03 val PER: 0.3587
2025-12-11 05:25:54,167: t15.2023.09.24 val PER: 0.3459
2025-12-11 05:25:54,167: t15.2023.09.29 val PER: 0.3350
2025-12-11 05:25:54,167: t15.2023.10.01 val PER: 0.4432
2025-12-11 05:25:54,167: t15.2023.10.06 val PER: 0.3111
2025-12-11 05:25:54,167: t15.2023.10.08 val PER: 0.4696
2025-12-11 05:25:54,167: t15.2023.10.13 val PER: 0.3894
2025-12-11 05:25:54,167: t15.2023.10.15 val PER: 0.3711
2025-12-11 05:25:54,167: t15.2023.10.20 val PER: 0.4396
2025-12-11 05:25:54,167: t15.2023.10.22 val PER: 0.3664
2025-12-11 05:25:54,167: t15.2023.11.03 val PER: 0.3915
2025-12-11 05:25:54,167: t15.2023.11.04 val PER: 0.1672
2025-12-11 05:25:54,167: t15.2023.11.17 val PER: 0.1788
2025-12-11 05:25:54,167: t15.2023.11.19 val PER: 0.2335
2025-12-11 05:25:54,167: t15.2023.11.26 val PER: 0.3659
2025-12-11 05:25:54,167: t15.2023.12.03 val PER: 0.3162
2025-12-11 05:25:54,167: t15.2023.12.08 val PER: 0.3336
2025-12-11 05:25:54,167: t15.2023.12.10 val PER: 0.2917
2025-12-11 05:25:54,167: t15.2023.12.17 val PER: 0.4595
2025-12-11 05:25:54,167: t15.2023.12.29 val PER: 0.4180
2025-12-11 05:25:54,168: t15.2024.02.25 val PER: 0.3764
2025-12-11 05:25:54,168: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:25:54,168: t15.2024.03.08 val PER: 0.4964
2025-12-11 05:25:54,168: t15.2024.03.15 val PER: 0.4271
2025-12-11 05:25:54,168: t15.2024.03.17 val PER: 0.3863
2025-12-11 05:25:54,168: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:25:54,168: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:25:54,168: t15.2024.05.10 val PER: 0.4413
2025-12-11 05:25:54,168: t15.2024.06.14 val PER: 0.4527
2025-12-11 05:25:54,168: t15.2024.07.19 val PER: 0.5887
2025-12-11 05:25:54,168: t15.2024.07.21 val PER: 0.3207
2025-12-11 05:25:54,168: t15.2024.07.28 val PER: 0.3838
2025-12-11 05:25:54,168: t15.2025.01.10 val PER: 0.5937
2025-12-11 05:25:54,168: t15.2025.01.12 val PER: 0.4057
2025-12-11 05:25:54,168: t15.2025.03.14 val PER: 0.6213
2025-12-11 05:25:54,168: t15.2025.03.16 val PER: 0.4725
2025-12-11 05:25:54,168: t15.2025.03.30 val PER: 0.5874
2025-12-11 05:25:54,168: t15.2025.04.13 val PER: 0.5235
2025-12-11 05:25:54,168: New best test PER 0.4088 --> 0.4011
2025-12-11 05:25:54,168: Checkpointing model
2025-12-11 05:25:55,414: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 05:26:59,090: Train batch 72200: loss: 16.04 grad norm: 68.69 time: 0.448
2025-12-11 05:28:02,344: Train batch 72400: loss: 7.66 grad norm: 54.28 time: 0.274
2025-12-11 05:29:05,814: Train batch 72600: loss: 8.40 grad norm: 47.77 time: 0.346
2025-12-11 05:30:07,729: Train batch 72800: loss: 7.87 grad norm: 56.98 time: 0.239
2025-12-11 05:31:11,368: Train batch 73000: loss: 17.19 grad norm: 115.77 time: 0.359
2025-12-11 05:32:15,193: Train batch 73200: loss: 12.98 grad norm: 276.51 time: 0.277
2025-12-11 05:33:19,043: Train batch 73400: loss: 14.22 grad norm: 237.91 time: 0.223
2025-12-11 05:34:22,616: Train batch 73600: loss: 7.56 grad norm: 46.68 time: 0.320
2025-12-11 05:35:26,866: Train batch 73800: loss: 5.78 grad norm: 521.28 time: 0.273
2025-12-11 05:36:29,498: Train batch 74000: loss: 9.46 grad norm: 56.07 time: 0.414
2025-12-11 05:36:29,499: Running test after training batch: 74000
2025-12-11 05:36:54,766: Val batch 74000: PER (avg): 0.3905 CTC Loss (avg): 51.7067 time: 25.267
2025-12-11 05:36:54,766: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:36:54,766: t15.2023.08.13 val PER: 0.4116
2025-12-11 05:36:54,766: t15.2023.08.18 val PER: 0.3630
2025-12-11 05:36:54,766: t15.2023.08.20 val PER: 0.3400
2025-12-11 05:36:54,766: t15.2023.08.25 val PER: 0.3870
2025-12-11 05:36:54,766: t15.2023.08.27 val PER: 0.4566
2025-12-11 05:36:54,766: t15.2023.09.01 val PER: 0.3076
2025-12-11 05:36:54,766: t15.2023.09.03 val PER: 0.3563
2025-12-11 05:36:54,766: t15.2023.09.24 val PER: 0.3422
2025-12-11 05:36:54,766: t15.2023.09.29 val PER: 0.3312
2025-12-11 05:36:54,767: t15.2023.10.01 val PER: 0.4115
2025-12-11 05:36:54,767: t15.2023.10.06 val PER: 0.3186
2025-12-11 05:36:54,767: t15.2023.10.08 val PER: 0.4520
2025-12-11 05:36:54,767: t15.2023.10.13 val PER: 0.3887
2025-12-11 05:36:54,767: t15.2023.10.15 val PER: 0.3738
2025-12-11 05:36:54,767: t15.2023.10.20 val PER: 0.4329
2025-12-11 05:36:54,767: t15.2023.10.22 val PER: 0.3474
2025-12-11 05:36:54,767: t15.2023.11.03 val PER: 0.3779
2025-12-11 05:36:54,767: t15.2023.11.04 val PER: 0.1570
2025-12-11 05:36:54,767: t15.2023.11.17 val PER: 0.1804
2025-12-11 05:36:54,767: t15.2023.11.19 val PER: 0.2575
2025-12-11 05:36:54,767: t15.2023.11.26 val PER: 0.3667
2025-12-11 05:36:54,767: t15.2023.12.03 val PER: 0.3067
2025-12-11 05:36:54,767: t15.2023.12.08 val PER: 0.3149
2025-12-11 05:36:54,767: t15.2023.12.10 val PER: 0.2957
2025-12-11 05:36:54,767: t15.2023.12.17 val PER: 0.4501
2025-12-11 05:36:54,767: t15.2023.12.29 val PER: 0.4166
2025-12-11 05:36:54,767: t15.2024.02.25 val PER: 0.3750
2025-12-11 05:36:54,768: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:36:54,768: t15.2024.03.08 val PER: 0.4723
2025-12-11 05:36:54,768: t15.2024.03.15 val PER: 0.4253
2025-12-11 05:36:54,768: t15.2024.03.17 val PER: 0.3570
2025-12-11 05:36:54,768: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:36:54,768: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:36:54,768: t15.2024.05.10 val PER: 0.4502
2025-12-11 05:36:54,768: t15.2024.06.14 val PER: 0.3943
2025-12-11 05:36:54,768: t15.2024.07.19 val PER: 0.5544
2025-12-11 05:36:54,768: t15.2024.07.21 val PER: 0.3172
2025-12-11 05:36:54,768: t15.2024.07.28 val PER: 0.3831
2025-12-11 05:36:54,768: t15.2025.01.10 val PER: 0.5730
2025-12-11 05:36:54,768: t15.2025.01.12 val PER: 0.3872
2025-12-11 05:36:54,768: t15.2025.03.14 val PER: 0.6213
2025-12-11 05:36:54,768: t15.2025.03.16 val PER: 0.4699
2025-12-11 05:36:54,768: t15.2025.03.30 val PER: 0.5897
2025-12-11 05:36:54,768: t15.2025.04.13 val PER: 0.5164
2025-12-11 05:36:54,768: New best test PER 0.4011 --> 0.3905
2025-12-11 05:36:54,768: Checkpointing model
2025-12-11 05:36:55,921: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 05:37:59,712: Train batch 74200: loss: 13.48 grad norm: 102.79 time: 0.342
2025-12-11 05:39:03,664: Train batch 74400: loss: 6.35 grad norm: 47.92 time: 0.339
2025-12-11 05:40:06,718: Train batch 74600: loss: 13.72 grad norm: 205.72 time: 0.298
2025-12-11 05:41:09,555: Train batch 74800: loss: 9.05 grad norm: 49.79 time: 0.286
2025-12-11 05:42:11,437: Train batch 75000: loss: 5.99 grad norm: 43.57 time: 0.296
2025-12-11 05:43:15,724: Train batch 75200: loss: 12.38 grad norm: 258.35 time: 0.272
2025-12-11 05:44:17,085: Train batch 75400: loss: 9.27 grad norm: 45.40 time: 0.248
2025-12-11 05:45:21,012: Train batch 75600: loss: 15.09 grad norm: 216.74 time: 0.419
2025-12-11 05:46:23,641: Train batch 75800: loss: 13.34 grad norm: 55.19 time: 0.270
2025-12-11 05:47:26,998: Train batch 76000: loss: 5.49 grad norm: 122.76 time: 0.368
2025-12-11 05:47:26,999: Running test after training batch: 76000
2025-12-11 05:47:52,186: Val batch 76000: PER (avg): 0.3957 CTC Loss (avg): 53.3322 time: 25.188
2025-12-11 05:47:52,187: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:47:52,187: t15.2023.08.13 val PER: 0.4283
2025-12-11 05:47:52,187: t15.2023.08.18 val PER: 0.3571
2025-12-11 05:47:52,187: t15.2023.08.20 val PER: 0.3527
2025-12-11 05:47:52,187: t15.2023.08.25 val PER: 0.3901
2025-12-11 05:47:52,187: t15.2023.08.27 val PER: 0.4614
2025-12-11 05:47:52,187: t15.2023.09.01 val PER: 0.3052
2025-12-11 05:47:52,187: t15.2023.09.03 val PER: 0.3420
2025-12-11 05:47:52,187: t15.2023.09.24 val PER: 0.3240
2025-12-11 05:47:52,187: t15.2023.09.29 val PER: 0.3382
2025-12-11 05:47:52,187: t15.2023.10.01 val PER: 0.4069
2025-12-11 05:47:52,187: t15.2023.10.06 val PER: 0.3143
2025-12-11 05:47:52,187: t15.2023.10.08 val PER: 0.4723
2025-12-11 05:47:52,187: t15.2023.10.13 val PER: 0.4081
2025-12-11 05:47:52,188: t15.2023.10.15 val PER: 0.3665
2025-12-11 05:47:52,188: t15.2023.10.20 val PER: 0.4497
2025-12-11 05:47:52,188: t15.2023.10.22 val PER: 0.3441
2025-12-11 05:47:52,188: t15.2023.11.03 val PER: 0.4132
2025-12-11 05:47:52,188: t15.2023.11.04 val PER: 0.1809
2025-12-11 05:47:52,188: t15.2023.11.17 val PER: 0.1742
2025-12-11 05:47:52,188: t15.2023.11.19 val PER: 0.2615
2025-12-11 05:47:52,188: t15.2023.11.26 val PER: 0.3587
2025-12-11 05:47:52,188: t15.2023.12.03 val PER: 0.3120
2025-12-11 05:47:52,188: t15.2023.12.08 val PER: 0.3162
2025-12-11 05:47:52,188: t15.2023.12.10 val PER: 0.2917
2025-12-11 05:47:52,188: t15.2023.12.17 val PER: 0.4563
2025-12-11 05:47:52,188: t15.2023.12.29 val PER: 0.4207
2025-12-11 05:47:52,188: t15.2024.02.25 val PER: 0.3539
2025-12-11 05:47:52,188: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:47:52,188: t15.2024.03.08 val PER: 0.4836
2025-12-11 05:47:52,188: t15.2024.03.15 val PER: 0.4390
2025-12-11 05:47:52,188: t15.2024.03.17 val PER: 0.3724
2025-12-11 05:47:52,189: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:47:52,189: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:47:52,189: t15.2024.05.10 val PER: 0.4294
2025-12-11 05:47:52,189: t15.2024.06.14 val PER: 0.4338
2025-12-11 05:47:52,189: t15.2024.07.19 val PER: 0.5755
2025-12-11 05:47:52,189: t15.2024.07.21 val PER: 0.3303
2025-12-11 05:47:52,189: t15.2024.07.28 val PER: 0.3713
2025-12-11 05:47:52,189: t15.2025.01.10 val PER: 0.5702
2025-12-11 05:47:52,189: t15.2025.01.12 val PER: 0.4188
2025-12-11 05:47:52,189: t15.2025.03.14 val PER: 0.6317
2025-12-11 05:47:52,189: t15.2025.03.16 val PER: 0.4594
2025-12-11 05:47:52,189: t15.2025.03.30 val PER: 0.5954
2025-12-11 05:47:52,189: t15.2025.04.13 val PER: 0.5136
2025-12-11 05:48:55,065: Train batch 76200: loss: 4.80 grad norm: 212.47 time: 0.286
2025-12-11 05:49:57,794: Train batch 76400: loss: 7.26 grad norm: 54.44 time: 0.323
2025-12-11 05:51:00,218: Train batch 76600: loss: 11.17 grad norm: 294.31 time: 0.369
2025-12-11 05:52:02,868: Train batch 76800: loss: 10.72 grad norm: 78.67 time: 0.402
2025-12-11 05:53:06,394: Train batch 77000: loss: 7.78 grad norm: 89.91 time: 0.303
2025-12-11 05:54:09,872: Train batch 77200: loss: 10.89 grad norm: 67.96 time: 0.243
2025-12-11 05:55:12,588: Train batch 77400: loss: 14.36 grad norm: 207.72 time: 0.272
2025-12-11 05:56:16,465: Train batch 77600: loss: 9.12 grad norm: 181.24 time: 0.255
2025-12-11 05:57:19,959: Train batch 77800: loss: 5.53 grad norm: 71.52 time: 0.356
2025-12-11 05:58:22,599: Train batch 78000: loss: 15.11 grad norm: 102.96 time: 0.348
2025-12-11 05:58:22,599: Running test after training batch: 78000
2025-12-11 05:58:47,833: Val batch 78000: PER (avg): 0.4101 CTC Loss (avg): 56.4457 time: 25.234
2025-12-11 05:58:47,834: t15.2023.08.11 val PER: 1.0000
2025-12-11 05:58:47,834: t15.2023.08.13 val PER: 0.4470
2025-12-11 05:58:47,834: t15.2023.08.18 val PER: 0.3655
2025-12-11 05:58:47,834: t15.2023.08.20 val PER: 0.3542
2025-12-11 05:58:47,834: t15.2023.08.25 val PER: 0.4202
2025-12-11 05:58:47,834: t15.2023.08.27 val PER: 0.5000
2025-12-11 05:58:47,834: t15.2023.09.01 val PER: 0.3385
2025-12-11 05:58:47,834: t15.2023.09.03 val PER: 0.3551
2025-12-11 05:58:47,834: t15.2023.09.24 val PER: 0.3410
2025-12-11 05:58:47,834: t15.2023.09.29 val PER: 0.3529
2025-12-11 05:58:47,834: t15.2023.10.01 val PER: 0.4300
2025-12-11 05:58:47,834: t15.2023.10.06 val PER: 0.3240
2025-12-11 05:58:47,834: t15.2023.10.08 val PER: 0.4871
2025-12-11 05:58:47,834: t15.2023.10.13 val PER: 0.4143
2025-12-11 05:58:47,834: t15.2023.10.15 val PER: 0.3718
2025-12-11 05:58:47,834: t15.2023.10.20 val PER: 0.4463
2025-12-11 05:58:47,835: t15.2023.10.22 val PER: 0.3563
2025-12-11 05:58:47,835: t15.2023.11.03 val PER: 0.4091
2025-12-11 05:58:47,835: t15.2023.11.04 val PER: 0.1911
2025-12-11 05:58:47,835: t15.2023.11.17 val PER: 0.1757
2025-12-11 05:58:47,835: t15.2023.11.19 val PER: 0.2814
2025-12-11 05:58:47,835: t15.2023.11.26 val PER: 0.3717
2025-12-11 05:58:47,835: t15.2023.12.03 val PER: 0.3246
2025-12-11 05:58:47,835: t15.2023.12.08 val PER: 0.3475
2025-12-11 05:58:47,835: t15.2023.12.10 val PER: 0.2996
2025-12-11 05:58:47,835: t15.2023.12.17 val PER: 0.4740
2025-12-11 05:58:47,835: t15.2023.12.29 val PER: 0.4345
2025-12-11 05:58:47,835: t15.2024.02.25 val PER: 0.3834
2025-12-11 05:58:47,835: t15.2024.03.03 val PER: 1.0000
2025-12-11 05:58:47,835: t15.2024.03.08 val PER: 0.4751
2025-12-11 05:58:47,835: t15.2024.03.15 val PER: 0.4484
2025-12-11 05:58:47,835: t15.2024.03.17 val PER: 0.4107
2025-12-11 05:58:47,835: t15.2024.04.25 val PER: 1.0000
2025-12-11 05:58:47,835: t15.2024.04.28 val PER: 1.0000
2025-12-11 05:58:47,835: t15.2024.05.10 val PER: 0.4547
2025-12-11 05:58:47,836: t15.2024.06.14 val PER: 0.4448
2025-12-11 05:58:47,836: t15.2024.07.19 val PER: 0.5913
2025-12-11 05:58:47,836: t15.2024.07.21 val PER: 0.3469
2025-12-11 05:58:47,836: t15.2024.07.28 val PER: 0.3963
2025-12-11 05:58:47,836: t15.2025.01.10 val PER: 0.5826
2025-12-11 05:58:47,836: t15.2025.01.12 val PER: 0.4380
2025-12-11 05:58:47,836: t15.2025.03.14 val PER: 0.6317
2025-12-11 05:58:47,836: t15.2025.03.16 val PER: 0.4673
2025-12-11 05:58:47,836: t15.2025.03.30 val PER: 0.5977
2025-12-11 05:58:47,836: t15.2025.04.13 val PER: 0.5007
2025-12-11 05:59:48,893: Train batch 78200: loss: 5.53 grad norm: 41.83 time: 0.302
2025-12-11 06:00:52,242: Train batch 78400: loss: 11.03 grad norm: 131.65 time: 0.270
2025-12-11 06:01:54,773: Train batch 78600: loss: 14.85 grad norm: 153.97 time: 0.256
2025-12-11 06:02:57,964: Train batch 78800: loss: 5.80 grad norm: 106.66 time: 0.305
2025-12-11 06:04:02,523: Train batch 79000: loss: 11.62 grad norm: 77.94 time: 0.282
2025-12-11 06:05:05,600: Train batch 79200: loss: 6.82 grad norm: 144.53 time: 0.265
2025-12-11 06:06:08,535: Train batch 79400: loss: 3.02 grad norm: 47.44 time: 0.308
2025-12-11 06:07:12,017: Train batch 79600: loss: 8.02 grad norm: 79.16 time: 0.272
2025-12-11 06:08:16,032: Train batch 79800: loss: 13.80 grad norm: 505.39 time: 0.201
2025-12-11 06:09:19,889: Train batch 80000: loss: 13.38 grad norm: 245.50 time: 0.382
2025-12-11 06:09:19,889: Running test after training batch: 80000
2025-12-11 06:09:45,201: Val batch 80000: PER (avg): 0.3856 CTC Loss (avg): 51.2340 time: 25.312
2025-12-11 06:09:45,201: t15.2023.08.11 val PER: 1.0000
2025-12-11 06:09:45,202: t15.2023.08.13 val PER: 0.4210
2025-12-11 06:09:45,202: t15.2023.08.18 val PER: 0.3412
2025-12-11 06:09:45,202: t15.2023.08.20 val PER: 0.3098
2025-12-11 06:09:45,202: t15.2023.08.25 val PER: 0.4036
2025-12-11 06:09:45,202: t15.2023.08.27 val PER: 0.4678
2025-12-11 06:09:45,202: t15.2023.09.01 val PER: 0.3239
2025-12-11 06:09:45,202: t15.2023.09.03 val PER: 0.3504
2025-12-11 06:09:45,202: t15.2023.09.24 val PER: 0.3289
2025-12-11 06:09:45,202: t15.2023.09.29 val PER: 0.3210
2025-12-11 06:09:45,202: t15.2023.10.01 val PER: 0.4069
2025-12-11 06:09:45,202: t15.2023.10.06 val PER: 0.2896
2025-12-11 06:09:45,202: t15.2023.10.08 val PER: 0.4655
2025-12-11 06:09:45,202: t15.2023.10.13 val PER: 0.3918
2025-12-11 06:09:45,202: t15.2023.10.15 val PER: 0.3533
2025-12-11 06:09:45,202: t15.2023.10.20 val PER: 0.4362
2025-12-11 06:09:45,202: t15.2023.10.22 val PER: 0.3408
2025-12-11 06:09:45,202: t15.2023.11.03 val PER: 0.3670
2025-12-11 06:09:45,202: t15.2023.11.04 val PER: 0.1433
2025-12-11 06:09:45,203: t15.2023.11.17 val PER: 0.1602
2025-12-11 06:09:45,203: t15.2023.11.19 val PER: 0.2475
2025-12-11 06:09:45,203: t15.2023.11.26 val PER: 0.3681
2025-12-11 06:09:45,203: t15.2023.12.03 val PER: 0.2994
2025-12-11 06:09:45,203: t15.2023.12.08 val PER: 0.2956
2025-12-11 06:09:45,203: t15.2023.12.10 val PER: 0.2602
2025-12-11 06:09:45,203: t15.2023.12.17 val PER: 0.4501
2025-12-11 06:09:45,203: t15.2023.12.29 val PER: 0.4063
2025-12-11 06:09:45,203: t15.2024.02.25 val PER: 0.3357
2025-12-11 06:09:45,203: t15.2024.03.03 val PER: 1.0000
2025-12-11 06:09:45,203: t15.2024.03.08 val PER: 0.4751
2025-12-11 06:09:45,203: t15.2024.03.15 val PER: 0.4303
2025-12-11 06:09:45,203: t15.2024.03.17 val PER: 0.3640
2025-12-11 06:09:45,203: t15.2024.04.25 val PER: 1.0000
2025-12-11 06:09:45,203: t15.2024.04.28 val PER: 1.0000
2025-12-11 06:09:45,203: t15.2024.05.10 val PER: 0.4577
2025-12-11 06:09:45,203: t15.2024.06.14 val PER: 0.4353
2025-12-11 06:09:45,203: t15.2024.07.19 val PER: 0.5643
2025-12-11 06:09:45,204: t15.2024.07.21 val PER: 0.3159
2025-12-11 06:09:45,204: t15.2024.07.28 val PER: 0.3735
2025-12-11 06:09:45,204: t15.2025.01.10 val PER: 0.5675
2025-12-11 06:09:45,204: t15.2025.01.12 val PER: 0.3880
2025-12-11 06:09:45,204: t15.2025.03.14 val PER: 0.6405
2025-12-11 06:09:45,204: t15.2025.03.16 val PER: 0.4555
2025-12-11 06:09:45,204: t15.2025.03.30 val PER: 0.5943
2025-12-11 06:09:45,204: t15.2025.04.13 val PER: 0.5036
2025-12-11 06:09:45,204: New best test PER 0.3905 --> 0.3856
2025-12-11 06:09:45,204: Checkpointing model
2025-12-11 06:09:46,387: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 06:10:49,343: Train batch 80200: loss: 8.61 grad norm: 177.83 time: 0.198
2025-12-11 06:11:52,296: Train batch 80400: loss: 10.22 grad norm: 60.14 time: 0.255
2025-12-11 06:12:55,254: Train batch 80600: loss: 13.69 grad norm: 75.65 time: 0.338
2025-12-11 06:13:59,144: Train batch 80800: loss: 9.70 grad norm: 107.88 time: 0.396
2025-12-11 06:15:02,467: Train batch 81000: loss: 7.09 grad norm: 82.14 time: 0.320
2025-12-11 06:16:05,113: Train batch 81200: loss: 2.27 grad norm: 207.40 time: 0.284
2025-12-11 06:17:07,760: Train batch 81400: loss: 10.87 grad norm: 62.20 time: 0.284
2025-12-11 06:18:10,734: Train batch 81600: loss: 11.55 grad norm: 82.56 time: 0.288
2025-12-11 06:19:14,008: Train batch 81800: loss: 9.27 grad norm: 949.58 time: 0.299
2025-12-11 06:20:18,038: Train batch 82000: loss: 4.94 grad norm: 41.44 time: 0.311
2025-12-11 06:20:18,039: Running test after training batch: 82000
2025-12-11 06:20:43,349: Val batch 82000: PER (avg): 0.4147 CTC Loss (avg): 56.8400 time: 25.310
2025-12-11 06:20:43,349: t15.2023.08.11 val PER: 1.0000
2025-12-11 06:20:43,349: t15.2023.08.13 val PER: 0.4470
2025-12-11 06:20:43,349: t15.2023.08.18 val PER: 0.3713
2025-12-11 06:20:43,349: t15.2023.08.20 val PER: 0.3590
2025-12-11 06:20:43,349: t15.2023.08.25 val PER: 0.4172
2025-12-11 06:20:43,349: t15.2023.08.27 val PER: 0.4855
2025-12-11 06:20:43,350: t15.2023.09.01 val PER: 0.3304
2025-12-11 06:20:43,350: t15.2023.09.03 val PER: 0.3895
2025-12-11 06:20:43,350: t15.2023.09.24 val PER: 0.3580
2025-12-11 06:20:43,350: t15.2023.09.29 val PER: 0.3631
2025-12-11 06:20:43,350: t15.2023.10.01 val PER: 0.4399
2025-12-11 06:20:43,350: t15.2023.10.06 val PER: 0.3402
2025-12-11 06:20:43,350: t15.2023.10.08 val PER: 0.4750
2025-12-11 06:20:43,350: t15.2023.10.13 val PER: 0.3995
2025-12-11 06:20:43,350: t15.2023.10.15 val PER: 0.3777
2025-12-11 06:20:43,350: t15.2023.10.20 val PER: 0.4564
2025-12-11 06:20:43,350: t15.2023.10.22 val PER: 0.3786
2025-12-11 06:20:43,350: t15.2023.11.03 val PER: 0.4016
2025-12-11 06:20:43,350: t15.2023.11.04 val PER: 0.1706
2025-12-11 06:20:43,350: t15.2023.11.17 val PER: 0.2224
2025-12-11 06:20:43,350: t15.2023.11.19 val PER: 0.2735
2025-12-11 06:20:43,350: t15.2023.11.26 val PER: 0.3841
2025-12-11 06:20:43,350: t15.2023.12.03 val PER: 0.3298
2025-12-11 06:20:43,351: t15.2023.12.08 val PER: 0.3289
2025-12-11 06:20:43,351: t15.2023.12.10 val PER: 0.2996
2025-12-11 06:20:43,351: t15.2023.12.17 val PER: 0.4719
2025-12-11 06:20:43,351: t15.2023.12.29 val PER: 0.4441
2025-12-11 06:20:43,351: t15.2024.02.25 val PER: 0.3961
2025-12-11 06:20:43,351: t15.2024.03.03 val PER: 1.0000
2025-12-11 06:20:43,351: t15.2024.03.08 val PER: 0.5078
2025-12-11 06:20:43,351: t15.2024.03.15 val PER: 0.4534
2025-12-11 06:20:43,351: t15.2024.03.17 val PER: 0.4045
2025-12-11 06:20:43,351: t15.2024.04.25 val PER: 1.0000
2025-12-11 06:20:43,351: t15.2024.04.28 val PER: 1.0000
2025-12-11 06:20:43,351: t15.2024.05.10 val PER: 0.4770
2025-12-11 06:20:43,351: t15.2024.06.14 val PER: 0.4479
2025-12-11 06:20:43,351: t15.2024.07.19 val PER: 0.5933
2025-12-11 06:20:43,351: t15.2024.07.21 val PER: 0.3428
2025-12-11 06:20:43,351: t15.2024.07.28 val PER: 0.4199
2025-12-11 06:20:43,352: t15.2025.01.10 val PER: 0.5758
2025-12-11 06:20:43,352: t15.2025.01.12 val PER: 0.4311
2025-12-11 06:20:43,352: t15.2025.03.14 val PER: 0.6331
2025-12-11 06:20:43,352: t15.2025.03.16 val PER: 0.4830
2025-12-11 06:20:43,352: t15.2025.03.30 val PER: 0.6046
2025-12-11 06:20:43,352: t15.2025.04.13 val PER: 0.5093
2025-12-11 06:21:45,738: Train batch 82200: loss: 11.47 grad norm: 304.27 time: 0.443
2025-12-11 06:22:48,954: Train batch 82400: loss: 8.66 grad norm: 158.07 time: 0.301
2025-12-11 06:23:51,805: Train batch 82600: loss: 10.78 grad norm: 851.70 time: 0.337
2025-12-11 06:24:54,178: Train batch 82800: loss: 6.48 grad norm: 53.93 time: 0.261
2025-12-11 06:25:58,251: Train batch 83000: loss: 6.83 grad norm: 106.65 time: 0.288
2025-12-11 06:27:02,556: Train batch 83200: loss: 5.02 grad norm: 62.34 time: 0.290
2025-12-11 06:28:06,231: Train batch 83400: loss: 5.16 grad norm: 63.13 time: 0.336
2025-12-11 06:29:09,971: Train batch 83600: loss: 8.06 grad norm: 79.32 time: 0.232
2025-12-11 06:30:12,797: Train batch 83800: loss: 8.81 grad norm: 674.28 time: 0.282
2025-12-11 06:31:14,461: Train batch 84000: loss: 10.23 grad norm: 53.15 time: 0.275
2025-12-11 06:31:14,461: Running test after training batch: 84000
2025-12-11 06:31:39,806: Val batch 84000: PER (avg): 0.4040 CTC Loss (avg): 54.6915 time: 25.344
2025-12-11 06:31:39,806: t15.2023.08.11 val PER: 1.0000
2025-12-11 06:31:39,806: t15.2023.08.13 val PER: 0.4335
2025-12-11 06:31:39,806: t15.2023.08.18 val PER: 0.3713
2025-12-11 06:31:39,806: t15.2023.08.20 val PER: 0.3455
2025-12-11 06:31:39,807: t15.2023.08.25 val PER: 0.4081
2025-12-11 06:31:39,807: t15.2023.08.27 val PER: 0.4823
2025-12-11 06:31:39,807: t15.2023.09.01 val PER: 0.3287
2025-12-11 06:31:39,807: t15.2023.09.03 val PER: 0.3717
2025-12-11 06:31:39,807: t15.2023.09.24 val PER: 0.3362
2025-12-11 06:31:39,807: t15.2023.09.29 val PER: 0.3452
2025-12-11 06:31:39,807: t15.2023.10.01 val PER: 0.4399
2025-12-11 06:31:39,807: t15.2023.10.06 val PER: 0.3348
2025-12-11 06:31:39,807: t15.2023.10.08 val PER: 0.4790
2025-12-11 06:31:39,807: t15.2023.10.13 val PER: 0.3957
2025-12-11 06:31:39,807: t15.2023.10.15 val PER: 0.3731
2025-12-11 06:31:39,807: t15.2023.10.20 val PER: 0.4631
2025-12-11 06:31:39,807: t15.2023.10.22 val PER: 0.3597
2025-12-11 06:31:39,807: t15.2023.11.03 val PER: 0.4057
2025-12-11 06:31:39,807: t15.2023.11.04 val PER: 0.1604
2025-12-11 06:31:39,807: t15.2023.11.17 val PER: 0.2006
2025-12-11 06:31:39,807: t15.2023.11.19 val PER: 0.2535
2025-12-11 06:31:39,807: t15.2023.11.26 val PER: 0.3616
2025-12-11 06:31:39,807: t15.2023.12.03 val PER: 0.3361
2025-12-11 06:31:39,808: t15.2023.12.08 val PER: 0.3202
2025-12-11 06:31:39,808: t15.2023.12.10 val PER: 0.2904
2025-12-11 06:31:39,808: t15.2023.12.17 val PER: 0.4563
2025-12-11 06:31:39,808: t15.2023.12.29 val PER: 0.4338
2025-12-11 06:31:39,808: t15.2024.02.25 val PER: 0.3708
2025-12-11 06:31:39,808: t15.2024.03.03 val PER: 1.0000
2025-12-11 06:31:39,808: t15.2024.03.08 val PER: 0.4993
2025-12-11 06:31:39,808: t15.2024.03.15 val PER: 0.4340
2025-12-11 06:31:39,808: t15.2024.03.17 val PER: 0.3801
2025-12-11 06:31:39,808: t15.2024.04.25 val PER: 1.0000
2025-12-11 06:31:39,808: t15.2024.04.28 val PER: 1.0000
2025-12-11 06:31:39,808: t15.2024.05.10 val PER: 0.4294
2025-12-11 06:31:39,808: t15.2024.06.14 val PER: 0.4227
2025-12-11 06:31:39,808: t15.2024.07.19 val PER: 0.5906
2025-12-11 06:31:39,808: t15.2024.07.21 val PER: 0.3379
2025-12-11 06:31:39,808: t15.2024.07.28 val PER: 0.3853
2025-12-11 06:31:39,808: t15.2025.01.10 val PER: 0.5826
2025-12-11 06:31:39,808: t15.2025.01.12 val PER: 0.4242
2025-12-11 06:31:39,808: t15.2025.03.14 val PER: 0.6376
2025-12-11 06:31:39,809: t15.2025.03.16 val PER: 0.4777
2025-12-11 06:31:39,809: t15.2025.03.30 val PER: 0.5954
2025-12-11 06:31:39,809: t15.2025.04.13 val PER: 0.4993
2025-12-11 06:32:43,951: Train batch 84200: loss: 6.18 grad norm: 84.93 time: 0.382
2025-12-11 06:33:48,659: Train batch 84400: loss: 8.94 grad norm: 48.52 time: 0.294
2025-12-11 06:34:52,121: Train batch 84600: loss: 10.38 grad norm: 125.70 time: 0.397
2025-12-11 06:35:55,379: Train batch 84800: loss: 3.96 grad norm: 600.36 time: 0.339
2025-12-11 06:36:58,783: Train batch 85000: loss: 3.24 grad norm: 477.15 time: 0.303
2025-12-11 06:38:01,610: Train batch 85200: loss: 8.26 grad norm: 84.52 time: 0.271
2025-12-11 06:39:05,183: Train batch 85400: loss: 7.36 grad norm: 78.65 time: 0.384
2025-12-11 06:40:09,477: Train batch 85600: loss: 10.54 grad norm: 70.79 time: 0.267
2025-12-11 06:41:11,890: Train batch 85800: loss: 7.07 grad norm: 47.61 time: 0.315
2025-12-11 06:42:15,371: Train batch 86000: loss: 5.03 grad norm: 43.43 time: 0.292
2025-12-11 06:42:15,371: Running test after training batch: 86000
2025-12-11 06:42:40,624: Val batch 86000: PER (avg): 0.3936 CTC Loss (avg): 53.1188 time: 25.253
2025-12-11 06:42:40,624: t15.2023.08.11 val PER: 1.0000
2025-12-11 06:42:40,625: t15.2023.08.13 val PER: 0.4148
2025-12-11 06:42:40,625: t15.2023.08.18 val PER: 0.3638
2025-12-11 06:42:40,625: t15.2023.08.20 val PER: 0.3249
2025-12-11 06:42:40,625: t15.2023.08.25 val PER: 0.4051
2025-12-11 06:42:40,625: t15.2023.08.27 val PER: 0.4614
2025-12-11 06:42:40,625: t15.2023.09.01 val PER: 0.2995
2025-12-11 06:42:40,625: t15.2023.09.03 val PER: 0.3563
2025-12-11 06:42:40,625: t15.2023.09.24 val PER: 0.3204
2025-12-11 06:42:40,625: t15.2023.09.29 val PER: 0.3459
2025-12-11 06:42:40,625: t15.2023.10.01 val PER: 0.4188
2025-12-11 06:42:40,625: t15.2023.10.06 val PER: 0.3079
2025-12-11 06:42:40,625: t15.2023.10.08 val PER: 0.4750
2025-12-11 06:42:40,625: t15.2023.10.13 val PER: 0.3964
2025-12-11 06:42:40,625: t15.2023.10.15 val PER: 0.3672
2025-12-11 06:42:40,625: t15.2023.10.20 val PER: 0.4362
2025-12-11 06:42:40,625: t15.2023.10.22 val PER: 0.3541
2025-12-11 06:42:40,625: t15.2023.11.03 val PER: 0.4043
2025-12-11 06:42:40,625: t15.2023.11.04 val PER: 0.1843
2025-12-11 06:42:40,625: t15.2023.11.17 val PER: 0.1835
2025-12-11 06:42:40,626: t15.2023.11.19 val PER: 0.2555
2025-12-11 06:42:40,626: t15.2023.11.26 val PER: 0.3601
2025-12-11 06:42:40,626: t15.2023.12.03 val PER: 0.2952
2025-12-11 06:42:40,626: t15.2023.12.08 val PER: 0.3103
2025-12-11 06:42:40,626: t15.2023.12.10 val PER: 0.2838
2025-12-11 06:42:40,626: t15.2023.12.17 val PER: 0.4168
2025-12-11 06:42:40,626: t15.2023.12.29 val PER: 0.4255
2025-12-11 06:42:40,626: t15.2024.02.25 val PER: 0.3722
2025-12-11 06:42:40,626: t15.2024.03.03 val PER: 1.0000
2025-12-11 06:42:40,626: t15.2024.03.08 val PER: 0.4751
2025-12-11 06:42:40,626: t15.2024.03.15 val PER: 0.4246
2025-12-11 06:42:40,626: t15.2024.03.17 val PER: 0.3828
2025-12-11 06:42:40,626: t15.2024.04.25 val PER: 1.0000
2025-12-11 06:42:40,626: t15.2024.04.28 val PER: 1.0000
2025-12-11 06:42:40,626: t15.2024.05.10 val PER: 0.4279
2025-12-11 06:42:40,626: t15.2024.06.14 val PER: 0.4022
2025-12-11 06:42:40,626: t15.2024.07.19 val PER: 0.5768
2025-12-11 06:42:40,626: t15.2024.07.21 val PER: 0.3331
2025-12-11 06:42:40,627: t15.2024.07.28 val PER: 0.3890
2025-12-11 06:42:40,627: t15.2025.01.10 val PER: 0.5744
2025-12-11 06:42:40,627: t15.2025.01.12 val PER: 0.4103
2025-12-11 06:42:40,627: t15.2025.03.14 val PER: 0.6317
2025-12-11 06:42:40,627: t15.2025.03.16 val PER: 0.4764
2025-12-11 06:42:40,627: t15.2025.03.30 val PER: 0.5862
2025-12-11 06:42:40,627: t15.2025.04.13 val PER: 0.4964
2025-12-11 06:43:43,462: Train batch 86200: loss: 17.76 grad norm: 333.74 time: 0.328
2025-12-11 06:44:46,557: Train batch 86400: loss: 12.90 grad norm: 99.15 time: 0.265
2025-12-11 06:45:49,540: Train batch 86600: loss: 10.13 grad norm: 86.56 time: 0.308
2025-12-11 06:46:52,406: Train batch 86800: loss: 11.62 grad norm: 132.98 time: 0.327
2025-12-11 06:47:56,392: Train batch 87000: loss: 5.17 grad norm: 148.29 time: 0.365
2025-12-11 06:48:58,915: Train batch 87200: loss: 6.18 grad norm: 60.35 time: 0.255
2025-12-11 06:50:01,968: Train batch 87400: loss: 9.92 grad norm: 167.14 time: 0.276
2025-12-11 06:51:06,239: Train batch 87600: loss: 9.36 grad norm: 300.68 time: 0.266
2025-12-11 06:52:09,715: Train batch 87800: loss: 8.66 grad norm: 60.00 time: 0.311
2025-12-11 06:53:14,158: Train batch 88000: loss: 7.04 grad norm: 356.36 time: 0.284
2025-12-11 06:53:14,158: Running test after training batch: 88000
2025-12-11 06:53:39,466: Val batch 88000: PER (avg): 0.4089 CTC Loss (avg): 56.5418 time: 25.308
2025-12-11 06:53:39,467: t15.2023.08.11 val PER: 1.0000
2025-12-11 06:53:39,467: t15.2023.08.13 val PER: 0.4314
2025-12-11 06:53:39,467: t15.2023.08.18 val PER: 0.3738
2025-12-11 06:53:39,467: t15.2023.08.20 val PER: 0.3447
2025-12-11 06:53:39,467: t15.2023.08.25 val PER: 0.4202
2025-12-11 06:53:39,467: t15.2023.08.27 val PER: 0.4743
2025-12-11 06:53:39,468: t15.2023.09.01 val PER: 0.3352
2025-12-11 06:53:39,468: t15.2023.09.03 val PER: 0.3705
2025-12-11 06:53:39,468: t15.2023.09.24 val PER: 0.3277
2025-12-11 06:53:39,468: t15.2023.09.29 val PER: 0.3440
2025-12-11 06:53:39,468: t15.2023.10.01 val PER: 0.4326
2025-12-11 06:53:39,468: t15.2023.10.06 val PER: 0.3294
2025-12-11 06:53:39,468: t15.2023.10.08 val PER: 0.4817
2025-12-11 06:53:39,468: t15.2023.10.13 val PER: 0.4236
2025-12-11 06:53:39,468: t15.2023.10.15 val PER: 0.3659
2025-12-11 06:53:39,468: t15.2023.10.20 val PER: 0.4597
2025-12-11 06:53:39,468: t15.2023.10.22 val PER: 0.3575
2025-12-11 06:53:39,468: t15.2023.11.03 val PER: 0.4199
2025-12-11 06:53:39,468: t15.2023.11.04 val PER: 0.1741
2025-12-11 06:53:39,468: t15.2023.11.17 val PER: 0.1866
2025-12-11 06:53:39,468: t15.2023.11.19 val PER: 0.2834
2025-12-11 06:53:39,468: t15.2023.11.26 val PER: 0.3870
2025-12-11 06:53:39,468: t15.2023.12.03 val PER: 0.3340
2025-12-11 06:53:39,468: t15.2023.12.08 val PER: 0.3269
2025-12-11 06:53:39,468: t15.2023.12.10 val PER: 0.2957
2025-12-11 06:53:39,468: t15.2023.12.17 val PER: 0.4740
2025-12-11 06:53:39,469: t15.2023.12.29 val PER: 0.4406
2025-12-11 06:53:39,469: t15.2024.02.25 val PER: 0.3876
2025-12-11 06:53:39,469: t15.2024.03.03 val PER: 1.0000
2025-12-11 06:53:39,469: t15.2024.03.08 val PER: 0.4865
2025-12-11 06:53:39,469: t15.2024.03.15 val PER: 0.4371
2025-12-11 06:53:39,469: t15.2024.03.17 val PER: 0.3870
2025-12-11 06:53:39,469: t15.2024.04.25 val PER: 1.0000
2025-12-11 06:53:39,469: t15.2024.04.28 val PER: 1.0000
2025-12-11 06:53:39,469: t15.2024.05.10 val PER: 0.4398
2025-12-11 06:53:39,469: t15.2024.06.14 val PER: 0.4416
2025-12-11 06:53:39,469: t15.2024.07.19 val PER: 0.5867
2025-12-11 06:53:39,469: t15.2024.07.21 val PER: 0.3566
2025-12-11 06:53:39,469: t15.2024.07.28 val PER: 0.3978
2025-12-11 06:53:39,469: t15.2025.01.10 val PER: 0.5702
2025-12-11 06:53:39,469: t15.2025.01.12 val PER: 0.4419
2025-12-11 06:53:39,469: t15.2025.03.14 val PER: 0.6272
2025-12-11 06:53:39,469: t15.2025.03.16 val PER: 0.4686
2025-12-11 06:53:39,469: t15.2025.03.30 val PER: 0.5931
2025-12-11 06:53:39,469: t15.2025.04.13 val PER: 0.5221
2025-12-11 06:54:42,772: Train batch 88200: loss: 10.61 grad norm: 121.86 time: 0.385
2025-12-11 06:55:46,228: Train batch 88400: loss: 9.82 grad norm: 539.87 time: 0.319
2025-12-11 06:56:49,746: Train batch 88600: loss: 7.64 grad norm: 446.07 time: 0.257
2025-12-11 06:57:52,679: Train batch 88800: loss: 6.02 grad norm: 515.67 time: 0.325
2025-12-11 06:58:56,045: Train batch 89000: loss: 7.87 grad norm: 50.02 time: 0.238
2025-12-11 06:59:58,643: Train batch 89200: loss: 4.17 grad norm: 34.80 time: 0.236
2025-12-11 07:01:03,205: Train batch 89400: loss: 9.74 grad norm: 123.56 time: 0.371
2025-12-11 07:02:05,176: Train batch 89600: loss: 10.65 grad norm: 182.79 time: 0.427
2025-12-11 07:03:09,096: Train batch 89800: loss: 3.79 grad norm: 196.91 time: 0.227
2025-12-11 07:04:12,912: Train batch 90000: loss: 7.53 grad norm: 55.06 time: 0.320
2025-12-11 07:04:12,913: Running test after training batch: 90000
2025-12-11 07:04:38,184: Val batch 90000: PER (avg): 0.3973 CTC Loss (avg): 53.9990 time: 25.272
2025-12-11 07:04:38,185: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:04:38,185: t15.2023.08.13 val PER: 0.4241
2025-12-11 07:04:38,185: t15.2023.08.18 val PER: 0.3596
2025-12-11 07:04:38,185: t15.2023.08.20 val PER: 0.3407
2025-12-11 07:04:38,185: t15.2023.08.25 val PER: 0.4157
2025-12-11 07:04:38,185: t15.2023.08.27 val PER: 0.4550
2025-12-11 07:04:38,185: t15.2023.09.01 val PER: 0.3206
2025-12-11 07:04:38,185: t15.2023.09.03 val PER: 0.3563
2025-12-11 07:04:38,185: t15.2023.09.24 val PER: 0.3350
2025-12-11 07:04:38,185: t15.2023.09.29 val PER: 0.3318
2025-12-11 07:04:38,185: t15.2023.10.01 val PER: 0.4280
2025-12-11 07:04:38,185: t15.2023.10.06 val PER: 0.3272
2025-12-11 07:04:38,185: t15.2023.10.08 val PER: 0.4709
2025-12-11 07:04:38,185: t15.2023.10.13 val PER: 0.4011
2025-12-11 07:04:38,185: t15.2023.10.15 val PER: 0.3599
2025-12-11 07:04:38,185: t15.2023.10.20 val PER: 0.4430
2025-12-11 07:04:38,185: t15.2023.10.22 val PER: 0.3619
2025-12-11 07:04:38,185: t15.2023.11.03 val PER: 0.3894
2025-12-11 07:04:38,186: t15.2023.11.04 val PER: 0.1536
2025-12-11 07:04:38,186: t15.2023.11.17 val PER: 0.1851
2025-12-11 07:04:38,186: t15.2023.11.19 val PER: 0.2675
2025-12-11 07:04:38,186: t15.2023.11.26 val PER: 0.3616
2025-12-11 07:04:38,186: t15.2023.12.03 val PER: 0.3141
2025-12-11 07:04:38,186: t15.2023.12.08 val PER: 0.3069
2025-12-11 07:04:38,186: t15.2023.12.10 val PER: 0.3009
2025-12-11 07:04:38,186: t15.2023.12.17 val PER: 0.4584
2025-12-11 07:04:38,186: t15.2023.12.29 val PER: 0.4221
2025-12-11 07:04:38,186: t15.2024.02.25 val PER: 0.3567
2025-12-11 07:04:38,186: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:04:38,186: t15.2024.03.08 val PER: 0.4822
2025-12-11 07:04:38,186: t15.2024.03.15 val PER: 0.4290
2025-12-11 07:04:38,186: t15.2024.03.17 val PER: 0.3815
2025-12-11 07:04:38,186: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:04:38,186: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:04:38,186: t15.2024.05.10 val PER: 0.4473
2025-12-11 07:04:38,186: t15.2024.06.14 val PER: 0.4211
2025-12-11 07:04:38,186: t15.2024.07.19 val PER: 0.5722
2025-12-11 07:04:38,187: t15.2024.07.21 val PER: 0.3366
2025-12-11 07:04:38,187: t15.2024.07.28 val PER: 0.3941
2025-12-11 07:04:38,187: t15.2025.01.10 val PER: 0.5730
2025-12-11 07:04:38,187: t15.2025.01.12 val PER: 0.4134
2025-12-11 07:04:38,187: t15.2025.03.14 val PER: 0.6287
2025-12-11 07:04:38,187: t15.2025.03.16 val PER: 0.4699
2025-12-11 07:04:38,187: t15.2025.03.30 val PER: 0.5885
2025-12-11 07:04:38,187: t15.2025.04.13 val PER: 0.5136
2025-12-11 07:05:41,673: Train batch 90200: loss: 5.88 grad norm: 48.30 time: 0.282
2025-12-11 07:06:45,093: Train batch 90400: loss: 10.08 grad norm: 128.10 time: 0.255
2025-12-11 07:07:47,264: Train batch 90600: loss: 6.10 grad norm: 115.55 time: 0.290
2025-12-11 07:08:50,304: Train batch 90800: loss: 10.08 grad norm: 90.76 time: 0.330
2025-12-11 07:09:52,732: Train batch 91000: loss: 16.05 grad norm: 308.62 time: 0.315
2025-12-11 07:10:55,163: Train batch 91200: loss: 13.08 grad norm: 247.20 time: 0.390
2025-12-11 07:11:58,209: Train batch 91400: loss: 13.03 grad norm: 78.64 time: 0.330
2025-12-11 07:13:01,711: Train batch 91600: loss: 10.09 grad norm: 240.88 time: 0.385
2025-12-11 07:14:06,120: Train batch 91800: loss: 13.09 grad norm: 197.10 time: 0.442
2025-12-11 07:15:09,693: Train batch 92000: loss: 8.90 grad norm: 46.67 time: 0.348
2025-12-11 07:15:09,693: Running test after training batch: 92000
2025-12-11 07:15:34,966: Val batch 92000: PER (avg): 0.3923 CTC Loss (avg): 53.1135 time: 25.273
2025-12-11 07:15:34,966: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:15:34,966: t15.2023.08.13 val PER: 0.4116
2025-12-11 07:15:34,966: t15.2023.08.18 val PER: 0.3604
2025-12-11 07:15:34,966: t15.2023.08.20 val PER: 0.3336
2025-12-11 07:15:34,966: t15.2023.08.25 val PER: 0.3840
2025-12-11 07:15:34,966: t15.2023.08.27 val PER: 0.4486
2025-12-11 07:15:34,967: t15.2023.09.01 val PER: 0.3174
2025-12-11 07:15:34,967: t15.2023.09.03 val PER: 0.3480
2025-12-11 07:15:34,967: t15.2023.09.24 val PER: 0.3240
2025-12-11 07:15:34,967: t15.2023.09.29 val PER: 0.3325
2025-12-11 07:15:34,967: t15.2023.10.01 val PER: 0.4168
2025-12-11 07:15:34,967: t15.2023.10.06 val PER: 0.3132
2025-12-11 07:15:34,967: t15.2023.10.08 val PER: 0.4750
2025-12-11 07:15:34,967: t15.2023.10.13 val PER: 0.4073
2025-12-11 07:15:34,967: t15.2023.10.15 val PER: 0.3579
2025-12-11 07:15:34,967: t15.2023.10.20 val PER: 0.4228
2025-12-11 07:15:34,967: t15.2023.10.22 val PER: 0.3474
2025-12-11 07:15:34,967: t15.2023.11.03 val PER: 0.3853
2025-12-11 07:15:34,967: t15.2023.11.04 val PER: 0.1468
2025-12-11 07:15:34,967: t15.2023.11.17 val PER: 0.1742
2025-12-11 07:15:34,967: t15.2023.11.19 val PER: 0.2355
2025-12-11 07:15:34,967: t15.2023.11.26 val PER: 0.3572
2025-12-11 07:15:34,967: t15.2023.12.03 val PER: 0.3109
2025-12-11 07:15:34,967: t15.2023.12.08 val PER: 0.3069
2025-12-11 07:15:34,967: t15.2023.12.10 val PER: 0.2786
2025-12-11 07:15:34,968: t15.2023.12.17 val PER: 0.4522
2025-12-11 07:15:34,968: t15.2023.12.29 val PER: 0.4248
2025-12-11 07:15:34,968: t15.2024.02.25 val PER: 0.3553
2025-12-11 07:15:34,968: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:15:34,968: t15.2024.03.08 val PER: 0.4637
2025-12-11 07:15:34,968: t15.2024.03.15 val PER: 0.4303
2025-12-11 07:15:34,968: t15.2024.03.17 val PER: 0.3808
2025-12-11 07:15:34,968: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:15:34,968: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:15:34,968: t15.2024.05.10 val PER: 0.4146
2025-12-11 07:15:34,968: t15.2024.06.14 val PER: 0.4132
2025-12-11 07:15:34,968: t15.2024.07.19 val PER: 0.5854
2025-12-11 07:15:34,968: t15.2024.07.21 val PER: 0.3345
2025-12-11 07:15:34,968: t15.2024.07.28 val PER: 0.3816
2025-12-11 07:15:34,968: t15.2025.01.10 val PER: 0.5661
2025-12-11 07:15:34,968: t15.2025.01.12 val PER: 0.4065
2025-12-11 07:15:34,968: t15.2025.03.14 val PER: 0.6435
2025-12-11 07:15:34,968: t15.2025.03.16 val PER: 0.4725
2025-12-11 07:15:34,968: t15.2025.03.30 val PER: 0.5793
2025-12-11 07:15:34,968: t15.2025.04.13 val PER: 0.5207
2025-12-11 07:16:38,288: Train batch 92200: loss: 10.89 grad norm: 149.41 time: 0.280
2025-12-11 07:17:42,693: Train batch 92400: loss: 8.84 grad norm: 579.50 time: 0.328
2025-12-11 07:18:46,662: Train batch 92600: loss: 10.52 grad norm: 80.72 time: 0.265
2025-12-11 07:19:49,784: Train batch 92800: loss: 6.16 grad norm: 437.31 time: 0.252
2025-12-11 07:20:54,105: Train batch 93000: loss: 4.21 grad norm: 244.30 time: 0.324
2025-12-11 07:21:56,991: Train batch 93200: loss: 8.12 grad norm: 97.18 time: 0.267
2025-12-11 07:22:59,900: Train batch 93400: loss: 7.57 grad norm: 128.43 time: 0.307
2025-12-11 07:24:03,458: Train batch 93600: loss: 7.20 grad norm: 110.62 time: 0.244
2025-12-11 07:25:07,132: Train batch 93800: loss: 12.32 grad norm: 136.29 time: 0.366
2025-12-11 07:26:10,530: Train batch 94000: loss: 9.66 grad norm: 66.23 time: 0.288
2025-12-11 07:26:10,530: Running test after training batch: 94000
2025-12-11 07:26:35,842: Val batch 94000: PER (avg): 0.3882 CTC Loss (avg): 52.3526 time: 25.312
2025-12-11 07:26:35,843: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:26:35,843: t15.2023.08.13 val PER: 0.4241
2025-12-11 07:26:35,843: t15.2023.08.18 val PER: 0.3504
2025-12-11 07:26:35,843: t15.2023.08.20 val PER: 0.3296
2025-12-11 07:26:35,843: t15.2023.08.25 val PER: 0.3840
2025-12-11 07:26:35,843: t15.2023.08.27 val PER: 0.4614
2025-12-11 07:26:35,843: t15.2023.09.01 val PER: 0.3044
2025-12-11 07:26:35,843: t15.2023.09.03 val PER: 0.3444
2025-12-11 07:26:35,843: t15.2023.09.24 val PER: 0.3252
2025-12-11 07:26:35,843: t15.2023.09.29 val PER: 0.3287
2025-12-11 07:26:35,843: t15.2023.10.01 val PER: 0.4168
2025-12-11 07:26:35,843: t15.2023.10.06 val PER: 0.2863
2025-12-11 07:26:35,843: t15.2023.10.08 val PER: 0.4628
2025-12-11 07:26:35,843: t15.2023.10.13 val PER: 0.3809
2025-12-11 07:26:35,843: t15.2023.10.15 val PER: 0.3599
2025-12-11 07:26:35,843: t15.2023.10.20 val PER: 0.4329
2025-12-11 07:26:35,844: t15.2023.10.22 val PER: 0.3430
2025-12-11 07:26:35,844: t15.2023.11.03 val PER: 0.3860
2025-12-11 07:26:35,844: t15.2023.11.04 val PER: 0.1365
2025-12-11 07:26:35,844: t15.2023.11.17 val PER: 0.1851
2025-12-11 07:26:35,844: t15.2023.11.19 val PER: 0.2395
2025-12-11 07:26:35,844: t15.2023.11.26 val PER: 0.3601
2025-12-11 07:26:35,844: t15.2023.12.03 val PER: 0.3162
2025-12-11 07:26:35,844: t15.2023.12.08 val PER: 0.3202
2025-12-11 07:26:35,844: t15.2023.12.10 val PER: 0.2812
2025-12-11 07:26:35,844: t15.2023.12.17 val PER: 0.4418
2025-12-11 07:26:35,844: t15.2023.12.29 val PER: 0.4187
2025-12-11 07:26:35,844: t15.2024.02.25 val PER: 0.3610
2025-12-11 07:26:35,844: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:26:35,844: t15.2024.03.08 val PER: 0.4794
2025-12-11 07:26:35,844: t15.2024.03.15 val PER: 0.4196
2025-12-11 07:26:35,844: t15.2024.03.17 val PER: 0.3703
2025-12-11 07:26:35,844: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:26:35,844: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:26:35,844: t15.2024.05.10 val PER: 0.4264
2025-12-11 07:26:35,845: t15.2024.06.14 val PER: 0.4054
2025-12-11 07:26:35,845: t15.2024.07.19 val PER: 0.5669
2025-12-11 07:26:35,845: t15.2024.07.21 val PER: 0.3297
2025-12-11 07:26:35,845: t15.2024.07.28 val PER: 0.3801
2025-12-11 07:26:35,845: t15.2025.01.10 val PER: 0.5785
2025-12-11 07:26:35,845: t15.2025.01.12 val PER: 0.3895
2025-12-11 07:26:35,845: t15.2025.03.14 val PER: 0.6317
2025-12-11 07:26:35,845: t15.2025.03.16 val PER: 0.4581
2025-12-11 07:26:35,845: t15.2025.03.30 val PER: 0.5782
2025-12-11 07:26:35,845: t15.2025.04.13 val PER: 0.4964
2025-12-11 07:27:39,444: Train batch 94200: loss: 21.20 grad norm: 367.86 time: 0.416
2025-12-11 07:28:41,635: Train batch 94400: loss: 6.06 grad norm: 143.83 time: 0.245
2025-12-11 07:29:44,270: Train batch 94600: loss: 5.26 grad norm: 157.72 time: 0.296
2025-12-11 07:30:46,621: Train batch 94800: loss: 5.55 grad norm: 70.65 time: 0.288
2025-12-11 07:31:49,959: Train batch 95000: loss: 10.82 grad norm: 2309.58 time: 0.274
2025-12-11 07:32:53,440: Train batch 95200: loss: 8.28 grad norm: 78.18 time: 0.271
2025-12-11 07:33:56,747: Train batch 95400: loss: 10.83 grad norm: 199.11 time: 0.398
2025-12-11 07:34:59,223: Train batch 95600: loss: 4.27 grad norm: 44.69 time: 0.262
2025-12-11 07:36:01,609: Train batch 95800: loss: 10.11 grad norm: 77.94 time: 0.247
2025-12-11 07:37:05,021: Train batch 96000: loss: 11.88 grad norm: 410.16 time: 0.279
2025-12-11 07:37:05,022: Running test after training batch: 96000
2025-12-11 07:37:30,239: Val batch 96000: PER (avg): 0.3895 CTC Loss (avg): 53.2439 time: 25.217
2025-12-11 07:37:30,239: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:37:30,239: t15.2023.08.13 val PER: 0.4168
2025-12-11 07:37:30,239: t15.2023.08.18 val PER: 0.3579
2025-12-11 07:37:30,239: t15.2023.08.20 val PER: 0.3368
2025-12-11 07:37:30,239: t15.2023.08.25 val PER: 0.3765
2025-12-11 07:37:30,239: t15.2023.08.27 val PER: 0.4614
2025-12-11 07:37:30,239: t15.2023.09.01 val PER: 0.3093
2025-12-11 07:37:30,239: t15.2023.09.03 val PER: 0.3385
2025-12-11 07:37:30,239: t15.2023.09.24 val PER: 0.3180
2025-12-11 07:37:30,240: t15.2023.09.29 val PER: 0.3280
2025-12-11 07:37:30,240: t15.2023.10.01 val PER: 0.4155
2025-12-11 07:37:30,240: t15.2023.10.06 val PER: 0.3122
2025-12-11 07:37:30,240: t15.2023.10.08 val PER: 0.4574
2025-12-11 07:37:30,240: t15.2023.10.13 val PER: 0.4019
2025-12-11 07:37:30,240: t15.2023.10.15 val PER: 0.3553
2025-12-11 07:37:30,240: t15.2023.10.20 val PER: 0.4362
2025-12-11 07:37:30,240: t15.2023.10.22 val PER: 0.3474
2025-12-11 07:37:30,240: t15.2023.11.03 val PER: 0.3860
2025-12-11 07:37:30,240: t15.2023.11.04 val PER: 0.1604
2025-12-11 07:37:30,240: t15.2023.11.17 val PER: 0.1788
2025-12-11 07:37:30,240: t15.2023.11.19 val PER: 0.2375
2025-12-11 07:37:30,240: t15.2023.11.26 val PER: 0.3442
2025-12-11 07:37:30,240: t15.2023.12.03 val PER: 0.3046
2025-12-11 07:37:30,240: t15.2023.12.08 val PER: 0.3056
2025-12-11 07:37:30,240: t15.2023.12.10 val PER: 0.2852
2025-12-11 07:37:30,241: t15.2023.12.17 val PER: 0.4418
2025-12-11 07:37:30,241: t15.2023.12.29 val PER: 0.4235
2025-12-11 07:37:30,241: t15.2024.02.25 val PER: 0.3624
2025-12-11 07:37:30,241: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:37:30,241: t15.2024.03.08 val PER: 0.4865
2025-12-11 07:37:30,241: t15.2024.03.15 val PER: 0.4353
2025-12-11 07:37:30,241: t15.2024.03.17 val PER: 0.3752
2025-12-11 07:37:30,241: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:37:30,241: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:37:30,241: t15.2024.05.10 val PER: 0.4220
2025-12-11 07:37:30,241: t15.2024.06.14 val PER: 0.4274
2025-12-11 07:37:30,241: t15.2024.07.19 val PER: 0.5662
2025-12-11 07:37:30,241: t15.2024.07.21 val PER: 0.3248
2025-12-11 07:37:30,241: t15.2024.07.28 val PER: 0.3875
2025-12-11 07:37:30,241: t15.2025.01.10 val PER: 0.5634
2025-12-11 07:37:30,241: t15.2025.01.12 val PER: 0.4034
2025-12-11 07:37:30,241: t15.2025.03.14 val PER: 0.6228
2025-12-11 07:37:30,242: t15.2025.03.16 val PER: 0.4529
2025-12-11 07:37:30,242: t15.2025.03.30 val PER: 0.5839
2025-12-11 07:37:30,242: t15.2025.04.13 val PER: 0.4950
2025-12-11 07:38:32,944: Train batch 96200: loss: 11.21 grad norm: 70.99 time: 0.420
2025-12-11 07:39:36,059: Train batch 96400: loss: 8.50 grad norm: 53.38 time: 0.370
2025-12-11 07:40:37,072: Train batch 96600: loss: 6.88 grad norm: 157.57 time: 0.287
2025-12-11 07:41:40,079: Train batch 96800: loss: 9.76 grad norm: 648.25 time: 0.359
2025-12-11 07:42:41,751: Train batch 97000: loss: 6.52 grad norm: 114.88 time: 0.234
2025-12-11 07:43:44,810: Train batch 97200: loss: 6.91 grad norm: 54.62 time: 0.265
2025-12-11 07:44:47,165: Train batch 97400: loss: 9.77 grad norm: 89.53 time: 0.260
2025-12-11 07:45:50,608: Train batch 97600: loss: 8.59 grad norm: 279.87 time: 0.309
2025-12-11 07:46:52,827: Train batch 97800: loss: 18.59 grad norm: 163.62 time: 0.329
2025-12-11 07:47:56,458: Train batch 98000: loss: 9.55 grad norm: 52.07 time: 0.326
2025-12-11 07:47:56,459: Running test after training batch: 98000
2025-12-11 07:48:21,683: Val batch 98000: PER (avg): 0.3791 CTC Loss (avg): 51.7458 time: 25.224
2025-12-11 07:48:21,683: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:48:21,683: t15.2023.08.13 val PER: 0.4127
2025-12-11 07:48:21,683: t15.2023.08.18 val PER: 0.3462
2025-12-11 07:48:21,683: t15.2023.08.20 val PER: 0.3233
2025-12-11 07:48:21,683: t15.2023.08.25 val PER: 0.3765
2025-12-11 07:48:21,684: t15.2023.08.27 val PER: 0.4486
2025-12-11 07:48:21,684: t15.2023.09.01 val PER: 0.2971
2025-12-11 07:48:21,684: t15.2023.09.03 val PER: 0.3325
2025-12-11 07:48:21,684: t15.2023.09.24 val PER: 0.3374
2025-12-11 07:48:21,684: t15.2023.09.29 val PER: 0.3223
2025-12-11 07:48:21,684: t15.2023.10.01 val PER: 0.4009
2025-12-11 07:48:21,684: t15.2023.10.06 val PER: 0.2766
2025-12-11 07:48:21,684: t15.2023.10.08 val PER: 0.4614
2025-12-11 07:48:21,684: t15.2023.10.13 val PER: 0.3840
2025-12-11 07:48:21,684: t15.2023.10.15 val PER: 0.3566
2025-12-11 07:48:21,684: t15.2023.10.20 val PER: 0.4161
2025-12-11 07:48:21,684: t15.2023.10.22 val PER: 0.3330
2025-12-11 07:48:21,684: t15.2023.11.03 val PER: 0.3609
2025-12-11 07:48:21,684: t15.2023.11.04 val PER: 0.1365
2025-12-11 07:48:21,684: t15.2023.11.17 val PER: 0.1602
2025-12-11 07:48:21,684: t15.2023.11.19 val PER: 0.2395
2025-12-11 07:48:21,684: t15.2023.11.26 val PER: 0.3428
2025-12-11 07:48:21,684: t15.2023.12.03 val PER: 0.3057
2025-12-11 07:48:21,684: t15.2023.12.08 val PER: 0.2883
2025-12-11 07:48:21,684: t15.2023.12.10 val PER: 0.2681
2025-12-11 07:48:21,685: t15.2023.12.17 val PER: 0.4418
2025-12-11 07:48:21,685: t15.2023.12.29 val PER: 0.4036
2025-12-11 07:48:21,685: t15.2024.02.25 val PER: 0.3525
2025-12-11 07:48:21,685: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:48:21,685: t15.2024.03.08 val PER: 0.4523
2025-12-11 07:48:21,685: t15.2024.03.15 val PER: 0.4196
2025-12-11 07:48:21,685: t15.2024.03.17 val PER: 0.3591
2025-12-11 07:48:21,685: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:48:21,685: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:48:21,685: t15.2024.05.10 val PER: 0.4190
2025-12-11 07:48:21,685: t15.2024.06.14 val PER: 0.4022
2025-12-11 07:48:21,685: t15.2024.07.19 val PER: 0.5623
2025-12-11 07:48:21,685: t15.2024.07.21 val PER: 0.3152
2025-12-11 07:48:21,685: t15.2024.07.28 val PER: 0.3750
2025-12-11 07:48:21,685: t15.2025.01.10 val PER: 0.5675
2025-12-11 07:48:21,685: t15.2025.01.12 val PER: 0.3957
2025-12-11 07:48:21,685: t15.2025.03.14 val PER: 0.6302
2025-12-11 07:48:21,685: t15.2025.03.16 val PER: 0.4411
2025-12-11 07:48:21,685: t15.2025.03.30 val PER: 0.5701
2025-12-11 07:48:21,686: t15.2025.04.13 val PER: 0.4936
2025-12-11 07:48:21,686: New best test PER 0.3856 --> 0.3791
2025-12-11 07:48:21,686: Checkpointing model
2025-12-11 07:48:22,839: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 07:49:24,955: Train batch 98200: loss: 7.12 grad norm: 295.70 time: 0.276
2025-12-11 07:50:27,480: Train batch 98400: loss: 8.35 grad norm: 495.59 time: 0.392
2025-12-11 07:51:29,889: Train batch 98600: loss: 13.94 grad norm: 199.43 time: 0.274
2025-12-11 07:52:32,334: Train batch 98800: loss: 9.61 grad norm: 165.51 time: 0.364
2025-12-11 07:53:34,791: Train batch 99000: loss: 9.08 grad norm: 67.17 time: 0.373
2025-12-11 07:54:37,680: Train batch 99200: loss: 10.83 grad norm: 301.25 time: 0.332
2025-12-11 07:55:39,742: Train batch 99400: loss: 8.59 grad norm: 119.93 time: 0.332
2025-12-11 07:56:42,452: Train batch 99600: loss: 5.70 grad norm: 640.64 time: 0.288
2025-12-11 07:57:45,994: Train batch 99800: loss: 6.30 grad norm: 51.99 time: 0.285
2025-12-11 07:58:48,162: Train batch 100000: loss: 5.61 grad norm: 91.52 time: 0.234
2025-12-11 07:58:48,163: Running test after training batch: 100000
2025-12-11 07:59:13,363: Val batch 100000: PER (avg): 0.4038 CTC Loss (avg): 56.3719 time: 25.200
2025-12-11 07:59:13,363: t15.2023.08.11 val PER: 1.0000
2025-12-11 07:59:13,363: t15.2023.08.13 val PER: 0.4366
2025-12-11 07:59:13,363: t15.2023.08.18 val PER: 0.3630
2025-12-11 07:59:13,363: t15.2023.08.20 val PER: 0.3392
2025-12-11 07:59:13,364: t15.2023.08.25 val PER: 0.3976
2025-12-11 07:59:13,364: t15.2023.08.27 val PER: 0.4839
2025-12-11 07:59:13,364: t15.2023.09.01 val PER: 0.3239
2025-12-11 07:59:13,364: t15.2023.09.03 val PER: 0.3563
2025-12-11 07:59:13,364: t15.2023.09.24 val PER: 0.3252
2025-12-11 07:59:13,364: t15.2023.09.29 val PER: 0.3472
2025-12-11 07:59:13,364: t15.2023.10.01 val PER: 0.4221
2025-12-11 07:59:13,364: t15.2023.10.06 val PER: 0.3315
2025-12-11 07:59:13,364: t15.2023.10.08 val PER: 0.4817
2025-12-11 07:59:13,364: t15.2023.10.13 val PER: 0.4119
2025-12-11 07:59:13,364: t15.2023.10.15 val PER: 0.3678
2025-12-11 07:59:13,364: t15.2023.10.20 val PER: 0.4497
2025-12-11 07:59:13,364: t15.2023.10.22 val PER: 0.3608
2025-12-11 07:59:13,364: t15.2023.11.03 val PER: 0.3989
2025-12-11 07:59:13,364: t15.2023.11.04 val PER: 0.1570
2025-12-11 07:59:13,364: t15.2023.11.17 val PER: 0.2053
2025-12-11 07:59:13,364: t15.2023.11.19 val PER: 0.2575
2025-12-11 07:59:13,364: t15.2023.11.26 val PER: 0.3732
2025-12-11 07:59:13,364: t15.2023.12.03 val PER: 0.3099
2025-12-11 07:59:13,365: t15.2023.12.08 val PER: 0.3249
2025-12-11 07:59:13,365: t15.2023.12.10 val PER: 0.2983
2025-12-11 07:59:13,365: t15.2023.12.17 val PER: 0.4709
2025-12-11 07:59:13,365: t15.2023.12.29 val PER: 0.4358
2025-12-11 07:59:13,365: t15.2024.02.25 val PER: 0.3862
2025-12-11 07:59:13,365: t15.2024.03.03 val PER: 1.0000
2025-12-11 07:59:13,365: t15.2024.03.08 val PER: 0.4780
2025-12-11 07:59:13,365: t15.2024.03.15 val PER: 0.4428
2025-12-11 07:59:13,365: t15.2024.03.17 val PER: 0.3801
2025-12-11 07:59:13,365: t15.2024.04.25 val PER: 1.0000
2025-12-11 07:59:13,365: t15.2024.04.28 val PER: 1.0000
2025-12-11 07:59:13,365: t15.2024.05.10 val PER: 0.4502
2025-12-11 07:59:13,365: t15.2024.06.14 val PER: 0.4495
2025-12-11 07:59:13,365: t15.2024.07.19 val PER: 0.5873
2025-12-11 07:59:13,365: t15.2024.07.21 val PER: 0.3497
2025-12-11 07:59:13,365: t15.2024.07.28 val PER: 0.4015
2025-12-11 07:59:13,365: t15.2025.01.10 val PER: 0.5689
2025-12-11 07:59:13,365: t15.2025.01.12 val PER: 0.4249
2025-12-11 07:59:13,365: t15.2025.03.14 val PER: 0.6317
2025-12-11 07:59:13,366: t15.2025.03.16 val PER: 0.4673
2025-12-11 07:59:13,366: t15.2025.03.30 val PER: 0.5805
2025-12-11 07:59:13,366: t15.2025.04.13 val PER: 0.5007
2025-12-11 08:00:16,373: Train batch 100200: loss: 11.48 grad norm: 224.92 time: 0.249
2025-12-11 08:01:19,669: Train batch 100400: loss: 4.42 grad norm: 79.42 time: 0.316
2025-12-11 08:02:23,799: Train batch 100600: loss: 6.19 grad norm: 205.14 time: 0.287
2025-12-11 08:03:26,660: Train batch 100800: loss: 5.28 grad norm: 47.18 time: 0.299
2025-12-11 08:04:30,537: Train batch 101000: loss: 3.88 grad norm: 98.94 time: 0.358
2025-12-11 08:05:34,291: Train batch 101200: loss: 10.39 grad norm: 63.23 time: 0.227
2025-12-11 08:06:37,767: Train batch 101400: loss: 3.35 grad norm: 282.18 time: 0.346
2025-12-11 08:07:40,494: Train batch 101600: loss: 14.38 grad norm: 127.28 time: 0.448
2025-12-11 08:08:42,133: Train batch 101800: loss: 8.16 grad norm: 113.45 time: 0.258
2025-12-11 08:09:45,484: Train batch 102000: loss: 6.43 grad norm: 52.75 time: 0.422
2025-12-11 08:09:45,485: Running test after training batch: 102000
2025-12-11 08:10:10,606: Val batch 102000: PER (avg): 0.3797 CTC Loss (avg): 52.0495 time: 25.120
2025-12-11 08:10:10,606: t15.2023.08.11 val PER: 1.0000
2025-12-11 08:10:10,607: t15.2023.08.13 val PER: 0.4137
2025-12-11 08:10:10,607: t15.2023.08.18 val PER: 0.3479
2025-12-11 08:10:10,607: t15.2023.08.20 val PER: 0.3114
2025-12-11 08:10:10,607: t15.2023.08.25 val PER: 0.3855
2025-12-11 08:10:10,607: t15.2023.08.27 val PER: 0.4598
2025-12-11 08:10:10,607: t15.2023.09.01 val PER: 0.2946
2025-12-11 08:10:10,607: t15.2023.09.03 val PER: 0.3420
2025-12-11 08:10:10,607: t15.2023.09.24 val PER: 0.3265
2025-12-11 08:10:10,607: t15.2023.09.29 val PER: 0.3267
2025-12-11 08:10:10,607: t15.2023.10.01 val PER: 0.4049
2025-12-11 08:10:10,607: t15.2023.10.06 val PER: 0.2982
2025-12-11 08:10:10,607: t15.2023.10.08 val PER: 0.4601
2025-12-11 08:10:10,607: t15.2023.10.13 val PER: 0.3856
2025-12-11 08:10:10,607: t15.2023.10.15 val PER: 0.3599
2025-12-11 08:10:10,607: t15.2023.10.20 val PER: 0.4262
2025-12-11 08:10:10,607: t15.2023.10.22 val PER: 0.3408
2025-12-11 08:10:10,607: t15.2023.11.03 val PER: 0.3772
2025-12-11 08:10:10,607: t15.2023.11.04 val PER: 0.1433
2025-12-11 08:10:10,608: t15.2023.11.17 val PER: 0.1664
2025-12-11 08:10:10,608: t15.2023.11.19 val PER: 0.2455
2025-12-11 08:10:10,608: t15.2023.11.26 val PER: 0.3399
2025-12-11 08:10:10,608: t15.2023.12.03 val PER: 0.2952
2025-12-11 08:10:10,608: t15.2023.12.08 val PER: 0.2856
2025-12-11 08:10:10,608: t15.2023.12.10 val PER: 0.2549
2025-12-11 08:10:10,608: t15.2023.12.17 val PER: 0.4376
2025-12-11 08:10:10,608: t15.2023.12.29 val PER: 0.4049
2025-12-11 08:10:10,608: t15.2024.02.25 val PER: 0.3455
2025-12-11 08:10:10,608: t15.2024.03.03 val PER: 1.0000
2025-12-11 08:10:10,608: t15.2024.03.08 val PER: 0.4481
2025-12-11 08:10:10,608: t15.2024.03.15 val PER: 0.4159
2025-12-11 08:10:10,608: t15.2024.03.17 val PER: 0.3591
2025-12-11 08:10:10,608: t15.2024.04.25 val PER: 1.0000
2025-12-11 08:10:10,608: t15.2024.04.28 val PER: 1.0000
2025-12-11 08:10:10,608: t15.2024.05.10 val PER: 0.4264
2025-12-11 08:10:10,608: t15.2024.06.14 val PER: 0.4022
2025-12-11 08:10:10,608: t15.2024.07.19 val PER: 0.5577
2025-12-11 08:10:10,608: t15.2024.07.21 val PER: 0.3166
2025-12-11 08:10:10,609: t15.2024.07.28 val PER: 0.3728
2025-12-11 08:10:10,609: t15.2025.01.10 val PER: 0.5592
2025-12-11 08:10:10,609: t15.2025.01.12 val PER: 0.3764
2025-12-11 08:10:10,609: t15.2025.03.14 val PER: 0.6302
2025-12-11 08:10:10,609: t15.2025.03.16 val PER: 0.4542
2025-12-11 08:10:10,609: t15.2025.03.30 val PER: 0.5805
2025-12-11 08:10:10,609: t15.2025.04.13 val PER: 0.4993
2025-12-11 08:11:12,515: Train batch 102200: loss: 13.10 grad norm: 275.30 time: 0.322
2025-12-11 08:12:14,715: Train batch 102400: loss: 3.33 grad norm: 70.81 time: 0.251
2025-12-11 08:13:17,712: Train batch 102600: loss: 6.85 grad norm: 48.50 time: 0.254
2025-12-11 08:14:17,654: Train batch 102800: loss: 9.39 grad norm: 376.01 time: 0.255
2025-12-11 08:15:17,578: Train batch 103000: loss: 11.54 grad norm: 387.44 time: 0.351
2025-12-11 08:16:17,381: Train batch 103200: loss: 9.04 grad norm: 96.35 time: 0.366
2025-12-11 08:17:21,768: Train batch 103400: loss: 1.59 grad norm: 45.18 time: 0.344
2025-12-11 08:18:23,446: Train batch 103600: loss: 8.06 grad norm: 73.56 time: 0.328
2025-12-11 08:19:24,225: Train batch 103800: loss: 5.82 grad norm: 143.11 time: 0.363
2025-12-11 08:20:27,637: Train batch 104000: loss: 11.16 grad norm: 54.23 time: 0.364
2025-12-11 08:20:27,638: Running test after training batch: 104000
2025-12-11 08:20:52,992: Val batch 104000: PER (avg): 0.3866 CTC Loss (avg): 53.4129 time: 25.354
2025-12-11 08:20:52,992: t15.2023.08.11 val PER: 1.0000
2025-12-11 08:20:52,992: t15.2023.08.13 val PER: 0.4241
2025-12-11 08:20:52,992: t15.2023.08.18 val PER: 0.3521
2025-12-11 08:20:52,993: t15.2023.08.20 val PER: 0.3264
2025-12-11 08:20:52,993: t15.2023.08.25 val PER: 0.3976
2025-12-11 08:20:52,993: t15.2023.08.27 val PER: 0.4791
2025-12-11 08:20:52,993: t15.2023.09.01 val PER: 0.2987
2025-12-11 08:20:52,993: t15.2023.09.03 val PER: 0.3480
2025-12-11 08:20:52,993: t15.2023.09.24 val PER: 0.3216
2025-12-11 08:20:52,993: t15.2023.09.29 val PER: 0.3287
2025-12-11 08:20:52,993: t15.2023.10.01 val PER: 0.4122
2025-12-11 08:20:52,993: t15.2023.10.06 val PER: 0.3046
2025-12-11 08:20:52,993: t15.2023.10.08 val PER: 0.4709
2025-12-11 08:20:52,993: t15.2023.10.13 val PER: 0.4034
2025-12-11 08:20:52,993: t15.2023.10.15 val PER: 0.3579
2025-12-11 08:20:52,993: t15.2023.10.20 val PER: 0.4530
2025-12-11 08:20:52,993: t15.2023.10.22 val PER: 0.3396
2025-12-11 08:20:52,993: t15.2023.11.03 val PER: 0.3820
2025-12-11 08:20:52,993: t15.2023.11.04 val PER: 0.1399
2025-12-11 08:20:52,993: t15.2023.11.17 val PER: 0.1726
2025-12-11 08:20:52,993: t15.2023.11.19 val PER: 0.2355
2025-12-11 08:20:52,993: t15.2023.11.26 val PER: 0.3529
2025-12-11 08:20:52,994: t15.2023.12.03 val PER: 0.2920
2025-12-11 08:20:52,994: t15.2023.12.08 val PER: 0.3063
2025-12-11 08:20:52,994: t15.2023.12.10 val PER: 0.2707
2025-12-11 08:20:52,994: t15.2023.12.17 val PER: 0.4387
2025-12-11 08:20:52,994: t15.2023.12.29 val PER: 0.4077
2025-12-11 08:20:52,994: t15.2024.02.25 val PER: 0.3638
2025-12-11 08:20:52,994: t15.2024.03.03 val PER: 1.0000
2025-12-11 08:20:52,994: t15.2024.03.08 val PER: 0.4694
2025-12-11 08:20:52,994: t15.2024.03.15 val PER: 0.4246
2025-12-11 08:20:52,994: t15.2024.03.17 val PER: 0.3654
2025-12-11 08:20:52,994: t15.2024.04.25 val PER: 1.0000
2025-12-11 08:20:52,994: t15.2024.04.28 val PER: 1.0000
2025-12-11 08:20:52,994: t15.2024.05.10 val PER: 0.4250
2025-12-11 08:20:52,994: t15.2024.06.14 val PER: 0.4101
2025-12-11 08:20:52,994: t15.2024.07.19 val PER: 0.5649
2025-12-11 08:20:52,994: t15.2024.07.21 val PER: 0.3221
2025-12-11 08:20:52,994: t15.2024.07.28 val PER: 0.3772
2025-12-11 08:20:52,994: t15.2025.01.10 val PER: 0.5675
2025-12-11 08:20:52,994: t15.2025.01.12 val PER: 0.3949
2025-12-11 08:20:52,995: t15.2025.03.14 val PER: 0.6257
2025-12-11 08:20:52,995: t15.2025.03.16 val PER: 0.4660
2025-12-11 08:20:52,995: t15.2025.03.30 val PER: 0.5690
2025-12-11 08:20:52,995: t15.2025.04.13 val PER: 0.4979
2025-12-11 08:21:54,510: Train batch 104200: loss: 5.22 grad norm: 62.30 time: 0.328
2025-12-11 08:22:58,432: Train batch 104400: loss: 7.74 grad norm: 50.87 time: 0.332
2025-12-11 08:24:01,037: Train batch 104600: loss: 7.15 grad norm: 66.41 time: 0.400
2025-12-11 08:25:04,244: Train batch 104800: loss: 9.78 grad norm: 142.18 time: 0.254
2025-12-11 08:26:07,499: Train batch 105000: loss: 9.26 grad norm: 71.00 time: 0.393
2025-12-11 08:27:10,719: Train batch 105200: loss: 6.29 grad norm: 109.05 time: 0.280
2025-12-11 08:28:14,537: Train batch 105400: loss: 4.95 grad norm: 88.24 time: 0.259
2025-12-11 08:29:17,914: Train batch 105600: loss: 5.28 grad norm: 45.04 time: 0.321
2025-12-11 08:30:19,193: Train batch 105800: loss: 10.36 grad norm: 102.57 time: 0.335
2025-12-11 08:31:21,366: Train batch 106000: loss: 5.73 grad norm: 40.72 time: 0.285
2025-12-11 08:31:21,367: Running test after training batch: 106000
2025-12-11 08:31:46,208: Val batch 106000: PER (avg): 0.3947 CTC Loss (avg): 54.8561 time: 24.841
2025-12-11 08:31:46,209: t15.2023.08.11 val PER: 1.0000
2025-12-11 08:31:46,209: t15.2023.08.13 val PER: 0.4335
2025-12-11 08:31:46,209: t15.2023.08.18 val PER: 0.3495
2025-12-11 08:31:46,209: t15.2023.08.20 val PER: 0.3376
2025-12-11 08:31:46,209: t15.2023.08.25 val PER: 0.4021
2025-12-11 08:31:46,209: t15.2023.08.27 val PER: 0.4727
2025-12-11 08:31:46,209: t15.2023.09.01 val PER: 0.3052
2025-12-11 08:31:46,209: t15.2023.09.03 val PER: 0.3432
2025-12-11 08:31:46,209: t15.2023.09.24 val PER: 0.3337
2025-12-11 08:31:46,209: t15.2023.09.29 val PER: 0.3299
2025-12-11 08:31:46,209: t15.2023.10.01 val PER: 0.4188
2025-12-11 08:31:46,209: t15.2023.10.06 val PER: 0.3154
2025-12-11 08:31:46,209: t15.2023.10.08 val PER: 0.4844
2025-12-11 08:31:46,209: t15.2023.10.13 val PER: 0.3988
2025-12-11 08:31:46,209: t15.2023.10.15 val PER: 0.3593
2025-12-11 08:31:46,210: t15.2023.10.20 val PER: 0.4396
2025-12-11 08:31:46,210: t15.2023.10.22 val PER: 0.3474
2025-12-11 08:31:46,210: t15.2023.11.03 val PER: 0.4050
2025-12-11 08:31:46,210: t15.2023.11.04 val PER: 0.1502
2025-12-11 08:31:46,210: t15.2023.11.17 val PER: 0.1757
2025-12-11 08:31:46,210: t15.2023.11.19 val PER: 0.2375
2025-12-11 08:31:46,210: t15.2023.11.26 val PER: 0.3587
2025-12-11 08:31:46,210: t15.2023.12.03 val PER: 0.3141
2025-12-11 08:31:46,210: t15.2023.12.08 val PER: 0.3202
2025-12-11 08:31:46,210: t15.2023.12.10 val PER: 0.2799
2025-12-11 08:31:46,210: t15.2023.12.17 val PER: 0.4574
2025-12-11 08:31:46,210: t15.2023.12.29 val PER: 0.4214
2025-12-11 08:31:46,210: t15.2024.02.25 val PER: 0.3638
2025-12-11 08:31:46,210: t15.2024.03.03 val PER: 1.0000
2025-12-11 08:31:46,210: t15.2024.03.08 val PER: 0.4680
2025-12-11 08:31:46,210: t15.2024.03.15 val PER: 0.4328
2025-12-11 08:31:46,210: t15.2024.03.17 val PER: 0.3689
2025-12-11 08:31:46,210: t15.2024.04.25 val PER: 1.0000
2025-12-11 08:31:46,211: t15.2024.04.28 val PER: 1.0000
2025-12-11 08:31:46,211: t15.2024.05.10 val PER: 0.4473
2025-12-11 08:31:46,211: t15.2024.06.14 val PER: 0.4211
2025-12-11 08:31:46,211: t15.2024.07.19 val PER: 0.5821
2025-12-11 08:31:46,211: t15.2024.07.21 val PER: 0.3386
2025-12-11 08:31:46,211: t15.2024.07.28 val PER: 0.3838
2025-12-11 08:31:46,211: t15.2025.01.10 val PER: 0.5689
2025-12-11 08:31:46,211: t15.2025.01.12 val PER: 0.4103
2025-12-11 08:31:46,211: t15.2025.03.14 val PER: 0.6302
2025-12-11 08:31:46,211: t15.2025.03.16 val PER: 0.4725
2025-12-11 08:31:46,211: t15.2025.03.30 val PER: 0.5793
2025-12-11 08:31:46,211: t15.2025.04.13 val PER: 0.4950
2025-12-11 08:32:48,694: Train batch 106200: loss: 9.31 grad norm: 274.66 time: 0.274
2025-12-11 08:33:51,538: Train batch 106400: loss: 4.98 grad norm: 147.38 time: 0.309
2025-12-11 08:34:52,540: Train batch 106600: loss: 7.49 grad norm: 838.32 time: 0.264
2025-12-11 08:35:54,056: Train batch 106800: loss: 5.30 grad norm: 213.79 time: 0.314
2025-12-11 08:36:57,235: Train batch 107000: loss: 6.69 grad norm: 61.95 time: 0.256
2025-12-11 08:38:00,289: Train batch 107200: loss: 2.03 grad norm: 25.21 time: 0.275
2025-12-11 08:39:02,240: Train batch 107400: loss: 8.12 grad norm: 821.73 time: 0.241
2025-12-11 08:40:03,251: Train batch 107600: loss: 6.28 grad norm: 54.91 time: 0.245
2025-12-11 08:41:04,929: Train batch 107800: loss: 2.75 grad norm: 63.90 time: 0.209
2025-12-11 08:42:05,512: Train batch 108000: loss: 5.99 grad norm: 87.32 time: 0.286
2025-12-11 08:42:05,512: Running test after training batch: 108000
2025-12-11 08:42:30,266: Val batch 108000: PER (avg): 0.3807 CTC Loss (avg): 52.4508 time: 24.754
2025-12-11 08:42:30,267: t15.2023.08.11 val PER: 1.0000
2025-12-11 08:42:30,267: t15.2023.08.13 val PER: 0.4096
2025-12-11 08:42:30,267: t15.2023.08.18 val PER: 0.3395
2025-12-11 08:42:30,267: t15.2023.08.20 val PER: 0.3233
2025-12-11 08:42:30,267: t15.2023.08.25 val PER: 0.3961
2025-12-11 08:42:30,267: t15.2023.08.27 val PER: 0.4550
2025-12-11 08:42:30,267: t15.2023.09.01 val PER: 0.2987
2025-12-11 08:42:30,267: t15.2023.09.03 val PER: 0.3361
2025-12-11 08:42:30,267: t15.2023.09.24 val PER: 0.3228
2025-12-11 08:42:30,267: t15.2023.09.29 val PER: 0.3159
2025-12-11 08:42:30,267: t15.2023.10.01 val PER: 0.4009
2025-12-11 08:42:30,267: t15.2023.10.06 val PER: 0.2992
2025-12-11 08:42:30,267: t15.2023.10.08 val PER: 0.4696
2025-12-11 08:42:30,267: t15.2023.10.13 val PER: 0.3809
2025-12-11 08:42:30,267: t15.2023.10.15 val PER: 0.3546
2025-12-11 08:42:30,268: t15.2023.10.20 val PER: 0.4262
2025-12-11 08:42:30,268: t15.2023.10.22 val PER: 0.3408
2025-12-11 08:42:30,268: t15.2023.11.03 val PER: 0.3826
2025-12-11 08:42:30,268: t15.2023.11.04 val PER: 0.1297
2025-12-11 08:42:30,268: t15.2023.11.17 val PER: 0.1633
2025-12-11 08:42:30,268: t15.2023.11.19 val PER: 0.2295
2025-12-11 08:42:30,268: t15.2023.11.26 val PER: 0.3319
2025-12-11 08:42:30,268: t15.2023.12.03 val PER: 0.2910
2025-12-11 08:42:30,268: t15.2023.12.08 val PER: 0.3029
2025-12-11 08:42:30,268: t15.2023.12.10 val PER: 0.2628
2025-12-11 08:42:30,268: t15.2023.12.17 val PER: 0.4324
2025-12-11 08:42:30,268: t15.2023.12.29 val PER: 0.4118
2025-12-11 08:42:30,268: t15.2024.02.25 val PER: 0.3581
2025-12-11 08:42:30,268: t15.2024.03.03 val PER: 1.0000
2025-12-11 08:42:30,268: t15.2024.03.08 val PER: 0.4595
2025-12-11 08:42:30,268: t15.2024.03.15 val PER: 0.4240
2025-12-11 08:42:30,268: t15.2024.03.17 val PER: 0.3577
2025-12-11 08:42:30,268: t15.2024.04.25 val PER: 1.0000
2025-12-11 08:42:30,268: t15.2024.04.28 val PER: 1.0000
2025-12-11 08:42:30,269: t15.2024.05.10 val PER: 0.4205
2025-12-11 08:42:30,269: t15.2024.06.14 val PER: 0.4148
2025-12-11 08:42:30,269: t15.2024.07.19 val PER: 0.5603
2025-12-11 08:42:30,269: t15.2024.07.21 val PER: 0.3193
2025-12-11 08:42:30,269: t15.2024.07.28 val PER: 0.3779
2025-12-11 08:42:30,269: t15.2025.01.10 val PER: 0.5510
2025-12-11 08:42:30,269: t15.2025.01.12 val PER: 0.3903
2025-12-11 08:42:30,269: t15.2025.03.14 val PER: 0.6302
2025-12-11 08:42:30,269: t15.2025.03.16 val PER: 0.4699
2025-12-11 08:42:30,269: t15.2025.03.30 val PER: 0.5724
2025-12-11 08:42:30,269: t15.2025.04.13 val PER: 0.4879
2025-12-11 08:43:32,696: Train batch 108200: loss: 6.37 grad norm: 113.60 time: 0.274
2025-12-11 08:44:33,177: Train batch 108400: loss: 9.85 grad norm: 158.53 time: 0.352
2025-12-11 08:45:34,476: Train batch 108600: loss: 10.46 grad norm: 55.23 time: 0.458
2025-12-11 08:46:37,070: Train batch 108800: loss: 7.86 grad norm: 81.74 time: 0.349
2025-12-11 08:47:38,045: Train batch 109000: loss: 7.12 grad norm: 92.43 time: 0.321
2025-12-11 08:48:40,750: Train batch 109200: loss: 6.86 grad norm: 122.93 time: 0.336
2025-12-11 08:49:42,774: Train batch 109400: loss: 5.33 grad norm: 60.93 time: 0.325
2025-12-11 08:50:43,585: Train batch 109600: loss: 4.39 grad norm: 55.93 time: 0.352
2025-12-11 08:51:45,140: Train batch 109800: loss: 13.59 grad norm: 85.21 time: 0.404
2025-12-11 08:52:46,250: Train batch 110000: loss: 11.61 grad norm: 114.35 time: 0.272
2025-12-11 08:52:46,251: Running test after training batch: 110000
2025-12-11 08:53:10,948: Val batch 110000: PER (avg): 0.3981 CTC Loss (avg): 55.2450 time: 24.698
2025-12-11 08:53:10,949: t15.2023.08.11 val PER: 1.0000
2025-12-11 08:53:10,949: t15.2023.08.13 val PER: 0.4262
2025-12-11 08:53:10,949: t15.2023.08.18 val PER: 0.3537
2025-12-11 08:53:10,949: t15.2023.08.20 val PER: 0.3360
2025-12-11 08:53:10,949: t15.2023.08.25 val PER: 0.4081
2025-12-11 08:53:10,949: t15.2023.08.27 val PER: 0.4839
2025-12-11 08:53:10,949: t15.2023.09.01 val PER: 0.3174
2025-12-11 08:53:10,949: t15.2023.09.03 val PER: 0.3551
2025-12-11 08:53:10,949: t15.2023.09.24 val PER: 0.3301
2025-12-11 08:53:10,949: t15.2023.09.29 val PER: 0.3350
2025-12-11 08:53:10,949: t15.2023.10.01 val PER: 0.4214
2025-12-11 08:53:10,949: t15.2023.10.06 val PER: 0.3219
2025-12-11 08:53:10,949: t15.2023.10.08 val PER: 0.4858
2025-12-11 08:53:10,949: t15.2023.10.13 val PER: 0.3980
2025-12-11 08:53:10,950: t15.2023.10.15 val PER: 0.3612
2025-12-11 08:53:10,950: t15.2023.10.20 val PER: 0.4396
2025-12-11 08:53:10,950: t15.2023.10.22 val PER: 0.3608
2025-12-11 08:53:10,950: t15.2023.11.03 val PER: 0.4084
2025-12-11 08:53:10,950: t15.2023.11.04 val PER: 0.1502
2025-12-11 08:53:10,950: t15.2023.11.17 val PER: 0.1804
2025-12-11 08:53:10,950: t15.2023.11.19 val PER: 0.2435
2025-12-11 08:53:10,950: t15.2023.11.26 val PER: 0.3536
2025-12-11 08:53:10,950: t15.2023.12.03 val PER: 0.3109
2025-12-11 08:53:10,950: t15.2023.12.08 val PER: 0.3242
2025-12-11 08:53:10,950: t15.2023.12.10 val PER: 0.2852
2025-12-11 08:53:10,950: t15.2023.12.17 val PER: 0.4626
2025-12-11 08:53:10,950: t15.2023.12.29 val PER: 0.4331
2025-12-11 08:53:10,950: t15.2024.02.25 val PER: 0.3722
2025-12-11 08:53:10,950: t15.2024.03.03 val PER: 1.0000
2025-12-11 08:53:10,950: t15.2024.03.08 val PER: 0.4737
2025-12-11 08:53:10,950: t15.2024.03.15 val PER: 0.4259
2025-12-11 08:53:10,950: t15.2024.03.17 val PER: 0.3759
2025-12-11 08:53:10,950: t15.2024.04.25 val PER: 1.0000
2025-12-11 08:53:10,951: t15.2024.04.28 val PER: 1.0000
2025-12-11 08:53:10,951: t15.2024.05.10 val PER: 0.4443
2025-12-11 08:53:10,951: t15.2024.06.14 val PER: 0.4385
2025-12-11 08:53:10,951: t15.2024.07.19 val PER: 0.5801
2025-12-11 08:53:10,951: t15.2024.07.21 val PER: 0.3366
2025-12-11 08:53:10,951: t15.2024.07.28 val PER: 0.3912
2025-12-11 08:53:10,951: t15.2025.01.10 val PER: 0.5758
2025-12-11 08:53:10,951: t15.2025.01.12 val PER: 0.4142
2025-12-11 08:53:10,951: t15.2025.03.14 val PER: 0.6331
2025-12-11 08:53:10,951: t15.2025.03.16 val PER: 0.4777
2025-12-11 08:53:10,951: t15.2025.03.30 val PER: 0.5874
2025-12-11 08:53:10,951: t15.2025.04.13 val PER: 0.4964
2025-12-11 08:54:13,663: Train batch 110200: loss: 6.28 grad norm: 136.64 time: 0.333
2025-12-11 08:55:15,529: Train batch 110400: loss: 8.69 grad norm: 64.84 time: 0.373
2025-12-11 08:56:17,903: Train batch 110600: loss: 4.23 grad norm: 46.11 time: 0.260
2025-12-11 08:57:19,561: Train batch 110800: loss: 7.69 grad norm: 100.36 time: 0.281
2025-12-11 08:58:20,503: Train batch 111000: loss: 7.70 grad norm: 145.26 time: 0.359
2025-12-11 08:59:23,934: Train batch 111200: loss: 7.69 grad norm: 65.00 time: 0.454
2025-12-11 09:00:25,863: Train batch 111400: loss: 10.70 grad norm: 785.25 time: 0.226
2025-12-11 09:01:28,789: Train batch 111600: loss: 8.80 grad norm: 84.29 time: 0.403
2025-12-11 09:02:29,907: Train batch 111800: loss: 9.29 grad norm: 365.21 time: 0.296
2025-12-11 09:03:31,027: Train batch 112000: loss: 8.52 grad norm: 405.02 time: 0.339
2025-12-11 09:03:31,027: Running test after training batch: 112000
2025-12-11 09:03:55,736: Val batch 112000: PER (avg): 0.3712 CTC Loss (avg): 51.1426 time: 24.708
2025-12-11 09:03:55,736: t15.2023.08.11 val PER: 1.0000
2025-12-11 09:03:55,736: t15.2023.08.13 val PER: 0.4033
2025-12-11 09:03:55,736: t15.2023.08.18 val PER: 0.3386
2025-12-11 09:03:55,736: t15.2023.08.20 val PER: 0.3257
2025-12-11 09:03:55,736: t15.2023.08.25 val PER: 0.3901
2025-12-11 09:03:55,736: t15.2023.08.27 val PER: 0.4518
2025-12-11 09:03:55,736: t15.2023.09.01 val PER: 0.2873
2025-12-11 09:03:55,736: t15.2023.09.03 val PER: 0.3325
2025-12-11 09:03:55,737: t15.2023.09.24 val PER: 0.3204
2025-12-11 09:03:55,737: t15.2023.09.29 val PER: 0.3101
2025-12-11 09:03:55,737: t15.2023.10.01 val PER: 0.3884
2025-12-11 09:03:55,737: t15.2023.10.06 val PER: 0.2820
2025-12-11 09:03:55,737: t15.2023.10.08 val PER: 0.4547
2025-12-11 09:03:55,737: t15.2023.10.13 val PER: 0.3701
2025-12-11 09:03:55,737: t15.2023.10.15 val PER: 0.3448
2025-12-11 09:03:55,737: t15.2023.10.20 val PER: 0.4195
2025-12-11 09:03:55,737: t15.2023.10.22 val PER: 0.3263
2025-12-11 09:03:55,737: t15.2023.11.03 val PER: 0.3684
2025-12-11 09:03:55,737: t15.2023.11.04 val PER: 0.1195
2025-12-11 09:03:55,737: t15.2023.11.17 val PER: 0.1540
2025-12-11 09:03:55,737: t15.2023.11.19 val PER: 0.2116
2025-12-11 09:03:55,737: t15.2023.11.26 val PER: 0.3290
2025-12-11 09:03:55,737: t15.2023.12.03 val PER: 0.2920
2025-12-11 09:03:55,737: t15.2023.12.08 val PER: 0.2903
2025-12-11 09:03:55,738: t15.2023.12.10 val PER: 0.2536
2025-12-11 09:03:55,738: t15.2023.12.17 val PER: 0.4137
2025-12-11 09:03:55,738: t15.2023.12.29 val PER: 0.3960
2025-12-11 09:03:55,738: t15.2024.02.25 val PER: 0.3413
2025-12-11 09:03:55,738: t15.2024.03.03 val PER: 1.0000
2025-12-11 09:03:55,738: t15.2024.03.08 val PER: 0.4566
2025-12-11 09:03:55,738: t15.2024.03.15 val PER: 0.4096
2025-12-11 09:03:55,738: t15.2024.03.17 val PER: 0.3515
2025-12-11 09:03:55,738: t15.2024.04.25 val PER: 1.0000
2025-12-11 09:03:55,738: t15.2024.04.28 val PER: 1.0000
2025-12-11 09:03:55,738: t15.2024.05.10 val PER: 0.4160
2025-12-11 09:03:55,738: t15.2024.06.14 val PER: 0.3959
2025-12-11 09:03:55,738: t15.2024.07.19 val PER: 0.5511
2025-12-11 09:03:55,738: t15.2024.07.21 val PER: 0.3014
2025-12-11 09:03:55,738: t15.2024.07.28 val PER: 0.3618
2025-12-11 09:03:55,738: t15.2025.01.10 val PER: 0.5510
2025-12-11 09:03:55,739: t15.2025.01.12 val PER: 0.3826
2025-12-11 09:03:55,739: t15.2025.03.14 val PER: 0.6154
2025-12-11 09:03:55,739: t15.2025.03.16 val PER: 0.4529
2025-12-11 09:03:55,739: t15.2025.03.30 val PER: 0.5667
2025-12-11 09:03:55,739: t15.2025.04.13 val PER: 0.4893
2025-12-11 09:03:55,739: New best test PER 0.3791 --> 0.3712
2025-12-11 09:03:55,739: Checkpointing model
2025-12-11 09:03:56,853: Saved model to checkpoint: trained_models/baseline_rnn_v2/checkpoint/best_checkpoint
2025-12-11 09:05:00,327: Train batch 112200: loss: 5.34 grad norm: 76.18 time: 0.377
2025-12-11 09:06:02,016: Train batch 112400: loss: 5.02 grad norm: 60.91 time: 0.364
2025-12-11 09:07:03,020: Train batch 112600: loss: 8.50 grad norm: 232.80 time: 0.327
2025-12-11 09:08:03,732: Train batch 112800: loss: 7.10 grad norm: 135.64 time: 0.305
2025-12-11 09:09:03,235: Train batch 113000: loss: 7.64 grad norm: 635.11 time: 0.322
2025-12-11 09:10:05,307: Train batch 113200: loss: 7.76 grad norm: 112.17 time: 0.233
2025-12-11 09:11:08,645: Train batch 113400: loss: 11.76 grad norm: 76.77 time: 0.260
2025-12-11 09:12:08,330: Train batch 113600: loss: 3.79 grad norm: 236.31 time: 0.266
2025-12-11 09:13:10,493: Train batch 113800: loss: 9.38 grad norm: 236.18 time: 0.356
2025-12-11 09:14:12,642: Train batch 114000: loss: 3.21 grad norm: 174.87 time: 0.332
2025-12-11 09:14:12,642: Running test after training batch: 114000
2025-12-11 09:14:37,308: Val batch 114000: PER (avg): 0.3778 CTC Loss (avg): 52.1860 time: 24.665
2025-12-11 09:14:37,308: t15.2023.08.11 val PER: 1.0000
2025-12-11 09:14:37,308: t15.2023.08.13 val PER: 0.4096
2025-12-11 09:14:37,308: t15.2023.08.18 val PER: 0.3512
2025-12-11 09:14:37,308: t15.2023.08.20 val PER: 0.3137
2025-12-11 09:14:37,308: t15.2023.08.25 val PER: 0.3870
2025-12-11 09:14:37,309: t15.2023.08.27 val PER: 0.4582
2025-12-11 09:14:37,309: t15.2023.09.01 val PER: 0.2930
2025-12-11 09:14:37,309: t15.2023.09.03 val PER: 0.3278
2025-12-11 09:14:37,309: t15.2023.09.24 val PER: 0.3204
2025-12-11 09:14:37,309: t15.2023.09.29 val PER: 0.3223
2025-12-11 09:14:37,309: t15.2023.10.01 val PER: 0.4003
2025-12-11 09:14:37,309: t15.2023.10.06 val PER: 0.2982
2025-12-11 09:14:37,309: t15.2023.10.08 val PER: 0.4601
2025-12-11 09:14:37,309: t15.2023.10.13 val PER: 0.3832
2025-12-11 09:14:37,309: t15.2023.10.15 val PER: 0.3514
2025-12-11 09:14:37,309: t15.2023.10.20 val PER: 0.4329
2025-12-11 09:14:37,309: t15.2023.10.22 val PER: 0.3263
2025-12-11 09:14:37,309: t15.2023.11.03 val PER: 0.3772
2025-12-11 09:14:37,309: t15.2023.11.04 val PER: 0.1263
2025-12-11 09:14:37,309: t15.2023.11.17 val PER: 0.1602
2025-12-11 09:14:37,309: t15.2023.11.19 val PER: 0.2236
2025-12-11 09:14:37,309: t15.2023.11.26 val PER: 0.3348
2025-12-11 09:14:37,309: t15.2023.12.03 val PER: 0.3057
2025-12-11 09:14:37,310: t15.2023.12.08 val PER: 0.2883
2025-12-11 09:14:37,310: t15.2023.12.10 val PER: 0.2576
2025-12-11 09:14:37,310: t15.2023.12.17 val PER: 0.4304
2025-12-11 09:14:37,310: t15.2023.12.29 val PER: 0.4036
2025-12-11 09:14:37,310: t15.2024.02.25 val PER: 0.3427
2025-12-11 09:14:37,310: t15.2024.03.03 val PER: 1.0000
2025-12-11 09:14:37,310: t15.2024.03.08 val PER: 0.4666
2025-12-11 09:14:37,310: t15.2024.03.15 val PER: 0.4184
2025-12-11 09:14:37,310: t15.2024.03.17 val PER: 0.3515
2025-12-11 09:14:37,310: t15.2024.04.25 val PER: 1.0000
2025-12-11 09:14:37,310: t15.2024.04.28 val PER: 1.0000
2025-12-11 09:14:37,310: t15.2024.05.10 val PER: 0.4190
2025-12-11 09:14:37,310: t15.2024.06.14 val PER: 0.4006
2025-12-11 09:14:37,310: t15.2024.07.19 val PER: 0.5603
2025-12-11 09:14:37,310: t15.2024.07.21 val PER: 0.3152
2025-12-11 09:14:37,310: t15.2024.07.28 val PER: 0.3706
2025-12-11 09:14:37,310: t15.2025.01.10 val PER: 0.5647
2025-12-11 09:14:37,310: t15.2025.01.12 val PER: 0.3888
2025-12-11 09:14:37,310: t15.2025.03.14 val PER: 0.6213
2025-12-11 09:14:37,311: t15.2025.03.16 val PER: 0.4594
2025-12-11 09:14:37,311: t15.2025.03.30 val PER: 0.5713
2025-12-11 09:14:37,311: t15.2025.04.13 val PER: 0.4879
2025-12-11 09:15:38,840: Train batch 114200: loss: 5.88 grad norm: 257.04 time: 0.268
2025-12-11 09:16:38,981: Train batch 114400: loss: 9.05 grad norm: 280.47 time: 0.308
2025-12-11 09:17:41,722: Train batch 114600: loss: 6.52 grad norm: 167.26 time: 0.367
2025-12-11 09:18:43,153: Train batch 114800: loss: 7.83 grad norm: 112.81 time: 0.257
2025-12-11 09:19:43,263: Train batch 115000: loss: 3.92 grad norm: 38.05 time: 0.369
2025-12-11 09:20:43,443: Train batch 115200: loss: 11.93 grad norm: 104.45 time: 0.287
2025-12-11 09:21:46,314: Train batch 115400: loss: 4.44 grad norm: 105.76 time: 0.235
2025-12-11 09:22:48,756: Train batch 115600: loss: 3.66 grad norm: 55.54 time: 0.360
2025-12-11 09:23:52,244: Train batch 115800: loss: 9.13 grad norm: 63.07 time: 0.269
2025-12-11 09:24:56,457: Train batch 116000: loss: 6.09 grad norm: 63.53 time: 0.316
2025-12-11 09:24:56,457: Running test after training batch: 116000
2025-12-11 09:25:21,400: Val batch 116000: PER (avg): 0.3814 CTC Loss (avg): 52.9218 time: 24.943
2025-12-11 09:25:21,401: t15.2023.08.11 val PER: 1.0000
2025-12-11 09:25:21,401: t15.2023.08.13 val PER: 0.4096
2025-12-11 09:25:21,401: t15.2023.08.18 val PER: 0.3403
2025-12-11 09:25:21,401: t15.2023.08.20 val PER: 0.3217
2025-12-11 09:25:21,401: t15.2023.08.25 val PER: 0.3870
2025-12-11 09:25:21,401: t15.2023.08.27 val PER: 0.4646
2025-12-11 09:25:21,401: t15.2023.09.01 val PER: 0.3060
2025-12-11 09:25:21,401: t15.2023.09.03 val PER: 0.3325
2025-12-11 09:25:21,402: t15.2023.09.24 val PER: 0.3228
2025-12-11 09:25:21,402: t15.2023.09.29 val PER: 0.3261
2025-12-11 09:25:21,402: t15.2023.10.01 val PER: 0.4055
2025-12-11 09:25:21,402: t15.2023.10.06 val PER: 0.3057
2025-12-11 09:25:21,402: t15.2023.10.08 val PER: 0.4696
2025-12-11 09:25:21,402: t15.2023.10.13 val PER: 0.3832
2025-12-11 09:25:21,402: t15.2023.10.15 val PER: 0.3527
2025-12-11 09:25:21,402: t15.2023.10.20 val PER: 0.4396
2025-12-11 09:25:21,402: t15.2023.10.22 val PER: 0.3374
2025-12-11 09:25:21,402: t15.2023.11.03 val PER: 0.3786
2025-12-11 09:25:21,402: t15.2023.11.04 val PER: 0.1331
2025-12-11 09:25:21,402: t15.2023.11.17 val PER: 0.1602
2025-12-11 09:25:21,402: t15.2023.11.19 val PER: 0.2315
2025-12-11 09:25:21,402: t15.2023.11.26 val PER: 0.3399
2025-12-11 09:25:21,402: t15.2023.12.03 val PER: 0.2994
2025-12-11 09:25:21,402: t15.2023.12.08 val PER: 0.2983
2025-12-11 09:25:21,402: t15.2023.12.10 val PER: 0.2615
2025-12-11 09:25:21,402: t15.2023.12.17 val PER: 0.4345
2025-12-11 09:25:21,402: t15.2023.12.29 val PER: 0.4070
2025-12-11 09:25:21,403: t15.2024.02.25 val PER: 0.3497
2025-12-11 09:25:21,403: t15.2024.03.03 val PER: 1.0000
2025-12-11 09:25:21,403: t15.2024.03.08 val PER: 0.4708
2025-12-11 09:25:21,403: t15.2024.03.15 val PER: 0.4209
2025-12-11 09:25:21,403: t15.2024.03.17 val PER: 0.3556
2025-12-11 09:25:21,403: t15.2024.04.25 val PER: 1.0000
2025-12-11 09:25:21,403: t15.2024.04.28 val PER: 1.0000
2025-12-11 09:25:21,403: t15.2024.05.10 val PER: 0.4175
2025-12-11 09:25:21,403: t15.2024.06.14 val PER: 0.4117
2025-12-11 09:25:21,403: t15.2024.07.19 val PER: 0.5577
2025-12-11 09:25:21,403: t15.2024.07.21 val PER: 0.3214
2025-12-11 09:25:21,403: t15.2024.07.28 val PER: 0.3706
2025-12-11 09:25:21,403: t15.2025.01.10 val PER: 0.5579
2025-12-11 09:25:21,403: t15.2025.01.12 val PER: 0.4011
2025-12-11 09:25:21,403: t15.2025.03.14 val PER: 0.6243
2025-12-11 09:25:21,403: t15.2025.03.16 val PER: 0.4647
2025-12-11 09:25:21,403: t15.2025.03.30 val PER: 0.5713
2025-12-11 09:25:21,403: t15.2025.04.13 val PER: 0.4936
2025-12-11 09:26:23,799: Train batch 116200: loss: 3.45 grad norm: 212.39 time: 0.316
2025-12-11 09:27:24,778: Train batch 116400: loss: 5.80 grad norm: 57.33 time: 0.291
2025-12-11 09:28:25,486: Train batch 116600: loss: 3.31 grad norm: 39.01 time: 0.242
2025-12-11 09:29:24,606: Train batch 116800: loss: 4.47 grad norm: 60.76 time: 0.326
2025-12-11 09:30:26,118: Train batch 117000: loss: 9.80 grad norm: 199.35 time: 0.421
2025-12-11 09:31:27,697: Train batch 117200: loss: 2.77 grad norm: 91.64 time: 0.272
2025-12-11 09:32:30,527: Train batch 117400: loss: 8.23 grad norm: 60.62 time: 0.327
2025-12-11 09:33:33,274: Train batch 117600: loss: 3.73 grad norm: 137.80 time: 0.251
2025-12-11 09:34:36,762: Train batch 117800: loss: 11.69 grad norm: 956.97 time: 0.317
2025-12-11 09:35:40,051: Train batch 118000: loss: 4.75 grad norm: 99.02 time: 0.238
2025-12-11 09:35:40,051: Running test after training batch: 118000
2025-12-11 09:36:04,746: Val batch 118000: PER (avg): 0.3848 CTC Loss (avg): 53.6412 time: 24.695
2025-12-11 09:36:04,747: t15.2023.08.11 val PER: 1.0000
2025-12-11 09:36:04,747: t15.2023.08.13 val PER: 0.4148
2025-12-11 09:36:04,747: t15.2023.08.18 val PER: 0.3403
2025-12-11 09:36:04,747: t15.2023.08.20 val PER: 0.3264
2025-12-11 09:36:04,747: t15.2023.08.25 val PER: 0.3961
2025-12-11 09:36:04,747: t15.2023.08.27 val PER: 0.4695
2025-12-11 09:36:04,747: t15.2023.09.01 val PER: 0.3003
2025-12-11 09:36:04,747: t15.2023.09.03 val PER: 0.3349
2025-12-11 09:36:04,747: t15.2023.09.24 val PER: 0.3252
2025-12-11 09:36:04,747: t15.2023.09.29 val PER: 0.3293
2025-12-11 09:36:04,747: t15.2023.10.01 val PER: 0.4075
2025-12-11 09:36:04,747: t15.2023.10.06 val PER: 0.3100
2025-12-11 09:36:04,747: t15.2023.10.08 val PER: 0.4614
2025-12-11 09:36:04,747: t15.2023.10.13 val PER: 0.3902
2025-12-11 09:36:04,747: t15.2023.10.15 val PER: 0.3593
2025-12-11 09:36:04,747: t15.2023.10.20 val PER: 0.4497
2025-12-11 09:36:04,747: t15.2023.10.22 val PER: 0.3408
2025-12-11 09:36:04,748: t15.2023.11.03 val PER: 0.3881
2025-12-11 09:36:04,748: t15.2023.11.04 val PER: 0.1433
2025-12-11 09:36:04,748: t15.2023.11.17 val PER: 0.1649
2025-12-11 09:36:04,748: t15.2023.11.19 val PER: 0.2295
2025-12-11 09:36:04,748: t15.2023.11.26 val PER: 0.3384
2025-12-11 09:36:04,748: t15.2023.12.03 val PER: 0.3046
2025-12-11 09:36:04,748: t15.2023.12.08 val PER: 0.3056
2025-12-11 09:36:04,748: t15.2023.12.10 val PER: 0.2668
2025-12-11 09:36:04,748: t15.2023.12.17 val PER: 0.4356
2025-12-11 09:36:04,748: t15.2023.12.29 val PER: 0.4097
2025-12-11 09:36:04,748: t15.2024.02.25 val PER: 0.3525
2025-12-11 09:36:04,748: t15.2024.03.03 val PER: 1.0000
2025-12-11 09:36:04,748: t15.2024.03.08 val PER: 0.4794
2025-12-11 09:36:04,748: t15.2024.03.15 val PER: 0.4228
2025-12-11 09:36:04,748: t15.2024.03.17 val PER: 0.3647
2025-12-11 09:36:04,748: t15.2024.04.25 val PER: 1.0000
2025-12-11 09:36:04,748: t15.2024.04.28 val PER: 1.0000
2025-12-11 09:36:04,748: t15.2024.05.10 val PER: 0.4131
2025-12-11 09:36:04,748: t15.2024.06.14 val PER: 0.4196
2025-12-11 09:36:04,749: t15.2024.07.19 val PER: 0.5623
2025-12-11 09:36:04,749: t15.2024.07.21 val PER: 0.3228
2025-12-11 09:36:04,749: t15.2024.07.28 val PER: 0.3765
2025-12-11 09:36:04,749: t15.2025.01.10 val PER: 0.5675
2025-12-11 09:36:04,749: t15.2025.01.12 val PER: 0.4057
2025-12-11 09:36:04,749: t15.2025.03.14 val PER: 0.6228
2025-12-11 09:36:04,749: t15.2025.03.16 val PER: 0.4634
2025-12-11 09:36:04,749: t15.2025.03.30 val PER: 0.5678
2025-12-11 09:36:04,749: t15.2025.04.13 val PER: 0.4893
2025-12-11 09:37:07,732: Train batch 118200: loss: 5.93 grad norm: 64.95 time: 0.296
2025-12-11 09:38:10,391: Train batch 118400: loss: 4.54 grad norm: 300.45 time: 0.386
2025-12-11 09:39:13,916: Train batch 118600: loss: 7.19 grad norm: 108.19 time: 0.343
2025-12-11 09:40:16,121: Train batch 118800: loss: 4.86 grad norm: 63.98 time: 0.250
2025-12-11 09:41:17,636: Train batch 119000: loss: 4.32 grad norm: 42.88 time: 0.289
2025-12-11 09:42:19,736: Train batch 119200: loss: 6.47 grad norm: 150.37 time: 0.341
2025-12-11 09:43:22,672: Train batch 119400: loss: 6.20 grad norm: 49.99 time: 0.312
2025-12-11 09:44:26,561: Train batch 119600: loss: 4.85 grad norm: 56.32 time: 0.348
2025-12-11 09:45:29,918: Train batch 119800: loss: 21.51 grad norm: 696.57 time: 0.341
2025-12-11 09:46:31,903: Running test after training batch: 119999
2025-12-11 09:46:57,074: Val batch 119999: PER (avg): 0.3759 CTC Loss (avg): 52.0307 time: 25.170
2025-12-11 09:46:57,074: t15.2023.08.11 val PER: 1.0000
2025-12-11 09:46:57,075: t15.2023.08.13 val PER: 0.4127
2025-12-11 09:46:57,075: t15.2023.08.18 val PER: 0.3386
2025-12-11 09:46:57,075: t15.2023.08.20 val PER: 0.3177
2025-12-11 09:46:57,075: t15.2023.08.25 val PER: 0.3946
2025-12-11 09:46:57,075: t15.2023.08.27 val PER: 0.4727
2025-12-11 09:46:57,075: t15.2023.09.01 val PER: 0.2938
2025-12-11 09:46:57,075: t15.2023.09.03 val PER: 0.3373
2025-12-11 09:46:57,075: t15.2023.09.24 val PER: 0.3107
2025-12-11 09:46:57,075: t15.2023.09.29 val PER: 0.3223
2025-12-11 09:46:57,075: t15.2023.10.01 val PER: 0.4029
2025-12-11 09:46:57,075: t15.2023.10.06 val PER: 0.2949
2025-12-11 09:46:57,075: t15.2023.10.08 val PER: 0.4655
2025-12-11 09:46:57,075: t15.2023.10.13 val PER: 0.3832
2025-12-11 09:46:57,075: t15.2023.10.15 val PER: 0.3474
2025-12-11 09:46:57,075: t15.2023.10.20 val PER: 0.4295
2025-12-11 09:46:57,075: t15.2023.10.22 val PER: 0.3396
2025-12-11 09:46:57,076: t15.2023.11.03 val PER: 0.3650
2025-12-11 09:46:57,076: t15.2023.11.04 val PER: 0.1297
2025-12-11 09:46:57,076: t15.2023.11.17 val PER: 0.1649
2025-12-11 09:46:57,076: t15.2023.11.19 val PER: 0.2315
2025-12-11 09:46:57,076: t15.2023.11.26 val PER: 0.3297
2025-12-11 09:46:57,076: t15.2023.12.03 val PER: 0.3057
2025-12-11 09:46:57,076: t15.2023.12.08 val PER: 0.2903
2025-12-11 09:46:57,076: t15.2023.12.10 val PER: 0.2615
2025-12-11 09:46:57,076: t15.2023.12.17 val PER: 0.4116
2025-12-11 09:46:57,076: t15.2023.12.29 val PER: 0.3995
2025-12-11 09:46:57,076: t15.2024.02.25 val PER: 0.3413
2025-12-11 09:46:57,076: t15.2024.03.03 val PER: 1.0000
2025-12-11 09:46:57,076: t15.2024.03.08 val PER: 0.4467
2025-12-11 09:46:57,076: t15.2024.03.15 val PER: 0.4071
2025-12-11 09:46:57,076: t15.2024.03.17 val PER: 0.3501
2025-12-11 09:46:57,076: t15.2024.04.25 val PER: 1.0000
2025-12-11 09:46:57,076: t15.2024.04.28 val PER: 1.0000
2025-12-11 09:46:57,077: t15.2024.05.10 val PER: 0.4190
2025-12-11 09:46:57,077: t15.2024.06.14 val PER: 0.4006
2025-12-11 09:46:57,077: t15.2024.07.19 val PER: 0.5550
2025-12-11 09:46:57,077: t15.2024.07.21 val PER: 0.3186
2025-12-11 09:46:57,077: t15.2024.07.28 val PER: 0.3676
2025-12-11 09:46:57,077: t15.2025.01.10 val PER: 0.5579
2025-12-11 09:46:57,077: t15.2025.01.12 val PER: 0.3818
2025-12-11 09:46:57,077: t15.2025.03.14 val PER: 0.6228
2025-12-11 09:46:57,077: t15.2025.03.16 val PER: 0.4568
2025-12-11 09:46:57,077: t15.2025.03.30 val PER: 0.5644
2025-12-11 09:46:57,077: t15.2025.04.13 val PER: 0.4922
2025-12-11 09:46:57,152: Best avg val PER achieved: 0.37116
2025-12-11 09:46:57,152: Total training time: 667.53 minutes
