2025-12-15 22:04:00,414: Using device: cuda:0
2025-12-15 22:04:00,483: Initialized RNN decoding model
2025-12-15 22:04:00,483: ConformerDecoder(
  (day_activation): Softsign()
  (day_dropout): Dropout(p=0.2, inplace=False)
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (input_proj): Sequential(
    (0): Linear(in_features=7168, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.2, inplace=False)
  )
  (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (conformer): Conformer(
    (conformer_layers): ModuleList(
      (0-3): 4 x ConformerLayer(
        (ffn1): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_dropout): Dropout(p=0.2, inplace=False)
        (conv_module): _ConvolutionModule(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (sequential): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
            (1): GLU(dim=1)
            (2): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): SiLU()
            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
            (6): Dropout(p=0.2, inplace=False)
          )
        )
        (ffn2): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_dropout): Dropout(p=0.1, inplace=False)
  (out): Linear(in_features=256, out_features=41, bias=True)
)
2025-12-15 22:04:00,486: Model has 17,641,001 parameters
2025-12-15 22:04:00,486: Model has 11,819,520 day-specific parameters | 67.00% of total parameters
2025-12-15 22:04:09,163: Successfully initialized datasets
2025-12-15 22:04:10,408: Train batch 0: loss: 28.57 grad norm: 13.02 time: 0.880
2025-12-15 22:04:10,408: Running test after training batch: 0
2025-12-15 22:04:22,027: Val batch 0: PER (avg): 4.5674 CTC Loss (avg): 24.7557 time: 11.619
2025-12-15 22:04:22,028: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:04:22,028: t15.2023.08.13 val PER: 3.6653
2025-12-15 22:04:22,028: t15.2023.08.18 val PER: 4.1802
2025-12-15 22:04:22,028: t15.2023.08.20 val PER: 3.8674
2025-12-15 22:04:22,028: t15.2023.08.25 val PER: 4.0452
2025-12-15 22:04:22,028: t15.2023.08.27 val PER: 3.6141
2025-12-15 22:04:22,028: t15.2023.09.01 val PER: 4.2127
2025-12-15 22:04:22,028: t15.2023.09.03 val PER: 3.9537
2025-12-15 22:04:22,028: t15.2023.09.24 val PER: 4.8823
2025-12-15 22:04:22,028: t15.2023.09.29 val PER: 4.8922
2025-12-15 22:04:22,028: t15.2023.10.01 val PER: 3.7483
2025-12-15 22:04:22,028: t15.2023.10.06 val PER: 4.7309
2025-12-15 22:04:22,028: t15.2023.10.08 val PER: 3.5737
2025-12-15 22:04:22,028: t15.2023.10.13 val PER: 4.4880
2025-12-15 22:04:22,028: t15.2023.10.15 val PER: 4.6948
2025-12-15 22:04:22,029: t15.2023.10.20 val PER: 4.9362
2025-12-15 22:04:22,029: t15.2023.10.22 val PER: 4.7283
2025-12-15 22:04:22,029: t15.2023.11.03 val PER: 5.3602
2025-12-15 22:04:22,029: t15.2023.11.04 val PER: 6.6416
2025-12-15 22:04:22,029: t15.2023.11.17 val PER: 6.2504
2025-12-15 22:04:22,029: t15.2023.11.19 val PER: 5.1697
2025-12-15 22:04:22,029: t15.2023.11.26 val PER: 4.5826
2025-12-15 22:04:22,029: t15.2023.12.03 val PER: 4.3834
2025-12-15 22:04:22,029: t15.2023.12.08 val PER: 4.7636
2025-12-15 22:04:22,029: t15.2023.12.10 val PER: 5.6071
2025-12-15 22:04:22,029: t15.2023.12.17 val PER: 4.2079
2025-12-15 22:04:22,029: t15.2023.12.29 val PER: 4.3892
2025-12-15 22:04:22,029: t15.2024.02.25 val PER: 4.6419
2025-12-15 22:04:22,029: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:04:22,029: t15.2024.03.08 val PER: 4.1110
2025-12-15 22:04:22,029: t15.2024.03.15 val PER: 4.3221
2025-12-15 22:04:22,029: t15.2024.03.17 val PER: 4.7755
2025-12-15 22:04:22,029: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:04:22,029: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:04:22,030: t15.2024.05.10 val PER: 4.4324
2025-12-15 22:04:22,030: t15.2024.06.14 val PER: 5.2082
2025-12-15 22:04:22,030: t15.2024.07.19 val PER: 3.2808
2025-12-15 22:04:22,030: t15.2024.07.21 val PER: 5.3828
2025-12-15 22:04:22,030: t15.2024.07.28 val PER: 5.4272
2025-12-15 22:04:22,030: t15.2025.01.10 val PER: 3.5220
2025-12-15 22:04:22,030: t15.2025.01.12 val PER: 5.7236
2025-12-15 22:04:22,030: t15.2025.03.14 val PER: 3.2899
2025-12-15 22:04:22,030: t15.2025.03.16 val PER: 5.9215
2025-12-15 22:04:22,030: t15.2025.03.30 val PER: 4.1655
2025-12-15 22:04:22,030: t15.2025.04.13 val PER: 5.1498
2025-12-15 22:04:22,030: New best test PER inf --> 4.5674
2025-12-15 22:04:22,030: Checkpointing model
2025-12-15 22:04:22,149: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:04:38,387: Train batch 200: loss: 23.34 grad norm: 27.71 time: 0.080
2025-12-15 22:04:53,344: Train batch 400: loss: 15.74 grad norm: 54.91 time: 0.084
2025-12-15 22:05:08,400: Train batch 600: loss: 8.71 grad norm: 49.02 time: 0.061
2025-12-15 22:05:23,353: Train batch 800: loss: 4.71 grad norm: 23.03 time: 0.056
2025-12-15 22:05:37,763: Train batch 1000: loss: 3.53 grad norm: 3.45 time: 0.070
2025-12-15 22:05:52,616: Train batch 1200: loss: 3.40 grad norm: 0.53 time: 0.085
2025-12-15 22:06:05,384: Train batch 1400: loss: 3.27 grad norm: 1.01 time: 0.033
2025-12-15 22:06:17,837: Train batch 1600: loss: 3.16 grad norm: 1.10 time: 0.069
2025-12-15 22:06:31,973: Train batch 1800: loss: 3.15 grad norm: 0.81 time: 0.071
2025-12-15 22:06:46,264: Train batch 2000: loss: 3.10 grad norm: 0.80 time: 0.076
2025-12-15 22:06:46,265: Running test after training batch: 2000
2025-12-15 22:06:55,530: Val batch 2000: PER (avg): 0.7772 CTC Loss (avg): 3.5678 time: 9.265
2025-12-15 22:06:55,530: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:06:55,530: t15.2023.08.13 val PER: 0.7630
2025-12-15 22:06:55,530: t15.2023.08.18 val PER: 0.7728
2025-12-15 22:06:55,531: t15.2023.08.20 val PER: 0.7705
2025-12-15 22:06:55,531: t15.2023.08.25 val PER: 0.7666
2025-12-15 22:06:55,531: t15.2023.08.27 val PER: 0.7878
2025-12-15 22:06:55,531: t15.2023.09.01 val PER: 0.7597
2025-12-15 22:06:55,531: t15.2023.09.03 val PER: 0.7732
2025-12-15 22:06:55,531: t15.2023.09.24 val PER: 0.7840
2025-12-15 22:06:55,531: t15.2023.09.29 val PER: 0.7869
2025-12-15 22:06:55,531: t15.2023.10.01 val PER: 0.7787
2025-12-15 22:06:55,531: t15.2023.10.06 val PER: 0.7632
2025-12-15 22:06:55,531: t15.2023.10.08 val PER: 0.8227
2025-12-15 22:06:55,531: t15.2023.10.13 val PER: 0.8146
2025-12-15 22:06:55,531: t15.2023.10.15 val PER: 0.7798
2025-12-15 22:06:55,531: t15.2023.10.20 val PER: 0.7550
2025-12-15 22:06:55,531: t15.2023.10.22 val PER: 0.7561
2025-12-15 22:06:55,531: t15.2023.11.03 val PER: 0.8060
2025-12-15 22:06:55,531: t15.2023.11.04 val PER: 0.7474
2025-12-15 22:06:55,531: t15.2023.11.17 val PER: 0.7885
2025-12-15 22:06:55,531: t15.2023.11.19 val PER: 0.7485
2025-12-15 22:06:55,531: t15.2023.11.26 val PER: 0.7949
2025-12-15 22:06:55,532: t15.2023.12.03 val PER: 0.7920
2025-12-15 22:06:55,532: t15.2023.12.08 val PER: 0.7870
2025-12-15 22:06:55,532: t15.2023.12.10 val PER: 0.7950
2025-12-15 22:06:55,532: t15.2023.12.17 val PER: 0.7890
2025-12-15 22:06:55,532: t15.2023.12.29 val PER: 0.7914
2025-12-15 22:06:55,532: t15.2024.02.25 val PER: 0.7612
2025-12-15 22:06:55,532: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:06:55,532: t15.2024.03.08 val PER: 0.7596
2025-12-15 22:06:55,532: t15.2024.03.15 val PER: 0.7986
2025-12-15 22:06:55,532: t15.2024.03.17 val PER: 0.7587
2025-12-15 22:06:55,532: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:06:55,532: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:06:55,532: t15.2024.05.10 val PER: 0.7533
2025-12-15 22:06:55,532: t15.2024.06.14 val PER: 0.7539
2025-12-15 22:06:55,532: t15.2024.07.19 val PER: 0.7673
2025-12-15 22:06:55,532: t15.2024.07.21 val PER: 0.7628
2025-12-15 22:06:55,532: t15.2024.07.28 val PER: 0.7713
2025-12-15 22:06:55,532: t15.2025.01.10 val PER: 0.7837
2025-12-15 22:06:55,532: t15.2025.01.12 val PER: 0.7567
2025-12-15 22:06:55,533: t15.2025.03.14 val PER: 0.7811
2025-12-15 22:06:55,533: t15.2025.03.16 val PER: 0.7461
2025-12-15 22:06:55,533: t15.2025.03.30 val PER: 0.7897
2025-12-15 22:06:55,533: t15.2025.04.13 val PER: 0.7489
2025-12-15 22:06:55,533: New best test PER 4.5674 --> 0.7772
2025-12-15 22:06:55,533: Checkpointing model
2025-12-15 22:06:55,959: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:07:10,610: Train batch 2200: loss: 3.08 grad norm: 1.03 time: 0.081
2025-12-15 22:07:25,184: Train batch 2400: loss: 3.10 grad norm: 1.09 time: 0.066
2025-12-15 22:07:39,528: Train batch 2600: loss: 2.96 grad norm: 1.83 time: 0.064
2025-12-15 22:07:53,044: Train batch 2800: loss: 2.88 grad norm: 1.62 time: 0.057
2025-12-15 22:08:07,223: Train batch 3000: loss: 2.84 grad norm: 1.87 time: 0.057
2025-12-15 22:08:22,066: Train batch 3200: loss: 2.87 grad norm: 1.69 time: 0.064
2025-12-15 22:08:35,728: Train batch 3400: loss: 2.81 grad norm: 1.92 time: 0.057
2025-12-15 22:08:48,970: Train batch 3600: loss: 2.75 grad norm: 1.58 time: 0.052
2025-12-15 22:09:03,039: Train batch 3800: loss: 2.57 grad norm: 1.67 time: 0.059
2025-12-15 22:09:16,483: Train batch 4000: loss: 2.51 grad norm: 1.92 time: 0.078
2025-12-15 22:09:16,484: Running test after training batch: 4000
2025-12-15 22:09:25,543: Val batch 4000: PER (avg): 0.5961 CTC Loss (avg): 2.8864 time: 9.059
2025-12-15 22:09:25,543: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:09:25,543: t15.2023.08.13 val PER: 0.5437
2025-12-15 22:09:25,544: t15.2023.08.18 val PER: 0.5516
2025-12-15 22:09:25,544: t15.2023.08.20 val PER: 0.5441
2025-12-15 22:09:25,544: t15.2023.08.25 val PER: 0.5512
2025-12-15 22:09:25,544: t15.2023.08.27 val PER: 0.6029
2025-12-15 22:09:25,544: t15.2023.09.01 val PER: 0.5373
2025-12-15 22:09:25,544: t15.2023.09.03 val PER: 0.5867
2025-12-15 22:09:25,544: t15.2023.09.24 val PER: 0.5862
2025-12-15 22:09:25,544: t15.2023.09.29 val PER: 0.5890
2025-12-15 22:09:25,544: t15.2023.10.01 val PER: 0.5971
2025-12-15 22:09:25,544: t15.2023.10.06 val PER: 0.5727
2025-12-15 22:09:25,544: t15.2023.10.08 val PER: 0.6238
2025-12-15 22:09:25,544: t15.2023.10.13 val PER: 0.6680
2025-12-15 22:09:25,544: t15.2023.10.15 val PER: 0.6012
2025-12-15 22:09:25,544: t15.2023.10.20 val PER: 0.5570
2025-12-15 22:09:25,544: t15.2023.10.22 val PER: 0.5702
2025-12-15 22:09:25,544: t15.2023.11.03 val PER: 0.6126
2025-12-15 22:09:25,544: t15.2023.11.04 val PER: 0.4744
2025-12-15 22:09:25,544: t15.2023.11.17 val PER: 0.5583
2025-12-15 22:09:25,544: t15.2023.11.19 val PER: 0.5190
2025-12-15 22:09:25,545: t15.2023.11.26 val PER: 0.6457
2025-12-15 22:09:25,545: t15.2023.12.03 val PER: 0.6071
2025-12-15 22:09:25,545: t15.2023.12.08 val PER: 0.6032
2025-12-15 22:09:25,545: t15.2023.12.10 val PER: 0.5926
2025-12-15 22:09:25,545: t15.2023.12.17 val PER: 0.5884
2025-12-15 22:09:25,545: t15.2023.12.29 val PER: 0.6019
2025-12-15 22:09:25,545: t15.2024.02.25 val PER: 0.5590
2025-12-15 22:09:25,545: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:09:25,545: t15.2024.03.08 val PER: 0.5889
2025-12-15 22:09:25,545: t15.2024.03.15 val PER: 0.6235
2025-12-15 22:09:25,545: t15.2024.03.17 val PER: 0.5941
2025-12-15 22:09:25,545: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:09:25,545: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:09:25,545: t15.2024.05.10 val PER: 0.6256
2025-12-15 22:09:25,545: t15.2024.06.14 val PER: 0.6057
2025-12-15 22:09:25,545: t15.2024.07.19 val PER: 0.6190
2025-12-15 22:09:25,545: t15.2024.07.21 val PER: 0.5883
2025-12-15 22:09:25,545: t15.2024.07.28 val PER: 0.6213
2025-12-15 22:09:25,545: t15.2025.01.10 val PER: 0.6364
2025-12-15 22:09:25,545: t15.2025.01.12 val PER: 0.5958
2025-12-15 22:09:25,546: t15.2025.03.14 val PER: 0.6538
2025-12-15 22:09:25,546: t15.2025.03.16 val PER: 0.6099
2025-12-15 22:09:25,546: t15.2025.03.30 val PER: 0.6322
2025-12-15 22:09:25,546: t15.2025.04.13 val PER: 0.6134
2025-12-15 22:09:25,546: New best test PER 0.7772 --> 0.5961
2025-12-15 22:09:25,546: Checkpointing model
2025-12-15 22:09:25,991: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:09:39,118: Train batch 4200: loss: 2.50 grad norm: 2.29 time: 0.076
2025-12-15 22:09:53,253: Train batch 4400: loss: 2.47 grad norm: 1.74 time: 0.033
2025-12-15 22:10:06,754: Train batch 4600: loss: 2.33 grad norm: 1.87 time: 0.078
2025-12-15 22:10:21,082: Train batch 4800: loss: 2.27 grad norm: 2.23 time: 0.056
2025-12-15 22:10:35,701: Train batch 5000: loss: 2.37 grad norm: 2.04 time: 0.057
2025-12-15 22:10:50,112: Train batch 5200: loss: 2.00 grad norm: 2.11 time: 0.063
2025-12-15 22:11:04,984: Train batch 5400: loss: 2.12 grad norm: 2.11 time: 0.069
2025-12-15 22:11:19,500: Train batch 5600: loss: 2.02 grad norm: 2.58 time: 0.074
2025-12-15 22:11:33,318: Train batch 5800: loss: 2.06 grad norm: 2.22 time: 0.076
2025-12-15 22:11:46,194: Train batch 6000: loss: 1.73 grad norm: 2.58 time: 0.071
2025-12-15 22:11:46,194: Running test after training batch: 6000
2025-12-15 22:11:55,484: Val batch 6000: PER (avg): 0.3648 CTC Loss (avg): 1.6478 time: 9.289
2025-12-15 22:11:55,484: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:11:55,484: t15.2023.08.13 val PER: 0.3514
2025-12-15 22:11:55,484: t15.2023.08.18 val PER: 0.3277
2025-12-15 22:11:55,484: t15.2023.08.20 val PER: 0.2986
2025-12-15 22:11:55,484: t15.2023.08.25 val PER: 0.3012
2025-12-15 22:11:55,484: t15.2023.08.27 val PER: 0.3923
2025-12-15 22:11:55,484: t15.2023.09.01 val PER: 0.2906
2025-12-15 22:11:55,485: t15.2023.09.03 val PER: 0.3456
2025-12-15 22:11:55,485: t15.2023.09.24 val PER: 0.3095
2025-12-15 22:11:55,485: t15.2023.09.29 val PER: 0.3504
2025-12-15 22:11:55,485: t15.2023.10.01 val PER: 0.3811
2025-12-15 22:11:55,485: t15.2023.10.06 val PER: 0.3208
2025-12-15 22:11:55,485: t15.2023.10.08 val PER: 0.3951
2025-12-15 22:11:55,485: t15.2023.10.13 val PER: 0.4321
2025-12-15 22:11:55,485: t15.2023.10.15 val PER: 0.3467
2025-12-15 22:11:55,485: t15.2023.10.20 val PER: 0.4228
2025-12-15 22:11:55,485: t15.2023.10.22 val PER: 0.3318
2025-12-15 22:11:55,485: t15.2023.11.03 val PER: 0.3752
2025-12-15 22:11:55,485: t15.2023.11.04 val PER: 0.1877
2025-12-15 22:11:55,485: t15.2023.11.17 val PER: 0.2644
2025-12-15 22:11:55,485: t15.2023.11.19 val PER: 0.2535
2025-12-15 22:11:55,485: t15.2023.11.26 val PER: 0.3928
2025-12-15 22:11:55,485: t15.2023.12.03 val PER: 0.3424
2025-12-15 22:11:55,485: t15.2023.12.08 val PER: 0.3615
2025-12-15 22:11:55,485: t15.2023.12.10 val PER: 0.3219
2025-12-15 22:11:55,486: t15.2023.12.17 val PER: 0.3898
2025-12-15 22:11:55,486: t15.2023.12.29 val PER: 0.3693
2025-12-15 22:11:55,486: t15.2024.02.25 val PER: 0.3315
2025-12-15 22:11:55,486: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:11:55,486: t15.2024.03.08 val PER: 0.4381
2025-12-15 22:11:55,486: t15.2024.03.15 val PER: 0.4021
2025-12-15 22:11:55,486: t15.2024.03.17 val PER: 0.3759
2025-12-15 22:11:55,486: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:11:55,486: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:11:55,486: t15.2024.05.10 val PER: 0.3403
2025-12-15 22:11:55,486: t15.2024.06.14 val PER: 0.3675
2025-12-15 22:11:55,486: t15.2024.07.19 val PER: 0.4278
2025-12-15 22:11:55,486: t15.2024.07.21 val PER: 0.3007
2025-12-15 22:11:55,486: t15.2024.07.28 val PER: 0.3360
2025-12-15 22:11:55,486: t15.2025.01.10 val PER: 0.4904
2025-12-15 22:11:55,486: t15.2025.01.12 val PER: 0.3580
2025-12-15 22:11:55,486: t15.2025.03.14 val PER: 0.5118
2025-12-15 22:11:55,486: t15.2025.03.16 val PER: 0.4215
2025-12-15 22:11:55,486: t15.2025.03.30 val PER: 0.4989
2025-12-15 22:11:55,487: t15.2025.04.13 val PER: 0.4251
2025-12-15 22:11:55,487: New best test PER 0.5961 --> 0.3648
2025-12-15 22:11:55,487: Checkpointing model
2025-12-15 22:11:55,915: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:12:10,732: Train batch 6200: loss: 1.87 grad norm: 3.23 time: 0.089
2025-12-15 22:12:24,999: Train batch 6400: loss: 1.69 grad norm: 2.21 time: 0.056
2025-12-15 22:12:38,790: Train batch 6600: loss: 1.81 grad norm: 2.76 time: 0.072
2025-12-15 22:12:52,480: Train batch 6800: loss: 1.88 grad norm: 2.85 time: 0.069
2025-12-15 22:13:06,402: Train batch 7000: loss: 1.75 grad norm: 2.62 time: 0.079
2025-12-15 22:13:20,808: Train batch 7200: loss: 1.74 grad norm: 3.67 time: 0.042
2025-12-15 22:13:35,517: Train batch 7400: loss: 1.75 grad norm: 2.58 time: 0.072
2025-12-15 22:13:50,252: Train batch 7600: loss: 1.66 grad norm: 2.84 time: 0.057
2025-12-15 22:14:04,404: Train batch 7800: loss: 1.67 grad norm: 2.83 time: 0.058
2025-12-15 22:14:18,520: Train batch 8000: loss: 1.54 grad norm: 2.80 time: 0.090
2025-12-15 22:14:18,520: Running test after training batch: 8000
2025-12-15 22:14:27,792: Val batch 8000: PER (avg): 0.2808 CTC Loss (avg): 1.1593 time: 9.272
2025-12-15 22:14:27,793: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:14:27,793: t15.2023.08.13 val PER: 0.2578
2025-12-15 22:14:27,793: t15.2023.08.18 val PER: 0.2372
2025-12-15 22:14:27,793: t15.2023.08.20 val PER: 0.2010
2025-12-15 22:14:27,793: t15.2023.08.25 val PER: 0.2123
2025-12-15 22:14:27,793: t15.2023.08.27 val PER: 0.3055
2025-12-15 22:14:27,793: t15.2023.09.01 val PER: 0.1989
2025-12-15 22:14:27,793: t15.2023.09.03 val PER: 0.2720
2025-12-15 22:14:27,793: t15.2023.09.24 val PER: 0.2197
2025-12-15 22:14:27,793: t15.2023.09.29 val PER: 0.2629
2025-12-15 22:14:27,793: t15.2023.10.01 val PER: 0.3025
2025-12-15 22:14:27,794: t15.2023.10.06 val PER: 0.2304
2025-12-15 22:14:27,794: t15.2023.10.08 val PER: 0.3410
2025-12-15 22:14:27,794: t15.2023.10.13 val PER: 0.3421
2025-12-15 22:14:27,794: t15.2023.10.15 val PER: 0.2604
2025-12-15 22:14:27,794: t15.2023.10.20 val PER: 0.3188
2025-12-15 22:14:27,794: t15.2023.10.22 val PER: 0.2639
2025-12-15 22:14:27,794: t15.2023.11.03 val PER: 0.2815
2025-12-15 22:14:27,794: t15.2023.11.04 val PER: 0.0887
2025-12-15 22:14:27,794: t15.2023.11.17 val PER: 0.1711
2025-12-15 22:14:27,794: t15.2023.11.19 val PER: 0.1697
2025-12-15 22:14:27,795: t15.2023.11.26 val PER: 0.2862
2025-12-15 22:14:27,795: t15.2023.12.03 val PER: 0.2605
2025-12-15 22:14:27,795: t15.2023.12.08 val PER: 0.2710
2025-12-15 22:14:27,795: t15.2023.12.10 val PER: 0.2365
2025-12-15 22:14:27,795: t15.2023.12.17 val PER: 0.3077
2025-12-15 22:14:27,795: t15.2023.12.29 val PER: 0.2601
2025-12-15 22:14:27,795: t15.2024.02.25 val PER: 0.2261
2025-12-15 22:14:27,795: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:14:27,796: t15.2024.03.08 val PER: 0.3442
2025-12-15 22:14:27,796: t15.2024.03.15 val PER: 0.3377
2025-12-15 22:14:27,796: t15.2024.03.17 val PER: 0.2762
2025-12-15 22:14:27,796: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:14:27,796: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:14:27,796: t15.2024.05.10 val PER: 0.3165
2025-12-15 22:14:27,796: t15.2024.06.14 val PER: 0.3107
2025-12-15 22:14:27,797: t15.2024.07.19 val PER: 0.3586
2025-12-15 22:14:27,797: t15.2024.07.21 val PER: 0.2269
2025-12-15 22:14:27,797: t15.2024.07.28 val PER: 0.2507
2025-12-15 22:14:27,797: t15.2025.01.10 val PER: 0.4146
2025-12-15 22:14:27,797: t15.2025.01.12 val PER: 0.2794
2025-12-15 22:14:27,797: t15.2025.03.14 val PER: 0.4453
2025-12-15 22:14:27,798: t15.2025.03.16 val PER: 0.3639
2025-12-15 22:14:27,798: t15.2025.03.30 val PER: 0.4069
2025-12-15 22:14:27,798: t15.2025.04.13 val PER: 0.3509
2025-12-15 22:14:27,798: New best test PER 0.3648 --> 0.2808
2025-12-15 22:14:27,798: Checkpointing model
2025-12-15 22:14:28,224: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:14:37,546: Train batch 8200: loss: 1.39 grad norm: 2.84 time: 0.046
2025-12-15 22:14:51,687: Train batch 8400: loss: 1.54 grad norm: 3.89 time: 0.085
2025-12-15 22:15:05,852: Train batch 8600: loss: 1.45 grad norm: 5.52 time: 0.068
2025-12-15 22:15:19,744: Train batch 8800: loss: 1.54 grad norm: 3.01 time: 0.085
2025-12-15 22:15:33,790: Train batch 9000: loss: 1.66 grad norm: 4.40 time: 0.075
2025-12-15 22:15:48,331: Train batch 9200: loss: 1.58 grad norm: 3.08 time: 0.062
2025-12-15 22:16:02,356: Train batch 9400: loss: 1.39 grad norm: 3.22 time: 0.056
2025-12-15 22:16:15,986: Train batch 9600: loss: 1.41 grad norm: 3.31 time: 0.088
2025-12-15 22:16:30,283: Train batch 9800: loss: 1.39 grad norm: 4.28 time: 0.101
2025-12-15 22:16:44,635: Train batch 10000: loss: 1.30 grad norm: 2.19 time: 0.073
2025-12-15 22:16:44,636: Running test after training batch: 10000
2025-12-15 22:16:54,085: Val batch 10000: PER (avg): 0.2395 CTC Loss (avg): 0.9457 time: 9.449
2025-12-15 22:16:54,086: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:16:54,086: t15.2023.08.13 val PER: 0.2110
2025-12-15 22:16:54,086: t15.2023.08.18 val PER: 0.1970
2025-12-15 22:16:54,086: t15.2023.08.20 val PER: 0.1644
2025-12-15 22:16:54,086: t15.2023.08.25 val PER: 0.1777
2025-12-15 22:16:54,086: t15.2023.08.27 val PER: 0.2701
2025-12-15 22:16:54,086: t15.2023.09.01 val PER: 0.1567
2025-12-15 22:16:54,086: t15.2023.09.03 val PER: 0.2352
2025-12-15 22:16:54,086: t15.2023.09.24 val PER: 0.1930
2025-12-15 22:16:54,086: t15.2023.09.29 val PER: 0.2221
2025-12-15 22:16:54,086: t15.2023.10.01 val PER: 0.2662
2025-12-15 22:16:54,086: t15.2023.10.06 val PER: 0.1959
2025-12-15 22:16:54,086: t15.2023.10.08 val PER: 0.2896
2025-12-15 22:16:54,086: t15.2023.10.13 val PER: 0.2746
2025-12-15 22:16:54,087: t15.2023.10.15 val PER: 0.2057
2025-12-15 22:16:54,087: t15.2023.10.20 val PER: 0.2718
2025-12-15 22:16:54,087: t15.2023.10.22 val PER: 0.2238
2025-12-15 22:16:54,087: t15.2023.11.03 val PER: 0.2456
2025-12-15 22:16:54,087: t15.2023.11.04 val PER: 0.0648
2025-12-15 22:16:54,087: t15.2023.11.17 val PER: 0.1291
2025-12-15 22:16:54,087: t15.2023.11.19 val PER: 0.1297
2025-12-15 22:16:54,087: t15.2023.11.26 val PER: 0.2413
2025-12-15 22:16:54,087: t15.2023.12.03 val PER: 0.1901
2025-12-15 22:16:54,087: t15.2023.12.08 val PER: 0.2150
2025-12-15 22:16:54,087: t15.2023.12.10 val PER: 0.1827
2025-12-15 22:16:54,087: t15.2023.12.17 val PER: 0.3025
2025-12-15 22:16:54,087: t15.2023.12.29 val PER: 0.2443
2025-12-15 22:16:54,087: t15.2024.02.25 val PER: 0.1826
2025-12-15 22:16:54,087: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:16:54,087: t15.2024.03.08 val PER: 0.3215
2025-12-15 22:16:54,087: t15.2024.03.15 val PER: 0.2996
2025-12-15 22:16:54,087: t15.2024.03.17 val PER: 0.2266
2025-12-15 22:16:54,088: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:16:54,088: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:16:54,088: t15.2024.05.10 val PER: 0.2437
2025-12-15 22:16:54,088: t15.2024.06.14 val PER: 0.2744
2025-12-15 22:16:54,088: t15.2024.07.19 val PER: 0.3092
2025-12-15 22:16:54,088: t15.2024.07.21 val PER: 0.1959
2025-12-15 22:16:54,088: t15.2024.07.28 val PER: 0.2066
2025-12-15 22:16:54,088: t15.2025.01.10 val PER: 0.3981
2025-12-15 22:16:54,088: t15.2025.01.12 val PER: 0.2525
2025-12-15 22:16:54,088: t15.2025.03.14 val PER: 0.4275
2025-12-15 22:16:54,088: t15.2025.03.16 val PER: 0.2618
2025-12-15 22:16:54,088: t15.2025.03.30 val PER: 0.3632
2025-12-15 22:16:54,088: t15.2025.04.13 val PER: 0.3295
2025-12-15 22:16:54,088: New best test PER 0.2808 --> 0.2395
2025-12-15 22:16:54,088: Checkpointing model
2025-12-15 22:16:54,529: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:17:08,466: Train batch 10200: loss: 1.44 grad norm: 2.95 time: 0.070
2025-12-15 22:17:23,018: Train batch 10400: loss: 1.26 grad norm: 1.90 time: 0.072
2025-12-15 22:17:37,527: Train batch 10600: loss: 1.27 grad norm: 2.77 time: 0.056
2025-12-15 22:17:51,209: Train batch 10800: loss: 1.23 grad norm: 2.02 time: 0.054
2025-12-15 22:18:05,236: Train batch 11000: loss: 1.19 grad norm: 3.24 time: 0.053
2025-12-15 22:18:19,389: Train batch 11200: loss: 1.03 grad norm: 1.71 time: 0.069
2025-12-15 22:18:33,293: Train batch 11400: loss: 1.36 grad norm: 3.68 time: 0.083
2025-12-15 22:18:47,307: Train batch 11600: loss: 1.16 grad norm: 5.59 time: 0.053
2025-12-15 22:18:58,995: Train batch 11800: loss: 1.36 grad norm: 6.09 time: 0.068
2025-12-15 22:19:13,057: Train batch 12000: loss: 1.16 grad norm: 1.96 time: 0.083
2025-12-15 22:19:13,058: Running test after training batch: 12000
2025-12-15 22:19:22,667: Val batch 12000: PER (avg): 0.2202 CTC Loss (avg): 0.8556 time: 9.609
2025-12-15 22:19:22,668: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:19:22,668: t15.2023.08.13 val PER: 0.1726
2025-12-15 22:19:22,668: t15.2023.08.18 val PER: 0.1819
2025-12-15 22:19:22,668: t15.2023.08.20 val PER: 0.1620
2025-12-15 22:19:22,668: t15.2023.08.25 val PER: 0.1687
2025-12-15 22:19:22,668: t15.2023.08.27 val PER: 0.2508
2025-12-15 22:19:22,668: t15.2023.09.01 val PER: 0.1250
2025-12-15 22:19:22,668: t15.2023.09.03 val PER: 0.2031
2025-12-15 22:19:22,668: t15.2023.09.24 val PER: 0.1723
2025-12-15 22:19:22,668: t15.2023.09.29 val PER: 0.1959
2025-12-15 22:19:22,668: t15.2023.10.01 val PER: 0.2173
2025-12-15 22:19:22,668: t15.2023.10.06 val PER: 0.1615
2025-12-15 22:19:22,668: t15.2023.10.08 val PER: 0.2476
2025-12-15 22:19:22,668: t15.2023.10.13 val PER: 0.2607
2025-12-15 22:19:22,668: t15.2023.10.15 val PER: 0.2083
2025-12-15 22:19:22,669: t15.2023.10.20 val PER: 0.2148
2025-12-15 22:19:22,669: t15.2023.10.22 val PER: 0.1659
2025-12-15 22:19:22,669: t15.2023.11.03 val PER: 0.2395
2025-12-15 22:19:22,669: t15.2023.11.04 val PER: 0.0580
2025-12-15 22:19:22,669: t15.2023.11.17 val PER: 0.1353
2025-12-15 22:19:22,669: t15.2023.11.19 val PER: 0.1138
2025-12-15 22:19:22,669: t15.2023.11.26 val PER: 0.2058
2025-12-15 22:19:22,669: t15.2023.12.03 val PER: 0.1901
2025-12-15 22:19:22,669: t15.2023.12.08 val PER: 0.1884
2025-12-15 22:19:22,669: t15.2023.12.10 val PER: 0.1735
2025-12-15 22:19:22,669: t15.2023.12.17 val PER: 0.2661
2025-12-15 22:19:22,669: t15.2023.12.29 val PER: 0.2237
2025-12-15 22:19:22,669: t15.2024.02.25 val PER: 0.1742
2025-12-15 22:19:22,669: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:19:22,669: t15.2024.03.08 val PER: 0.2987
2025-12-15 22:19:22,669: t15.2024.03.15 val PER: 0.2883
2025-12-15 22:19:22,669: t15.2024.03.17 val PER: 0.2134
2025-12-15 22:19:22,669: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:19:22,669: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:19:22,670: t15.2024.05.10 val PER: 0.2600
2025-12-15 22:19:22,670: t15.2024.06.14 val PER: 0.2681
2025-12-15 22:19:22,670: t15.2024.07.19 val PER: 0.3019
2025-12-15 22:19:22,670: t15.2024.07.21 val PER: 0.1621
2025-12-15 22:19:22,670: t15.2024.07.28 val PER: 0.2022
2025-12-15 22:19:22,670: t15.2025.01.10 val PER: 0.3664
2025-12-15 22:19:22,670: t15.2025.01.12 val PER: 0.2325
2025-12-15 22:19:22,670: t15.2025.03.14 val PER: 0.3876
2025-12-15 22:19:22,670: t15.2025.03.16 val PER: 0.2919
2025-12-15 22:19:22,670: t15.2025.03.30 val PER: 0.3609
2025-12-15 22:19:22,670: t15.2025.04.13 val PER: 0.2853
2025-12-15 22:19:22,670: New best test PER 0.2395 --> 0.2202
2025-12-15 22:19:22,670: Checkpointing model
2025-12-15 22:19:23,088: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:19:37,571: Train batch 12200: loss: 1.35 grad norm: 2.83 time: 0.055
2025-12-15 22:19:51,922: Train batch 12400: loss: 1.31 grad norm: 3.03 time: 0.054
2025-12-15 22:20:05,224: Train batch 12600: loss: 1.06 grad norm: 2.15 time: 0.061
2025-12-15 22:20:18,057: Train batch 12800: loss: 1.23 grad norm: 4.93 time: 0.057
2025-12-15 22:20:32,482: Train batch 13000: loss: 1.39 grad norm: 5.75 time: 0.055
2025-12-15 22:20:46,706: Train batch 13200: loss: 1.19 grad norm: 2.18 time: 0.077
2025-12-15 22:21:01,050: Train batch 13400: loss: 1.07 grad norm: 2.14 time: 0.067
2025-12-15 22:21:15,500: Train batch 13600: loss: 1.08 grad norm: 1.92 time: 0.053
2025-12-15 22:21:30,050: Train batch 13800: loss: 1.16 grad norm: 1.54 time: 0.072
2025-12-15 22:21:43,850: Train batch 14000: loss: 1.28 grad norm: 2.78 time: 0.081
2025-12-15 22:21:43,851: Running test after training batch: 14000
2025-12-15 22:21:52,977: Val batch 14000: PER (avg): 0.2014 CTC Loss (avg): 0.8048 time: 9.126
2025-12-15 22:21:52,977: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:21:52,977: t15.2023.08.13 val PER: 0.1642
2025-12-15 22:21:52,977: t15.2023.08.18 val PER: 0.1626
2025-12-15 22:21:52,977: t15.2023.08.20 val PER: 0.1541
2025-12-15 22:21:52,977: t15.2023.08.25 val PER: 0.1581
2025-12-15 22:21:52,977: t15.2023.08.27 val PER: 0.2251
2025-12-15 22:21:52,978: t15.2023.09.01 val PER: 0.1209
2025-12-15 22:21:52,978: t15.2023.09.03 val PER: 0.2114
2025-12-15 22:21:52,978: t15.2023.09.24 val PER: 0.1299
2025-12-15 22:21:52,978: t15.2023.09.29 val PER: 0.1863
2025-12-15 22:21:52,978: t15.2023.10.01 val PER: 0.2252
2025-12-15 22:21:52,978: t15.2023.10.06 val PER: 0.1529
2025-12-15 22:21:52,978: t15.2023.10.08 val PER: 0.2585
2025-12-15 22:21:52,978: t15.2023.10.13 val PER: 0.2265
2025-12-15 22:21:52,978: t15.2023.10.15 val PER: 0.1898
2025-12-15 22:21:52,978: t15.2023.10.20 val PER: 0.2450
2025-12-15 22:21:52,978: t15.2023.10.22 val PER: 0.1559
2025-12-15 22:21:52,978: t15.2023.11.03 val PER: 0.2300
2025-12-15 22:21:52,978: t15.2023.11.04 val PER: 0.0375
2025-12-15 22:21:52,978: t15.2023.11.17 val PER: 0.0918
2025-12-15 22:21:52,978: t15.2023.11.19 val PER: 0.1098
2025-12-15 22:21:52,978: t15.2023.11.26 val PER: 0.1928
2025-12-15 22:21:52,978: t15.2023.12.03 val PER: 0.1681
2025-12-15 22:21:52,978: t15.2023.12.08 val PER: 0.1644
2025-12-15 22:21:52,978: t15.2023.12.10 val PER: 0.1577
2025-12-15 22:21:52,979: t15.2023.12.17 val PER: 0.2100
2025-12-15 22:21:52,979: t15.2023.12.29 val PER: 0.1970
2025-12-15 22:21:52,979: t15.2024.02.25 val PER: 0.1531
2025-12-15 22:21:52,979: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:21:52,979: t15.2024.03.08 val PER: 0.2916
2025-12-15 22:21:52,979: t15.2024.03.15 val PER: 0.2614
2025-12-15 22:21:52,979: t15.2024.03.17 val PER: 0.1980
2025-12-15 22:21:52,979: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:21:52,979: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:21:52,979: t15.2024.05.10 val PER: 0.2110
2025-12-15 22:21:52,979: t15.2024.06.14 val PER: 0.2334
2025-12-15 22:21:52,979: t15.2024.07.19 val PER: 0.2584
2025-12-15 22:21:52,979: t15.2024.07.21 val PER: 0.1517
2025-12-15 22:21:52,979: t15.2024.07.28 val PER: 0.1713
2025-12-15 22:21:52,979: t15.2025.01.10 val PER: 0.3526
2025-12-15 22:21:52,979: t15.2025.01.12 val PER: 0.2032
2025-12-15 22:21:52,979: t15.2025.03.14 val PER: 0.3609
2025-12-15 22:21:52,979: t15.2025.03.16 val PER: 0.2644
2025-12-15 22:21:52,979: t15.2025.03.30 val PER: 0.3276
2025-12-15 22:21:52,980: t15.2025.04.13 val PER: 0.2924
2025-12-15 22:21:52,980: New best test PER 0.2202 --> 0.2014
2025-12-15 22:21:52,980: Checkpointing model
2025-12-15 22:21:53,422: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:22:06,711: Train batch 14200: loss: 1.11 grad norm: 2.61 time: 0.063
2025-12-15 22:22:17,543: Train batch 14400: loss: 1.00 grad norm: 1.75 time: 0.069
2025-12-15 22:22:31,759: Train batch 14600: loss: 1.25 grad norm: 2.66 time: 0.086
2025-12-15 22:22:44,313: Train batch 14800: loss: 1.08 grad norm: 2.58 time: 0.061
2025-12-15 22:22:56,759: Train batch 15000: loss: 1.17 grad norm: 2.14 time: 0.054
2025-12-15 22:23:09,393: Train batch 15200: loss: 1.14 grad norm: 2.30 time: 0.074
2025-12-15 22:23:21,975: Train batch 15400: loss: 1.11 grad norm: 2.04 time: 0.061
2025-12-15 22:23:35,590: Train batch 15600: loss: 0.96 grad norm: 2.12 time: 0.038
2025-12-15 22:23:49,637: Train batch 15800: loss: 1.07 grad norm: 1.77 time: 0.074
2025-12-15 22:24:01,449: Train batch 16000: loss: 0.95 grad norm: 4.27 time: 0.050
2025-12-15 22:24:01,450: Running test after training batch: 16000
2025-12-15 22:24:10,545: Val batch 16000: PER (avg): 0.1929 CTC Loss (avg): 0.7945 time: 9.094
2025-12-15 22:24:10,545: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:24:10,545: t15.2023.08.13 val PER: 0.1497
2025-12-15 22:24:10,545: t15.2023.08.18 val PER: 0.1509
2025-12-15 22:24:10,545: t15.2023.08.20 val PER: 0.1382
2025-12-15 22:24:10,545: t15.2023.08.25 val PER: 0.1310
2025-12-15 22:24:10,545: t15.2023.08.27 val PER: 0.2203
2025-12-15 22:24:10,545: t15.2023.09.01 val PER: 0.1047
2025-12-15 22:24:10,545: t15.2023.09.03 val PER: 0.1746
2025-12-15 22:24:10,545: t15.2023.09.24 val PER: 0.1638
2025-12-15 22:24:10,545: t15.2023.09.29 val PER: 0.1812
2025-12-15 22:24:10,545: t15.2023.10.01 val PER: 0.2107
2025-12-15 22:24:10,546: t15.2023.10.06 val PER: 0.1410
2025-12-15 22:24:10,546: t15.2023.10.08 val PER: 0.2355
2025-12-15 22:24:10,546: t15.2023.10.13 val PER: 0.2389
2025-12-15 22:24:10,546: t15.2023.10.15 val PER: 0.1912
2025-12-15 22:24:10,546: t15.2023.10.20 val PER: 0.2248
2025-12-15 22:24:10,546: t15.2023.10.22 val PER: 0.1392
2025-12-15 22:24:10,546: t15.2023.11.03 val PER: 0.2144
2025-12-15 22:24:10,546: t15.2023.11.04 val PER: 0.0205
2025-12-15 22:24:10,546: t15.2023.11.17 val PER: 0.0902
2025-12-15 22:24:10,546: t15.2023.11.19 val PER: 0.0958
2025-12-15 22:24:10,546: t15.2023.11.26 val PER: 0.1986
2025-12-15 22:24:10,546: t15.2023.12.03 val PER: 0.1565
2025-12-15 22:24:10,546: t15.2023.12.08 val PER: 0.1585
2025-12-15 22:24:10,546: t15.2023.12.10 val PER: 0.1432
2025-12-15 22:24:10,546: t15.2023.12.17 val PER: 0.2079
2025-12-15 22:24:10,546: t15.2023.12.29 val PER: 0.1812
2025-12-15 22:24:10,546: t15.2024.02.25 val PER: 0.1545
2025-12-15 22:24:10,546: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:24:10,546: t15.2024.03.08 val PER: 0.2802
2025-12-15 22:24:10,547: t15.2024.03.15 val PER: 0.2570
2025-12-15 22:24:10,547: t15.2024.03.17 val PER: 0.1778
2025-12-15 22:24:10,547: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:24:10,547: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:24:10,547: t15.2024.05.10 val PER: 0.2021
2025-12-15 22:24:10,547: t15.2024.06.14 val PER: 0.2461
2025-12-15 22:24:10,547: t15.2024.07.19 val PER: 0.2459
2025-12-15 22:24:10,547: t15.2024.07.21 val PER: 0.1310
2025-12-15 22:24:10,547: t15.2024.07.28 val PER: 0.1860
2025-12-15 22:24:10,547: t15.2025.01.10 val PER: 0.3609
2025-12-15 22:24:10,547: t15.2025.01.12 val PER: 0.1863
2025-12-15 22:24:10,547: t15.2025.03.14 val PER: 0.3565
2025-12-15 22:24:10,547: t15.2025.03.16 val PER: 0.2369
2025-12-15 22:24:10,547: t15.2025.03.30 val PER: 0.3161
2025-12-15 22:24:10,547: t15.2025.04.13 val PER: 0.2996
2025-12-15 22:24:10,547: New best test PER 0.2014 --> 0.1929
2025-12-15 22:24:10,547: Checkpointing model
2025-12-15 22:24:10,980: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:24:24,439: Train batch 16200: loss: 1.11 grad norm: 7.48 time: 0.074
2025-12-15 22:24:38,373: Train batch 16400: loss: 0.99 grad norm: 1.89 time: 0.061
2025-12-15 22:24:51,925: Train batch 16600: loss: 1.17 grad norm: 4.20 time: 0.046
2025-12-15 22:25:05,430: Train batch 16800: loss: 1.16 grad norm: 3.40 time: 0.068
2025-12-15 22:25:19,838: Train batch 17000: loss: 1.01 grad norm: 2.16 time: 0.068
2025-12-15 22:25:34,235: Train batch 17200: loss: 0.89 grad norm: 1.44 time: 0.073
2025-12-15 22:25:48,927: Train batch 17400: loss: 1.18 grad norm: 2.79 time: 0.056
2025-12-15 22:26:03,177: Train batch 17600: loss: 1.06 grad norm: 4.40 time: 0.050
2025-12-15 22:26:16,522: Train batch 17800: loss: 1.26 grad norm: 7.86 time: 0.060
2025-12-15 22:26:30,802: Train batch 18000: loss: 1.17 grad norm: 3.15 time: 0.055
2025-12-15 22:26:30,803: Running test after training batch: 18000
2025-12-15 22:26:39,999: Val batch 18000: PER (avg): 0.1860 CTC Loss (avg): 0.8163 time: 9.196
2025-12-15 22:26:40,000: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:26:40,000: t15.2023.08.13 val PER: 0.1528
2025-12-15 22:26:40,000: t15.2023.08.18 val PER: 0.1676
2025-12-15 22:26:40,000: t15.2023.08.20 val PER: 0.1223
2025-12-15 22:26:40,000: t15.2023.08.25 val PER: 0.1401
2025-12-15 22:26:40,000: t15.2023.08.27 val PER: 0.2090
2025-12-15 22:26:40,000: t15.2023.09.01 val PER: 0.1023
2025-12-15 22:26:40,000: t15.2023.09.03 val PER: 0.1936
2025-12-15 22:26:40,000: t15.2023.09.24 val PER: 0.1444
2025-12-15 22:26:40,000: t15.2023.09.29 val PER: 0.1768
2025-12-15 22:26:40,000: t15.2023.10.01 val PER: 0.2166
2025-12-15 22:26:40,000: t15.2023.10.06 val PER: 0.1432
2025-12-15 22:26:40,000: t15.2023.10.08 val PER: 0.2219
2025-12-15 22:26:40,000: t15.2023.10.13 val PER: 0.2444
2025-12-15 22:26:40,001: t15.2023.10.15 val PER: 0.1635
2025-12-15 22:26:40,001: t15.2023.10.20 val PER: 0.2450
2025-12-15 22:26:40,001: t15.2023.10.22 val PER: 0.1359
2025-12-15 22:26:40,001: t15.2023.11.03 val PER: 0.2218
2025-12-15 22:26:40,001: t15.2023.11.04 val PER: 0.0375
2025-12-15 22:26:40,001: t15.2023.11.17 val PER: 0.0731
2025-12-15 22:26:40,001: t15.2023.11.19 val PER: 0.1098
2025-12-15 22:26:40,001: t15.2023.11.26 val PER: 0.1580
2025-12-15 22:26:40,001: t15.2023.12.03 val PER: 0.1576
2025-12-15 22:26:40,001: t15.2023.12.08 val PER: 0.1538
2025-12-15 22:26:40,001: t15.2023.12.10 val PER: 0.1367
2025-12-15 22:26:40,001: t15.2023.12.17 val PER: 0.1778
2025-12-15 22:26:40,001: t15.2023.12.29 val PER: 0.1784
2025-12-15 22:26:40,001: t15.2024.02.25 val PER: 0.1376
2025-12-15 22:26:40,001: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:26:40,001: t15.2024.03.08 val PER: 0.2603
2025-12-15 22:26:40,001: t15.2024.03.15 val PER: 0.2445
2025-12-15 22:26:40,001: t15.2024.03.17 val PER: 0.1632
2025-12-15 22:26:40,001: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:26:40,002: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:26:40,002: t15.2024.05.10 val PER: 0.2080
2025-12-15 22:26:40,002: t15.2024.06.14 val PER: 0.2366
2025-12-15 22:26:40,002: t15.2024.07.19 val PER: 0.2393
2025-12-15 22:26:40,002: t15.2024.07.21 val PER: 0.1255
2025-12-15 22:26:40,002: t15.2024.07.28 val PER: 0.1824
2025-12-15 22:26:40,002: t15.2025.01.10 val PER: 0.3209
2025-12-15 22:26:40,002: t15.2025.01.12 val PER: 0.1817
2025-12-15 22:26:40,002: t15.2025.03.14 val PER: 0.3772
2025-12-15 22:26:40,002: t15.2025.03.16 val PER: 0.2382
2025-12-15 22:26:40,002: t15.2025.03.30 val PER: 0.2920
2025-12-15 22:26:40,002: t15.2025.04.13 val PER: 0.2625
2025-12-15 22:26:40,002: New best test PER 0.1929 --> 0.1860
2025-12-15 22:26:40,002: Checkpointing model
2025-12-15 22:26:40,429: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:26:54,322: Train batch 18200: loss: 0.93 grad norm: 2.17 time: 0.088
2025-12-15 22:27:09,058: Train batch 18400: loss: 1.03 grad norm: 3.19 time: 0.076
2025-12-15 22:27:23,664: Train batch 18600: loss: 1.02 grad norm: 4.18 time: 0.053
2025-12-15 22:27:38,219: Train batch 18800: loss: 1.04 grad norm: 1.53 time: 0.073
2025-12-15 22:27:52,807: Train batch 19000: loss: 0.95 grad norm: 1.90 time: 0.070
2025-12-15 22:28:07,381: Train batch 19200: loss: 1.04 grad norm: 2.83 time: 0.073
2025-12-15 22:28:21,991: Train batch 19400: loss: 1.13 grad norm: 6.44 time: 0.065
2025-12-15 22:28:36,379: Train batch 19600: loss: 1.05 grad norm: 3.01 time: 0.074
2025-12-15 22:28:50,922: Train batch 19800: loss: 1.08 grad norm: 2.35 time: 0.080
2025-12-15 22:29:05,420: Train batch 20000: loss: 1.04 grad norm: 2.25 time: 0.088
2025-12-15 22:29:05,420: Running test after training batch: 20000
2025-12-15 22:29:14,547: Val batch 20000: PER (avg): 0.1817 CTC Loss (avg): 0.8026 time: 9.127
2025-12-15 22:29:14,547: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:29:14,547: t15.2023.08.13 val PER: 0.1455
2025-12-15 22:29:14,547: t15.2023.08.18 val PER: 0.1324
2025-12-15 22:29:14,548: t15.2023.08.20 val PER: 0.1176
2025-12-15 22:29:14,548: t15.2023.08.25 val PER: 0.1355
2025-12-15 22:29:14,548: t15.2023.08.27 val PER: 0.1913
2025-12-15 22:29:14,548: t15.2023.09.01 val PER: 0.0909
2025-12-15 22:29:14,548: t15.2023.09.03 val PER: 0.1900
2025-12-15 22:29:14,548: t15.2023.09.24 val PER: 0.1274
2025-12-15 22:29:14,548: t15.2023.09.29 val PER: 0.1615
2025-12-15 22:29:14,548: t15.2023.10.01 val PER: 0.2048
2025-12-15 22:29:14,548: t15.2023.10.06 val PER: 0.1399
2025-12-15 22:29:14,548: t15.2023.10.08 val PER: 0.2395
2025-12-15 22:29:14,548: t15.2023.10.13 val PER: 0.2560
2025-12-15 22:29:14,548: t15.2023.10.15 val PER: 0.1721
2025-12-15 22:29:14,548: t15.2023.10.20 val PER: 0.2416
2025-12-15 22:29:14,548: t15.2023.10.22 val PER: 0.1325
2025-12-15 22:29:14,548: t15.2023.11.03 val PER: 0.2028
2025-12-15 22:29:14,548: t15.2023.11.04 val PER: 0.0375
2025-12-15 22:29:14,548: t15.2023.11.17 val PER: 0.0762
2025-12-15 22:29:14,548: t15.2023.11.19 val PER: 0.0838
2025-12-15 22:29:14,548: t15.2023.11.26 val PER: 0.1746
2025-12-15 22:29:14,549: t15.2023.12.03 val PER: 0.1565
2025-12-15 22:29:14,549: t15.2023.12.08 val PER: 0.1571
2025-12-15 22:29:14,549: t15.2023.12.10 val PER: 0.1261
2025-12-15 22:29:14,549: t15.2023.12.17 val PER: 0.1788
2025-12-15 22:29:14,549: t15.2023.12.29 val PER: 0.1682
2025-12-15 22:29:14,549: t15.2024.02.25 val PER: 0.1306
2025-12-15 22:29:14,549: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:29:14,549: t15.2024.03.08 val PER: 0.2532
2025-12-15 22:29:14,549: t15.2024.03.15 val PER: 0.2458
2025-12-15 22:29:14,549: t15.2024.03.17 val PER: 0.1618
2025-12-15 22:29:14,549: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:29:14,549: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:29:14,549: t15.2024.05.10 val PER: 0.2021
2025-12-15 22:29:14,549: t15.2024.06.14 val PER: 0.2066
2025-12-15 22:29:14,549: t15.2024.07.19 val PER: 0.2446
2025-12-15 22:29:14,549: t15.2024.07.21 val PER: 0.1359
2025-12-15 22:29:14,549: t15.2024.07.28 val PER: 0.1529
2025-12-15 22:29:14,549: t15.2025.01.10 val PER: 0.3278
2025-12-15 22:29:14,549: t15.2025.01.12 val PER: 0.1878
2025-12-15 22:29:14,549: t15.2025.03.14 val PER: 0.3432
2025-12-15 22:29:14,550: t15.2025.03.16 val PER: 0.2552
2025-12-15 22:29:14,550: t15.2025.03.30 val PER: 0.3000
2025-12-15 22:29:14,550: t15.2025.04.13 val PER: 0.2696
2025-12-15 22:29:14,550: New best test PER 0.1860 --> 0.1817
2025-12-15 22:29:14,550: Checkpointing model
2025-12-15 22:29:14,957: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:29:28,053: Train batch 20200: loss: 1.01 grad norm: 2.23 time: 0.065
2025-12-15 22:29:41,902: Train batch 20400: loss: 1.13 grad norm: 1.75 time: 0.079
2025-12-15 22:29:56,254: Train batch 20600: loss: 0.95 grad norm: 1.34 time: 0.071
2025-12-15 22:30:10,905: Train batch 20800: loss: 0.96 grad norm: 2.04 time: 0.071
2025-12-15 22:30:25,453: Train batch 21000: loss: 1.13 grad norm: 2.95 time: 0.052
2025-12-15 22:30:39,105: Train batch 21200: loss: 1.25 grad norm: 3.42 time: 0.069
2025-12-15 22:30:53,295: Train batch 21400: loss: 0.99 grad norm: 1.63 time: 0.055
2025-12-15 22:31:07,789: Train batch 21600: loss: 1.09 grad norm: 2.61 time: 0.054
2025-12-15 22:31:21,095: Train batch 21800: loss: 0.97 grad norm: 3.14 time: 0.069
2025-12-15 22:31:35,202: Train batch 22000: loss: 1.02 grad norm: 1.70 time: 0.071
2025-12-15 22:31:35,202: Running test after training batch: 22000
2025-12-15 22:31:44,252: Val batch 22000: PER (avg): 0.1732 CTC Loss (avg): 0.7885 time: 9.049
2025-12-15 22:31:44,252: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:31:44,252: t15.2023.08.13 val PER: 0.1341
2025-12-15 22:31:44,252: t15.2023.08.18 val PER: 0.1282
2025-12-15 22:31:44,252: t15.2023.08.20 val PER: 0.1072
2025-12-15 22:31:44,252: t15.2023.08.25 val PER: 0.1265
2025-12-15 22:31:44,252: t15.2023.08.27 val PER: 0.2154
2025-12-15 22:31:44,252: t15.2023.09.01 val PER: 0.0771
2025-12-15 22:31:44,252: t15.2023.09.03 val PER: 0.1698
2025-12-15 22:31:44,252: t15.2023.09.24 val PER: 0.1311
2025-12-15 22:31:44,252: t15.2023.09.29 val PER: 0.1640
2025-12-15 22:31:44,252: t15.2023.10.01 val PER: 0.2034
2025-12-15 22:31:44,253: t15.2023.10.06 val PER: 0.1302
2025-12-15 22:31:44,253: t15.2023.10.08 val PER: 0.2260
2025-12-15 22:31:44,253: t15.2023.10.13 val PER: 0.2172
2025-12-15 22:31:44,253: t15.2023.10.15 val PER: 0.1740
2025-12-15 22:31:44,253: t15.2023.10.20 val PER: 0.2383
2025-12-15 22:31:44,253: t15.2023.10.22 val PER: 0.1325
2025-12-15 22:31:44,253: t15.2023.11.03 val PER: 0.1927
2025-12-15 22:31:44,253: t15.2023.11.04 val PER: 0.0307
2025-12-15 22:31:44,253: t15.2023.11.17 val PER: 0.0778
2025-12-15 22:31:44,253: t15.2023.11.19 val PER: 0.0858
2025-12-15 22:31:44,253: t15.2023.11.26 val PER: 0.1442
2025-12-15 22:31:44,253: t15.2023.12.03 val PER: 0.1460
2025-12-15 22:31:44,253: t15.2023.12.08 val PER: 0.1372
2025-12-15 22:31:44,253: t15.2023.12.10 val PER: 0.1301
2025-12-15 22:31:44,253: t15.2023.12.17 val PER: 0.1798
2025-12-15 22:31:44,253: t15.2023.12.29 val PER: 0.1592
2025-12-15 22:31:44,253: t15.2024.02.25 val PER: 0.1348
2025-12-15 22:31:44,253: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:31:44,253: t15.2024.03.08 val PER: 0.2646
2025-12-15 22:31:44,254: t15.2024.03.15 val PER: 0.2276
2025-12-15 22:31:44,254: t15.2024.03.17 val PER: 0.1729
2025-12-15 22:31:44,254: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:31:44,254: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:31:44,254: t15.2024.05.10 val PER: 0.1872
2025-12-15 22:31:44,254: t15.2024.06.14 val PER: 0.2192
2025-12-15 22:31:44,254: t15.2024.07.19 val PER: 0.2268
2025-12-15 22:31:44,254: t15.2024.07.21 val PER: 0.1124
2025-12-15 22:31:44,254: t15.2024.07.28 val PER: 0.1551
2025-12-15 22:31:44,254: t15.2025.01.10 val PER: 0.3168
2025-12-15 22:31:44,254: t15.2025.01.12 val PER: 0.1717
2025-12-15 22:31:44,254: t15.2025.03.14 val PER: 0.3432
2025-12-15 22:31:44,254: t15.2025.03.16 val PER: 0.2251
2025-12-15 22:31:44,254: t15.2025.03.30 val PER: 0.2920
2025-12-15 22:31:44,254: t15.2025.04.13 val PER: 0.2582
2025-12-15 22:31:44,254: New best test PER 0.1817 --> 0.1732
2025-12-15 22:31:44,254: Checkpointing model
2025-12-15 22:31:44,671: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:31:59,139: Train batch 22200: loss: 0.97 grad norm: 1.53 time: 0.056
2025-12-15 22:32:13,445: Train batch 22400: loss: 0.88 grad norm: 7.13 time: 0.063
2025-12-15 22:32:27,955: Train batch 22600: loss: 0.92 grad norm: 1.59 time: 0.074
2025-12-15 22:32:42,549: Train batch 22800: loss: 1.02 grad norm: 2.71 time: 0.066
2025-12-15 22:32:56,835: Train batch 23000: loss: 1.11 grad norm: 3.12 time: 0.056
2025-12-15 22:33:10,795: Train batch 23200: loss: 1.01 grad norm: 2.36 time: 0.054
2025-12-15 22:33:25,547: Train batch 23400: loss: 1.02 grad norm: 2.75 time: 0.057
2025-12-15 22:33:40,201: Train batch 23600: loss: 0.96 grad norm: 1.41 time: 0.068
2025-12-15 22:33:54,787: Train batch 23800: loss: 0.99 grad norm: 2.30 time: 0.071
2025-12-15 22:34:09,319: Train batch 24000: loss: 0.95 grad norm: 1.75 time: 0.073
2025-12-15 22:34:09,319: Running test after training batch: 24000
2025-12-15 22:34:18,322: Val batch 24000: PER (avg): 0.1711 CTC Loss (avg): 0.8027 time: 9.002
2025-12-15 22:34:18,322: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:34:18,322: t15.2023.08.13 val PER: 0.1351
2025-12-15 22:34:18,322: t15.2023.08.18 val PER: 0.1199
2025-12-15 22:34:18,322: t15.2023.08.20 val PER: 0.1112
2025-12-15 22:34:18,323: t15.2023.08.25 val PER: 0.1370
2025-12-15 22:34:18,323: t15.2023.08.27 val PER: 0.1768
2025-12-15 22:34:18,323: t15.2023.09.01 val PER: 0.0771
2025-12-15 22:34:18,323: t15.2023.09.03 val PER: 0.1698
2025-12-15 22:34:18,323: t15.2023.09.24 val PER: 0.1311
2025-12-15 22:34:18,323: t15.2023.09.29 val PER: 0.1602
2025-12-15 22:34:18,323: t15.2023.10.01 val PER: 0.2081
2025-12-15 22:34:18,323: t15.2023.10.06 val PER: 0.1195
2025-12-15 22:34:18,323: t15.2023.10.08 val PER: 0.2368
2025-12-15 22:34:18,323: t15.2023.10.13 val PER: 0.2118
2025-12-15 22:34:18,323: t15.2023.10.15 val PER: 0.1655
2025-12-15 22:34:18,323: t15.2023.10.20 val PER: 0.2315
2025-12-15 22:34:18,323: t15.2023.10.22 val PER: 0.1370
2025-12-15 22:34:18,323: t15.2023.11.03 val PER: 0.1872
2025-12-15 22:34:18,323: t15.2023.11.04 val PER: 0.0341
2025-12-15 22:34:18,323: t15.2023.11.17 val PER: 0.0669
2025-12-15 22:34:18,323: t15.2023.11.19 val PER: 0.0758
2025-12-15 22:34:18,323: t15.2023.11.26 val PER: 0.1435
2025-12-15 22:34:18,323: t15.2023.12.03 val PER: 0.1145
2025-12-15 22:34:18,324: t15.2023.12.08 val PER: 0.1232
2025-12-15 22:34:18,324: t15.2023.12.10 val PER: 0.1209
2025-12-15 22:34:18,324: t15.2023.12.17 val PER: 0.1757
2025-12-15 22:34:18,324: t15.2023.12.29 val PER: 0.1599
2025-12-15 22:34:18,324: t15.2024.02.25 val PER: 0.1334
2025-12-15 22:34:18,324: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:34:18,324: t15.2024.03.08 val PER: 0.2432
2025-12-15 22:34:18,324: t15.2024.03.15 val PER: 0.2289
2025-12-15 22:34:18,324: t15.2024.03.17 val PER: 0.1520
2025-12-15 22:34:18,324: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:34:18,324: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:34:18,324: t15.2024.05.10 val PER: 0.2080
2025-12-15 22:34:18,324: t15.2024.06.14 val PER: 0.2240
2025-12-15 22:34:18,324: t15.2024.07.19 val PER: 0.2235
2025-12-15 22:34:18,324: t15.2024.07.21 val PER: 0.1255
2025-12-15 22:34:18,324: t15.2024.07.28 val PER: 0.1507
2025-12-15 22:34:18,324: t15.2025.01.10 val PER: 0.3444
2025-12-15 22:34:18,324: t15.2025.01.12 val PER: 0.1747
2025-12-15 22:34:18,324: t15.2025.03.14 val PER: 0.3757
2025-12-15 22:34:18,325: t15.2025.03.16 val PER: 0.2291
2025-12-15 22:34:18,325: t15.2025.03.30 val PER: 0.2885
2025-12-15 22:34:18,325: t15.2025.04.13 val PER: 0.2682
2025-12-15 22:34:18,325: New best test PER 0.1732 --> 0.1711
2025-12-15 22:34:18,325: Checkpointing model
2025-12-15 22:34:18,936: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:34:33,647: Train batch 24200: loss: 0.86 grad norm: 1.95 time: 0.069
2025-12-15 22:34:47,769: Train batch 24400: loss: 0.97 grad norm: 1.07 time: 0.055
2025-12-15 22:35:02,083: Train batch 24600: loss: 1.02 grad norm: 2.49 time: 0.078
2025-12-15 22:35:15,938: Train batch 24800: loss: 0.99 grad norm: 2.78 time: 0.070
2025-12-15 22:35:30,114: Train batch 25000: loss: 0.91 grad norm: 0.95 time: 0.066
2025-12-15 22:35:44,761: Train batch 25200: loss: 0.88 grad norm: 1.25 time: 0.071
2025-12-15 22:35:57,626: Train batch 25400: loss: 0.91 grad norm: 1.41 time: 0.040
2025-12-15 22:36:10,393: Train batch 25600: loss: 1.01 grad norm: 4.58 time: 0.033
2025-12-15 22:36:21,010: Train batch 25800: loss: 0.92 grad norm: 1.19 time: 0.060
2025-12-15 22:36:31,162: Train batch 26000: loss: 0.94 grad norm: 2.33 time: 0.046
2025-12-15 22:36:31,163: Running test after training batch: 26000
2025-12-15 22:36:40,092: Val batch 26000: PER (avg): 0.1700 CTC Loss (avg): 0.8354 time: 8.929
2025-12-15 22:36:40,092: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:36:40,092: t15.2023.08.13 val PER: 0.1216
2025-12-15 22:36:40,092: t15.2023.08.18 val PER: 0.1266
2025-12-15 22:36:40,092: t15.2023.08.20 val PER: 0.1128
2025-12-15 22:36:40,092: t15.2023.08.25 val PER: 0.1250
2025-12-15 22:36:40,092: t15.2023.08.27 val PER: 0.1704
2025-12-15 22:36:40,092: t15.2023.09.01 val PER: 0.0828
2025-12-15 22:36:40,092: t15.2023.09.03 val PER: 0.1651
2025-12-15 22:36:40,092: t15.2023.09.24 val PER: 0.1299
2025-12-15 22:36:40,092: t15.2023.09.29 val PER: 0.1544
2025-12-15 22:36:40,092: t15.2023.10.01 val PER: 0.1915
2025-12-15 22:36:40,093: t15.2023.10.06 val PER: 0.1302
2025-12-15 22:36:40,093: t15.2023.10.08 val PER: 0.2327
2025-12-15 22:36:40,093: t15.2023.10.13 val PER: 0.2203
2025-12-15 22:36:40,093: t15.2023.10.15 val PER: 0.1628
2025-12-15 22:36:40,093: t15.2023.10.20 val PER: 0.2148
2025-12-15 22:36:40,093: t15.2023.10.22 val PER: 0.1314
2025-12-15 22:36:40,093: t15.2023.11.03 val PER: 0.1995
2025-12-15 22:36:40,093: t15.2023.11.04 val PER: 0.0273
2025-12-15 22:36:40,093: t15.2023.11.17 val PER: 0.0715
2025-12-15 22:36:40,093: t15.2023.11.19 val PER: 0.0858
2025-12-15 22:36:40,093: t15.2023.11.26 val PER: 0.1312
2025-12-15 22:36:40,093: t15.2023.12.03 val PER: 0.1092
2025-12-15 22:36:40,093: t15.2023.12.08 val PER: 0.1365
2025-12-15 22:36:40,093: t15.2023.12.10 val PER: 0.1314
2025-12-15 22:36:40,093: t15.2023.12.17 val PER: 0.1746
2025-12-15 22:36:40,093: t15.2023.12.29 val PER: 0.1496
2025-12-15 22:36:40,093: t15.2024.02.25 val PER: 0.1278
2025-12-15 22:36:40,093: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:36:40,093: t15.2024.03.08 val PER: 0.2262
2025-12-15 22:36:40,093: t15.2024.03.15 val PER: 0.2295
2025-12-15 22:36:40,094: t15.2024.03.17 val PER: 0.1632
2025-12-15 22:36:40,094: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:36:40,094: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:36:40,094: t15.2024.05.10 val PER: 0.2021
2025-12-15 22:36:40,094: t15.2024.06.14 val PER: 0.2303
2025-12-15 22:36:40,094: t15.2024.07.19 val PER: 0.2439
2025-12-15 22:36:40,094: t15.2024.07.21 val PER: 0.1193
2025-12-15 22:36:40,094: t15.2024.07.28 val PER: 0.1485
2025-12-15 22:36:40,094: t15.2025.01.10 val PER: 0.3223
2025-12-15 22:36:40,094: t15.2025.01.12 val PER: 0.1563
2025-12-15 22:36:40,094: t15.2025.03.14 val PER: 0.3817
2025-12-15 22:36:40,094: t15.2025.03.16 val PER: 0.2251
2025-12-15 22:36:40,094: t15.2025.03.30 val PER: 0.2862
2025-12-15 22:36:40,094: t15.2025.04.13 val PER: 0.2710
2025-12-15 22:36:40,094: New best test PER 0.1711 --> 0.1700
2025-12-15 22:36:40,094: Checkpointing model
2025-12-15 22:36:40,539: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:36:54,863: Train batch 26200: loss: 0.91 grad norm: 1.63 time: 0.042
2025-12-15 22:37:06,830: Train batch 26400: loss: 0.88 grad norm: 1.33 time: 0.044
2025-12-15 22:37:21,111: Train batch 26600: loss: 0.84 grad norm: 1.99 time: 0.067
2025-12-15 22:37:35,076: Train batch 26800: loss: 0.91 grad norm: 4.08 time: 0.058
2025-12-15 22:37:49,568: Train batch 27000: loss: 0.84 grad norm: 0.81 time: 0.057
2025-12-15 22:38:04,009: Train batch 27200: loss: 0.88 grad norm: 4.29 time: 0.082
2025-12-15 22:38:18,047: Train batch 27400: loss: 1.02 grad norm: 3.50 time: 0.063
2025-12-15 22:38:31,652: Train batch 27600: loss: 0.87 grad norm: 1.14 time: 0.077
2025-12-15 22:38:46,002: Train batch 27800: loss: 0.97 grad norm: 2.20 time: 0.070
2025-12-15 22:39:00,685: Train batch 28000: loss: 0.98 grad norm: 2.37 time: 0.081
2025-12-15 22:39:00,686: Running test after training batch: 28000
2025-12-15 22:39:09,689: Val batch 28000: PER (avg): 0.1682 CTC Loss (avg): 0.8313 time: 9.003
2025-12-15 22:39:09,689: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:39:09,689: t15.2023.08.13 val PER: 0.1289
2025-12-15 22:39:09,689: t15.2023.08.18 val PER: 0.1308
2025-12-15 22:39:09,689: t15.2023.08.20 val PER: 0.1160
2025-12-15 22:39:09,690: t15.2023.08.25 val PER: 0.1431
2025-12-15 22:39:09,690: t15.2023.08.27 val PER: 0.1608
2025-12-15 22:39:09,690: t15.2023.09.01 val PER: 0.0909
2025-12-15 22:39:09,690: t15.2023.09.03 val PER: 0.1556
2025-12-15 22:39:09,690: t15.2023.09.24 val PER: 0.1214
2025-12-15 22:39:09,690: t15.2023.09.29 val PER: 0.1551
2025-12-15 22:39:09,690: t15.2023.10.01 val PER: 0.1948
2025-12-15 22:39:09,690: t15.2023.10.06 val PER: 0.1378
2025-12-15 22:39:09,690: t15.2023.10.08 val PER: 0.2165
2025-12-15 22:39:09,690: t15.2023.10.13 val PER: 0.2118
2025-12-15 22:39:09,690: t15.2023.10.15 val PER: 0.1608
2025-12-15 22:39:09,690: t15.2023.10.20 val PER: 0.2181
2025-12-15 22:39:09,690: t15.2023.10.22 val PER: 0.1325
2025-12-15 22:39:09,690: t15.2023.11.03 val PER: 0.1839
2025-12-15 22:39:09,690: t15.2023.11.04 val PER: 0.0546
2025-12-15 22:39:09,690: t15.2023.11.17 val PER: 0.0591
2025-12-15 22:39:09,690: t15.2023.11.19 val PER: 0.0758
2025-12-15 22:39:09,690: t15.2023.11.26 val PER: 0.1435
2025-12-15 22:39:09,691: t15.2023.12.03 val PER: 0.1113
2025-12-15 22:39:09,691: t15.2023.12.08 val PER: 0.1292
2025-12-15 22:39:09,691: t15.2023.12.10 val PER: 0.1183
2025-12-15 22:39:09,691: t15.2023.12.17 val PER: 0.1788
2025-12-15 22:39:09,691: t15.2023.12.29 val PER: 0.1373
2025-12-15 22:39:09,691: t15.2024.02.25 val PER: 0.1222
2025-12-15 22:39:09,691: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:39:09,691: t15.2024.03.08 val PER: 0.2660
2025-12-15 22:39:09,691: t15.2024.03.15 val PER: 0.2320
2025-12-15 22:39:09,691: t15.2024.03.17 val PER: 0.1513
2025-12-15 22:39:09,691: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:39:09,691: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:39:09,691: t15.2024.05.10 val PER: 0.1828
2025-12-15 22:39:09,691: t15.2024.06.14 val PER: 0.2066
2025-12-15 22:39:09,691: t15.2024.07.19 val PER: 0.2307
2025-12-15 22:39:09,691: t15.2024.07.21 val PER: 0.1159
2025-12-15 22:39:09,691: t15.2024.07.28 val PER: 0.1544
2025-12-15 22:39:09,691: t15.2025.01.10 val PER: 0.3140
2025-12-15 22:39:09,691: t15.2025.01.12 val PER: 0.1694
2025-12-15 22:39:09,692: t15.2025.03.14 val PER: 0.3417
2025-12-15 22:39:09,692: t15.2025.03.16 val PER: 0.2343
2025-12-15 22:39:09,692: t15.2025.03.30 val PER: 0.2874
2025-12-15 22:39:09,692: t15.2025.04.13 val PER: 0.2782
2025-12-15 22:39:09,692: New best test PER 0.1700 --> 0.1682
2025-12-15 22:39:09,692: Checkpointing model
2025-12-15 22:39:10,103: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:39:24,865: Train batch 28200: loss: 0.84 grad norm: 1.92 time: 0.056
2025-12-15 22:39:39,726: Train batch 28400: loss: 0.97 grad norm: 2.43 time: 0.077
2025-12-15 22:39:54,336: Train batch 28600: loss: 0.90 grad norm: 2.14 time: 0.070
2025-12-15 22:40:09,089: Train batch 28800: loss: 0.90 grad norm: 1.93 time: 0.080
2025-12-15 22:40:23,832: Train batch 29000: loss: 0.93 grad norm: 1.36 time: 0.056
2025-12-15 22:40:38,532: Train batch 29200: loss: 0.97 grad norm: 2.19 time: 0.070
2025-12-15 22:40:52,815: Train batch 29400: loss: 0.88 grad norm: 0.61 time: 0.086
2025-12-15 22:41:05,844: Train batch 29600: loss: 0.85 grad norm: 1.46 time: 0.042
2025-12-15 22:41:18,623: Train batch 29800: loss: 0.93 grad norm: 1.04 time: 0.054
2025-12-15 22:41:32,409: Train batch 30000: loss: 0.82 grad norm: 0.96 time: 0.072
2025-12-15 22:41:32,410: Running test after training batch: 30000
2025-12-15 22:41:41,522: Val batch 30000: PER (avg): 0.1624 CTC Loss (avg): 0.8319 time: 9.111
2025-12-15 22:41:41,522: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:41:41,522: t15.2023.08.13 val PER: 0.1227
2025-12-15 22:41:41,522: t15.2023.08.18 val PER: 0.1241
2025-12-15 22:41:41,522: t15.2023.08.20 val PER: 0.1128
2025-12-15 22:41:41,522: t15.2023.08.25 val PER: 0.1145
2025-12-15 22:41:41,522: t15.2023.08.27 val PER: 0.1768
2025-12-15 22:41:41,522: t15.2023.09.01 val PER: 0.0844
2025-12-15 22:41:41,522: t15.2023.09.03 val PER: 0.1532
2025-12-15 22:41:41,522: t15.2023.09.24 val PER: 0.1092
2025-12-15 22:41:41,522: t15.2023.09.29 val PER: 0.1442
2025-12-15 22:41:41,522: t15.2023.10.01 val PER: 0.1929
2025-12-15 22:41:41,522: t15.2023.10.06 val PER: 0.1206
2025-12-15 22:41:41,523: t15.2023.10.08 val PER: 0.2124
2025-12-15 22:41:41,523: t15.2023.10.13 val PER: 0.2219
2025-12-15 22:41:41,523: t15.2023.10.15 val PER: 0.1747
2025-12-15 22:41:41,523: t15.2023.10.20 val PER: 0.2148
2025-12-15 22:41:41,523: t15.2023.10.22 val PER: 0.1136
2025-12-15 22:41:41,523: t15.2023.11.03 val PER: 0.1805
2025-12-15 22:41:41,523: t15.2023.11.04 val PER: 0.0444
2025-12-15 22:41:41,523: t15.2023.11.17 val PER: 0.0700
2025-12-15 22:41:41,523: t15.2023.11.19 val PER: 0.0778
2025-12-15 22:41:41,523: t15.2023.11.26 val PER: 0.1391
2025-12-15 22:41:41,523: t15.2023.12.03 val PER: 0.1134
2025-12-15 22:41:41,523: t15.2023.12.08 val PER: 0.1119
2025-12-15 22:41:41,523: t15.2023.12.10 val PER: 0.1091
2025-12-15 22:41:41,523: t15.2023.12.17 val PER: 0.1850
2025-12-15 22:41:41,523: t15.2023.12.29 val PER: 0.1455
2025-12-15 22:41:41,523: t15.2024.02.25 val PER: 0.1152
2025-12-15 22:41:41,523: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:41:41,523: t15.2024.03.08 val PER: 0.2304
2025-12-15 22:41:41,523: t15.2024.03.15 val PER: 0.2083
2025-12-15 22:41:41,524: t15.2024.03.17 val PER: 0.1499
2025-12-15 22:41:41,524: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:41:41,524: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:41:41,524: t15.2024.05.10 val PER: 0.1753
2025-12-15 22:41:41,524: t15.2024.06.14 val PER: 0.1877
2025-12-15 22:41:41,524: t15.2024.07.19 val PER: 0.2129
2025-12-15 22:41:41,524: t15.2024.07.21 val PER: 0.1083
2025-12-15 22:41:41,524: t15.2024.07.28 val PER: 0.1419
2025-12-15 22:41:41,524: t15.2025.01.10 val PER: 0.2989
2025-12-15 22:41:41,524: t15.2025.01.12 val PER: 0.1678
2025-12-15 22:41:41,524: t15.2025.03.14 val PER: 0.3683
2025-12-15 22:41:41,524: t15.2025.03.16 val PER: 0.2225
2025-12-15 22:41:41,524: t15.2025.03.30 val PER: 0.2931
2025-12-15 22:41:41,524: t15.2025.04.13 val PER: 0.2625
2025-12-15 22:41:41,524: New best test PER 0.1682 --> 0.1624
2025-12-15 22:41:41,524: Checkpointing model
2025-12-15 22:41:41,947: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:41:55,733: Train batch 30200: loss: 1.05 grad norm: 2.24 time: 0.071
2025-12-15 22:42:10,339: Train batch 30400: loss: 0.92 grad norm: 1.68 time: 0.092
2025-12-15 22:42:24,557: Train batch 30600: loss: 0.97 grad norm: 2.23 time: 0.054
2025-12-15 22:42:37,049: Train batch 30800: loss: 0.88 grad norm: 1.19 time: 0.068
2025-12-15 22:42:51,079: Train batch 31000: loss: 1.11 grad norm: 1.81 time: 0.072
2025-12-15 22:43:05,320: Train batch 31200: loss: 1.00 grad norm: 1.23 time: 0.067
2025-12-15 22:43:19,474: Train batch 31400: loss: 0.97 grad norm: 1.84 time: 0.072
2025-12-15 22:43:29,789: Train batch 31600: loss: 0.95 grad norm: 0.55 time: 0.054
2025-12-15 22:43:43,988: Train batch 31800: loss: 0.91 grad norm: 1.35 time: 0.088
2025-12-15 22:43:57,319: Train batch 32000: loss: 0.87 grad norm: 1.50 time: 0.064
2025-12-15 22:43:57,319: Running test after training batch: 32000
2025-12-15 22:44:06,419: Val batch 32000: PER (avg): 0.1619 CTC Loss (avg): 0.8293 time: 9.100
2025-12-15 22:44:06,420: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:44:06,420: t15.2023.08.13 val PER: 0.1185
2025-12-15 22:44:06,420: t15.2023.08.18 val PER: 0.1282
2025-12-15 22:44:06,420: t15.2023.08.20 val PER: 0.1033
2025-12-15 22:44:06,420: t15.2023.08.25 val PER: 0.1250
2025-12-15 22:44:06,420: t15.2023.08.27 val PER: 0.1720
2025-12-15 22:44:06,420: t15.2023.09.01 val PER: 0.0885
2025-12-15 22:44:06,420: t15.2023.09.03 val PER: 0.1639
2025-12-15 22:44:06,420: t15.2023.09.24 val PER: 0.1201
2025-12-15 22:44:06,420: t15.2023.09.29 val PER: 0.1423
2025-12-15 22:44:06,420: t15.2023.10.01 val PER: 0.2028
2025-12-15 22:44:06,420: t15.2023.10.06 val PER: 0.1033
2025-12-15 22:44:06,420: t15.2023.10.08 val PER: 0.2084
2025-12-15 22:44:06,420: t15.2023.10.13 val PER: 0.2335
2025-12-15 22:44:06,420: t15.2023.10.15 val PER: 0.1556
2025-12-15 22:44:06,420: t15.2023.10.20 val PER: 0.2248
2025-12-15 22:44:06,421: t15.2023.10.22 val PER: 0.1225
2025-12-15 22:44:06,421: t15.2023.11.03 val PER: 0.2022
2025-12-15 22:44:06,421: t15.2023.11.04 val PER: 0.0341
2025-12-15 22:44:06,421: t15.2023.11.17 val PER: 0.0560
2025-12-15 22:44:06,421: t15.2023.11.19 val PER: 0.0838
2025-12-15 22:44:06,421: t15.2023.11.26 val PER: 0.1391
2025-12-15 22:44:06,421: t15.2023.12.03 val PER: 0.1208
2025-12-15 22:44:06,421: t15.2023.12.08 val PER: 0.1052
2025-12-15 22:44:06,421: t15.2023.12.10 val PER: 0.1261
2025-12-15 22:44:06,421: t15.2023.12.17 val PER: 0.1653
2025-12-15 22:44:06,421: t15.2023.12.29 val PER: 0.1283
2025-12-15 22:44:06,421: t15.2024.02.25 val PER: 0.1124
2025-12-15 22:44:06,421: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:44:06,421: t15.2024.03.08 val PER: 0.2475
2025-12-15 22:44:06,421: t15.2024.03.15 val PER: 0.2164
2025-12-15 22:44:06,421: t15.2024.03.17 val PER: 0.1485
2025-12-15 22:44:06,421: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:44:06,421: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:44:06,422: t15.2024.05.10 val PER: 0.1828
2025-12-15 22:44:06,422: t15.2024.06.14 val PER: 0.1987
2025-12-15 22:44:06,422: t15.2024.07.19 val PER: 0.2109
2025-12-15 22:44:06,422: t15.2024.07.21 val PER: 0.1131
2025-12-15 22:44:06,422: t15.2024.07.28 val PER: 0.1316
2025-12-15 22:44:06,422: t15.2025.01.10 val PER: 0.3154
2025-12-15 22:44:06,422: t15.2025.01.12 val PER: 0.1624
2025-12-15 22:44:06,422: t15.2025.03.14 val PER: 0.3743
2025-12-15 22:44:06,422: t15.2025.03.16 val PER: 0.2134
2025-12-15 22:44:06,422: t15.2025.03.30 val PER: 0.2724
2025-12-15 22:44:06,422: t15.2025.04.13 val PER: 0.2340
2025-12-15 22:44:06,422: New best test PER 0.1624 --> 0.1619
2025-12-15 22:44:06,422: Checkpointing model
2025-12-15 22:44:06,851: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:44:19,664: Train batch 32200: loss: 1.07 grad norm: 1.77 time: 0.081
2025-12-15 22:44:34,194: Train batch 32400: loss: 0.95 grad norm: 1.32 time: 0.072
2025-12-15 22:44:48,684: Train batch 32600: loss: 0.93 grad norm: 1.71 time: 0.083
2025-12-15 22:45:03,017: Train batch 32800: loss: 0.84 grad norm: 0.77 time: 0.067
2025-12-15 22:45:17,553: Train batch 33000: loss: 0.87 grad norm: 1.03 time: 0.072
2025-12-15 22:45:31,730: Train batch 33200: loss: 0.90 grad norm: 1.48 time: 0.056
2025-12-15 22:45:45,408: Train batch 33400: loss: 0.86 grad norm: 1.31 time: 0.078
2025-12-15 22:45:58,860: Train batch 33600: loss: 0.88 grad norm: 1.28 time: 0.069
2025-12-15 22:46:13,180: Train batch 33800: loss: 0.94 grad norm: 2.46 time: 0.087
2025-12-15 22:46:26,026: Train batch 34000: loss: 0.94 grad norm: 1.84 time: 0.070
2025-12-15 22:46:26,027: Running test after training batch: 34000
2025-12-15 22:46:35,114: Val batch 34000: PER (avg): 0.1577 CTC Loss (avg): 0.8529 time: 9.087
2025-12-15 22:46:35,115: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:46:35,115: t15.2023.08.13 val PER: 0.1331
2025-12-15 22:46:35,115: t15.2023.08.18 val PER: 0.1182
2025-12-15 22:46:35,115: t15.2023.08.20 val PER: 0.1104
2025-12-15 22:46:35,115: t15.2023.08.25 val PER: 0.1205
2025-12-15 22:46:35,115: t15.2023.08.27 val PER: 0.1801
2025-12-15 22:46:35,115: t15.2023.09.01 val PER: 0.0812
2025-12-15 22:46:35,115: t15.2023.09.03 val PER: 0.1591
2025-12-15 22:46:35,115: t15.2023.09.24 val PER: 0.1019
2025-12-15 22:46:35,115: t15.2023.09.29 val PER: 0.1474
2025-12-15 22:46:35,115: t15.2023.10.01 val PER: 0.1889
2025-12-15 22:46:35,115: t15.2023.10.06 val PER: 0.1130
2025-12-15 22:46:35,115: t15.2023.10.08 val PER: 0.2084
2025-12-15 22:46:35,115: t15.2023.10.13 val PER: 0.1939
2025-12-15 22:46:35,115: t15.2023.10.15 val PER: 0.1549
2025-12-15 22:46:35,115: t15.2023.10.20 val PER: 0.2114
2025-12-15 22:46:35,115: t15.2023.10.22 val PER: 0.1214
2025-12-15 22:46:35,116: t15.2023.11.03 val PER: 0.2022
2025-12-15 22:46:35,116: t15.2023.11.04 val PER: 0.0478
2025-12-15 22:46:35,116: t15.2023.11.17 val PER: 0.0529
2025-12-15 22:46:35,116: t15.2023.11.19 val PER: 0.0818
2025-12-15 22:46:35,116: t15.2023.11.26 val PER: 0.1290
2025-12-15 22:46:35,116: t15.2023.12.03 val PER: 0.1261
2025-12-15 22:46:35,116: t15.2023.12.08 val PER: 0.1152
2025-12-15 22:46:35,116: t15.2023.12.10 val PER: 0.1117
2025-12-15 22:46:35,116: t15.2023.12.17 val PER: 0.1497
2025-12-15 22:46:35,116: t15.2023.12.29 val PER: 0.1345
2025-12-15 22:46:35,116: t15.2024.02.25 val PER: 0.1138
2025-12-15 22:46:35,116: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:46:35,116: t15.2024.03.08 val PER: 0.2304
2025-12-15 22:46:35,116: t15.2024.03.15 val PER: 0.2089
2025-12-15 22:46:35,116: t15.2024.03.17 val PER: 0.1437
2025-12-15 22:46:35,116: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:46:35,116: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:46:35,116: t15.2024.05.10 val PER: 0.1872
2025-12-15 22:46:35,116: t15.2024.06.14 val PER: 0.1845
2025-12-15 22:46:35,116: t15.2024.07.19 val PER: 0.2096
2025-12-15 22:46:35,117: t15.2024.07.21 val PER: 0.1090
2025-12-15 22:46:35,117: t15.2024.07.28 val PER: 0.1272
2025-12-15 22:46:35,117: t15.2025.01.10 val PER: 0.3030
2025-12-15 22:46:35,117: t15.2025.01.12 val PER: 0.1409
2025-12-15 22:46:35,117: t15.2025.03.14 val PER: 0.3550
2025-12-15 22:46:35,117: t15.2025.03.16 val PER: 0.2212
2025-12-15 22:46:35,117: t15.2025.03.30 val PER: 0.2724
2025-12-15 22:46:35,117: t15.2025.04.13 val PER: 0.2439
2025-12-15 22:46:35,117: New best test PER 0.1619 --> 0.1577
2025-12-15 22:46:35,117: Checkpointing model
2025-12-15 22:46:35,533: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:46:50,164: Train batch 34200: loss: 0.83 grad norm: 1.52 time: 0.070
2025-12-15 22:47:04,858: Train batch 34400: loss: 0.95 grad norm: 1.15 time: 0.068
2025-12-15 22:47:16,793: Train batch 34600: loss: 0.83 grad norm: 1.70 time: 0.064
2025-12-15 22:47:30,415: Train batch 34800: loss: 0.90 grad norm: 1.76 time: 0.056
2025-12-15 22:47:44,884: Train batch 35000: loss: 0.98 grad norm: 1.61 time: 0.074
2025-12-15 22:47:59,465: Train batch 35200: loss: 0.97 grad norm: 1.23 time: 0.057
2025-12-15 22:48:13,171: Train batch 35400: loss: 1.00 grad norm: 2.94 time: 0.031
2025-12-15 22:48:25,798: Train batch 35600: loss: 0.90 grad norm: 1.03 time: 0.059
2025-12-15 22:48:40,078: Train batch 35800: loss: 0.91 grad norm: 1.38 time: 0.058
2025-12-15 22:48:54,611: Train batch 36000: loss: 0.90 grad norm: 1.22 time: 0.073
2025-12-15 22:48:54,612: Running test after training batch: 36000
2025-12-15 22:49:03,738: Val batch 36000: PER (avg): 0.1570 CTC Loss (avg): 0.8732 time: 9.126
2025-12-15 22:49:03,738: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:49:03,738: t15.2023.08.13 val PER: 0.1216
2025-12-15 22:49:03,738: t15.2023.08.18 val PER: 0.1006
2025-12-15 22:49:03,738: t15.2023.08.20 val PER: 0.0890
2025-12-15 22:49:03,739: t15.2023.08.25 val PER: 0.1114
2025-12-15 22:49:03,739: t15.2023.08.27 val PER: 0.1704
2025-12-15 22:49:03,739: t15.2023.09.01 val PER: 0.0804
2025-12-15 22:49:03,739: t15.2023.09.03 val PER: 0.1603
2025-12-15 22:49:03,739: t15.2023.09.24 val PER: 0.1032
2025-12-15 22:49:03,739: t15.2023.09.29 val PER: 0.1353
2025-12-15 22:49:03,739: t15.2023.10.01 val PER: 0.1856
2025-12-15 22:49:03,739: t15.2023.10.06 val PER: 0.1076
2025-12-15 22:49:03,739: t15.2023.10.08 val PER: 0.2124
2025-12-15 22:49:03,739: t15.2023.10.13 val PER: 0.2095
2025-12-15 22:49:03,739: t15.2023.10.15 val PER: 0.1589
2025-12-15 22:49:03,739: t15.2023.10.20 val PER: 0.2248
2025-12-15 22:49:03,739: t15.2023.10.22 val PER: 0.1292
2025-12-15 22:49:03,739: t15.2023.11.03 val PER: 0.1961
2025-12-15 22:49:03,739: t15.2023.11.04 val PER: 0.0444
2025-12-15 22:49:03,739: t15.2023.11.17 val PER: 0.0591
2025-12-15 22:49:03,739: t15.2023.11.19 val PER: 0.0858
2025-12-15 22:49:03,739: t15.2023.11.26 val PER: 0.1486
2025-12-15 22:49:03,740: t15.2023.12.03 val PER: 0.1061
2025-12-15 22:49:03,740: t15.2023.12.08 val PER: 0.1192
2025-12-15 22:49:03,740: t15.2023.12.10 val PER: 0.1130
2025-12-15 22:49:03,740: t15.2023.12.17 val PER: 0.1975
2025-12-15 22:49:03,740: t15.2023.12.29 val PER: 0.1421
2025-12-15 22:49:03,740: t15.2024.02.25 val PER: 0.1096
2025-12-15 22:49:03,740: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:49:03,740: t15.2024.03.08 val PER: 0.2333
2025-12-15 22:49:03,740: t15.2024.03.15 val PER: 0.2226
2025-12-15 22:49:03,740: t15.2024.03.17 val PER: 0.1311
2025-12-15 22:49:03,740: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:49:03,740: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:49:03,740: t15.2024.05.10 val PER: 0.1857
2025-12-15 22:49:03,740: t15.2024.06.14 val PER: 0.1877
2025-12-15 22:49:03,740: t15.2024.07.19 val PER: 0.2070
2025-12-15 22:49:03,740: t15.2024.07.21 val PER: 0.0993
2025-12-15 22:49:03,740: t15.2024.07.28 val PER: 0.1184
2025-12-15 22:49:03,740: t15.2025.01.10 val PER: 0.3140
2025-12-15 22:49:03,740: t15.2025.01.12 val PER: 0.1493
2025-12-15 22:49:03,741: t15.2025.03.14 val PER: 0.3225
2025-12-15 22:49:03,741: t15.2025.03.16 val PER: 0.1990
2025-12-15 22:49:03,741: t15.2025.03.30 val PER: 0.3000
2025-12-15 22:49:03,741: t15.2025.04.13 val PER: 0.2211
2025-12-15 22:49:03,741: New best test PER 0.1577 --> 0.1570
2025-12-15 22:49:03,741: Checkpointing model
2025-12-15 22:49:04,148: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:49:17,882: Train batch 36200: loss: 0.99 grad norm: 1.42 time: 0.044
2025-12-15 22:49:32,357: Train batch 36400: loss: 0.94 grad norm: 3.35 time: 0.055
2025-12-15 22:49:45,549: Train batch 36600: loss: 0.91 grad norm: 1.96 time: 0.056
2025-12-15 22:50:00,432: Train batch 36800: loss: 0.82 grad norm: 1.04 time: 0.070
2025-12-15 22:50:14,299: Train batch 37000: loss: 0.93 grad norm: 1.44 time: 0.063
2025-12-15 22:50:27,243: Train batch 37200: loss: 0.84 grad norm: 1.47 time: 0.062
2025-12-15 22:50:41,664: Train batch 37400: loss: 0.82 grad norm: 1.53 time: 0.067
2025-12-15 22:50:55,671: Train batch 37600: loss: 0.87 grad norm: 1.67 time: 0.069
2025-12-15 22:51:08,744: Train batch 37800: loss: 0.90 grad norm: 1.23 time: 0.070
2025-12-15 22:51:21,605: Train batch 38000: loss: 0.84 grad norm: 2.12 time: 0.069
2025-12-15 22:51:21,606: Running test after training batch: 38000
2025-12-15 22:51:30,884: Val batch 38000: PER (avg): 0.1559 CTC Loss (avg): 0.8528 time: 9.278
2025-12-15 22:51:30,885: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:51:30,885: t15.2023.08.13 val PER: 0.1258
2025-12-15 22:51:30,885: t15.2023.08.18 val PER: 0.0997
2025-12-15 22:51:30,885: t15.2023.08.20 val PER: 0.0953
2025-12-15 22:51:30,885: t15.2023.08.25 val PER: 0.1054
2025-12-15 22:51:30,885: t15.2023.08.27 val PER: 0.1865
2025-12-15 22:51:30,885: t15.2023.09.01 val PER: 0.0682
2025-12-15 22:51:30,885: t15.2023.09.03 val PER: 0.1544
2025-12-15 22:51:30,885: t15.2023.09.24 val PER: 0.1056
2025-12-15 22:51:30,885: t15.2023.09.29 val PER: 0.1436
2025-12-15 22:51:30,885: t15.2023.10.01 val PER: 0.1770
2025-12-15 22:51:30,885: t15.2023.10.06 val PER: 0.1109
2025-12-15 22:51:30,885: t15.2023.10.08 val PER: 0.1976
2025-12-15 22:51:30,885: t15.2023.10.13 val PER: 0.2056
2025-12-15 22:51:30,885: t15.2023.10.15 val PER: 0.1628
2025-12-15 22:51:30,885: t15.2023.10.20 val PER: 0.1980
2025-12-15 22:51:30,885: t15.2023.10.22 val PER: 0.1269
2025-12-15 22:51:30,885: t15.2023.11.03 val PER: 0.1811
2025-12-15 22:51:30,886: t15.2023.11.04 val PER: 0.0546
2025-12-15 22:51:30,886: t15.2023.11.17 val PER: 0.0653
2025-12-15 22:51:30,886: t15.2023.11.19 val PER: 0.0798
2025-12-15 22:51:30,886: t15.2023.11.26 val PER: 0.1275
2025-12-15 22:51:30,886: t15.2023.12.03 val PER: 0.0998
2025-12-15 22:51:30,886: t15.2023.12.08 val PER: 0.1105
2025-12-15 22:51:30,886: t15.2023.12.10 val PER: 0.1143
2025-12-15 22:51:30,886: t15.2023.12.17 val PER: 0.1684
2025-12-15 22:51:30,886: t15.2023.12.29 val PER: 0.1441
2025-12-15 22:51:30,886: t15.2024.02.25 val PER: 0.1152
2025-12-15 22:51:30,886: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:51:30,886: t15.2024.03.08 val PER: 0.2233
2025-12-15 22:51:30,886: t15.2024.03.15 val PER: 0.2114
2025-12-15 22:51:30,886: t15.2024.03.17 val PER: 0.1457
2025-12-15 22:51:30,886: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:51:30,886: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:51:30,886: t15.2024.05.10 val PER: 0.1887
2025-12-15 22:51:30,886: t15.2024.06.14 val PER: 0.1987
2025-12-15 22:51:30,886: t15.2024.07.19 val PER: 0.2169
2025-12-15 22:51:30,886: t15.2024.07.21 val PER: 0.1007
2025-12-15 22:51:30,887: t15.2024.07.28 val PER: 0.1228
2025-12-15 22:51:30,887: t15.2025.01.10 val PER: 0.3140
2025-12-15 22:51:30,887: t15.2025.01.12 val PER: 0.1478
2025-12-15 22:51:30,887: t15.2025.03.14 val PER: 0.3476
2025-12-15 22:51:30,887: t15.2025.03.16 val PER: 0.2212
2025-12-15 22:51:30,887: t15.2025.03.30 val PER: 0.2782
2025-12-15 22:51:30,887: t15.2025.04.13 val PER: 0.2468
2025-12-15 22:51:30,887: New best test PER 0.1570 --> 0.1559
2025-12-15 22:51:30,887: Checkpointing model
2025-12-15 22:51:31,295: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:51:45,882: Train batch 38200: loss: 0.83 grad norm: 0.76 time: 0.038
2025-12-15 22:51:59,546: Train batch 38400: loss: 1.07 grad norm: 2.74 time: 0.044
2025-12-15 22:52:10,465: Train batch 38600: loss: 0.90 grad norm: 1.63 time: 0.038
2025-12-15 22:52:23,922: Train batch 38800: loss: 0.94 grad norm: 1.30 time: 0.068
2025-12-15 22:52:38,030: Train batch 39000: loss: 0.80 grad norm: 2.01 time: 0.055
2025-12-15 22:52:51,729: Train batch 39200: loss: 0.94 grad norm: 1.27 time: 0.065
2025-12-15 22:53:03,431: Train batch 39400: loss: 0.75 grad norm: 0.95 time: 0.089
2025-12-15 22:53:17,467: Train batch 39600: loss: 0.84 grad norm: 1.18 time: 0.074
2025-12-15 22:53:31,340: Train batch 39800: loss: 0.85 grad norm: 1.20 time: 0.056
2025-12-15 22:53:46,112: Train batch 40000: loss: 0.85 grad norm: 1.04 time: 0.054
2025-12-15 22:53:46,113: Running test after training batch: 40000
2025-12-15 22:53:55,274: Val batch 40000: PER (avg): 0.1529 CTC Loss (avg): 0.8773 time: 9.161
2025-12-15 22:53:55,274: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:53:55,274: t15.2023.08.13 val PER: 0.1185
2025-12-15 22:53:55,274: t15.2023.08.18 val PER: 0.1174
2025-12-15 22:53:55,274: t15.2023.08.20 val PER: 0.0898
2025-12-15 22:53:55,274: t15.2023.08.25 val PER: 0.1160
2025-12-15 22:53:55,274: t15.2023.08.27 val PER: 0.1913
2025-12-15 22:53:55,275: t15.2023.09.01 val PER: 0.0771
2025-12-15 22:53:55,275: t15.2023.09.03 val PER: 0.1508
2025-12-15 22:53:55,275: t15.2023.09.24 val PER: 0.1165
2025-12-15 22:53:55,275: t15.2023.09.29 val PER: 0.1366
2025-12-15 22:53:55,275: t15.2023.10.01 val PER: 0.1691
2025-12-15 22:53:55,275: t15.2023.10.06 val PER: 0.1055
2025-12-15 22:53:55,275: t15.2023.10.08 val PER: 0.2179
2025-12-15 22:53:55,275: t15.2023.10.13 val PER: 0.2033
2025-12-15 22:53:55,275: t15.2023.10.15 val PER: 0.1582
2025-12-15 22:53:55,275: t15.2023.10.20 val PER: 0.2248
2025-12-15 22:53:55,275: t15.2023.10.22 val PER: 0.1180
2025-12-15 22:53:55,275: t15.2023.11.03 val PER: 0.1839
2025-12-15 22:53:55,275: t15.2023.11.04 val PER: 0.0375
2025-12-15 22:53:55,275: t15.2023.11.17 val PER: 0.0638
2025-12-15 22:53:55,275: t15.2023.11.19 val PER: 0.0699
2025-12-15 22:53:55,275: t15.2023.11.26 val PER: 0.1268
2025-12-15 22:53:55,275: t15.2023.12.03 val PER: 0.1166
2025-12-15 22:53:55,275: t15.2023.12.08 val PER: 0.1019
2025-12-15 22:53:55,276: t15.2023.12.10 val PER: 0.1130
2025-12-15 22:53:55,276: t15.2023.12.17 val PER: 0.1590
2025-12-15 22:53:55,276: t15.2023.12.29 val PER: 0.1222
2025-12-15 22:53:55,276: t15.2024.02.25 val PER: 0.1138
2025-12-15 22:53:55,276: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:53:55,276: t15.2024.03.08 val PER: 0.2504
2025-12-15 22:53:55,276: t15.2024.03.15 val PER: 0.2039
2025-12-15 22:53:55,276: t15.2024.03.17 val PER: 0.1423
2025-12-15 22:53:55,276: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:53:55,276: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:53:55,276: t15.2024.05.10 val PER: 0.1560
2025-12-15 22:53:55,276: t15.2024.06.14 val PER: 0.2066
2025-12-15 22:53:55,276: t15.2024.07.19 val PER: 0.1931
2025-12-15 22:53:55,276: t15.2024.07.21 val PER: 0.1048
2025-12-15 22:53:55,276: t15.2024.07.28 val PER: 0.1228
2025-12-15 22:53:55,276: t15.2025.01.10 val PER: 0.2893
2025-12-15 22:53:55,276: t15.2025.01.12 val PER: 0.1509
2025-12-15 22:53:55,276: t15.2025.03.14 val PER: 0.3432
2025-12-15 22:53:55,276: t15.2025.03.16 val PER: 0.2029
2025-12-15 22:53:55,276: t15.2025.03.30 val PER: 0.2736
2025-12-15 22:53:55,277: t15.2025.04.13 val PER: 0.2311
2025-12-15 22:53:55,277: New best test PER 0.1559 --> 0.1529
2025-12-15 22:53:55,277: Checkpointing model
2025-12-15 22:53:55,703: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:54:09,047: Train batch 40200: loss: 0.94 grad norm: 0.53 time: 0.058
2025-12-15 22:54:23,126: Train batch 40400: loss: 0.87 grad norm: 0.64 time: 0.056
2025-12-15 22:54:37,716: Train batch 40600: loss: 0.90 grad norm: 1.74 time: 0.056
2025-12-15 22:54:52,423: Train batch 40800: loss: 0.91 grad norm: 1.78 time: 0.062
2025-12-15 22:55:07,278: Train batch 41000: loss: 1.05 grad norm: 0.63 time: 0.072
2025-12-15 22:55:21,356: Train batch 41200: loss: 0.77 grad norm: 1.03 time: 0.069
2025-12-15 22:55:35,278: Train batch 41400: loss: 0.95 grad norm: 2.18 time: 0.076
2025-12-15 22:55:48,944: Train batch 41600: loss: 0.80 grad norm: 0.82 time: 0.073
2025-12-15 22:56:03,143: Train batch 41800: loss: 0.82 grad norm: 1.74 time: 0.065
2025-12-15 22:56:17,751: Train batch 42000: loss: 0.89 grad norm: 1.36 time: 0.058
2025-12-15 22:56:17,752: Running test after training batch: 42000
2025-12-15 22:56:27,265: Val batch 42000: PER (avg): 0.1516 CTC Loss (avg): 0.9156 time: 9.513
2025-12-15 22:56:27,265: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:56:27,265: t15.2023.08.13 val PER: 0.1154
2025-12-15 22:56:27,265: t15.2023.08.18 val PER: 0.1199
2025-12-15 22:56:27,265: t15.2023.08.20 val PER: 0.0969
2025-12-15 22:56:27,265: t15.2023.08.25 val PER: 0.0964
2025-12-15 22:56:27,265: t15.2023.08.27 val PER: 0.1656
2025-12-15 22:56:27,265: t15.2023.09.01 val PER: 0.0690
2025-12-15 22:56:27,265: t15.2023.09.03 val PER: 0.1663
2025-12-15 22:56:27,265: t15.2023.09.24 val PER: 0.1141
2025-12-15 22:56:27,265: t15.2023.09.29 val PER: 0.1461
2025-12-15 22:56:27,266: t15.2023.10.01 val PER: 0.1783
2025-12-15 22:56:27,266: t15.2023.10.06 val PER: 0.1184
2025-12-15 22:56:27,266: t15.2023.10.08 val PER: 0.1867
2025-12-15 22:56:27,266: t15.2023.10.13 val PER: 0.2110
2025-12-15 22:56:27,266: t15.2023.10.15 val PER: 0.1602
2025-12-15 22:56:27,266: t15.2023.10.20 val PER: 0.2282
2025-12-15 22:56:27,266: t15.2023.10.22 val PER: 0.1136
2025-12-15 22:56:27,266: t15.2023.11.03 val PER: 0.1811
2025-12-15 22:56:27,266: t15.2023.11.04 val PER: 0.0444
2025-12-15 22:56:27,266: t15.2023.11.17 val PER: 0.0482
2025-12-15 22:56:27,266: t15.2023.11.19 val PER: 0.0758
2025-12-15 22:56:27,266: t15.2023.11.26 val PER: 0.1254
2025-12-15 22:56:27,266: t15.2023.12.03 val PER: 0.1019
2025-12-15 22:56:27,266: t15.2023.12.08 val PER: 0.1119
2025-12-15 22:56:27,266: t15.2023.12.10 val PER: 0.1038
2025-12-15 22:56:27,266: t15.2023.12.17 val PER: 0.1570
2025-12-15 22:56:27,266: t15.2023.12.29 val PER: 0.1242
2025-12-15 22:56:27,266: t15.2024.02.25 val PER: 0.1096
2025-12-15 22:56:27,267: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:56:27,267: t15.2024.03.08 val PER: 0.2361
2025-12-15 22:56:27,267: t15.2024.03.15 val PER: 0.2139
2025-12-15 22:56:27,267: t15.2024.03.17 val PER: 0.1276
2025-12-15 22:56:27,267: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:56:27,267: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:56:27,267: t15.2024.05.10 val PER: 0.1575
2025-12-15 22:56:27,267: t15.2024.06.14 val PER: 0.1877
2025-12-15 22:56:27,267: t15.2024.07.19 val PER: 0.1938
2025-12-15 22:56:27,267: t15.2024.07.21 val PER: 0.1028
2025-12-15 22:56:27,267: t15.2024.07.28 val PER: 0.1221
2025-12-15 22:56:27,267: t15.2025.01.10 val PER: 0.2906
2025-12-15 22:56:27,267: t15.2025.01.12 val PER: 0.1478
2025-12-15 22:56:27,267: t15.2025.03.14 val PER: 0.3476
2025-12-15 22:56:27,267: t15.2025.03.16 val PER: 0.2081
2025-12-15 22:56:27,267: t15.2025.03.30 val PER: 0.2517
2025-12-15 22:56:27,267: t15.2025.04.13 val PER: 0.2340
2025-12-15 22:56:27,267: New best test PER 0.1529 --> 0.1516
2025-12-15 22:56:27,267: Checkpointing model
2025-12-15 22:56:27,682: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 22:56:42,244: Train batch 42200: loss: 0.87 grad norm: 1.31 time: 0.068
2025-12-15 22:56:56,589: Train batch 42400: loss: 1.00 grad norm: 1.57 time: 0.061
2025-12-15 22:57:10,142: Train batch 42600: loss: 0.78 grad norm: 1.21 time: 0.068
2025-12-15 22:57:24,424: Train batch 42800: loss: 0.84 grad norm: 1.37 time: 0.055
2025-12-15 22:57:38,148: Train batch 43000: loss: 0.84 grad norm: 0.97 time: 0.057
2025-12-15 22:57:52,853: Train batch 43200: loss: 0.97 grad norm: 2.07 time: 0.081
2025-12-15 22:58:07,307: Train batch 43400: loss: 0.93 grad norm: 1.68 time: 0.085
2025-12-15 22:58:21,688: Train batch 43600: loss: 0.81 grad norm: 0.94 time: 0.067
2025-12-15 22:58:36,011: Train batch 43800: loss: 0.91 grad norm: 1.29 time: 0.058
2025-12-15 22:58:50,047: Train batch 44000: loss: 0.82 grad norm: 1.59 time: 0.057
2025-12-15 22:58:50,047: Running test after training batch: 44000
2025-12-15 22:58:59,829: Val batch 44000: PER (avg): 0.1533 CTC Loss (avg): 0.9436 time: 9.781
2025-12-15 22:58:59,829: t15.2023.08.11 val PER: 1.0000
2025-12-15 22:58:59,829: t15.2023.08.13 val PER: 0.1164
2025-12-15 22:58:59,829: t15.2023.08.18 val PER: 0.1065
2025-12-15 22:58:59,829: t15.2023.08.20 val PER: 0.1009
2025-12-15 22:58:59,829: t15.2023.08.25 val PER: 0.1145
2025-12-15 22:58:59,829: t15.2023.08.27 val PER: 0.1785
2025-12-15 22:58:59,829: t15.2023.09.01 val PER: 0.0755
2025-12-15 22:58:59,829: t15.2023.09.03 val PER: 0.1532
2025-12-15 22:58:59,829: t15.2023.09.24 val PER: 0.1177
2025-12-15 22:58:59,829: t15.2023.09.29 val PER: 0.1372
2025-12-15 22:58:59,829: t15.2023.10.01 val PER: 0.1737
2025-12-15 22:58:59,829: t15.2023.10.06 val PER: 0.0990
2025-12-15 22:58:59,830: t15.2023.10.08 val PER: 0.2070
2025-12-15 22:58:59,830: t15.2023.10.13 val PER: 0.2040
2025-12-15 22:58:59,830: t15.2023.10.15 val PER: 0.1562
2025-12-15 22:58:59,830: t15.2023.10.20 val PER: 0.2047
2025-12-15 22:58:59,830: t15.2023.10.22 val PER: 0.1336
2025-12-15 22:58:59,830: t15.2023.11.03 val PER: 0.1818
2025-12-15 22:58:59,830: t15.2023.11.04 val PER: 0.0580
2025-12-15 22:58:59,830: t15.2023.11.17 val PER: 0.0591
2025-12-15 22:58:59,830: t15.2023.11.19 val PER: 0.0639
2025-12-15 22:58:59,830: t15.2023.11.26 val PER: 0.1471
2025-12-15 22:58:59,830: t15.2023.12.03 val PER: 0.1071
2025-12-15 22:58:59,830: t15.2023.12.08 val PER: 0.1152
2025-12-15 22:58:59,830: t15.2023.12.10 val PER: 0.0972
2025-12-15 22:58:59,830: t15.2023.12.17 val PER: 0.1861
2025-12-15 22:58:59,830: t15.2023.12.29 val PER: 0.1297
2025-12-15 22:58:59,830: t15.2024.02.25 val PER: 0.1081
2025-12-15 22:58:59,830: t15.2024.03.03 val PER: 1.0000
2025-12-15 22:58:59,830: t15.2024.03.08 val PER: 0.2404
2025-12-15 22:58:59,830: t15.2024.03.15 val PER: 0.2101
2025-12-15 22:58:59,831: t15.2024.03.17 val PER: 0.1339
2025-12-15 22:58:59,831: t15.2024.04.25 val PER: 1.0000
2025-12-15 22:58:59,831: t15.2024.04.28 val PER: 1.0000
2025-12-15 22:58:59,831: t15.2024.05.10 val PER: 0.1634
2025-12-15 22:58:59,831: t15.2024.06.14 val PER: 0.1956
2025-12-15 22:58:59,831: t15.2024.07.19 val PER: 0.1971
2025-12-15 22:58:59,831: t15.2024.07.21 val PER: 0.0993
2025-12-15 22:58:59,831: t15.2024.07.28 val PER: 0.1346
2025-12-15 22:58:59,831: t15.2025.01.10 val PER: 0.2755
2025-12-15 22:58:59,831: t15.2025.01.12 val PER: 0.1370
2025-12-15 22:58:59,831: t15.2025.03.14 val PER: 0.3225
2025-12-15 22:58:59,831: t15.2025.03.16 val PER: 0.2003
2025-12-15 22:58:59,831: t15.2025.03.30 val PER: 0.2793
2025-12-15 22:58:59,831: t15.2025.04.13 val PER: 0.2397
2025-12-15 22:59:10,051: Train batch 44200: loss: 0.92 grad norm: 1.50 time: 0.056
2025-12-15 22:59:24,324: Train batch 44400: loss: 0.90 grad norm: 1.00 time: 0.066
2025-12-15 22:59:37,251: Train batch 44600: loss: 0.79 grad norm: 1.18 time: 0.054
2025-12-15 22:59:50,992: Train batch 44800: loss: 0.91 grad norm: 1.21 time: 0.069
2025-12-15 23:00:04,088: Train batch 45000: loss: 0.87 grad norm: 1.94 time: 0.066
2025-12-15 23:00:18,115: Train batch 45200: loss: 0.79 grad norm: 2.51 time: 0.042
2025-12-15 23:00:30,075: Train batch 45400: loss: 0.82 grad norm: 2.19 time: 0.049
2025-12-15 23:00:43,995: Train batch 45600: loss: 0.98 grad norm: 0.89 time: 0.057
2025-12-15 23:00:57,689: Train batch 45800: loss: 0.85 grad norm: 1.29 time: 0.070
2025-12-15 23:01:09,588: Train batch 46000: loss: 1.09 grad norm: 2.20 time: 0.083
2025-12-15 23:01:09,588: Running test after training batch: 46000
2025-12-15 23:01:19,366: Val batch 46000: PER (avg): 0.1529 CTC Loss (avg): 0.9253 time: 9.777
2025-12-15 23:01:19,366: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:01:19,367: t15.2023.08.13 val PER: 0.1195
2025-12-15 23:01:19,367: t15.2023.08.18 val PER: 0.1165
2025-12-15 23:01:19,367: t15.2023.08.20 val PER: 0.1048
2025-12-15 23:01:19,367: t15.2023.08.25 val PER: 0.1190
2025-12-15 23:01:19,367: t15.2023.08.27 val PER: 0.1688
2025-12-15 23:01:19,367: t15.2023.09.01 val PER: 0.0722
2025-12-15 23:01:19,367: t15.2023.09.03 val PER: 0.1615
2025-12-15 23:01:19,367: t15.2023.09.24 val PER: 0.1056
2025-12-15 23:01:19,367: t15.2023.09.29 val PER: 0.1500
2025-12-15 23:01:19,367: t15.2023.10.01 val PER: 0.1909
2025-12-15 23:01:19,367: t15.2023.10.06 val PER: 0.1119
2025-12-15 23:01:19,367: t15.2023.10.08 val PER: 0.2097
2025-12-15 23:01:19,367: t15.2023.10.13 val PER: 0.2002
2025-12-15 23:01:19,367: t15.2023.10.15 val PER: 0.1575
2025-12-15 23:01:19,367: t15.2023.10.20 val PER: 0.2181
2025-12-15 23:01:19,367: t15.2023.10.22 val PER: 0.1225
2025-12-15 23:01:19,367: t15.2023.11.03 val PER: 0.1730
2025-12-15 23:01:19,368: t15.2023.11.04 val PER: 0.0341
2025-12-15 23:01:19,368: t15.2023.11.17 val PER: 0.0451
2025-12-15 23:01:19,368: t15.2023.11.19 val PER: 0.0858
2025-12-15 23:01:19,368: t15.2023.11.26 val PER: 0.1167
2025-12-15 23:01:19,368: t15.2023.12.03 val PER: 0.1008
2025-12-15 23:01:19,368: t15.2023.12.08 val PER: 0.1218
2025-12-15 23:01:19,368: t15.2023.12.10 val PER: 0.0959
2025-12-15 23:01:19,368: t15.2023.12.17 val PER: 0.1622
2025-12-15 23:01:19,368: t15.2023.12.29 val PER: 0.1235
2025-12-15 23:01:19,368: t15.2024.02.25 val PER: 0.1081
2025-12-15 23:01:19,368: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:01:19,368: t15.2024.03.08 val PER: 0.2290
2025-12-15 23:01:19,368: t15.2024.03.15 val PER: 0.2133
2025-12-15 23:01:19,368: t15.2024.03.17 val PER: 0.1318
2025-12-15 23:01:19,368: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:01:19,368: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:01:19,368: t15.2024.05.10 val PER: 0.1724
2025-12-15 23:01:19,368: t15.2024.06.14 val PER: 0.1909
2025-12-15 23:01:19,368: t15.2024.07.19 val PER: 0.1925
2025-12-15 23:01:19,369: t15.2024.07.21 val PER: 0.1117
2025-12-15 23:01:19,369: t15.2024.07.28 val PER: 0.1169
2025-12-15 23:01:19,369: t15.2025.01.10 val PER: 0.3099
2025-12-15 23:01:19,369: t15.2025.01.12 val PER: 0.1447
2025-12-15 23:01:19,369: t15.2025.03.14 val PER: 0.3447
2025-12-15 23:01:19,369: t15.2025.03.16 val PER: 0.1937
2025-12-15 23:01:19,369: t15.2025.03.30 val PER: 0.2747
2025-12-15 23:01:19,369: t15.2025.04.13 val PER: 0.2225
2025-12-15 23:01:31,780: Train batch 46200: loss: 0.93 grad norm: 2.29 time: 0.063
2025-12-15 23:01:45,191: Train batch 46400: loss: 0.93 grad norm: 2.48 time: 0.061
2025-12-15 23:01:58,574: Train batch 46600: loss: 0.84 grad norm: 0.88 time: 0.057
2025-12-15 23:02:13,288: Train batch 46800: loss: 1.03 grad norm: 0.99 time: 0.069
2025-12-15 23:02:27,477: Train batch 47000: loss: 0.88 grad norm: 1.26 time: 0.076
2025-12-15 23:02:41,124: Train batch 47200: loss: 0.91 grad norm: 1.18 time: 0.085
2025-12-15 23:02:55,367: Train batch 47400: loss: 0.80 grad norm: 0.69 time: 0.079
2025-12-15 23:03:07,145: Train batch 47600: loss: 0.91 grad norm: 1.78 time: 0.069
2025-12-15 23:03:18,329: Train batch 47800: loss: 0.96 grad norm: 1.89 time: 0.035
2025-12-15 23:03:32,147: Train batch 48000: loss: 0.98 grad norm: 0.96 time: 0.071
2025-12-15 23:03:32,148: Running test after training batch: 48000
2025-12-15 23:03:41,290: Val batch 48000: PER (avg): 0.1488 CTC Loss (avg): 0.9091 time: 9.142
2025-12-15 23:03:41,291: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:03:41,291: t15.2023.08.13 val PER: 0.1102
2025-12-15 23:03:41,291: t15.2023.08.18 val PER: 0.1215
2025-12-15 23:03:41,291: t15.2023.08.20 val PER: 0.0929
2025-12-15 23:03:41,291: t15.2023.08.25 val PER: 0.1175
2025-12-15 23:03:41,291: t15.2023.08.27 val PER: 0.1672
2025-12-15 23:03:41,291: t15.2023.09.01 val PER: 0.0690
2025-12-15 23:03:41,291: t15.2023.09.03 val PER: 0.1354
2025-12-15 23:03:41,291: t15.2023.09.24 val PER: 0.1117
2025-12-15 23:03:41,291: t15.2023.09.29 val PER: 0.1321
2025-12-15 23:03:41,291: t15.2023.10.01 val PER: 0.1843
2025-12-15 23:03:41,291: t15.2023.10.06 val PER: 0.0969
2025-12-15 23:03:41,291: t15.2023.10.08 val PER: 0.1867
2025-12-15 23:03:41,291: t15.2023.10.13 val PER: 0.1955
2025-12-15 23:03:41,291: t15.2023.10.15 val PER: 0.1543
2025-12-15 23:03:41,291: t15.2023.10.20 val PER: 0.2013
2025-12-15 23:03:41,292: t15.2023.10.22 val PER: 0.1325
2025-12-15 23:03:41,292: t15.2023.11.03 val PER: 0.1757
2025-12-15 23:03:41,292: t15.2023.11.04 val PER: 0.0546
2025-12-15 23:03:41,292: t15.2023.11.17 val PER: 0.0451
2025-12-15 23:03:41,292: t15.2023.11.19 val PER: 0.0739
2025-12-15 23:03:41,292: t15.2023.11.26 val PER: 0.1123
2025-12-15 23:03:41,292: t15.2023.12.03 val PER: 0.1029
2025-12-15 23:03:41,292: t15.2023.12.08 val PER: 0.1178
2025-12-15 23:03:41,292: t15.2023.12.10 val PER: 0.1104
2025-12-15 23:03:41,292: t15.2023.12.17 val PER: 0.1798
2025-12-15 23:03:41,292: t15.2023.12.29 val PER: 0.1270
2025-12-15 23:03:41,292: t15.2024.02.25 val PER: 0.1222
2025-12-15 23:03:41,292: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:03:41,292: t15.2024.03.08 val PER: 0.2390
2025-12-15 23:03:41,292: t15.2024.03.15 val PER: 0.1976
2025-12-15 23:03:41,292: t15.2024.03.17 val PER: 0.1276
2025-12-15 23:03:41,292: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:03:41,292: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:03:41,292: t15.2024.05.10 val PER: 0.1679
2025-12-15 23:03:41,293: t15.2024.06.14 val PER: 0.1940
2025-12-15 23:03:41,293: t15.2024.07.19 val PER: 0.1945
2025-12-15 23:03:41,293: t15.2024.07.21 val PER: 0.1028
2025-12-15 23:03:41,293: t15.2024.07.28 val PER: 0.1265
2025-12-15 23:03:41,293: t15.2025.01.10 val PER: 0.2948
2025-12-15 23:03:41,293: t15.2025.01.12 val PER: 0.1355
2025-12-15 23:03:41,293: t15.2025.03.14 val PER: 0.3151
2025-12-15 23:03:41,293: t15.2025.03.16 val PER: 0.1898
2025-12-15 23:03:41,293: t15.2025.03.30 val PER: 0.2494
2025-12-15 23:03:41,293: t15.2025.04.13 val PER: 0.2240
2025-12-15 23:03:41,293: New best test PER 0.1516 --> 0.1488
2025-12-15 23:03:41,293: Checkpointing model
2025-12-15 23:03:41,744: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:03:55,289: Train batch 48200: loss: 0.80 grad norm: 1.19 time: 0.069
2025-12-15 23:04:09,801: Train batch 48400: loss: 0.85 grad norm: 0.39 time: 0.084
2025-12-15 23:04:24,419: Train batch 48600: loss: 0.90 grad norm: 1.41 time: 0.052
2025-12-15 23:04:39,123: Train batch 48800: loss: 0.82 grad norm: 0.67 time: 0.068
2025-12-15 23:04:53,789: Train batch 49000: loss: 0.84 grad norm: 1.16 time: 0.083
2025-12-15 23:05:08,477: Train batch 49200: loss: 0.84 grad norm: 0.85 time: 0.071
2025-12-15 23:05:23,075: Train batch 49400: loss: 0.89 grad norm: 1.04 time: 0.064
2025-12-15 23:05:37,439: Train batch 49600: loss: 0.91 grad norm: 1.08 time: 0.071
2025-12-15 23:05:50,692: Train batch 49800: loss: 0.85 grad norm: 0.97 time: 0.083
2025-12-15 23:06:04,944: Train batch 50000: loss: 0.82 grad norm: 3.95 time: 0.067
2025-12-15 23:06:04,945: Running test after training batch: 50000
2025-12-15 23:06:14,105: Val batch 50000: PER (avg): 0.1488 CTC Loss (avg): 0.9441 time: 9.160
2025-12-15 23:06:14,106: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:06:14,106: t15.2023.08.13 val PER: 0.1112
2025-12-15 23:06:14,106: t15.2023.08.18 val PER: 0.1224
2025-12-15 23:06:14,106: t15.2023.08.20 val PER: 0.0993
2025-12-15 23:06:14,106: t15.2023.08.25 val PER: 0.1054
2025-12-15 23:06:14,106: t15.2023.08.27 val PER: 0.1801
2025-12-15 23:06:14,106: t15.2023.09.01 val PER: 0.0609
2025-12-15 23:06:14,106: t15.2023.09.03 val PER: 0.1413
2025-12-15 23:06:14,106: t15.2023.09.24 val PER: 0.1165
2025-12-15 23:06:14,106: t15.2023.09.29 val PER: 0.1391
2025-12-15 23:06:14,106: t15.2023.10.01 val PER: 0.1770
2025-12-15 23:06:14,106: t15.2023.10.06 val PER: 0.0990
2025-12-15 23:06:14,106: t15.2023.10.08 val PER: 0.2030
2025-12-15 23:06:14,106: t15.2023.10.13 val PER: 0.1939
2025-12-15 23:06:14,106: t15.2023.10.15 val PER: 0.1536
2025-12-15 23:06:14,106: t15.2023.10.20 val PER: 0.2416
2025-12-15 23:06:14,106: t15.2023.10.22 val PER: 0.1192
2025-12-15 23:06:14,107: t15.2023.11.03 val PER: 0.1771
2025-12-15 23:06:14,107: t15.2023.11.04 val PER: 0.0273
2025-12-15 23:06:14,107: t15.2023.11.17 val PER: 0.0638
2025-12-15 23:06:14,107: t15.2023.11.19 val PER: 0.0659
2025-12-15 23:06:14,107: t15.2023.11.26 val PER: 0.1225
2025-12-15 23:06:14,107: t15.2023.12.03 val PER: 0.0987
2025-12-15 23:06:14,107: t15.2023.12.08 val PER: 0.1065
2025-12-15 23:06:14,107: t15.2023.12.10 val PER: 0.0959
2025-12-15 23:06:14,107: t15.2023.12.17 val PER: 0.1601
2025-12-15 23:06:14,107: t15.2023.12.29 val PER: 0.1242
2025-12-15 23:06:14,107: t15.2024.02.25 val PER: 0.1194
2025-12-15 23:06:14,107: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:06:14,107: t15.2024.03.08 val PER: 0.2404
2025-12-15 23:06:14,107: t15.2024.03.15 val PER: 0.2001
2025-12-15 23:06:14,107: t15.2024.03.17 val PER: 0.1290
2025-12-15 23:06:14,107: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:06:14,107: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:06:14,107: t15.2024.05.10 val PER: 0.1798
2025-12-15 23:06:14,107: t15.2024.06.14 val PER: 0.1782
2025-12-15 23:06:14,108: t15.2024.07.19 val PER: 0.1951
2025-12-15 23:06:14,108: t15.2024.07.21 val PER: 0.1007
2025-12-15 23:06:14,108: t15.2024.07.28 val PER: 0.1096
2025-12-15 23:06:14,108: t15.2025.01.10 val PER: 0.3044
2025-12-15 23:06:14,108: t15.2025.01.12 val PER: 0.1401
2025-12-15 23:06:14,108: t15.2025.03.14 val PER: 0.3240
2025-12-15 23:06:14,108: t15.2025.03.16 val PER: 0.2068
2025-12-15 23:06:14,108: t15.2025.03.30 val PER: 0.2701
2025-12-15 23:06:14,108: t15.2025.04.13 val PER: 0.2111
2025-12-15 23:06:27,641: Train batch 50200: loss: 0.87 grad norm: 0.88 time: 0.053
2025-12-15 23:06:42,432: Train batch 50400: loss: 0.96 grad norm: 1.42 time: 0.070
2025-12-15 23:06:57,045: Train batch 50600: loss: 0.98 grad norm: 2.08 time: 0.067
2025-12-15 23:07:11,367: Train batch 50800: loss: 0.99 grad norm: 1.49 time: 0.062
2025-12-15 23:07:25,978: Train batch 51000: loss: 0.89 grad norm: 3.82 time: 0.072
2025-12-15 23:07:40,559: Train batch 51200: loss: 0.87 grad norm: 0.69 time: 0.079
2025-12-15 23:07:54,922: Train batch 51400: loss: 0.92 grad norm: 0.95 time: 0.066
2025-12-15 23:08:08,432: Train batch 51600: loss: 0.92 grad norm: 1.24 time: 0.074
2025-12-15 23:08:23,105: Train batch 51800: loss: 0.77 grad norm: 1.14 time: 0.054
2025-12-15 23:08:35,400: Train batch 52000: loss: 0.90 grad norm: 1.91 time: 0.086
2025-12-15 23:08:35,401: Running test after training batch: 52000
2025-12-15 23:08:44,551: Val batch 52000: PER (avg): 0.1444 CTC Loss (avg): 0.9528 time: 9.151
2025-12-15 23:08:44,552: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:08:44,552: t15.2023.08.13 val PER: 0.1008
2025-12-15 23:08:44,552: t15.2023.08.18 val PER: 0.1023
2025-12-15 23:08:44,552: t15.2023.08.20 val PER: 0.0905
2025-12-15 23:08:44,552: t15.2023.08.25 val PER: 0.0994
2025-12-15 23:08:44,552: t15.2023.08.27 val PER: 0.1801
2025-12-15 23:08:44,552: t15.2023.09.01 val PER: 0.0674
2025-12-15 23:08:44,552: t15.2023.09.03 val PER: 0.1401
2025-12-15 23:08:44,552: t15.2023.09.24 val PER: 0.1080
2025-12-15 23:08:44,552: t15.2023.09.29 val PER: 0.1327
2025-12-15 23:08:44,552: t15.2023.10.01 val PER: 0.1645
2025-12-15 23:08:44,552: t15.2023.10.06 val PER: 0.0926
2025-12-15 23:08:44,552: t15.2023.10.08 val PER: 0.2070
2025-12-15 23:08:44,552: t15.2023.10.13 val PER: 0.1916
2025-12-15 23:08:44,553: t15.2023.10.15 val PER: 0.1622
2025-12-15 23:08:44,553: t15.2023.10.20 val PER: 0.1946
2025-12-15 23:08:44,553: t15.2023.10.22 val PER: 0.1203
2025-12-15 23:08:44,553: t15.2023.11.03 val PER: 0.1798
2025-12-15 23:08:44,553: t15.2023.11.04 val PER: 0.0307
2025-12-15 23:08:44,553: t15.2023.11.17 val PER: 0.0638
2025-12-15 23:08:44,553: t15.2023.11.19 val PER: 0.0798
2025-12-15 23:08:44,553: t15.2023.11.26 val PER: 0.1232
2025-12-15 23:08:44,553: t15.2023.12.03 val PER: 0.0977
2025-12-15 23:08:44,553: t15.2023.12.08 val PER: 0.0965
2025-12-15 23:08:44,553: t15.2023.12.10 val PER: 0.0933
2025-12-15 23:08:44,553: t15.2023.12.17 val PER: 0.1341
2025-12-15 23:08:44,553: t15.2023.12.29 val PER: 0.1290
2025-12-15 23:08:44,553: t15.2024.02.25 val PER: 0.1053
2025-12-15 23:08:44,553: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:08:44,553: t15.2024.03.08 val PER: 0.2319
2025-12-15 23:08:44,553: t15.2024.03.15 val PER: 0.2051
2025-12-15 23:08:44,553: t15.2024.03.17 val PER: 0.1123
2025-12-15 23:08:44,553: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:08:44,554: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:08:44,554: t15.2024.05.10 val PER: 0.1382
2025-12-15 23:08:44,554: t15.2024.06.14 val PER: 0.1767
2025-12-15 23:08:44,554: t15.2024.07.19 val PER: 0.1872
2025-12-15 23:08:44,554: t15.2024.07.21 val PER: 0.1014
2025-12-15 23:08:44,554: t15.2024.07.28 val PER: 0.1250
2025-12-15 23:08:44,554: t15.2025.01.10 val PER: 0.2782
2025-12-15 23:08:44,554: t15.2025.01.12 val PER: 0.1401
2025-12-15 23:08:44,554: t15.2025.03.14 val PER: 0.3195
2025-12-15 23:08:44,554: t15.2025.03.16 val PER: 0.2042
2025-12-15 23:08:44,554: t15.2025.03.30 val PER: 0.2609
2025-12-15 23:08:44,554: t15.2025.04.13 val PER: 0.2211
2025-12-15 23:08:44,554: New best test PER 0.1488 --> 0.1444
2025-12-15 23:08:44,554: Checkpointing model
2025-12-15 23:08:44,985: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:08:57,560: Train batch 52200: loss: 0.79 grad norm: 0.42 time: 0.091
2025-12-15 23:09:11,672: Train batch 52400: loss: 0.84 grad norm: 0.37 time: 0.056
2025-12-15 23:09:26,276: Train batch 52600: loss: 0.76 grad norm: 0.39 time: 0.067
2025-12-15 23:09:40,813: Train batch 52800: loss: 0.79 grad norm: 1.42 time: 0.052
2025-12-15 23:09:55,192: Train batch 53000: loss: 0.94 grad norm: 1.10 time: 0.056
2025-12-15 23:10:09,202: Train batch 53200: loss: 0.80 grad norm: 0.64 time: 0.059
2025-12-15 23:10:23,465: Train batch 53400: loss: 1.04 grad norm: 1.43 time: 0.057
2025-12-15 23:10:37,545: Train batch 53600: loss: 0.84 grad norm: 2.19 time: 0.067
2025-12-15 23:10:50,538: Train batch 53800: loss: 0.91 grad norm: 1.50 time: 0.070
2025-12-15 23:11:04,749: Train batch 54000: loss: 0.89 grad norm: 1.39 time: 0.072
2025-12-15 23:11:04,750: Running test after training batch: 54000
2025-12-15 23:11:13,924: Val batch 54000: PER (avg): 0.1458 CTC Loss (avg): 0.9499 time: 9.175
2025-12-15 23:11:13,925: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:11:13,925: t15.2023.08.13 val PER: 0.1154
2025-12-15 23:11:13,925: t15.2023.08.18 val PER: 0.1065
2025-12-15 23:11:13,925: t15.2023.08.20 val PER: 0.1048
2025-12-15 23:11:13,925: t15.2023.08.25 val PER: 0.1175
2025-12-15 23:11:13,925: t15.2023.08.27 val PER: 0.1608
2025-12-15 23:11:13,925: t15.2023.09.01 val PER: 0.0706
2025-12-15 23:11:13,925: t15.2023.09.03 val PER: 0.1378
2025-12-15 23:11:13,925: t15.2023.09.24 val PER: 0.1214
2025-12-15 23:11:13,925: t15.2023.09.29 val PER: 0.1340
2025-12-15 23:11:13,925: t15.2023.10.01 val PER: 0.1770
2025-12-15 23:11:13,925: t15.2023.10.06 val PER: 0.0861
2025-12-15 23:11:13,925: t15.2023.10.08 val PER: 0.2003
2025-12-15 23:11:13,925: t15.2023.10.13 val PER: 0.2002
2025-12-15 23:11:13,926: t15.2023.10.15 val PER: 0.1503
2025-12-15 23:11:13,926: t15.2023.10.20 val PER: 0.1980
2025-12-15 23:11:13,926: t15.2023.10.22 val PER: 0.1147
2025-12-15 23:11:13,926: t15.2023.11.03 val PER: 0.1601
2025-12-15 23:11:13,926: t15.2023.11.04 val PER: 0.0375
2025-12-15 23:11:13,926: t15.2023.11.17 val PER: 0.0715
2025-12-15 23:11:13,926: t15.2023.11.19 val PER: 0.0858
2025-12-15 23:11:13,926: t15.2023.11.26 val PER: 0.1290
2025-12-15 23:11:13,926: t15.2023.12.03 val PER: 0.1029
2025-12-15 23:11:13,926: t15.2023.12.08 val PER: 0.1012
2025-12-15 23:11:13,927: t15.2023.12.10 val PER: 0.0894
2025-12-15 23:11:13,927: t15.2023.12.17 val PER: 0.1466
2025-12-15 23:11:13,927: t15.2023.12.29 val PER: 0.1290
2025-12-15 23:11:13,928: t15.2024.02.25 val PER: 0.0941
2025-12-15 23:11:13,928: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:11:13,929: t15.2024.03.08 val PER: 0.2233
2025-12-15 23:11:13,929: t15.2024.03.15 val PER: 0.1982
2025-12-15 23:11:13,929: t15.2024.03.17 val PER: 0.1158
2025-12-15 23:11:13,929: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:11:13,930: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:11:13,930: t15.2024.05.10 val PER: 0.1664
2025-12-15 23:11:13,930: t15.2024.06.14 val PER: 0.1562
2025-12-15 23:11:13,930: t15.2024.07.19 val PER: 0.1931
2025-12-15 23:11:13,930: t15.2024.07.21 val PER: 0.0979
2025-12-15 23:11:13,930: t15.2024.07.28 val PER: 0.1191
2025-12-15 23:11:13,930: t15.2025.01.10 val PER: 0.2906
2025-12-15 23:11:13,930: t15.2025.01.12 val PER: 0.1401
2025-12-15 23:11:13,930: t15.2025.03.14 val PER: 0.3121
2025-12-15 23:11:13,930: t15.2025.03.16 val PER: 0.2029
2025-12-15 23:11:13,931: t15.2025.03.30 val PER: 0.2713
2025-12-15 23:11:13,931: t15.2025.04.13 val PER: 0.2297
2025-12-15 23:11:27,869: Train batch 54200: loss: 0.78 grad norm: 0.43 time: 0.081
2025-12-15 23:11:42,130: Train batch 54400: loss: 0.88 grad norm: 1.12 time: 0.045
2025-12-15 23:11:56,391: Train batch 54600: loss: 0.73 grad norm: 1.31 time: 0.080
2025-12-15 23:12:10,562: Train batch 54800: loss: 1.06 grad norm: 1.44 time: 0.067
2025-12-15 23:12:24,011: Train batch 55000: loss: 0.85 grad norm: 0.49 time: 0.056
2025-12-15 23:12:38,104: Train batch 55200: loss: 0.76 grad norm: 0.86 time: 0.066
2025-12-15 23:12:51,957: Train batch 55400: loss: 0.84 grad norm: 1.70 time: 0.055
2025-12-15 23:13:04,038: Train batch 55600: loss: 0.90 grad norm: 0.61 time: 0.070
2025-12-15 23:13:18,541: Train batch 55800: loss: 0.86 grad norm: 0.15 time: 0.066
2025-12-15 23:13:32,973: Train batch 56000: loss: 0.88 grad norm: 1.41 time: 0.053
2025-12-15 23:13:32,973: Running test after training batch: 56000
2025-12-15 23:13:42,225: Val batch 56000: PER (avg): 0.1456 CTC Loss (avg): 0.9619 time: 9.252
2025-12-15 23:13:42,225: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:13:42,226: t15.2023.08.13 val PER: 0.1071
2025-12-15 23:13:42,226: t15.2023.08.18 val PER: 0.1039
2025-12-15 23:13:42,226: t15.2023.08.20 val PER: 0.0882
2025-12-15 23:13:42,226: t15.2023.08.25 val PER: 0.0979
2025-12-15 23:13:42,226: t15.2023.08.27 val PER: 0.1752
2025-12-15 23:13:42,226: t15.2023.09.01 val PER: 0.0666
2025-12-15 23:13:42,226: t15.2023.09.03 val PER: 0.1496
2025-12-15 23:13:42,226: t15.2023.09.24 val PER: 0.1092
2025-12-15 23:13:42,226: t15.2023.09.29 val PER: 0.1429
2025-12-15 23:13:42,226: t15.2023.10.01 val PER: 0.1876
2025-12-15 23:13:42,226: t15.2023.10.06 val PER: 0.1001
2025-12-15 23:13:42,226: t15.2023.10.08 val PER: 0.1949
2025-12-15 23:13:42,226: t15.2023.10.13 val PER: 0.1939
2025-12-15 23:13:42,226: t15.2023.10.15 val PER: 0.1457
2025-12-15 23:13:42,226: t15.2023.10.20 val PER: 0.1980
2025-12-15 23:13:42,226: t15.2023.10.22 val PER: 0.1258
2025-12-15 23:13:42,226: t15.2023.11.03 val PER: 0.1696
2025-12-15 23:13:42,226: t15.2023.11.04 val PER: 0.0239
2025-12-15 23:13:42,227: t15.2023.11.17 val PER: 0.0544
2025-12-15 23:13:42,227: t15.2023.11.19 val PER: 0.0719
2025-12-15 23:13:42,227: t15.2023.11.26 val PER: 0.1370
2025-12-15 23:13:42,227: t15.2023.12.03 val PER: 0.0977
2025-12-15 23:13:42,227: t15.2023.12.08 val PER: 0.0899
2025-12-15 23:13:42,227: t15.2023.12.10 val PER: 0.0828
2025-12-15 23:13:42,227: t15.2023.12.17 val PER: 0.1362
2025-12-15 23:13:42,227: t15.2023.12.29 val PER: 0.1174
2025-12-15 23:13:42,227: t15.2024.02.25 val PER: 0.1096
2025-12-15 23:13:42,227: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:13:42,227: t15.2024.03.08 val PER: 0.2205
2025-12-15 23:13:42,227: t15.2024.03.15 val PER: 0.2076
2025-12-15 23:13:42,227: t15.2024.03.17 val PER: 0.1165
2025-12-15 23:13:42,227: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:13:42,227: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:13:42,227: t15.2024.05.10 val PER: 0.1501
2025-12-15 23:13:42,227: t15.2024.06.14 val PER: 0.1688
2025-12-15 23:13:42,227: t15.2024.07.19 val PER: 0.1905
2025-12-15 23:13:42,227: t15.2024.07.21 val PER: 0.0993
2025-12-15 23:13:42,228: t15.2024.07.28 val PER: 0.1184
2025-12-15 23:13:42,228: t15.2025.01.10 val PER: 0.2893
2025-12-15 23:13:42,228: t15.2025.01.12 val PER: 0.1501
2025-12-15 23:13:42,228: t15.2025.03.14 val PER: 0.3417
2025-12-15 23:13:42,228: t15.2025.03.16 val PER: 0.1976
2025-12-15 23:13:42,228: t15.2025.03.30 val PER: 0.2621
2025-12-15 23:13:42,228: t15.2025.04.13 val PER: 0.2411
2025-12-15 23:13:56,125: Train batch 56200: loss: 0.83 grad norm: 0.54 time: 0.075
2025-12-15 23:14:10,710: Train batch 56400: loss: 0.82 grad norm: 0.15 time: 0.068
2025-12-15 23:14:24,484: Train batch 56600: loss: 0.86 grad norm: 0.99 time: 0.055
2025-12-15 23:14:38,931: Train batch 56800: loss: 0.79 grad norm: 0.79 time: 0.068
2025-12-15 23:14:53,527: Train batch 57000: loss: 0.83 grad norm: 0.98 time: 0.057
2025-12-15 23:15:07,962: Train batch 57200: loss: 0.99 grad norm: 1.90 time: 0.058
2025-12-15 23:15:21,747: Train batch 57400: loss: 0.79 grad norm: 0.78 time: 0.074
2025-12-15 23:15:34,806: Train batch 57600: loss: 0.90 grad norm: 1.06 time: 0.083
2025-12-15 23:15:49,327: Train batch 57800: loss: 0.92 grad norm: 0.63 time: 0.050
2025-12-15 23:16:04,734: Train batch 58000: loss: 0.91 grad norm: 0.59 time: 0.073
2025-12-15 23:16:04,734: Running test after training batch: 58000
2025-12-15 23:16:14,165: Val batch 58000: PER (avg): 0.1440 CTC Loss (avg): 0.9820 time: 9.431
2025-12-15 23:16:14,166: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:16:14,166: t15.2023.08.13 val PER: 0.1091
2025-12-15 23:16:14,166: t15.2023.08.18 val PER: 0.1048
2025-12-15 23:16:14,166: t15.2023.08.20 val PER: 0.0953
2025-12-15 23:16:14,166: t15.2023.08.25 val PER: 0.0979
2025-12-15 23:16:14,166: t15.2023.08.27 val PER: 0.1688
2025-12-15 23:16:14,166: t15.2023.09.01 val PER: 0.0657
2025-12-15 23:16:14,166: t15.2023.09.03 val PER: 0.1413
2025-12-15 23:16:14,166: t15.2023.09.24 val PER: 0.0910
2025-12-15 23:16:14,166: t15.2023.09.29 val PER: 0.1270
2025-12-15 23:16:14,166: t15.2023.10.01 val PER: 0.1744
2025-12-15 23:16:14,166: t15.2023.10.06 val PER: 0.0807
2025-12-15 23:16:14,166: t15.2023.10.08 val PER: 0.1976
2025-12-15 23:16:14,166: t15.2023.10.13 val PER: 0.1994
2025-12-15 23:16:14,166: t15.2023.10.15 val PER: 0.1523
2025-12-15 23:16:14,167: t15.2023.10.20 val PER: 0.2114
2025-12-15 23:16:14,167: t15.2023.10.22 val PER: 0.1225
2025-12-15 23:16:14,167: t15.2023.11.03 val PER: 0.1696
2025-12-15 23:16:14,167: t15.2023.11.04 val PER: 0.0205
2025-12-15 23:16:14,167: t15.2023.11.17 val PER: 0.0498
2025-12-15 23:16:14,167: t15.2023.11.19 val PER: 0.0858
2025-12-15 23:16:14,167: t15.2023.11.26 val PER: 0.1145
2025-12-15 23:16:14,167: t15.2023.12.03 val PER: 0.1029
2025-12-15 23:16:14,167: t15.2023.12.08 val PER: 0.0999
2025-12-15 23:16:14,167: t15.2023.12.10 val PER: 0.0867
2025-12-15 23:16:14,167: t15.2023.12.17 val PER: 0.1538
2025-12-15 23:16:14,167: t15.2023.12.29 val PER: 0.1181
2025-12-15 23:16:14,167: t15.2024.02.25 val PER: 0.1096
2025-12-15 23:16:14,167: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:16:14,167: t15.2024.03.08 val PER: 0.2233
2025-12-15 23:16:14,167: t15.2024.03.15 val PER: 0.2001
2025-12-15 23:16:14,167: t15.2024.03.17 val PER: 0.1192
2025-12-15 23:16:14,167: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:16:14,167: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:16:14,168: t15.2024.05.10 val PER: 0.1530
2025-12-15 23:16:14,168: t15.2024.06.14 val PER: 0.1735
2025-12-15 23:16:14,168: t15.2024.07.19 val PER: 0.1905
2025-12-15 23:16:14,168: t15.2024.07.21 val PER: 0.1034
2025-12-15 23:16:14,168: t15.2024.07.28 val PER: 0.1375
2025-12-15 23:16:14,168: t15.2025.01.10 val PER: 0.2824
2025-12-15 23:16:14,168: t15.2025.01.12 val PER: 0.1301
2025-12-15 23:16:14,168: t15.2025.03.14 val PER: 0.3284
2025-12-15 23:16:14,168: t15.2025.03.16 val PER: 0.1976
2025-12-15 23:16:14,168: t15.2025.03.30 val PER: 0.2598
2025-12-15 23:16:14,168: t15.2025.04.13 val PER: 0.2282
2025-12-15 23:16:14,168: New best test PER 0.1444 --> 0.1440
2025-12-15 23:16:14,168: Checkpointing model
2025-12-15 23:16:14,601: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:16:29,759: Train batch 58200: loss: 0.92 grad norm: 1.12 time: 0.068
2025-12-15 23:16:44,238: Train batch 58400: loss: 0.74 grad norm: 0.64 time: 0.058
2025-12-15 23:16:58,606: Train batch 58600: loss: 0.80 grad norm: 1.25 time: 0.074
2025-12-15 23:17:11,632: Train batch 58800: loss: 0.91 grad norm: 0.43 time: 0.067
2025-12-15 23:17:22,558: Train batch 59000: loss: 0.93 grad norm: 0.76 time: 0.046
2025-12-15 23:17:32,567: Train batch 59200: loss: 0.87 grad norm: 1.05 time: 0.080
2025-12-15 23:17:44,817: Train batch 59400: loss: 0.94 grad norm: 0.38 time: 0.081
2025-12-15 23:17:59,013: Train batch 59600: loss: 0.87 grad norm: 0.80 time: 0.071
2025-12-15 23:18:13,252: Train batch 59800: loss: 0.87 grad norm: 1.20 time: 0.067
2025-12-15 23:18:25,787: Train batch 60000: loss: 0.97 grad norm: 0.23 time: 0.046
2025-12-15 23:18:25,787: Running test after training batch: 60000
2025-12-15 23:18:35,489: Val batch 60000: PER (avg): 0.1437 CTC Loss (avg): 0.9802 time: 9.702
2025-12-15 23:18:35,489: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:18:35,489: t15.2023.08.13 val PER: 0.1071
2025-12-15 23:18:35,489: t15.2023.08.18 val PER: 0.1081
2025-12-15 23:18:35,490: t15.2023.08.20 val PER: 0.0905
2025-12-15 23:18:35,490: t15.2023.08.25 val PER: 0.0919
2025-12-15 23:18:35,490: t15.2023.08.27 val PER: 0.1608
2025-12-15 23:18:35,490: t15.2023.09.01 val PER: 0.0722
2025-12-15 23:18:35,490: t15.2023.09.03 val PER: 0.1354
2025-12-15 23:18:35,490: t15.2023.09.24 val PER: 0.1153
2025-12-15 23:18:35,490: t15.2023.09.29 val PER: 0.1378
2025-12-15 23:18:35,490: t15.2023.10.01 val PER: 0.1671
2025-12-15 23:18:35,490: t15.2023.10.06 val PER: 0.0926
2025-12-15 23:18:35,490: t15.2023.10.08 val PER: 0.1908
2025-12-15 23:18:35,490: t15.2023.10.13 val PER: 0.2048
2025-12-15 23:18:35,490: t15.2023.10.15 val PER: 0.1345
2025-12-15 23:18:35,490: t15.2023.10.20 val PER: 0.1913
2025-12-15 23:18:35,490: t15.2023.10.22 val PER: 0.1269
2025-12-15 23:18:35,490: t15.2023.11.03 val PER: 0.1737
2025-12-15 23:18:35,490: t15.2023.11.04 val PER: 0.0375
2025-12-15 23:18:35,490: t15.2023.11.17 val PER: 0.0638
2025-12-15 23:18:35,490: t15.2023.11.19 val PER: 0.0818
2025-12-15 23:18:35,490: t15.2023.11.26 val PER: 0.1275
2025-12-15 23:18:35,491: t15.2023.12.03 val PER: 0.0945
2025-12-15 23:18:35,491: t15.2023.12.08 val PER: 0.0839
2025-12-15 23:18:35,491: t15.2023.12.10 val PER: 0.0828
2025-12-15 23:18:35,491: t15.2023.12.17 val PER: 0.1455
2025-12-15 23:18:35,491: t15.2023.12.29 val PER: 0.1222
2025-12-15 23:18:35,491: t15.2024.02.25 val PER: 0.1025
2025-12-15 23:18:35,491: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:18:35,491: t15.2024.03.08 val PER: 0.2347
2025-12-15 23:18:35,491: t15.2024.03.15 val PER: 0.1989
2025-12-15 23:18:35,491: t15.2024.03.17 val PER: 0.1130
2025-12-15 23:18:35,491: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:18:35,491: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:18:35,491: t15.2024.05.10 val PER: 0.1590
2025-12-15 23:18:35,491: t15.2024.06.14 val PER: 0.1735
2025-12-15 23:18:35,491: t15.2024.07.19 val PER: 0.1958
2025-12-15 23:18:35,491: t15.2024.07.21 val PER: 0.0959
2025-12-15 23:18:35,491: t15.2024.07.28 val PER: 0.1228
2025-12-15 23:18:35,491: t15.2025.01.10 val PER: 0.2851
2025-12-15 23:18:35,491: t15.2025.01.12 val PER: 0.1286
2025-12-15 23:18:35,492: t15.2025.03.14 val PER: 0.3269
2025-12-15 23:18:35,492: t15.2025.03.16 val PER: 0.1911
2025-12-15 23:18:35,492: t15.2025.03.30 val PER: 0.2621
2025-12-15 23:18:35,492: t15.2025.04.13 val PER: 0.2525
2025-12-15 23:18:35,492: New best test PER 0.1440 --> 0.1437
2025-12-15 23:18:35,492: Checkpointing model
2025-12-15 23:18:35,952: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:18:49,310: Train batch 60200: loss: 0.80 grad norm: 1.26 time: 0.066
2025-12-15 23:19:03,046: Train batch 60400: loss: 0.96 grad norm: 0.54 time: 0.049
2025-12-15 23:19:17,565: Train batch 60600: loss: 0.87 grad norm: 0.57 time: 0.077
2025-12-15 23:19:31,879: Train batch 60800: loss: 0.97 grad norm: 1.43 time: 0.073
2025-12-15 23:19:46,414: Train batch 61000: loss: 0.84 grad norm: 1.92 time: 0.038
2025-12-15 23:20:01,080: Train batch 61200: loss: 0.88 grad norm: 0.75 time: 0.077
2025-12-15 23:20:15,423: Train batch 61400: loss: 0.91 grad norm: 1.75 time: 0.070
2025-12-15 23:20:26,229: Train batch 61600: loss: 0.85 grad norm: 1.03 time: 0.070
2025-12-15 23:20:39,419: Train batch 61800: loss: 0.88 grad norm: 0.12 time: 0.069
2025-12-15 23:20:52,892: Train batch 62000: loss: 0.85 grad norm: 0.87 time: 0.052
2025-12-15 23:20:52,893: Running test after training batch: 62000
2025-12-15 23:21:02,843: Val batch 62000: PER (avg): 0.1417 CTC Loss (avg): 0.9834 time: 9.950
2025-12-15 23:21:02,843: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:21:02,843: t15.2023.08.13 val PER: 0.1206
2025-12-15 23:21:02,843: t15.2023.08.18 val PER: 0.0914
2025-12-15 23:21:02,843: t15.2023.08.20 val PER: 0.0866
2025-12-15 23:21:02,843: t15.2023.08.25 val PER: 0.0919
2025-12-15 23:21:02,843: t15.2023.08.27 val PER: 0.1576
2025-12-15 23:21:02,843: t15.2023.09.01 val PER: 0.0706
2025-12-15 23:21:02,843: t15.2023.09.03 val PER: 0.1318
2025-12-15 23:21:02,843: t15.2023.09.24 val PER: 0.1056
2025-12-15 23:21:02,843: t15.2023.09.29 val PER: 0.1206
2025-12-15 23:21:02,844: t15.2023.10.01 val PER: 0.1869
2025-12-15 23:21:02,844: t15.2023.10.06 val PER: 0.0893
2025-12-15 23:21:02,844: t15.2023.10.08 val PER: 0.1881
2025-12-15 23:21:02,844: t15.2023.10.13 val PER: 0.1955
2025-12-15 23:21:02,844: t15.2023.10.15 val PER: 0.1437
2025-12-15 23:21:02,844: t15.2023.10.20 val PER: 0.2013
2025-12-15 23:21:02,844: t15.2023.10.22 val PER: 0.1192
2025-12-15 23:21:02,844: t15.2023.11.03 val PER: 0.1737
2025-12-15 23:21:02,844: t15.2023.11.04 val PER: 0.0410
2025-12-15 23:21:02,844: t15.2023.11.17 val PER: 0.0560
2025-12-15 23:21:02,844: t15.2023.11.19 val PER: 0.0719
2025-12-15 23:21:02,844: t15.2023.11.26 val PER: 0.1297
2025-12-15 23:21:02,844: t15.2023.12.03 val PER: 0.1008
2025-12-15 23:21:02,844: t15.2023.12.08 val PER: 0.1065
2025-12-15 23:21:02,844: t15.2023.12.10 val PER: 0.0854
2025-12-15 23:21:02,844: t15.2023.12.17 val PER: 0.1320
2025-12-15 23:21:02,844: t15.2023.12.29 val PER: 0.1030
2025-12-15 23:21:02,844: t15.2024.02.25 val PER: 0.1096
2025-12-15 23:21:02,844: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:21:02,845: t15.2024.03.08 val PER: 0.2162
2025-12-15 23:21:02,845: t15.2024.03.15 val PER: 0.1957
2025-12-15 23:21:02,845: t15.2024.03.17 val PER: 0.1053
2025-12-15 23:21:02,845: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:21:02,845: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:21:02,845: t15.2024.05.10 val PER: 0.1501
2025-12-15 23:21:02,845: t15.2024.06.14 val PER: 0.1672
2025-12-15 23:21:02,845: t15.2024.07.19 val PER: 0.1905
2025-12-15 23:21:02,845: t15.2024.07.21 val PER: 0.1007
2025-12-15 23:21:02,845: t15.2024.07.28 val PER: 0.1213
2025-12-15 23:21:02,845: t15.2025.01.10 val PER: 0.3113
2025-12-15 23:21:02,845: t15.2025.01.12 val PER: 0.1316
2025-12-15 23:21:02,845: t15.2025.03.14 val PER: 0.3284
2025-12-15 23:21:02,845: t15.2025.03.16 val PER: 0.1872
2025-12-15 23:21:02,845: t15.2025.03.30 val PER: 0.2575
2025-12-15 23:21:02,845: t15.2025.04.13 val PER: 0.2168
2025-12-15 23:21:02,845: New best test PER 0.1437 --> 0.1417
2025-12-15 23:21:02,845: Checkpointing model
2025-12-15 23:21:03,237: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:21:17,750: Train batch 62200: loss: 0.89 grad norm: 1.71 time: 0.069
2025-12-15 23:21:32,227: Train batch 62400: loss: 0.82 grad norm: 0.16 time: 0.070
2025-12-15 23:21:46,690: Train batch 62600: loss: 0.73 grad norm: 0.96 time: 0.068
2025-12-15 23:22:00,789: Train batch 62800: loss: 0.91 grad norm: 0.69 time: 0.056
2025-12-15 23:22:14,852: Train batch 63000: loss: 0.79 grad norm: 0.64 time: 0.071
2025-12-15 23:22:28,140: Train batch 63200: loss: 0.83 grad norm: 0.44 time: 0.075
2025-12-15 23:22:42,606: Train batch 63400: loss: 0.92 grad norm: 0.73 time: 0.056
2025-12-15 23:22:56,996: Train batch 63600: loss: 0.86 grad norm: 1.45 time: 0.082
2025-12-15 23:23:11,477: Train batch 63800: loss: 0.82 grad norm: 1.14 time: 0.066
2025-12-15 23:23:23,577: Train batch 64000: loss: 0.95 grad norm: 0.66 time: 0.083
2025-12-15 23:23:23,577: Running test after training batch: 64000
2025-12-15 23:23:32,968: Val batch 64000: PER (avg): 0.1435 CTC Loss (avg): 1.0257 time: 9.391
2025-12-15 23:23:32,968: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:23:32,968: t15.2023.08.13 val PER: 0.1216
2025-12-15 23:23:32,968: t15.2023.08.18 val PER: 0.1073
2025-12-15 23:23:32,968: t15.2023.08.20 val PER: 0.0731
2025-12-15 23:23:32,968: t15.2023.08.25 val PER: 0.1009
2025-12-15 23:23:32,969: t15.2023.08.27 val PER: 0.1801
2025-12-15 23:23:32,969: t15.2023.09.01 val PER: 0.0731
2025-12-15 23:23:32,969: t15.2023.09.03 val PER: 0.1306
2025-12-15 23:23:32,969: t15.2023.09.24 val PER: 0.1044
2025-12-15 23:23:32,969: t15.2023.09.29 val PER: 0.1315
2025-12-15 23:23:32,969: t15.2023.10.01 val PER: 0.1671
2025-12-15 23:23:32,969: t15.2023.10.06 val PER: 0.0980
2025-12-15 23:23:32,969: t15.2023.10.08 val PER: 0.2030
2025-12-15 23:23:32,969: t15.2023.10.13 val PER: 0.2017
2025-12-15 23:23:32,969: t15.2023.10.15 val PER: 0.1430
2025-12-15 23:23:32,969: t15.2023.10.20 val PER: 0.1946
2025-12-15 23:23:32,969: t15.2023.10.22 val PER: 0.1247
2025-12-15 23:23:32,969: t15.2023.11.03 val PER: 0.1757
2025-12-15 23:23:32,969: t15.2023.11.04 val PER: 0.0307
2025-12-15 23:23:32,969: t15.2023.11.17 val PER: 0.0622
2025-12-15 23:23:32,969: t15.2023.11.19 val PER: 0.0758
2025-12-15 23:23:32,969: t15.2023.11.26 val PER: 0.1297
2025-12-15 23:23:32,969: t15.2023.12.03 val PER: 0.0767
2025-12-15 23:23:32,969: t15.2023.12.08 val PER: 0.0965
2025-12-15 23:23:32,970: t15.2023.12.10 val PER: 0.0907
2025-12-15 23:23:32,970: t15.2023.12.17 val PER: 0.1476
2025-12-15 23:23:32,970: t15.2023.12.29 val PER: 0.1132
2025-12-15 23:23:32,970: t15.2024.02.25 val PER: 0.1067
2025-12-15 23:23:32,970: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:23:32,970: t15.2024.03.08 val PER: 0.2176
2025-12-15 23:23:32,970: t15.2024.03.15 val PER: 0.1957
2025-12-15 23:23:32,970: t15.2024.03.17 val PER: 0.1109
2025-12-15 23:23:32,970: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:23:32,970: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:23:32,970: t15.2024.05.10 val PER: 0.1620
2025-12-15 23:23:32,970: t15.2024.06.14 val PER: 0.1703
2025-12-15 23:23:32,970: t15.2024.07.19 val PER: 0.1971
2025-12-15 23:23:32,970: t15.2024.07.21 val PER: 0.0952
2025-12-15 23:23:32,970: t15.2024.07.28 val PER: 0.1287
2025-12-15 23:23:32,970: t15.2025.01.10 val PER: 0.2975
2025-12-15 23:23:32,970: t15.2025.01.12 val PER: 0.1386
2025-12-15 23:23:32,970: t15.2025.03.14 val PER: 0.3299
2025-12-15 23:23:32,970: t15.2025.03.16 val PER: 0.2003
2025-12-15 23:23:32,971: t15.2025.03.30 val PER: 0.2483
2025-12-15 23:23:32,971: t15.2025.04.13 val PER: 0.2211
2025-12-15 23:23:43,863: Train batch 64200: loss: 0.93 grad norm: 0.90 time: 0.057
2025-12-15 23:23:57,325: Train batch 64400: loss: 0.83 grad norm: 1.20 time: 0.035
2025-12-15 23:24:11,562: Train batch 64600: loss: 0.97 grad norm: 0.62 time: 0.056
2025-12-15 23:24:25,809: Train batch 64800: loss: 0.76 grad norm: 1.06 time: 0.063
2025-12-15 23:24:39,119: Train batch 65000: loss: 0.83 grad norm: 0.22 time: 0.073
2025-12-15 23:24:53,398: Train batch 65200: loss: 0.85 grad norm: 0.40 time: 0.067
2025-12-15 23:25:07,823: Train batch 65400: loss: 0.83 grad norm: 1.21 time: 0.083
2025-12-15 23:25:21,918: Train batch 65600: loss: 0.76 grad norm: 0.55 time: 0.059
2025-12-15 23:25:35,682: Train batch 65800: loss: 0.89 grad norm: 0.70 time: 0.063
2025-12-15 23:25:49,441: Train batch 66000: loss: 0.83 grad norm: 1.23 time: 0.038
2025-12-15 23:25:49,441: Running test after training batch: 66000
2025-12-15 23:25:58,669: Val batch 66000: PER (avg): 0.1382 CTC Loss (avg): 0.9857 time: 9.227
2025-12-15 23:25:58,669: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:25:58,669: t15.2023.08.13 val PER: 0.1133
2025-12-15 23:25:58,669: t15.2023.08.18 val PER: 0.1023
2025-12-15 23:25:58,669: t15.2023.08.20 val PER: 0.0834
2025-12-15 23:25:58,669: t15.2023.08.25 val PER: 0.1084
2025-12-15 23:25:58,669: t15.2023.08.27 val PER: 0.1672
2025-12-15 23:25:58,669: t15.2023.09.01 val PER: 0.0625
2025-12-15 23:25:58,669: t15.2023.09.03 val PER: 0.1342
2025-12-15 23:25:58,669: t15.2023.09.24 val PER: 0.0922
2025-12-15 23:25:58,669: t15.2023.09.29 val PER: 0.1257
2025-12-15 23:25:58,669: t15.2023.10.01 val PER: 0.1645
2025-12-15 23:25:58,670: t15.2023.10.06 val PER: 0.0969
2025-12-15 23:25:58,670: t15.2023.10.08 val PER: 0.1976
2025-12-15 23:25:58,670: t15.2023.10.13 val PER: 0.2040
2025-12-15 23:25:58,670: t15.2023.10.15 val PER: 0.1444
2025-12-15 23:25:58,670: t15.2023.10.20 val PER: 0.1745
2025-12-15 23:25:58,670: t15.2023.10.22 val PER: 0.1114
2025-12-15 23:25:58,670: t15.2023.11.03 val PER: 0.1682
2025-12-15 23:25:58,670: t15.2023.11.04 val PER: 0.0205
2025-12-15 23:25:58,670: t15.2023.11.17 val PER: 0.0467
2025-12-15 23:25:58,670: t15.2023.11.19 val PER: 0.0679
2025-12-15 23:25:58,670: t15.2023.11.26 val PER: 0.1188
2025-12-15 23:25:58,670: t15.2023.12.03 val PER: 0.0809
2025-12-15 23:25:58,670: t15.2023.12.08 val PER: 0.0912
2025-12-15 23:25:58,670: t15.2023.12.10 val PER: 0.0828
2025-12-15 23:25:58,670: t15.2023.12.17 val PER: 0.1372
2025-12-15 23:25:58,670: t15.2023.12.29 val PER: 0.1105
2025-12-15 23:25:58,670: t15.2024.02.25 val PER: 0.0955
2025-12-15 23:25:58,670: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:25:58,670: t15.2024.03.08 val PER: 0.2390
2025-12-15 23:25:58,670: t15.2024.03.15 val PER: 0.1939
2025-12-15 23:25:58,671: t15.2024.03.17 val PER: 0.0969
2025-12-15 23:25:58,671: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:25:58,671: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:25:58,671: t15.2024.05.10 val PER: 0.1649
2025-12-15 23:25:58,671: t15.2024.06.14 val PER: 0.1546
2025-12-15 23:25:58,671: t15.2024.07.19 val PER: 0.1839
2025-12-15 23:25:58,671: t15.2024.07.21 val PER: 0.0945
2025-12-15 23:25:58,671: t15.2024.07.28 val PER: 0.1272
2025-12-15 23:25:58,671: t15.2025.01.10 val PER: 0.2961
2025-12-15 23:25:58,671: t15.2025.01.12 val PER: 0.1263
2025-12-15 23:25:58,671: t15.2025.03.14 val PER: 0.3062
2025-12-15 23:25:58,671: t15.2025.03.16 val PER: 0.1846
2025-12-15 23:25:58,671: t15.2025.03.30 val PER: 0.2506
2025-12-15 23:25:58,671: t15.2025.04.13 val PER: 0.2083
2025-12-15 23:25:58,671: New best test PER 0.1417 --> 0.1382
2025-12-15 23:25:58,671: Checkpointing model
2025-12-15 23:25:59,054: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:26:13,719: Train batch 66200: loss: 0.83 grad norm: 0.41 time: 0.069
2025-12-15 23:26:27,884: Train batch 66400: loss: 0.78 grad norm: 0.75 time: 0.087
2025-12-15 23:26:42,254: Train batch 66600: loss: 0.78 grad norm: 0.98 time: 0.066
2025-12-15 23:26:55,742: Train batch 66800: loss: 0.80 grad norm: 0.08 time: 0.067
2025-12-15 23:27:10,012: Train batch 67000: loss: 0.81 grad norm: 0.05 time: 0.070
2025-12-15 23:27:24,726: Train batch 67200: loss: 0.76 grad norm: 0.71 time: 0.080
2025-12-15 23:27:39,474: Train batch 67400: loss: 0.77 grad norm: 1.25 time: 0.070
2025-12-15 23:27:53,790: Train batch 67600: loss: 0.83 grad norm: 0.80 time: 0.074
2025-12-15 23:28:08,252: Train batch 67800: loss: 0.97 grad norm: 0.20 time: 0.057
2025-12-15 23:28:21,759: Train batch 68000: loss: 0.93 grad norm: 0.95 time: 0.083
2025-12-15 23:28:21,760: Running test after training batch: 68000
2025-12-15 23:28:31,001: Val batch 68000: PER (avg): 0.1390 CTC Loss (avg): 0.9944 time: 9.241
2025-12-15 23:28:31,002: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:28:31,002: t15.2023.08.13 val PER: 0.1237
2025-12-15 23:28:31,002: t15.2023.08.18 val PER: 0.0964
2025-12-15 23:28:31,002: t15.2023.08.20 val PER: 0.0699
2025-12-15 23:28:31,002: t15.2023.08.25 val PER: 0.1009
2025-12-15 23:28:31,002: t15.2023.08.27 val PER: 0.1752
2025-12-15 23:28:31,002: t15.2023.09.01 val PER: 0.0633
2025-12-15 23:28:31,002: t15.2023.09.03 val PER: 0.1461
2025-12-15 23:28:31,002: t15.2023.09.24 val PER: 0.1129
2025-12-15 23:28:31,002: t15.2023.09.29 val PER: 0.1251
2025-12-15 23:28:31,002: t15.2023.10.01 val PER: 0.1737
2025-12-15 23:28:31,002: t15.2023.10.06 val PER: 0.1012
2025-12-15 23:28:31,002: t15.2023.10.08 val PER: 0.2030
2025-12-15 23:28:31,002: t15.2023.10.13 val PER: 0.1955
2025-12-15 23:28:31,002: t15.2023.10.15 val PER: 0.1411
2025-12-15 23:28:31,002: t15.2023.10.20 val PER: 0.1946
2025-12-15 23:28:31,002: t15.2023.10.22 val PER: 0.1281
2025-12-15 23:28:31,002: t15.2023.11.03 val PER: 0.1784
2025-12-15 23:28:31,003: t15.2023.11.04 val PER: 0.0341
2025-12-15 23:28:31,003: t15.2023.11.17 val PER: 0.0373
2025-12-15 23:28:31,003: t15.2023.11.19 val PER: 0.0739
2025-12-15 23:28:31,003: t15.2023.11.26 val PER: 0.1138
2025-12-15 23:28:31,003: t15.2023.12.03 val PER: 0.0924
2025-12-15 23:28:31,003: t15.2023.12.08 val PER: 0.0919
2025-12-15 23:28:31,003: t15.2023.12.10 val PER: 0.0775
2025-12-15 23:28:31,003: t15.2023.12.17 val PER: 0.1414
2025-12-15 23:28:31,003: t15.2023.12.29 val PER: 0.1181
2025-12-15 23:28:31,003: t15.2024.02.25 val PER: 0.0969
2025-12-15 23:28:31,003: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:28:31,003: t15.2024.03.08 val PER: 0.2105
2025-12-15 23:28:31,003: t15.2024.03.15 val PER: 0.2014
2025-12-15 23:28:31,003: t15.2024.03.17 val PER: 0.1004
2025-12-15 23:28:31,003: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:28:31,003: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:28:31,003: t15.2024.05.10 val PER: 0.1560
2025-12-15 23:28:31,003: t15.2024.06.14 val PER: 0.1577
2025-12-15 23:28:31,003: t15.2024.07.19 val PER: 0.1806
2025-12-15 23:28:31,004: t15.2024.07.21 val PER: 0.0952
2025-12-15 23:28:31,004: t15.2024.07.28 val PER: 0.1162
2025-12-15 23:28:31,004: t15.2025.01.10 val PER: 0.2741
2025-12-15 23:28:31,004: t15.2025.01.12 val PER: 0.1270
2025-12-15 23:28:31,004: t15.2025.03.14 val PER: 0.3195
2025-12-15 23:28:31,004: t15.2025.03.16 val PER: 0.1846
2025-12-15 23:28:31,004: t15.2025.03.30 val PER: 0.2471
2025-12-15 23:28:31,004: t15.2025.04.13 val PER: 0.2026
2025-12-15 23:28:44,382: Train batch 68200: loss: 0.89 grad norm: 0.22 time: 0.065
2025-12-15 23:28:55,961: Train batch 68400: loss: 0.83 grad norm: 1.15 time: 0.054
2025-12-15 23:29:10,133: Train batch 68600: loss: 0.86 grad norm: 0.27 time: 0.070
2025-12-15 23:29:24,609: Train batch 68800: loss: 0.85 grad norm: 1.06 time: 0.070
2025-12-15 23:29:39,364: Train batch 69000: loss: 0.85 grad norm: 1.76 time: 0.057
2025-12-15 23:29:52,496: Train batch 69200: loss: 0.78 grad norm: 0.98 time: 0.069
2025-12-15 23:30:06,914: Train batch 69400: loss: 0.77 grad norm: 0.81 time: 0.071
2025-12-15 23:30:20,629: Train batch 69600: loss: 0.92 grad norm: 0.21 time: 0.054
2025-12-15 23:30:34,061: Train batch 69800: loss: 0.79 grad norm: 1.13 time: 0.081
2025-12-15 23:30:48,598: Train batch 70000: loss: 0.95 grad norm: 1.02 time: 0.067
2025-12-15 23:30:48,599: Running test after training batch: 70000
2025-12-15 23:30:57,783: Val batch 70000: PER (avg): 0.1421 CTC Loss (avg): 1.0571 time: 9.184
2025-12-15 23:30:57,783: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:30:57,783: t15.2023.08.13 val PER: 0.1206
2025-12-15 23:30:57,783: t15.2023.08.18 val PER: 0.1014
2025-12-15 23:30:57,783: t15.2023.08.20 val PER: 0.0810
2025-12-15 23:30:57,783: t15.2023.08.25 val PER: 0.1250
2025-12-15 23:30:57,783: t15.2023.08.27 val PER: 0.1881
2025-12-15 23:30:57,783: t15.2023.09.01 val PER: 0.0690
2025-12-15 23:30:57,783: t15.2023.09.03 val PER: 0.1437
2025-12-15 23:30:57,784: t15.2023.09.24 val PER: 0.1056
2025-12-15 23:30:57,784: t15.2023.09.29 val PER: 0.1340
2025-12-15 23:30:57,784: t15.2023.10.01 val PER: 0.1678
2025-12-15 23:30:57,784: t15.2023.10.06 val PER: 0.0936
2025-12-15 23:30:57,784: t15.2023.10.08 val PER: 0.2003
2025-12-15 23:30:57,784: t15.2023.10.13 val PER: 0.1939
2025-12-15 23:30:57,784: t15.2023.10.15 val PER: 0.1444
2025-12-15 23:30:57,784: t15.2023.10.20 val PER: 0.1913
2025-12-15 23:30:57,784: t15.2023.10.22 val PER: 0.1225
2025-12-15 23:30:57,784: t15.2023.11.03 val PER: 0.1703
2025-12-15 23:30:57,784: t15.2023.11.04 val PER: 0.0375
2025-12-15 23:30:57,784: t15.2023.11.17 val PER: 0.0560
2025-12-15 23:30:57,784: t15.2023.11.19 val PER: 0.0818
2025-12-15 23:30:57,784: t15.2023.11.26 val PER: 0.1203
2025-12-15 23:30:57,784: t15.2023.12.03 val PER: 0.0977
2025-12-15 23:30:57,784: t15.2023.12.08 val PER: 0.0979
2025-12-15 23:30:57,784: t15.2023.12.10 val PER: 0.0802
2025-12-15 23:30:57,784: t15.2023.12.17 val PER: 0.1476
2025-12-15 23:30:57,785: t15.2023.12.29 val PER: 0.1208
2025-12-15 23:30:57,785: t15.2024.02.25 val PER: 0.1067
2025-12-15 23:30:57,785: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:30:57,785: t15.2024.03.08 val PER: 0.2105
2025-12-15 23:30:57,785: t15.2024.03.15 val PER: 0.1976
2025-12-15 23:30:57,785: t15.2024.03.17 val PER: 0.1102
2025-12-15 23:30:57,785: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:30:57,785: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:30:57,785: t15.2024.05.10 val PER: 0.1649
2025-12-15 23:30:57,785: t15.2024.06.14 val PER: 0.1735
2025-12-15 23:30:57,785: t15.2024.07.19 val PER: 0.1859
2025-12-15 23:30:57,785: t15.2024.07.21 val PER: 0.0993
2025-12-15 23:30:57,785: t15.2024.07.28 val PER: 0.1118
2025-12-15 23:30:57,785: t15.2025.01.10 val PER: 0.2879
2025-12-15 23:30:57,785: t15.2025.01.12 val PER: 0.1093
2025-12-15 23:30:57,785: t15.2025.03.14 val PER: 0.3240
2025-12-15 23:30:57,785: t15.2025.03.16 val PER: 0.2081
2025-12-15 23:30:57,785: t15.2025.03.30 val PER: 0.2379
2025-12-15 23:30:57,786: t15.2025.04.13 val PER: 0.2225
2025-12-15 23:31:12,146: Train batch 70200: loss: 0.84 grad norm: 0.46 time: 0.056
2025-12-15 23:31:26,738: Train batch 70400: loss: 0.97 grad norm: 0.42 time: 0.074
2025-12-15 23:31:40,131: Train batch 70600: loss: 0.89 grad norm: 0.76 time: 0.042
2025-12-15 23:31:53,570: Train batch 70800: loss: 0.90 grad norm: 0.54 time: 0.039
2025-12-15 23:32:06,750: Train batch 71000: loss: 0.82 grad norm: 0.93 time: 0.064
2025-12-15 23:32:20,720: Train batch 71200: loss: 0.77 grad norm: 0.45 time: 0.067
2025-12-15 23:32:35,553: Train batch 71400: loss: 0.92 grad norm: 0.63 time: 0.051
2025-12-15 23:32:49,766: Train batch 71600: loss: 0.86 grad norm: 0.18 time: 0.073
2025-12-15 23:33:04,018: Train batch 71800: loss: 0.85 grad norm: 0.90 time: 0.067
2025-12-15 23:33:16,795: Train batch 72000: loss: 0.86 grad norm: 1.09 time: 0.054
2025-12-15 23:33:16,795: Running test after training batch: 72000
2025-12-15 23:33:26,315: Val batch 72000: PER (avg): 0.1387 CTC Loss (avg): 1.0418 time: 9.519
2025-12-15 23:33:26,315: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:33:26,315: t15.2023.08.13 val PER: 0.1123
2025-12-15 23:33:26,315: t15.2023.08.18 val PER: 0.0997
2025-12-15 23:33:26,315: t15.2023.08.20 val PER: 0.0699
2025-12-15 23:33:26,315: t15.2023.08.25 val PER: 0.1175
2025-12-15 23:33:26,315: t15.2023.08.27 val PER: 0.1688
2025-12-15 23:33:26,315: t15.2023.09.01 val PER: 0.0625
2025-12-15 23:33:26,315: t15.2023.09.03 val PER: 0.1295
2025-12-15 23:33:26,315: t15.2023.09.24 val PER: 0.1068
2025-12-15 23:33:26,315: t15.2023.09.29 val PER: 0.1308
2025-12-15 23:33:26,315: t15.2023.10.01 val PER: 0.1651
2025-12-15 23:33:26,315: t15.2023.10.06 val PER: 0.0840
2025-12-15 23:33:26,316: t15.2023.10.08 val PER: 0.2097
2025-12-15 23:33:26,316: t15.2023.10.13 val PER: 0.1971
2025-12-15 23:33:26,316: t15.2023.10.15 val PER: 0.1457
2025-12-15 23:33:26,316: t15.2023.10.20 val PER: 0.2081
2025-12-15 23:33:26,316: t15.2023.10.22 val PER: 0.1225
2025-12-15 23:33:26,316: t15.2023.11.03 val PER: 0.1723
2025-12-15 23:33:26,316: t15.2023.11.04 val PER: 0.0239
2025-12-15 23:33:26,316: t15.2023.11.17 val PER: 0.0575
2025-12-15 23:33:26,316: t15.2023.11.19 val PER: 0.0938
2025-12-15 23:33:26,316: t15.2023.11.26 val PER: 0.1130
2025-12-15 23:33:26,316: t15.2023.12.03 val PER: 0.0903
2025-12-15 23:33:26,316: t15.2023.12.08 val PER: 0.0919
2025-12-15 23:33:26,316: t15.2023.12.10 val PER: 0.0933
2025-12-15 23:33:26,316: t15.2023.12.17 val PER: 0.1476
2025-12-15 23:33:26,316: t15.2023.12.29 val PER: 0.1084
2025-12-15 23:33:26,316: t15.2024.02.25 val PER: 0.0997
2025-12-15 23:33:26,316: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:33:26,316: t15.2024.03.08 val PER: 0.2119
2025-12-15 23:33:26,316: t15.2024.03.15 val PER: 0.1939
2025-12-15 23:33:26,317: t15.2024.03.17 val PER: 0.1102
2025-12-15 23:33:26,317: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:33:26,317: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:33:26,317: t15.2024.05.10 val PER: 0.1620
2025-12-15 23:33:26,317: t15.2024.06.14 val PER: 0.1625
2025-12-15 23:33:26,317: t15.2024.07.19 val PER: 0.1800
2025-12-15 23:33:26,317: t15.2024.07.21 val PER: 0.0938
2025-12-15 23:33:26,317: t15.2024.07.28 val PER: 0.1029
2025-12-15 23:33:26,317: t15.2025.01.10 val PER: 0.2893
2025-12-15 23:33:26,317: t15.2025.01.12 val PER: 0.1116
2025-12-15 23:33:26,317: t15.2025.03.14 val PER: 0.3033
2025-12-15 23:33:26,317: t15.2025.03.16 val PER: 0.1950
2025-12-15 23:33:26,317: t15.2025.03.30 val PER: 0.2471
2025-12-15 23:33:26,317: t15.2025.04.13 val PER: 0.2340
2025-12-15 23:33:40,646: Train batch 72200: loss: 0.96 grad norm: 0.19 time: 0.056
2025-12-15 23:33:55,124: Train batch 72400: loss: 0.92 grad norm: 1.55 time: 0.040
2025-12-15 23:34:09,608: Train batch 72600: loss: 0.86 grad norm: 2.32 time: 0.058
2025-12-15 23:34:23,727: Train batch 72800: loss: 0.86 grad norm: 0.36 time: 0.065
2025-12-15 23:34:37,200: Train batch 73000: loss: 0.90 grad norm: 1.57 time: 0.061
2025-12-15 23:34:51,261: Train batch 73200: loss: 0.73 grad norm: 0.89 time: 0.078
2025-12-15 23:35:06,237: Train batch 73400: loss: 0.88 grad norm: 0.56 time: 0.075
2025-12-15 23:35:20,188: Train batch 73600: loss: 0.82 grad norm: 1.00 time: 0.068
2025-12-15 23:35:34,589: Train batch 73800: loss: 0.84 grad norm: 1.06 time: 0.067
2025-12-15 23:35:49,388: Train batch 74000: loss: 0.94 grad norm: 0.15 time: 0.061
2025-12-15 23:35:49,390: Running test after training batch: 74000
2025-12-15 23:35:59,074: Val batch 74000: PER (avg): 0.1352 CTC Loss (avg): 1.0610 time: 9.684
2025-12-15 23:35:59,074: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:35:59,074: t15.2023.08.13 val PER: 0.1102
2025-12-15 23:35:59,074: t15.2023.08.18 val PER: 0.0847
2025-12-15 23:35:59,074: t15.2023.08.20 val PER: 0.0667
2025-12-15 23:35:59,074: t15.2023.08.25 val PER: 0.1024
2025-12-15 23:35:59,074: t15.2023.08.27 val PER: 0.1736
2025-12-15 23:35:59,074: t15.2023.09.01 val PER: 0.0617
2025-12-15 23:35:59,074: t15.2023.09.03 val PER: 0.1140
2025-12-15 23:35:59,075: t15.2023.09.24 val PER: 0.1056
2025-12-15 23:35:59,075: t15.2023.09.29 val PER: 0.1270
2025-12-15 23:35:59,075: t15.2023.10.01 val PER: 0.1697
2025-12-15 23:35:59,075: t15.2023.10.06 val PER: 0.0850
2025-12-15 23:35:59,075: t15.2023.10.08 val PER: 0.1813
2025-12-15 23:35:59,075: t15.2023.10.13 val PER: 0.1831
2025-12-15 23:35:59,075: t15.2023.10.15 val PER: 0.1358
2025-12-15 23:35:59,075: t15.2023.10.20 val PER: 0.1879
2025-12-15 23:35:59,075: t15.2023.10.22 val PER: 0.1102
2025-12-15 23:35:59,075: t15.2023.11.03 val PER: 0.1784
2025-12-15 23:35:59,075: t15.2023.11.04 val PER: 0.0171
2025-12-15 23:35:59,075: t15.2023.11.17 val PER: 0.0575
2025-12-15 23:35:59,075: t15.2023.11.19 val PER: 0.0559
2025-12-15 23:35:59,075: t15.2023.11.26 val PER: 0.1116
2025-12-15 23:35:59,075: t15.2023.12.03 val PER: 0.0830
2025-12-15 23:35:59,075: t15.2023.12.08 val PER: 0.0905
2025-12-15 23:35:59,075: t15.2023.12.10 val PER: 0.0828
2025-12-15 23:35:59,075: t15.2023.12.17 val PER: 0.1486
2025-12-15 23:35:59,075: t15.2023.12.29 val PER: 0.1167
2025-12-15 23:35:59,076: t15.2024.02.25 val PER: 0.1053
2025-12-15 23:35:59,076: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:35:59,076: t15.2024.03.08 val PER: 0.2276
2025-12-15 23:35:59,076: t15.2024.03.15 val PER: 0.2014
2025-12-15 23:35:59,076: t15.2024.03.17 val PER: 0.1011
2025-12-15 23:35:59,076: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:35:59,076: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:35:59,076: t15.2024.05.10 val PER: 0.1516
2025-12-15 23:35:59,076: t15.2024.06.14 val PER: 0.1751
2025-12-15 23:35:59,076: t15.2024.07.19 val PER: 0.1688
2025-12-15 23:35:59,076: t15.2024.07.21 val PER: 0.0897
2025-12-15 23:35:59,076: t15.2024.07.28 val PER: 0.1221
2025-12-15 23:35:59,076: t15.2025.01.10 val PER: 0.2796
2025-12-15 23:35:59,076: t15.2025.01.12 val PER: 0.1186
2025-12-15 23:35:59,076: t15.2025.03.14 val PER: 0.2885
2025-12-15 23:35:59,076: t15.2025.03.16 val PER: 0.1885
2025-12-15 23:35:59,076: t15.2025.03.30 val PER: 0.2425
2025-12-15 23:35:59,076: t15.2025.04.13 val PER: 0.2154
2025-12-15 23:35:59,076: New best test PER 0.1382 --> 0.1352
2025-12-15 23:35:59,076: Checkpointing model
2025-12-15 23:35:59,478: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:36:13,258: Train batch 74200: loss: 0.76 grad norm: 0.83 time: 0.087
2025-12-15 23:36:27,759: Train batch 74400: loss: 0.84 grad norm: 0.81 time: 0.073
2025-12-15 23:36:42,445: Train batch 74600: loss: 0.88 grad norm: 0.67 time: 0.079
2025-12-15 23:36:57,006: Train batch 74800: loss: 0.71 grad norm: 0.64 time: 0.058
2025-12-15 23:37:11,231: Train batch 75000: loss: 0.70 grad norm: 0.63 time: 0.056
2025-12-15 23:37:25,132: Train batch 75200: loss: 0.72 grad norm: 1.09 time: 0.048
2025-12-15 23:37:39,243: Train batch 75400: loss: 0.90 grad norm: 1.78 time: 0.073
2025-12-15 23:37:52,351: Train batch 75600: loss: 0.74 grad norm: 1.45 time: 0.085
2025-12-15 23:38:06,274: Train batch 75800: loss: 1.03 grad norm: 0.63 time: 0.056
2025-12-15 23:38:18,848: Train batch 76000: loss: 0.85 grad norm: 0.60 time: 0.068
2025-12-15 23:38:18,848: Running test after training batch: 76000
2025-12-15 23:38:28,035: Val batch 76000: PER (avg): 0.1354 CTC Loss (avg): 1.0812 time: 9.186
2025-12-15 23:38:28,035: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:38:28,035: t15.2023.08.13 val PER: 0.1060
2025-12-15 23:38:28,036: t15.2023.08.18 val PER: 0.0939
2025-12-15 23:38:28,036: t15.2023.08.20 val PER: 0.0691
2025-12-15 23:38:28,036: t15.2023.08.25 val PER: 0.0949
2025-12-15 23:38:28,036: t15.2023.08.27 val PER: 0.1576
2025-12-15 23:38:28,036: t15.2023.09.01 val PER: 0.0609
2025-12-15 23:38:28,036: t15.2023.09.03 val PER: 0.1295
2025-12-15 23:38:28,036: t15.2023.09.24 val PER: 0.0971
2025-12-15 23:38:28,036: t15.2023.09.29 val PER: 0.1270
2025-12-15 23:38:28,036: t15.2023.10.01 val PER: 0.1691
2025-12-15 23:38:28,036: t15.2023.10.06 val PER: 0.0936
2025-12-15 23:38:28,036: t15.2023.10.08 val PER: 0.1867
2025-12-15 23:38:28,036: t15.2023.10.13 val PER: 0.1916
2025-12-15 23:38:28,036: t15.2023.10.15 val PER: 0.1417
2025-12-15 23:38:28,036: t15.2023.10.20 val PER: 0.1980
2025-12-15 23:38:28,036: t15.2023.10.22 val PER: 0.1236
2025-12-15 23:38:28,036: t15.2023.11.03 val PER: 0.1716
2025-12-15 23:38:28,036: t15.2023.11.04 val PER: 0.0102
2025-12-15 23:38:28,036: t15.2023.11.17 val PER: 0.0513
2025-12-15 23:38:28,036: t15.2023.11.19 val PER: 0.0599
2025-12-15 23:38:28,037: t15.2023.11.26 val PER: 0.1036
2025-12-15 23:38:28,037: t15.2023.12.03 val PER: 0.0798
2025-12-15 23:38:28,037: t15.2023.12.08 val PER: 0.0945
2025-12-15 23:38:28,037: t15.2023.12.10 val PER: 0.0788
2025-12-15 23:38:28,037: t15.2023.12.17 val PER: 0.1455
2025-12-15 23:38:28,037: t15.2023.12.29 val PER: 0.1126
2025-12-15 23:38:28,037: t15.2024.02.25 val PER: 0.1025
2025-12-15 23:38:28,037: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:38:28,037: t15.2024.03.08 val PER: 0.2205
2025-12-15 23:38:28,037: t15.2024.03.15 val PER: 0.1995
2025-12-15 23:38:28,037: t15.2024.03.17 val PER: 0.1046
2025-12-15 23:38:28,037: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:38:28,037: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:38:28,037: t15.2024.05.10 val PER: 0.1738
2025-12-15 23:38:28,037: t15.2024.06.14 val PER: 0.1719
2025-12-15 23:38:28,037: t15.2024.07.19 val PER: 0.1714
2025-12-15 23:38:28,037: t15.2024.07.21 val PER: 0.0917
2025-12-15 23:38:28,037: t15.2024.07.28 val PER: 0.1250
2025-12-15 23:38:28,037: t15.2025.01.10 val PER: 0.2837
2025-12-15 23:38:28,038: t15.2025.01.12 val PER: 0.1101
2025-12-15 23:38:28,038: t15.2025.03.14 val PER: 0.2870
2025-12-15 23:38:28,038: t15.2025.03.16 val PER: 0.1754
2025-12-15 23:38:28,038: t15.2025.03.30 val PER: 0.2322
2025-12-15 23:38:28,038: t15.2025.04.13 val PER: 0.2240
2025-12-15 23:38:42,584: Train batch 76200: loss: 0.94 grad norm: 0.64 time: 0.084
2025-12-15 23:38:55,848: Train batch 76400: loss: 0.85 grad norm: 1.04 time: 0.058
2025-12-15 23:39:08,728: Train batch 76600: loss: 0.87 grad norm: 0.17 time: 0.068
2025-12-15 23:39:22,762: Train batch 76800: loss: 0.76 grad norm: 0.48 time: 0.069
2025-12-15 23:39:36,989: Train batch 77000: loss: 0.85 grad norm: 0.33 time: 0.081
2025-12-15 23:39:51,227: Train batch 77200: loss: 0.85 grad norm: 0.55 time: 0.044
2025-12-15 23:40:05,601: Train batch 77400: loss: 0.72 grad norm: 0.17 time: 0.068
2025-12-15 23:40:20,332: Train batch 77600: loss: 0.75 grad norm: 1.74 time: 0.070
2025-12-15 23:40:34,829: Train batch 77800: loss: 0.99 grad norm: 0.04 time: 0.053
2025-12-15 23:40:49,428: Train batch 78000: loss: 0.80 grad norm: 0.30 time: 0.080
2025-12-15 23:40:49,429: Running test after training batch: 78000
2025-12-15 23:40:58,634: Val batch 78000: PER (avg): 0.1351 CTC Loss (avg): 1.0815 time: 9.205
2025-12-15 23:40:58,634: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:40:58,634: t15.2023.08.13 val PER: 0.1029
2025-12-15 23:40:58,635: t15.2023.08.18 val PER: 0.0972
2025-12-15 23:40:58,635: t15.2023.08.20 val PER: 0.0786
2025-12-15 23:40:58,635: t15.2023.08.25 val PER: 0.0873
2025-12-15 23:40:58,635: t15.2023.08.27 val PER: 0.1704
2025-12-15 23:40:58,635: t15.2023.09.01 val PER: 0.0584
2025-12-15 23:40:58,635: t15.2023.09.03 val PER: 0.1342
2025-12-15 23:40:58,635: t15.2023.09.24 val PER: 0.0934
2025-12-15 23:40:58,635: t15.2023.09.29 val PER: 0.1270
2025-12-15 23:40:58,635: t15.2023.10.01 val PER: 0.1612
2025-12-15 23:40:58,635: t15.2023.10.06 val PER: 0.0969
2025-12-15 23:40:58,635: t15.2023.10.08 val PER: 0.1813
2025-12-15 23:40:58,635: t15.2023.10.13 val PER: 0.1846
2025-12-15 23:40:58,635: t15.2023.10.15 val PER: 0.1437
2025-12-15 23:40:58,635: t15.2023.10.20 val PER: 0.1980
2025-12-15 23:40:58,635: t15.2023.10.22 val PER: 0.1214
2025-12-15 23:40:58,635: t15.2023.11.03 val PER: 0.1635
2025-12-15 23:40:58,635: t15.2023.11.04 val PER: 0.0205
2025-12-15 23:40:58,635: t15.2023.11.17 val PER: 0.0435
2025-12-15 23:40:58,636: t15.2023.11.19 val PER: 0.0659
2025-12-15 23:40:58,636: t15.2023.11.26 val PER: 0.1225
2025-12-15 23:40:58,636: t15.2023.12.03 val PER: 0.0830
2025-12-15 23:40:58,636: t15.2023.12.08 val PER: 0.0812
2025-12-15 23:40:58,636: t15.2023.12.10 val PER: 0.0815
2025-12-15 23:40:58,636: t15.2023.12.17 val PER: 0.1424
2025-12-15 23:40:58,636: t15.2023.12.29 val PER: 0.1119
2025-12-15 23:40:58,636: t15.2024.02.25 val PER: 0.1053
2025-12-15 23:40:58,636: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:40:58,636: t15.2024.03.08 val PER: 0.2191
2025-12-15 23:40:58,636: t15.2024.03.15 val PER: 0.2039
2025-12-15 23:40:58,636: t15.2024.03.17 val PER: 0.1067
2025-12-15 23:40:58,636: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:40:58,636: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:40:58,636: t15.2024.05.10 val PER: 0.1441
2025-12-15 23:40:58,636: t15.2024.06.14 val PER: 0.1814
2025-12-15 23:40:58,636: t15.2024.07.19 val PER: 0.1635
2025-12-15 23:40:58,636: t15.2024.07.21 val PER: 0.0903
2025-12-15 23:40:58,636: t15.2024.07.28 val PER: 0.1169
2025-12-15 23:40:58,636: t15.2025.01.10 val PER: 0.2906
2025-12-15 23:40:58,637: t15.2025.01.12 val PER: 0.1193
2025-12-15 23:40:58,637: t15.2025.03.14 val PER: 0.2959
2025-12-15 23:40:58,637: t15.2025.03.16 val PER: 0.1819
2025-12-15 23:40:58,637: t15.2025.03.30 val PER: 0.2391
2025-12-15 23:40:58,637: t15.2025.04.13 val PER: 0.2126
2025-12-15 23:40:58,637: New best test PER 0.1352 --> 0.1351
2025-12-15 23:40:58,637: Checkpointing model
2025-12-15 23:40:59,073: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:41:13,594: Train batch 78200: loss: 0.79 grad norm: 0.27 time: 0.068
2025-12-15 23:41:27,723: Train batch 78400: loss: 0.87 grad norm: 0.26 time: 0.068
2025-12-15 23:41:41,818: Train batch 78600: loss: 0.86 grad norm: 0.02 time: 0.048
2025-12-15 23:41:55,281: Train batch 78800: loss: 0.81 grad norm: 0.62 time: 0.054
2025-12-15 23:42:09,419: Train batch 79000: loss: 1.07 grad norm: 0.56 time: 0.063
2025-12-15 23:42:22,757: Train batch 79200: loss: 0.86 grad norm: 1.07 time: 0.053
2025-12-15 23:42:35,533: Train batch 79400: loss: 0.89 grad norm: 0.20 time: 0.058
2025-12-15 23:42:47,849: Train batch 79600: loss: 0.82 grad norm: 0.25 time: 0.052
2025-12-15 23:43:01,026: Train batch 79800: loss: 0.77 grad norm: 0.50 time: 0.055
2025-12-15 23:43:15,035: Train batch 80000: loss: 0.88 grad norm: 1.15 time: 0.071
2025-12-15 23:43:15,036: Running test after training batch: 80000
2025-12-15 23:43:24,474: Val batch 80000: PER (avg): 0.1355 CTC Loss (avg): 1.1032 time: 9.438
2025-12-15 23:43:24,474: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:43:24,474: t15.2023.08.13 val PER: 0.1071
2025-12-15 23:43:24,474: t15.2023.08.18 val PER: 0.0930
2025-12-15 23:43:24,475: t15.2023.08.20 val PER: 0.0786
2025-12-15 23:43:24,475: t15.2023.08.25 val PER: 0.0994
2025-12-15 23:43:24,475: t15.2023.08.27 val PER: 0.1736
2025-12-15 23:43:24,475: t15.2023.09.01 val PER: 0.0666
2025-12-15 23:43:24,475: t15.2023.09.03 val PER: 0.1318
2025-12-15 23:43:24,475: t15.2023.09.24 val PER: 0.0959
2025-12-15 23:43:24,475: t15.2023.09.29 val PER: 0.1251
2025-12-15 23:43:24,475: t15.2023.10.01 val PER: 0.1711
2025-12-15 23:43:24,475: t15.2023.10.06 val PER: 0.0958
2025-12-15 23:43:24,475: t15.2023.10.08 val PER: 0.1881
2025-12-15 23:43:24,475: t15.2023.10.13 val PER: 0.1792
2025-12-15 23:43:24,475: t15.2023.10.15 val PER: 0.1338
2025-12-15 23:43:24,475: t15.2023.10.20 val PER: 0.1812
2025-12-15 23:43:24,475: t15.2023.10.22 val PER: 0.1314
2025-12-15 23:43:24,475: t15.2023.11.03 val PER: 0.1723
2025-12-15 23:43:24,475: t15.2023.11.04 val PER: 0.0205
2025-12-15 23:43:24,475: t15.2023.11.17 val PER: 0.0513
2025-12-15 23:43:24,475: t15.2023.11.19 val PER: 0.0719
2025-12-15 23:43:24,476: t15.2023.11.26 val PER: 0.1159
2025-12-15 23:43:24,476: t15.2023.12.03 val PER: 0.0851
2025-12-15 23:43:24,476: t15.2023.12.08 val PER: 0.0832
2025-12-15 23:43:24,476: t15.2023.12.10 val PER: 0.0894
2025-12-15 23:43:24,476: t15.2023.12.17 val PER: 0.1403
2025-12-15 23:43:24,476: t15.2023.12.29 val PER: 0.1043
2025-12-15 23:43:24,476: t15.2024.02.25 val PER: 0.1081
2025-12-15 23:43:24,476: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:43:24,476: t15.2024.03.08 val PER: 0.2404
2025-12-15 23:43:24,476: t15.2024.03.15 val PER: 0.1914
2025-12-15 23:43:24,476: t15.2024.03.17 val PER: 0.0997
2025-12-15 23:43:24,476: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:43:24,476: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:43:24,476: t15.2024.05.10 val PER: 0.1426
2025-12-15 23:43:24,476: t15.2024.06.14 val PER: 0.1593
2025-12-15 23:43:24,476: t15.2024.07.19 val PER: 0.1773
2025-12-15 23:43:24,476: t15.2024.07.21 val PER: 0.0938
2025-12-15 23:43:24,476: t15.2024.07.28 val PER: 0.1206
2025-12-15 23:43:24,476: t15.2025.01.10 val PER: 0.2796
2025-12-15 23:43:24,477: t15.2025.01.12 val PER: 0.1232
2025-12-15 23:43:24,477: t15.2025.03.14 val PER: 0.3047
2025-12-15 23:43:24,477: t15.2025.03.16 val PER: 0.1741
2025-12-15 23:43:24,477: t15.2025.03.30 val PER: 0.2276
2025-12-15 23:43:24,477: t15.2025.04.13 val PER: 0.2140
2025-12-15 23:43:39,028: Train batch 80200: loss: 1.00 grad norm: 0.42 time: 0.050
2025-12-15 23:43:51,333: Train batch 80400: loss: 0.75 grad norm: 1.13 time: 0.065
2025-12-15 23:44:05,120: Train batch 80600: loss: 0.97 grad norm: 2.66 time: 0.078
2025-12-15 23:44:16,891: Train batch 80800: loss: 0.87 grad norm: 0.81 time: 0.050
2025-12-15 23:44:30,436: Train batch 81000: loss: 0.78 grad norm: 0.57 time: 0.058
2025-12-15 23:44:43,695: Train batch 81200: loss: 0.81 grad norm: 1.37 time: 0.066
2025-12-15 23:44:58,294: Train batch 81400: loss: 0.83 grad norm: 0.23 time: 0.082
2025-12-15 23:45:12,890: Train batch 81600: loss: 0.83 grad norm: 0.64 time: 0.056
2025-12-15 23:45:25,647: Train batch 81800: loss: 0.74 grad norm: 0.66 time: 0.033
2025-12-15 23:45:39,342: Train batch 82000: loss: 0.89 grad norm: 0.59 time: 0.088
2025-12-15 23:45:39,343: Running test after training batch: 82000
2025-12-15 23:45:48,603: Val batch 82000: PER (avg): 0.1344 CTC Loss (avg): 1.1250 time: 9.260
2025-12-15 23:45:48,604: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:45:48,604: t15.2023.08.13 val PER: 0.0998
2025-12-15 23:45:48,604: t15.2023.08.18 val PER: 0.0964
2025-12-15 23:45:48,604: t15.2023.08.20 val PER: 0.0770
2025-12-15 23:45:48,604: t15.2023.08.25 val PER: 0.1084
2025-12-15 23:45:48,604: t15.2023.08.27 val PER: 0.1624
2025-12-15 23:45:48,604: t15.2023.09.01 val PER: 0.0593
2025-12-15 23:45:48,604: t15.2023.09.03 val PER: 0.1378
2025-12-15 23:45:48,604: t15.2023.09.24 val PER: 0.1019
2025-12-15 23:45:48,604: t15.2023.09.29 val PER: 0.1232
2025-12-15 23:45:48,604: t15.2023.10.01 val PER: 0.1605
2025-12-15 23:45:48,604: t15.2023.10.06 val PER: 0.0840
2025-12-15 23:45:48,604: t15.2023.10.08 val PER: 0.1908
2025-12-15 23:45:48,604: t15.2023.10.13 val PER: 0.1963
2025-12-15 23:45:48,604: t15.2023.10.15 val PER: 0.1463
2025-12-15 23:45:48,604: t15.2023.10.20 val PER: 0.1711
2025-12-15 23:45:48,604: t15.2023.10.22 val PER: 0.1203
2025-12-15 23:45:48,605: t15.2023.11.03 val PER: 0.1744
2025-12-15 23:45:48,605: t15.2023.11.04 val PER: 0.0205
2025-12-15 23:45:48,605: t15.2023.11.17 val PER: 0.0513
2025-12-15 23:45:48,605: t15.2023.11.19 val PER: 0.0659
2025-12-15 23:45:48,605: t15.2023.11.26 val PER: 0.1087
2025-12-15 23:45:48,605: t15.2023.12.03 val PER: 0.0798
2025-12-15 23:45:48,605: t15.2023.12.08 val PER: 0.0806
2025-12-15 23:45:48,605: t15.2023.12.10 val PER: 0.0867
2025-12-15 23:45:48,605: t15.2023.12.17 val PER: 0.1403
2025-12-15 23:45:48,605: t15.2023.12.29 val PER: 0.1084
2025-12-15 23:45:48,605: t15.2024.02.25 val PER: 0.0899
2025-12-15 23:45:48,605: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:45:48,605: t15.2024.03.08 val PER: 0.2233
2025-12-15 23:45:48,605: t15.2024.03.15 val PER: 0.1932
2025-12-15 23:45:48,605: t15.2024.03.17 val PER: 0.0962
2025-12-15 23:45:48,605: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:45:48,605: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:45:48,605: t15.2024.05.10 val PER: 0.1590
2025-12-15 23:45:48,605: t15.2024.06.14 val PER: 0.1798
2025-12-15 23:45:48,606: t15.2024.07.19 val PER: 0.1734
2025-12-15 23:45:48,606: t15.2024.07.21 val PER: 0.0897
2025-12-15 23:45:48,606: t15.2024.07.28 val PER: 0.1147
2025-12-15 23:45:48,606: t15.2025.01.10 val PER: 0.2741
2025-12-15 23:45:48,606: t15.2025.01.12 val PER: 0.1101
2025-12-15 23:45:48,606: t15.2025.03.14 val PER: 0.3077
2025-12-15 23:45:48,606: t15.2025.03.16 val PER: 0.1688
2025-12-15 23:45:48,606: t15.2025.03.30 val PER: 0.2425
2025-12-15 23:45:48,606: t15.2025.04.13 val PER: 0.2240
2025-12-15 23:45:48,606: New best test PER 0.1351 --> 0.1344
2025-12-15 23:45:48,606: Checkpointing model
2025-12-15 23:45:49,021: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:46:02,751: Train batch 82200: loss: 0.73 grad norm: 0.62 time: 0.054
2025-12-15 23:46:16,843: Train batch 82400: loss: 0.85 grad norm: 0.15 time: 0.056
2025-12-15 23:46:30,947: Train batch 82600: loss: 0.83 grad norm: 0.55 time: 0.092
2025-12-15 23:46:45,687: Train batch 82800: loss: 0.85 grad norm: 0.43 time: 0.057
2025-12-15 23:47:00,285: Train batch 83000: loss: 0.86 grad norm: 0.85 time: 0.087
2025-12-15 23:47:14,895: Train batch 83200: loss: 0.96 grad norm: 1.53 time: 0.075
2025-12-15 23:47:29,592: Train batch 83400: loss: 0.78 grad norm: 0.45 time: 0.073
2025-12-15 23:47:44,123: Train batch 83600: loss: 0.96 grad norm: 1.23 time: 0.072
2025-12-15 23:47:58,566: Train batch 83800: loss: 0.80 grad norm: 1.05 time: 0.075
2025-12-15 23:48:13,098: Train batch 84000: loss: 0.84 grad norm: 0.60 time: 0.038
2025-12-15 23:48:13,099: Running test after training batch: 84000
2025-12-15 23:48:22,371: Val batch 84000: PER (avg): 0.1363 CTC Loss (avg): 1.1088 time: 9.272
2025-12-15 23:48:22,371: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:48:22,371: t15.2023.08.13 val PER: 0.1060
2025-12-15 23:48:22,371: t15.2023.08.18 val PER: 0.0905
2025-12-15 23:48:22,371: t15.2023.08.20 val PER: 0.0786
2025-12-15 23:48:22,371: t15.2023.08.25 val PER: 0.1009
2025-12-15 23:48:22,371: t15.2023.08.27 val PER: 0.1592
2025-12-15 23:48:22,371: t15.2023.09.01 val PER: 0.0641
2025-12-15 23:48:22,372: t15.2023.09.03 val PER: 0.1449
2025-12-15 23:48:22,372: t15.2023.09.24 val PER: 0.1068
2025-12-15 23:48:22,372: t15.2023.09.29 val PER: 0.1244
2025-12-15 23:48:22,372: t15.2023.10.01 val PER: 0.1704
2025-12-15 23:48:22,372: t15.2023.10.06 val PER: 0.0936
2025-12-15 23:48:22,372: t15.2023.10.08 val PER: 0.1867
2025-12-15 23:48:22,372: t15.2023.10.13 val PER: 0.1963
2025-12-15 23:48:22,372: t15.2023.10.15 val PER: 0.1490
2025-12-15 23:48:22,372: t15.2023.10.20 val PER: 0.1711
2025-12-15 23:48:22,372: t15.2023.10.22 val PER: 0.1214
2025-12-15 23:48:22,372: t15.2023.11.03 val PER: 0.1703
2025-12-15 23:48:22,372: t15.2023.11.04 val PER: 0.0273
2025-12-15 23:48:22,372: t15.2023.11.17 val PER: 0.0451
2025-12-15 23:48:22,372: t15.2023.11.19 val PER: 0.0619
2025-12-15 23:48:22,372: t15.2023.11.26 val PER: 0.1159
2025-12-15 23:48:22,372: t15.2023.12.03 val PER: 0.0746
2025-12-15 23:48:22,372: t15.2023.12.08 val PER: 0.0939
2025-12-15 23:48:22,372: t15.2023.12.10 val PER: 0.0762
2025-12-15 23:48:22,372: t15.2023.12.17 val PER: 0.1424
2025-12-15 23:48:22,372: t15.2023.12.29 val PER: 0.1078
2025-12-15 23:48:22,373: t15.2024.02.25 val PER: 0.0885
2025-12-15 23:48:22,373: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:48:22,373: t15.2024.03.08 val PER: 0.2347
2025-12-15 23:48:22,373: t15.2024.03.15 val PER: 0.2064
2025-12-15 23:48:22,373: t15.2024.03.17 val PER: 0.1046
2025-12-15 23:48:22,373: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:48:22,373: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:48:22,373: t15.2024.05.10 val PER: 0.1560
2025-12-15 23:48:22,373: t15.2024.06.14 val PER: 0.1782
2025-12-15 23:48:22,373: t15.2024.07.19 val PER: 0.1714
2025-12-15 23:48:22,373: t15.2024.07.21 val PER: 0.0897
2025-12-15 23:48:22,373: t15.2024.07.28 val PER: 0.1081
2025-12-15 23:48:22,373: t15.2025.01.10 val PER: 0.2645
2025-12-15 23:48:22,373: t15.2025.01.12 val PER: 0.1216
2025-12-15 23:48:22,373: t15.2025.03.14 val PER: 0.3136
2025-12-15 23:48:22,373: t15.2025.03.16 val PER: 0.1780
2025-12-15 23:48:22,373: t15.2025.03.30 val PER: 0.2391
2025-12-15 23:48:22,373: t15.2025.04.13 val PER: 0.2140
2025-12-15 23:48:35,919: Train batch 84200: loss: 0.95 grad norm: 0.96 time: 0.058
2025-12-15 23:48:50,654: Train batch 84400: loss: 0.82 grad norm: 0.63 time: 0.064
2025-12-15 23:49:02,981: Train batch 84600: loss: 0.84 grad norm: 0.77 time: 0.065
2025-12-15 23:49:15,961: Train batch 84800: loss: 0.86 grad norm: 0.67 time: 0.056
2025-12-15 23:49:29,671: Train batch 85000: loss: 0.88 grad norm: 0.17 time: 0.047
2025-12-15 23:49:43,967: Train batch 85200: loss: 0.79 grad norm: 0.89 time: 0.089
2025-12-15 23:49:58,039: Train batch 85400: loss: 0.83 grad norm: 0.34 time: 0.056
2025-12-15 23:50:11,304: Train batch 85600: loss: 0.80 grad norm: 0.07 time: 0.043
2025-12-15 23:50:24,312: Train batch 85800: loss: 0.95 grad norm: 0.92 time: 0.052
2025-12-15 23:50:36,427: Train batch 86000: loss: 0.72 grad norm: 1.17 time: 0.071
2025-12-15 23:50:36,428: Running test after training batch: 86000
2025-12-15 23:50:45,598: Val batch 86000: PER (avg): 0.1347 CTC Loss (avg): 1.1417 time: 9.169
2025-12-15 23:50:45,598: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:50:45,598: t15.2023.08.13 val PER: 0.1112
2025-12-15 23:50:45,598: t15.2023.08.18 val PER: 0.0872
2025-12-15 23:50:45,598: t15.2023.08.20 val PER: 0.0683
2025-12-15 23:50:45,598: t15.2023.08.25 val PER: 0.1069
2025-12-15 23:50:45,598: t15.2023.08.27 val PER: 0.1768
2025-12-15 23:50:45,598: t15.2023.09.01 val PER: 0.0560
2025-12-15 23:50:45,598: t15.2023.09.03 val PER: 0.1247
2025-12-15 23:50:45,599: t15.2023.09.24 val PER: 0.1032
2025-12-15 23:50:45,599: t15.2023.09.29 val PER: 0.1251
2025-12-15 23:50:45,599: t15.2023.10.01 val PER: 0.1684
2025-12-15 23:50:45,599: t15.2023.10.06 val PER: 0.0904
2025-12-15 23:50:45,599: t15.2023.10.08 val PER: 0.1908
2025-12-15 23:50:45,599: t15.2023.10.13 val PER: 0.1893
2025-12-15 23:50:45,599: t15.2023.10.15 val PER: 0.1404
2025-12-15 23:50:45,599: t15.2023.10.20 val PER: 0.1812
2025-12-15 23:50:45,599: t15.2023.10.22 val PER: 0.1180
2025-12-15 23:50:45,599: t15.2023.11.03 val PER: 0.1662
2025-12-15 23:50:45,599: t15.2023.11.04 val PER: 0.0171
2025-12-15 23:50:45,599: t15.2023.11.17 val PER: 0.0498
2025-12-15 23:50:45,599: t15.2023.11.19 val PER: 0.0499
2025-12-15 23:50:45,599: t15.2023.11.26 val PER: 0.0949
2025-12-15 23:50:45,599: t15.2023.12.03 val PER: 0.0851
2025-12-15 23:50:45,599: t15.2023.12.08 val PER: 0.0839
2025-12-15 23:50:45,599: t15.2023.12.10 val PER: 0.0788
2025-12-15 23:50:45,599: t15.2023.12.17 val PER: 0.1622
2025-12-15 23:50:45,599: t15.2023.12.29 val PER: 0.1160
2025-12-15 23:50:45,600: t15.2024.02.25 val PER: 0.0997
2025-12-15 23:50:45,600: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:50:45,600: t15.2024.03.08 val PER: 0.2248
2025-12-15 23:50:45,600: t15.2024.03.15 val PER: 0.2026
2025-12-15 23:50:45,600: t15.2024.03.17 val PER: 0.1039
2025-12-15 23:50:45,600: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:50:45,600: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:50:45,600: t15.2024.05.10 val PER: 0.1634
2025-12-15 23:50:45,600: t15.2024.06.14 val PER: 0.1767
2025-12-15 23:50:45,600: t15.2024.07.19 val PER: 0.1714
2025-12-15 23:50:45,600: t15.2024.07.21 val PER: 0.0876
2025-12-15 23:50:45,600: t15.2024.07.28 val PER: 0.1132
2025-12-15 23:50:45,600: t15.2025.01.10 val PER: 0.2727
2025-12-15 23:50:45,600: t15.2025.01.12 val PER: 0.1209
2025-12-15 23:50:45,600: t15.2025.03.14 val PER: 0.3092
2025-12-15 23:50:45,600: t15.2025.03.16 val PER: 0.1885
2025-12-15 23:50:45,600: t15.2025.03.30 val PER: 0.2276
2025-12-15 23:50:45,600: t15.2025.04.13 val PER: 0.2054
2025-12-15 23:50:59,477: Train batch 86200: loss: 0.92 grad norm: 1.27 time: 0.059
2025-12-15 23:51:12,593: Train batch 86400: loss: 0.77 grad norm: 0.04 time: 0.067
2025-12-15 23:51:26,468: Train batch 86600: loss: 0.84 grad norm: 0.35 time: 0.045
2025-12-15 23:51:40,925: Train batch 86800: loss: 0.81 grad norm: 0.09 time: 0.063
2025-12-15 23:51:55,205: Train batch 87000: loss: 1.03 grad norm: 0.33 time: 0.056
2025-12-15 23:52:09,595: Train batch 87200: loss: 0.82 grad norm: 1.33 time: 0.081
2025-12-15 23:52:23,590: Train batch 87400: loss: 0.82 grad norm: 1.68 time: 0.050
2025-12-15 23:52:37,826: Train batch 87600: loss: 0.73 grad norm: 0.08 time: 0.056
2025-12-15 23:52:50,534: Train batch 87800: loss: 0.81 grad norm: 0.03 time: 0.087
2025-12-15 23:53:04,979: Train batch 88000: loss: 0.81 grad norm: 0.59 time: 0.063
2025-12-15 23:53:04,980: Running test after training batch: 88000
2025-12-15 23:53:14,135: Val batch 88000: PER (avg): 0.1332 CTC Loss (avg): 1.1186 time: 9.155
2025-12-15 23:53:14,135: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:53:14,135: t15.2023.08.13 val PER: 0.1123
2025-12-15 23:53:14,135: t15.2023.08.18 val PER: 0.0939
2025-12-15 23:53:14,135: t15.2023.08.20 val PER: 0.0747
2025-12-15 23:53:14,136: t15.2023.08.25 val PER: 0.1054
2025-12-15 23:53:14,136: t15.2023.08.27 val PER: 0.1624
2025-12-15 23:53:14,136: t15.2023.09.01 val PER: 0.0609
2025-12-15 23:53:14,136: t15.2023.09.03 val PER: 0.1318
2025-12-15 23:53:14,136: t15.2023.09.24 val PER: 0.1129
2025-12-15 23:53:14,136: t15.2023.09.29 val PER: 0.1244
2025-12-15 23:53:14,136: t15.2023.10.01 val PER: 0.1757
2025-12-15 23:53:14,136: t15.2023.10.06 val PER: 0.1033
2025-12-15 23:53:14,136: t15.2023.10.08 val PER: 0.1881
2025-12-15 23:53:14,136: t15.2023.10.13 val PER: 0.1792
2025-12-15 23:53:14,136: t15.2023.10.15 val PER: 0.1424
2025-12-15 23:53:14,136: t15.2023.10.20 val PER: 0.1611
2025-12-15 23:53:14,136: t15.2023.10.22 val PER: 0.1203
2025-12-15 23:53:14,136: t15.2023.11.03 val PER: 0.1710
2025-12-15 23:53:14,136: t15.2023.11.04 val PER: 0.0239
2025-12-15 23:53:14,136: t15.2023.11.17 val PER: 0.0575
2025-12-15 23:53:14,136: t15.2023.11.19 val PER: 0.0579
2025-12-15 23:53:14,136: t15.2023.11.26 val PER: 0.0877
2025-12-15 23:53:14,136: t15.2023.12.03 val PER: 0.0777
2025-12-15 23:53:14,136: t15.2023.12.08 val PER: 0.0852
2025-12-15 23:53:14,137: t15.2023.12.10 val PER: 0.0802
2025-12-15 23:53:14,137: t15.2023.12.17 val PER: 0.1372
2025-12-15 23:53:14,137: t15.2023.12.29 val PER: 0.1112
2025-12-15 23:53:14,137: t15.2024.02.25 val PER: 0.0913
2025-12-15 23:53:14,137: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:53:14,137: t15.2024.03.08 val PER: 0.2205
2025-12-15 23:53:14,137: t15.2024.03.15 val PER: 0.2026
2025-12-15 23:53:14,137: t15.2024.03.17 val PER: 0.1025
2025-12-15 23:53:14,137: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:53:14,137: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:53:14,137: t15.2024.05.10 val PER: 0.1530
2025-12-15 23:53:14,137: t15.2024.06.14 val PER: 0.1751
2025-12-15 23:53:14,137: t15.2024.07.19 val PER: 0.1575
2025-12-15 23:53:14,137: t15.2024.07.21 val PER: 0.0897
2025-12-15 23:53:14,137: t15.2024.07.28 val PER: 0.1066
2025-12-15 23:53:14,137: t15.2025.01.10 val PER: 0.2741
2025-12-15 23:53:14,137: t15.2025.01.12 val PER: 0.1178
2025-12-15 23:53:14,137: t15.2025.03.14 val PER: 0.2885
2025-12-15 23:53:14,137: t15.2025.03.16 val PER: 0.1793
2025-12-15 23:53:14,138: t15.2025.03.30 val PER: 0.2310
2025-12-15 23:53:14,138: t15.2025.04.13 val PER: 0.2054
2025-12-15 23:53:14,138: New best test PER 0.1344 --> 0.1332
2025-12-15 23:53:14,138: Checkpointing model
2025-12-15 23:53:14,548: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:53:28,990: Train batch 88200: loss: 0.72 grad norm: 0.46 time: 0.058
2025-12-15 23:53:43,571: Train batch 88400: loss: 0.92 grad norm: 0.61 time: 0.056
2025-12-15 23:53:57,102: Train batch 88600: loss: 0.88 grad norm: 0.50 time: 0.084
2025-12-15 23:54:11,396: Train batch 88800: loss: 0.84 grad norm: 0.15 time: 0.059
2025-12-15 23:54:25,413: Train batch 89000: loss: 0.85 grad norm: 0.13 time: 0.068
2025-12-15 23:54:39,088: Train batch 89200: loss: 0.92 grad norm: 0.30 time: 0.036
2025-12-15 23:54:52,058: Train batch 89400: loss: 0.79 grad norm: 0.61 time: 0.050
2025-12-15 23:55:05,059: Train batch 89600: loss: 0.73 grad norm: 0.54 time: 0.069
2025-12-15 23:55:18,987: Train batch 89800: loss: 0.80 grad norm: 0.54 time: 0.057
2025-12-15 23:55:33,675: Train batch 90000: loss: 0.77 grad norm: 0.05 time: 0.054
2025-12-15 23:55:33,676: Running test after training batch: 90000
2025-12-15 23:55:42,877: Val batch 90000: PER (avg): 0.1342 CTC Loss (avg): 1.1022 time: 9.201
2025-12-15 23:55:42,878: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:55:42,878: t15.2023.08.13 val PER: 0.0936
2025-12-15 23:55:42,878: t15.2023.08.18 val PER: 0.0905
2025-12-15 23:55:42,878: t15.2023.08.20 val PER: 0.0755
2025-12-15 23:55:42,878: t15.2023.08.25 val PER: 0.0949
2025-12-15 23:55:42,878: t15.2023.08.27 val PER: 0.1608
2025-12-15 23:55:42,878: t15.2023.09.01 val PER: 0.0601
2025-12-15 23:55:42,878: t15.2023.09.03 val PER: 0.1437
2025-12-15 23:55:42,878: t15.2023.09.24 val PER: 0.1032
2025-12-15 23:55:42,878: t15.2023.09.29 val PER: 0.1257
2025-12-15 23:55:42,878: t15.2023.10.01 val PER: 0.1678
2025-12-15 23:55:42,878: t15.2023.10.06 val PER: 0.1001
2025-12-15 23:55:42,878: t15.2023.10.08 val PER: 0.1786
2025-12-15 23:55:42,879: t15.2023.10.13 val PER: 0.1846
2025-12-15 23:55:42,879: t15.2023.10.15 val PER: 0.1424
2025-12-15 23:55:42,879: t15.2023.10.20 val PER: 0.1913
2025-12-15 23:55:42,879: t15.2023.10.22 val PER: 0.1225
2025-12-15 23:55:42,879: t15.2023.11.03 val PER: 0.1682
2025-12-15 23:55:42,879: t15.2023.11.04 val PER: 0.0273
2025-12-15 23:55:42,879: t15.2023.11.17 val PER: 0.0420
2025-12-15 23:55:42,879: t15.2023.11.19 val PER: 0.0479
2025-12-15 23:55:42,879: t15.2023.11.26 val PER: 0.0906
2025-12-15 23:55:42,879: t15.2023.12.03 val PER: 0.0788
2025-12-15 23:55:42,879: t15.2023.12.08 val PER: 0.0779
2025-12-15 23:55:42,879: t15.2023.12.10 val PER: 0.0880
2025-12-15 23:55:42,879: t15.2023.12.17 val PER: 0.1486
2025-12-15 23:55:42,879: t15.2023.12.29 val PER: 0.1057
2025-12-15 23:55:42,879: t15.2024.02.25 val PER: 0.0997
2025-12-15 23:55:42,879: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:55:42,879: t15.2024.03.08 val PER: 0.2191
2025-12-15 23:55:42,879: t15.2024.03.15 val PER: 0.1895
2025-12-15 23:55:42,879: t15.2024.03.17 val PER: 0.1032
2025-12-15 23:55:42,880: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:55:42,880: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:55:42,880: t15.2024.05.10 val PER: 0.1590
2025-12-15 23:55:42,880: t15.2024.06.14 val PER: 0.1688
2025-12-15 23:55:42,880: t15.2024.07.19 val PER: 0.1760
2025-12-15 23:55:42,880: t15.2024.07.21 val PER: 0.0917
2025-12-15 23:55:42,880: t15.2024.07.28 val PER: 0.1147
2025-12-15 23:55:42,880: t15.2025.01.10 val PER: 0.2782
2025-12-15 23:55:42,880: t15.2025.01.12 val PER: 0.1139
2025-12-15 23:55:42,880: t15.2025.03.14 val PER: 0.3284
2025-12-15 23:55:42,880: t15.2025.03.16 val PER: 0.1846
2025-12-15 23:55:42,880: t15.2025.03.30 val PER: 0.2414
2025-12-15 23:55:42,880: t15.2025.04.13 val PER: 0.2340
2025-12-15 23:55:56,974: Train batch 90200: loss: 0.80 grad norm: 0.97 time: 0.055
2025-12-15 23:56:11,279: Train batch 90400: loss: 0.91 grad norm: 0.07 time: 0.056
2025-12-15 23:56:25,911: Train batch 90600: loss: 0.83 grad norm: 0.46 time: 0.071
2025-12-15 23:56:38,738: Train batch 90800: loss: 0.89 grad norm: 0.49 time: 0.068
2025-12-15 23:56:53,395: Train batch 91000: loss: 0.74 grad norm: 0.69 time: 0.057
2025-12-15 23:57:07,834: Train batch 91200: loss: 0.80 grad norm: 0.66 time: 0.044
2025-12-15 23:57:22,297: Train batch 91400: loss: 0.89 grad norm: 0.40 time: 0.081
2025-12-15 23:57:37,052: Train batch 91600: loss: 0.80 grad norm: 1.17 time: 0.056
2025-12-15 23:57:51,654: Train batch 91800: loss: 0.74 grad norm: 0.48 time: 0.066
2025-12-15 23:58:04,627: Train batch 92000: loss: 0.72 grad norm: 0.77 time: 0.070
2025-12-15 23:58:04,627: Running test after training batch: 92000
2025-12-15 23:58:13,746: Val batch 92000: PER (avg): 0.1316 CTC Loss (avg): 1.1297 time: 9.118
2025-12-15 23:58:13,746: t15.2023.08.11 val PER: 1.0000
2025-12-15 23:58:13,746: t15.2023.08.13 val PER: 0.1019
2025-12-15 23:58:13,746: t15.2023.08.18 val PER: 0.0838
2025-12-15 23:58:13,746: t15.2023.08.20 val PER: 0.0707
2025-12-15 23:58:13,746: t15.2023.08.25 val PER: 0.0979
2025-12-15 23:58:13,747: t15.2023.08.27 val PER: 0.1608
2025-12-15 23:58:13,747: t15.2023.09.01 val PER: 0.0495
2025-12-15 23:58:13,747: t15.2023.09.03 val PER: 0.1378
2025-12-15 23:58:13,747: t15.2023.09.24 val PER: 0.1032
2025-12-15 23:58:13,747: t15.2023.09.29 val PER: 0.1347
2025-12-15 23:58:13,747: t15.2023.10.01 val PER: 0.1664
2025-12-15 23:58:13,747: t15.2023.10.06 val PER: 0.0872
2025-12-15 23:58:13,747: t15.2023.10.08 val PER: 0.1813
2025-12-15 23:58:13,747: t15.2023.10.13 val PER: 0.1777
2025-12-15 23:58:13,747: t15.2023.10.15 val PER: 0.1391
2025-12-15 23:58:13,747: t15.2023.10.20 val PER: 0.1779
2025-12-15 23:58:13,747: t15.2023.10.22 val PER: 0.1091
2025-12-15 23:58:13,747: t15.2023.11.03 val PER: 0.1649
2025-12-15 23:58:13,747: t15.2023.11.04 val PER: 0.0273
2025-12-15 23:58:13,747: t15.2023.11.17 val PER: 0.0358
2025-12-15 23:58:13,747: t15.2023.11.19 val PER: 0.0519
2025-12-15 23:58:13,747: t15.2023.11.26 val PER: 0.1116
2025-12-15 23:58:13,747: t15.2023.12.03 val PER: 0.0840
2025-12-15 23:58:13,748: t15.2023.12.08 val PER: 0.0786
2025-12-15 23:58:13,748: t15.2023.12.10 val PER: 0.0736
2025-12-15 23:58:13,748: t15.2023.12.17 val PER: 0.1528
2025-12-15 23:58:13,748: t15.2023.12.29 val PER: 0.1016
2025-12-15 23:58:13,748: t15.2024.02.25 val PER: 0.1025
2025-12-15 23:58:13,748: t15.2024.03.03 val PER: 1.0000
2025-12-15 23:58:13,748: t15.2024.03.08 val PER: 0.2404
2025-12-15 23:58:13,748: t15.2024.03.15 val PER: 0.2026
2025-12-15 23:58:13,748: t15.2024.03.17 val PER: 0.0934
2025-12-15 23:58:13,748: t15.2024.04.25 val PER: 1.0000
2025-12-15 23:58:13,748: t15.2024.04.28 val PER: 1.0000
2025-12-15 23:58:13,748: t15.2024.05.10 val PER: 0.1560
2025-12-15 23:58:13,748: t15.2024.06.14 val PER: 0.1640
2025-12-15 23:58:13,748: t15.2024.07.19 val PER: 0.1608
2025-12-15 23:58:13,748: t15.2024.07.21 val PER: 0.0938
2025-12-15 23:58:13,748: t15.2024.07.28 val PER: 0.1074
2025-12-15 23:58:13,748: t15.2025.01.10 val PER: 0.2631
2025-12-15 23:58:13,748: t15.2025.01.12 val PER: 0.1109
2025-12-15 23:58:13,748: t15.2025.03.14 val PER: 0.2929
2025-12-15 23:58:13,749: t15.2025.03.16 val PER: 0.1872
2025-12-15 23:58:13,749: t15.2025.03.30 val PER: 0.2333
2025-12-15 23:58:13,749: t15.2025.04.13 val PER: 0.2111
2025-12-15 23:58:13,749: New best test PER 0.1332 --> 0.1316
2025-12-15 23:58:13,749: Checkpointing model
2025-12-15 23:58:14,149: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-15 23:58:28,290: Train batch 92200: loss: 0.75 grad norm: 0.18 time: 0.084
2025-12-15 23:58:42,777: Train batch 92400: loss: 0.80 grad norm: 0.39 time: 0.091
2025-12-15 23:58:57,044: Train batch 92600: loss: 0.76 grad norm: 0.07 time: 0.054
2025-12-15 23:59:11,039: Train batch 92800: loss: 0.82 grad norm: 0.97 time: 0.068
2025-12-15 23:59:25,521: Train batch 93000: loss: 0.78 grad norm: 0.86 time: 0.069
2025-12-15 23:59:40,154: Train batch 93200: loss: 0.89 grad norm: 0.70 time: 0.058
2025-12-15 23:59:54,707: Train batch 93400: loss: 0.76 grad norm: 1.03 time: 0.070
2025-12-16 00:00:09,004: Train batch 93600: loss: 0.89 grad norm: 0.09 time: 0.071
2025-12-16 00:00:23,590: Train batch 93800: loss: 0.96 grad norm: 0.07 time: 0.060
2025-12-16 00:00:38,260: Train batch 94000: loss: 0.86 grad norm: 0.24 time: 0.057
2025-12-16 00:00:38,261: Running test after training batch: 94000
2025-12-16 00:00:47,506: Val batch 94000: PER (avg): 0.1303 CTC Loss (avg): 1.1248 time: 9.245
2025-12-16 00:00:47,506: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:00:47,506: t15.2023.08.13 val PER: 0.0925
2025-12-16 00:00:47,506: t15.2023.08.18 val PER: 0.0939
2025-12-16 00:00:47,507: t15.2023.08.20 val PER: 0.0731
2025-12-16 00:00:47,507: t15.2023.08.25 val PER: 0.0873
2025-12-16 00:00:47,507: t15.2023.08.27 val PER: 0.1463
2025-12-16 00:00:47,507: t15.2023.09.01 val PER: 0.0568
2025-12-16 00:00:47,507: t15.2023.09.03 val PER: 0.1366
2025-12-16 00:00:47,507: t15.2023.09.24 val PER: 0.1032
2025-12-16 00:00:47,507: t15.2023.09.29 val PER: 0.1327
2025-12-16 00:00:47,507: t15.2023.10.01 val PER: 0.1671
2025-12-16 00:00:47,507: t15.2023.10.06 val PER: 0.0936
2025-12-16 00:00:47,507: t15.2023.10.08 val PER: 0.1813
2025-12-16 00:00:47,507: t15.2023.10.13 val PER: 0.1823
2025-12-16 00:00:47,507: t15.2023.10.15 val PER: 0.1411
2025-12-16 00:00:47,507: t15.2023.10.20 val PER: 0.1745
2025-12-16 00:00:47,507: t15.2023.10.22 val PER: 0.0980
2025-12-16 00:00:47,507: t15.2023.11.03 val PER: 0.1682
2025-12-16 00:00:47,507: t15.2023.11.04 val PER: 0.0239
2025-12-16 00:00:47,507: t15.2023.11.17 val PER: 0.0420
2025-12-16 00:00:47,507: t15.2023.11.19 val PER: 0.0599
2025-12-16 00:00:47,507: t15.2023.11.26 val PER: 0.0949
2025-12-16 00:00:47,508: t15.2023.12.03 val PER: 0.0683
2025-12-16 00:00:47,508: t15.2023.12.08 val PER: 0.0759
2025-12-16 00:00:47,508: t15.2023.12.10 val PER: 0.0736
2025-12-16 00:00:47,508: t15.2023.12.17 val PER: 0.1372
2025-12-16 00:00:47,508: t15.2023.12.29 val PER: 0.1009
2025-12-16 00:00:47,508: t15.2024.02.25 val PER: 0.0927
2025-12-16 00:00:47,508: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:00:47,508: t15.2024.03.08 val PER: 0.2219
2025-12-16 00:00:47,508: t15.2024.03.15 val PER: 0.1914
2025-12-16 00:00:47,508: t15.2024.03.17 val PER: 0.0983
2025-12-16 00:00:47,508: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:00:47,508: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:00:47,508: t15.2024.05.10 val PER: 0.1575
2025-12-16 00:00:47,508: t15.2024.06.14 val PER: 0.1672
2025-12-16 00:00:47,508: t15.2024.07.19 val PER: 0.1707
2025-12-16 00:00:47,508: t15.2024.07.21 val PER: 0.0869
2025-12-16 00:00:47,508: t15.2024.07.28 val PER: 0.1125
2025-12-16 00:00:47,508: t15.2025.01.10 val PER: 0.2713
2025-12-16 00:00:47,508: t15.2025.01.12 val PER: 0.1085
2025-12-16 00:00:47,509: t15.2025.03.14 val PER: 0.3107
2025-12-16 00:00:47,509: t15.2025.03.16 val PER: 0.1702
2025-12-16 00:00:47,509: t15.2025.03.30 val PER: 0.2299
2025-12-16 00:00:47,509: t15.2025.04.13 val PER: 0.2225
2025-12-16 00:00:47,509: New best test PER 0.1316 --> 0.1303
2025-12-16 00:00:47,509: Checkpointing model
2025-12-16 00:00:47,925: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-16 00:01:02,503: Train batch 94200: loss: 0.76 grad norm: 0.37 time: 0.062
2025-12-16 00:01:16,715: Train batch 94400: loss: 0.90 grad norm: 0.02 time: 0.070
2025-12-16 00:01:31,222: Train batch 94600: loss: 0.78 grad norm: 0.56 time: 0.073
2025-12-16 00:01:44,730: Train batch 94800: loss: 0.99 grad norm: 0.74 time: 0.054
2025-12-16 00:01:59,129: Train batch 95000: loss: 0.85 grad norm: 0.28 time: 0.066
2025-12-16 00:02:13,697: Train batch 95200: loss: 0.82 grad norm: 0.20 time: 0.087
2025-12-16 00:02:28,399: Train batch 95400: loss: 0.84 grad norm: 0.12 time: 0.085
2025-12-16 00:02:42,958: Train batch 95600: loss: 0.82 grad norm: 0.89 time: 0.064
2025-12-16 00:02:57,423: Train batch 95800: loss: 0.86 grad norm: 0.27 time: 0.076
2025-12-16 00:03:11,290: Train batch 96000: loss: 0.85 grad norm: 0.42 time: 0.075
2025-12-16 00:03:11,290: Running test after training batch: 96000
2025-12-16 00:03:20,329: Val batch 96000: PER (avg): 0.1322 CTC Loss (avg): 1.1691 time: 9.039
2025-12-16 00:03:20,329: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:03:20,329: t15.2023.08.13 val PER: 0.1008
2025-12-16 00:03:20,329: t15.2023.08.18 val PER: 0.0947
2025-12-16 00:03:20,329: t15.2023.08.20 val PER: 0.0755
2025-12-16 00:03:20,329: t15.2023.08.25 val PER: 0.0889
2025-12-16 00:03:20,329: t15.2023.08.27 val PER: 0.1543
2025-12-16 00:03:20,329: t15.2023.09.01 val PER: 0.0536
2025-12-16 00:03:20,329: t15.2023.09.03 val PER: 0.1271
2025-12-16 00:03:20,329: t15.2023.09.24 val PER: 0.1044
2025-12-16 00:03:20,329: t15.2023.09.29 val PER: 0.1327
2025-12-16 00:03:20,329: t15.2023.10.01 val PER: 0.1658
2025-12-16 00:03:20,329: t15.2023.10.06 val PER: 0.0893
2025-12-16 00:03:20,329: t15.2023.10.08 val PER: 0.1813
2025-12-16 00:03:20,330: t15.2023.10.13 val PER: 0.1800
2025-12-16 00:03:20,330: t15.2023.10.15 val PER: 0.1417
2025-12-16 00:03:20,330: t15.2023.10.20 val PER: 0.1611
2025-12-16 00:03:20,330: t15.2023.10.22 val PER: 0.1036
2025-12-16 00:03:20,330: t15.2023.11.03 val PER: 0.1662
2025-12-16 00:03:20,330: t15.2023.11.04 val PER: 0.0137
2025-12-16 00:03:20,330: t15.2023.11.17 val PER: 0.0389
2025-12-16 00:03:20,330: t15.2023.11.19 val PER: 0.0599
2025-12-16 00:03:20,330: t15.2023.11.26 val PER: 0.0949
2025-12-16 00:03:20,330: t15.2023.12.03 val PER: 0.0683
2025-12-16 00:03:20,330: t15.2023.12.08 val PER: 0.0799
2025-12-16 00:03:20,330: t15.2023.12.10 val PER: 0.0788
2025-12-16 00:03:20,330: t15.2023.12.17 val PER: 0.1383
2025-12-16 00:03:20,330: t15.2023.12.29 val PER: 0.1139
2025-12-16 00:03:20,330: t15.2024.02.25 val PER: 0.0969
2025-12-16 00:03:20,330: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:03:20,330: t15.2024.03.08 val PER: 0.2219
2025-12-16 00:03:20,330: t15.2024.03.15 val PER: 0.1982
2025-12-16 00:03:20,330: t15.2024.03.17 val PER: 0.1018
2025-12-16 00:03:20,330: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:03:20,331: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:03:20,331: t15.2024.05.10 val PER: 0.1560
2025-12-16 00:03:20,331: t15.2024.06.14 val PER: 0.1751
2025-12-16 00:03:20,331: t15.2024.07.19 val PER: 0.1661
2025-12-16 00:03:20,331: t15.2024.07.21 val PER: 0.0924
2025-12-16 00:03:20,331: t15.2024.07.28 val PER: 0.1191
2025-12-16 00:03:20,331: t15.2025.01.10 val PER: 0.2727
2025-12-16 00:03:20,331: t15.2025.01.12 val PER: 0.1124
2025-12-16 00:03:20,331: t15.2025.03.14 val PER: 0.3092
2025-12-16 00:03:20,331: t15.2025.03.16 val PER: 0.1832
2025-12-16 00:03:20,331: t15.2025.03.30 val PER: 0.2379
2025-12-16 00:03:20,331: t15.2025.04.13 val PER: 0.2225
2025-12-16 00:03:34,568: Train batch 96200: loss: 0.87 grad norm: 0.05 time: 0.084
2025-12-16 00:03:48,170: Train batch 96400: loss: 0.84 grad norm: 0.85 time: 0.082
2025-12-16 00:04:02,835: Train batch 96600: loss: 0.77 grad norm: 0.45 time: 0.071
2025-12-16 00:04:16,980: Train batch 96800: loss: 0.74 grad norm: 0.03 time: 0.054
2025-12-16 00:04:31,482: Train batch 97000: loss: 0.90 grad norm: 0.45 time: 0.070
2025-12-16 00:04:45,678: Train batch 97200: loss: 1.00 grad norm: 2.15 time: 0.056
2025-12-16 00:04:59,710: Train batch 97400: loss: 0.88 grad norm: 0.23 time: 0.067
2025-12-16 00:05:13,074: Train batch 97600: loss: 0.76 grad norm: 0.40 time: 0.069
2025-12-16 00:05:27,174: Train batch 97800: loss: 0.86 grad norm: 0.88 time: 0.062
2025-12-16 00:05:41,308: Train batch 98000: loss: 0.88 grad norm: 0.44 time: 0.054
2025-12-16 00:05:41,309: Running test after training batch: 98000
2025-12-16 00:05:50,552: Val batch 98000: PER (avg): 0.1307 CTC Loss (avg): 1.1776 time: 9.243
2025-12-16 00:05:50,552: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:05:50,552: t15.2023.08.13 val PER: 0.0988
2025-12-16 00:05:50,552: t15.2023.08.18 val PER: 0.0914
2025-12-16 00:05:50,552: t15.2023.08.20 val PER: 0.0707
2025-12-16 00:05:50,552: t15.2023.08.25 val PER: 0.0934
2025-12-16 00:05:50,552: t15.2023.08.27 val PER: 0.1656
2025-12-16 00:05:50,552: t15.2023.09.01 val PER: 0.0471
2025-12-16 00:05:50,552: t15.2023.09.03 val PER: 0.1318
2025-12-16 00:05:50,552: t15.2023.09.24 val PER: 0.1007
2025-12-16 00:05:50,552: t15.2023.09.29 val PER: 0.1308
2025-12-16 00:05:50,552: t15.2023.10.01 val PER: 0.1651
2025-12-16 00:05:50,552: t15.2023.10.06 val PER: 0.0915
2025-12-16 00:05:50,553: t15.2023.10.08 val PER: 0.1976
2025-12-16 00:05:50,553: t15.2023.10.13 val PER: 0.1792
2025-12-16 00:05:50,553: t15.2023.10.15 val PER: 0.1397
2025-12-16 00:05:50,553: t15.2023.10.20 val PER: 0.1980
2025-12-16 00:05:50,553: t15.2023.10.22 val PER: 0.1125
2025-12-16 00:05:50,553: t15.2023.11.03 val PER: 0.1655
2025-12-16 00:05:50,553: t15.2023.11.04 val PER: 0.0137
2025-12-16 00:05:50,553: t15.2023.11.17 val PER: 0.0435
2025-12-16 00:05:50,553: t15.2023.11.19 val PER: 0.0659
2025-12-16 00:05:50,553: t15.2023.11.26 val PER: 0.1043
2025-12-16 00:05:50,553: t15.2023.12.03 val PER: 0.0798
2025-12-16 00:05:50,553: t15.2023.12.08 val PER: 0.0859
2025-12-16 00:05:50,553: t15.2023.12.10 val PER: 0.0696
2025-12-16 00:05:50,553: t15.2023.12.17 val PER: 0.1414
2025-12-16 00:05:50,553: t15.2023.12.29 val PER: 0.1112
2025-12-16 00:05:50,553: t15.2024.02.25 val PER: 0.0913
2025-12-16 00:05:50,553: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:05:50,553: t15.2024.03.08 val PER: 0.2376
2025-12-16 00:05:50,553: t15.2024.03.15 val PER: 0.1882
2025-12-16 00:05:50,553: t15.2024.03.17 val PER: 0.0962
2025-12-16 00:05:50,554: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:05:50,554: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:05:50,554: t15.2024.05.10 val PER: 0.1367
2025-12-16 00:05:50,554: t15.2024.06.14 val PER: 0.1656
2025-12-16 00:05:50,554: t15.2024.07.19 val PER: 0.1490
2025-12-16 00:05:50,554: t15.2024.07.21 val PER: 0.0869
2025-12-16 00:05:50,554: t15.2024.07.28 val PER: 0.1037
2025-12-16 00:05:50,554: t15.2025.01.10 val PER: 0.2741
2025-12-16 00:05:50,554: t15.2025.01.12 val PER: 0.1078
2025-12-16 00:05:50,554: t15.2025.03.14 val PER: 0.3003
2025-12-16 00:05:50,554: t15.2025.03.16 val PER: 0.1780
2025-12-16 00:05:50,554: t15.2025.03.30 val PER: 0.2414
2025-12-16 00:05:50,554: t15.2025.04.13 val PER: 0.2183
2025-12-16 00:06:04,570: Train batch 98200: loss: 0.81 grad norm: 1.35 time: 0.069
2025-12-16 00:06:18,320: Train batch 98400: loss: 0.76 grad norm: 0.03 time: 0.049
2025-12-16 00:06:29,379: Train batch 98600: loss: 0.90 grad norm: 0.07 time: 0.052
2025-12-16 00:06:41,389: Train batch 98800: loss: 0.93 grad norm: 0.61 time: 0.070
2025-12-16 00:06:56,172: Train batch 99000: loss: 0.74 grad norm: 0.69 time: 0.065
2025-12-16 00:07:10,940: Train batch 99200: loss: 0.76 grad norm: 0.59 time: 0.070
2025-12-16 00:07:24,317: Train batch 99400: loss: 0.70 grad norm: 0.35 time: 0.040
2025-12-16 00:07:36,547: Train batch 99600: loss: 0.73 grad norm: 0.18 time: 0.071
2025-12-16 00:07:50,695: Train batch 99800: loss: 0.80 grad norm: 0.71 time: 0.058
2025-12-16 00:08:05,127: Train batch 100000: loss: 0.86 grad norm: 0.25 time: 0.051
2025-12-16 00:08:05,128: Running test after training batch: 100000
2025-12-16 00:08:14,226: Val batch 100000: PER (avg): 0.1312 CTC Loss (avg): 1.1915 time: 9.098
2025-12-16 00:08:14,227: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:08:14,227: t15.2023.08.13 val PER: 0.0925
2025-12-16 00:08:14,227: t15.2023.08.18 val PER: 0.0880
2025-12-16 00:08:14,227: t15.2023.08.20 val PER: 0.0747
2025-12-16 00:08:14,227: t15.2023.08.25 val PER: 0.0768
2025-12-16 00:08:14,227: t15.2023.08.27 val PER: 0.1592
2025-12-16 00:08:14,227: t15.2023.09.01 val PER: 0.0609
2025-12-16 00:08:14,227: t15.2023.09.03 val PER: 0.1271
2025-12-16 00:08:14,227: t15.2023.09.24 val PER: 0.0959
2025-12-16 00:08:14,227: t15.2023.09.29 val PER: 0.1264
2025-12-16 00:08:14,227: t15.2023.10.01 val PER: 0.1605
2025-12-16 00:08:14,227: t15.2023.10.06 val PER: 0.0947
2025-12-16 00:08:14,227: t15.2023.10.08 val PER: 0.1962
2025-12-16 00:08:14,227: t15.2023.10.13 val PER: 0.1870
2025-12-16 00:08:14,227: t15.2023.10.15 val PER: 0.1371
2025-12-16 00:08:14,227: t15.2023.10.20 val PER: 0.1611
2025-12-16 00:08:14,228: t15.2023.10.22 val PER: 0.1058
2025-12-16 00:08:14,228: t15.2023.11.03 val PER: 0.1628
2025-12-16 00:08:14,228: t15.2023.11.04 val PER: 0.0137
2025-12-16 00:08:14,228: t15.2023.11.17 val PER: 0.0435
2025-12-16 00:08:14,228: t15.2023.11.19 val PER: 0.0619
2025-12-16 00:08:14,228: t15.2023.11.26 val PER: 0.1065
2025-12-16 00:08:14,228: t15.2023.12.03 val PER: 0.0767
2025-12-16 00:08:14,228: t15.2023.12.08 val PER: 0.0839
2025-12-16 00:08:14,228: t15.2023.12.10 val PER: 0.0762
2025-12-16 00:08:14,228: t15.2023.12.17 val PER: 0.1663
2025-12-16 00:08:14,228: t15.2023.12.29 val PER: 0.1084
2025-12-16 00:08:14,228: t15.2024.02.25 val PER: 0.1011
2025-12-16 00:08:14,228: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:08:14,228: t15.2024.03.08 val PER: 0.2361
2025-12-16 00:08:14,228: t15.2024.03.15 val PER: 0.1845
2025-12-16 00:08:14,228: t15.2024.03.17 val PER: 0.1039
2025-12-16 00:08:14,228: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:08:14,228: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:08:14,228: t15.2024.05.10 val PER: 0.1530
2025-12-16 00:08:14,228: t15.2024.06.14 val PER: 0.1498
2025-12-16 00:08:14,229: t15.2024.07.19 val PER: 0.1655
2025-12-16 00:08:14,229: t15.2024.07.21 val PER: 0.0862
2025-12-16 00:08:14,229: t15.2024.07.28 val PER: 0.1037
2025-12-16 00:08:14,229: t15.2025.01.10 val PER: 0.2617
2025-12-16 00:08:14,229: t15.2025.01.12 val PER: 0.1155
2025-12-16 00:08:14,229: t15.2025.03.14 val PER: 0.3062
2025-12-16 00:08:14,229: t15.2025.03.16 val PER: 0.1859
2025-12-16 00:08:14,229: t15.2025.03.30 val PER: 0.2391
2025-12-16 00:08:14,229: t15.2025.04.13 val PER: 0.2097
2025-12-16 00:08:28,771: Train batch 100200: loss: 0.91 grad norm: 0.03 time: 0.068
2025-12-16 00:08:43,328: Train batch 100400: loss: 0.84 grad norm: 0.07 time: 0.059
2025-12-16 00:08:57,697: Train batch 100600: loss: 0.87 grad norm: 0.65 time: 0.056
2025-12-16 00:09:12,114: Train batch 100800: loss: 0.83 grad norm: 0.25 time: 0.068
2025-12-16 00:09:26,990: Train batch 101000: loss: 0.89 grad norm: 0.07 time: 0.099
2025-12-16 00:09:41,430: Train batch 101200: loss: 0.93 grad norm: 0.48 time: 0.066
2025-12-16 00:09:54,977: Train batch 101400: loss: 0.77 grad norm: 0.10 time: 0.059
2025-12-16 00:10:08,457: Train batch 101600: loss: 0.92 grad norm: 0.35 time: 0.030
2025-12-16 00:10:19,435: Train batch 101800: loss: 0.81 grad norm: 0.33 time: 0.039
2025-12-16 00:10:32,479: Train batch 102000: loss: 0.77 grad norm: 0.68 time: 0.060
2025-12-16 00:10:32,479: Running test after training batch: 102000
2025-12-16 00:10:41,667: Val batch 102000: PER (avg): 0.1305 CTC Loss (avg): 1.2048 time: 9.188
2025-12-16 00:10:41,667: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:10:41,667: t15.2023.08.13 val PER: 0.0894
2025-12-16 00:10:41,667: t15.2023.08.18 val PER: 0.0956
2025-12-16 00:10:41,667: t15.2023.08.20 val PER: 0.0667
2025-12-16 00:10:41,667: t15.2023.08.25 val PER: 0.0828
2025-12-16 00:10:41,667: t15.2023.08.27 val PER: 0.1672
2025-12-16 00:10:41,667: t15.2023.09.01 val PER: 0.0519
2025-12-16 00:10:41,667: t15.2023.09.03 val PER: 0.1271
2025-12-16 00:10:41,667: t15.2023.09.24 val PER: 0.1007
2025-12-16 00:10:41,667: t15.2023.09.29 val PER: 0.1295
2025-12-16 00:10:41,667: t15.2023.10.01 val PER: 0.1664
2025-12-16 00:10:41,668: t15.2023.10.06 val PER: 0.0840
2025-12-16 00:10:41,668: t15.2023.10.08 val PER: 0.1867
2025-12-16 00:10:41,668: t15.2023.10.13 val PER: 0.1753
2025-12-16 00:10:41,668: t15.2023.10.15 val PER: 0.1384
2025-12-16 00:10:41,668: t15.2023.10.20 val PER: 0.1779
2025-12-16 00:10:41,668: t15.2023.10.22 val PER: 0.1125
2025-12-16 00:10:41,668: t15.2023.11.03 val PER: 0.1608
2025-12-16 00:10:41,668: t15.2023.11.04 val PER: 0.0137
2025-12-16 00:10:41,668: t15.2023.11.17 val PER: 0.0420
2025-12-16 00:10:41,668: t15.2023.11.19 val PER: 0.0599
2025-12-16 00:10:41,668: t15.2023.11.26 val PER: 0.1080
2025-12-16 00:10:41,668: t15.2023.12.03 val PER: 0.0809
2025-12-16 00:10:41,668: t15.2023.12.08 val PER: 0.0832
2025-12-16 00:10:41,668: t15.2023.12.10 val PER: 0.0710
2025-12-16 00:10:41,668: t15.2023.12.17 val PER: 0.1497
2025-12-16 00:10:41,668: t15.2023.12.29 val PER: 0.1091
2025-12-16 00:10:41,668: t15.2024.02.25 val PER: 0.0885
2025-12-16 00:10:41,668: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:10:41,668: t15.2024.03.08 val PER: 0.2319
2025-12-16 00:10:41,669: t15.2024.03.15 val PER: 0.1895
2025-12-16 00:10:41,669: t15.2024.03.17 val PER: 0.0969
2025-12-16 00:10:41,669: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:10:41,669: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:10:41,669: t15.2024.05.10 val PER: 0.1441
2025-12-16 00:10:41,669: t15.2024.06.14 val PER: 0.1562
2025-12-16 00:10:41,669: t15.2024.07.19 val PER: 0.1648
2025-12-16 00:10:41,669: t15.2024.07.21 val PER: 0.0821
2025-12-16 00:10:41,669: t15.2024.07.28 val PER: 0.1051
2025-12-16 00:10:41,669: t15.2025.01.10 val PER: 0.2782
2025-12-16 00:10:41,669: t15.2025.01.12 val PER: 0.1085
2025-12-16 00:10:41,669: t15.2025.03.14 val PER: 0.3047
2025-12-16 00:10:41,669: t15.2025.03.16 val PER: 0.1976
2025-12-16 00:10:41,669: t15.2025.03.30 val PER: 0.2425
2025-12-16 00:10:41,669: t15.2025.04.13 val PER: 0.2140
2025-12-16 00:10:53,821: Train batch 102200: loss: 0.91 grad norm: 1.32 time: 0.067
2025-12-16 00:11:08,413: Train batch 102400: loss: 0.87 grad norm: 0.01 time: 0.058
2025-12-16 00:11:22,858: Train batch 102600: loss: 0.94 grad norm: 0.83 time: 0.059
2025-12-16 00:11:37,225: Train batch 102800: loss: 0.87 grad norm: 0.04 time: 0.055
2025-12-16 00:11:48,520: Train batch 103000: loss: 0.84 grad norm: 1.00 time: 0.069
2025-12-16 00:12:02,362: Train batch 103200: loss: 0.83 grad norm: 0.96 time: 0.073
2025-12-16 00:12:15,749: Train batch 103400: loss: 0.71 grad norm: 0.48 time: 0.057
2025-12-16 00:12:29,086: Train batch 103600: loss: 0.97 grad norm: 0.11 time: 0.069
2025-12-16 00:12:43,508: Train batch 103800: loss: 0.93 grad norm: 0.26 time: 0.057
2025-12-16 00:12:57,795: Train batch 104000: loss: 0.91 grad norm: 0.75 time: 0.056
2025-12-16 00:12:57,795: Running test after training batch: 104000
2025-12-16 00:13:07,033: Val batch 104000: PER (avg): 0.1306 CTC Loss (avg): 1.1641 time: 9.237
2025-12-16 00:13:07,033: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:13:07,033: t15.2023.08.13 val PER: 0.0894
2025-12-16 00:13:07,033: t15.2023.08.18 val PER: 0.0956
2025-12-16 00:13:07,033: t15.2023.08.20 val PER: 0.0683
2025-12-16 00:13:07,033: t15.2023.08.25 val PER: 0.0873
2025-12-16 00:13:07,033: t15.2023.08.27 val PER: 0.1527
2025-12-16 00:13:07,033: t15.2023.09.01 val PER: 0.0552
2025-12-16 00:13:07,033: t15.2023.09.03 val PER: 0.1318
2025-12-16 00:13:07,034: t15.2023.09.24 val PER: 0.1068
2025-12-16 00:13:07,034: t15.2023.09.29 val PER: 0.1321
2025-12-16 00:13:07,034: t15.2023.10.01 val PER: 0.1697
2025-12-16 00:13:07,034: t15.2023.10.06 val PER: 0.0797
2025-12-16 00:13:07,034: t15.2023.10.08 val PER: 0.1894
2025-12-16 00:13:07,034: t15.2023.10.13 val PER: 0.1738
2025-12-16 00:13:07,034: t15.2023.10.15 val PER: 0.1490
2025-12-16 00:13:07,034: t15.2023.10.20 val PER: 0.1812
2025-12-16 00:13:07,034: t15.2023.10.22 val PER: 0.1080
2025-12-16 00:13:07,034: t15.2023.11.03 val PER: 0.1581
2025-12-16 00:13:07,034: t15.2023.11.04 val PER: 0.0137
2025-12-16 00:13:07,034: t15.2023.11.17 val PER: 0.0404
2025-12-16 00:13:07,034: t15.2023.11.19 val PER: 0.0659
2025-12-16 00:13:07,034: t15.2023.11.26 val PER: 0.0993
2025-12-16 00:13:07,034: t15.2023.12.03 val PER: 0.0798
2025-12-16 00:13:07,034: t15.2023.12.08 val PER: 0.0866
2025-12-16 00:13:07,034: t15.2023.12.10 val PER: 0.0657
2025-12-16 00:13:07,034: t15.2023.12.17 val PER: 0.1518
2025-12-16 00:13:07,034: t15.2023.12.29 val PER: 0.1043
2025-12-16 00:13:07,035: t15.2024.02.25 val PER: 0.0983
2025-12-16 00:13:07,035: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:13:07,035: t15.2024.03.08 val PER: 0.2376
2025-12-16 00:13:07,035: t15.2024.03.15 val PER: 0.1932
2025-12-16 00:13:07,035: t15.2024.03.17 val PER: 0.0997
2025-12-16 00:13:07,035: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:13:07,035: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:13:07,035: t15.2024.05.10 val PER: 0.1367
2025-12-16 00:13:07,035: t15.2024.06.14 val PER: 0.1609
2025-12-16 00:13:07,035: t15.2024.07.19 val PER: 0.1727
2025-12-16 00:13:07,035: t15.2024.07.21 val PER: 0.0862
2025-12-16 00:13:07,035: t15.2024.07.28 val PER: 0.1015
2025-12-16 00:13:07,035: t15.2025.01.10 val PER: 0.2631
2025-12-16 00:13:07,035: t15.2025.01.12 val PER: 0.1062
2025-12-16 00:13:07,035: t15.2025.03.14 val PER: 0.2944
2025-12-16 00:13:07,035: t15.2025.03.16 val PER: 0.1885
2025-12-16 00:13:07,035: t15.2025.03.30 val PER: 0.2425
2025-12-16 00:13:07,035: t15.2025.04.13 val PER: 0.2040
2025-12-16 00:13:21,419: Train batch 104200: loss: 0.92 grad norm: 2.10 time: 0.078
2025-12-16 00:13:35,915: Train batch 104400: loss: 0.87 grad norm: 0.89 time: 0.080
2025-12-16 00:13:50,578: Train batch 104600: loss: 0.75 grad norm: 0.68 time: 0.085
2025-12-16 00:14:05,086: Train batch 104800: loss: 0.72 grad norm: 0.04 time: 0.058
2025-12-16 00:14:18,686: Train batch 105000: loss: 0.87 grad norm: 0.44 time: 0.067
2025-12-16 00:14:33,198: Train batch 105200: loss: 0.90 grad norm: 0.43 time: 0.053
2025-12-16 00:14:47,022: Train batch 105400: loss: 0.81 grad norm: 0.28 time: 0.072
2025-12-16 00:15:01,616: Train batch 105600: loss: 0.83 grad norm: 0.35 time: 0.062
2025-12-16 00:15:16,189: Train batch 105800: loss: 0.83 grad norm: 0.07 time: 0.070
2025-12-16 00:15:30,682: Train batch 106000: loss: 0.80 grad norm: 0.55 time: 0.083
2025-12-16 00:15:30,683: Running test after training batch: 106000
2025-12-16 00:15:39,858: Val batch 106000: PER (avg): 0.1292 CTC Loss (avg): 1.1723 time: 9.175
2025-12-16 00:15:39,859: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:15:39,859: t15.2023.08.13 val PER: 0.0946
2025-12-16 00:15:39,859: t15.2023.08.18 val PER: 0.0981
2025-12-16 00:15:39,859: t15.2023.08.20 val PER: 0.0707
2025-12-16 00:15:39,859: t15.2023.08.25 val PER: 0.0873
2025-12-16 00:15:39,859: t15.2023.08.27 val PER: 0.1559
2025-12-16 00:15:39,859: t15.2023.09.01 val PER: 0.0560
2025-12-16 00:15:39,859: t15.2023.09.03 val PER: 0.1306
2025-12-16 00:15:39,859: t15.2023.09.24 val PER: 0.1044
2025-12-16 00:15:39,859: t15.2023.09.29 val PER: 0.1276
2025-12-16 00:15:39,859: t15.2023.10.01 val PER: 0.1612
2025-12-16 00:15:39,859: t15.2023.10.06 val PER: 0.0786
2025-12-16 00:15:39,859: t15.2023.10.08 val PER: 0.1881
2025-12-16 00:15:39,859: t15.2023.10.13 val PER: 0.1877
2025-12-16 00:15:39,859: t15.2023.10.15 val PER: 0.1417
2025-12-16 00:15:39,859: t15.2023.10.20 val PER: 0.1745
2025-12-16 00:15:39,859: t15.2023.10.22 val PER: 0.1013
2025-12-16 00:15:39,859: t15.2023.11.03 val PER: 0.1608
2025-12-16 00:15:39,860: t15.2023.11.04 val PER: 0.0102
2025-12-16 00:15:39,860: t15.2023.11.17 val PER: 0.0404
2025-12-16 00:15:39,860: t15.2023.11.19 val PER: 0.0619
2025-12-16 00:15:39,860: t15.2023.11.26 val PER: 0.0993
2025-12-16 00:15:39,860: t15.2023.12.03 val PER: 0.0809
2025-12-16 00:15:39,860: t15.2023.12.08 val PER: 0.0812
2025-12-16 00:15:39,860: t15.2023.12.10 val PER: 0.0788
2025-12-16 00:15:39,860: t15.2023.12.17 val PER: 0.1538
2025-12-16 00:15:39,860: t15.2023.12.29 val PER: 0.0988
2025-12-16 00:15:39,860: t15.2024.02.25 val PER: 0.0955
2025-12-16 00:15:39,860: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:15:39,860: t15.2024.03.08 val PER: 0.2276
2025-12-16 00:15:39,860: t15.2024.03.15 val PER: 0.1870
2025-12-16 00:15:39,860: t15.2024.03.17 val PER: 0.0955
2025-12-16 00:15:39,860: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:15:39,860: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:15:39,860: t15.2024.05.10 val PER: 0.1426
2025-12-16 00:15:39,860: t15.2024.06.14 val PER: 0.1577
2025-12-16 00:15:39,860: t15.2024.07.19 val PER: 0.1707
2025-12-16 00:15:39,861: t15.2024.07.21 val PER: 0.0841
2025-12-16 00:15:39,861: t15.2024.07.28 val PER: 0.0993
2025-12-16 00:15:39,861: t15.2025.01.10 val PER: 0.2645
2025-12-16 00:15:39,861: t15.2025.01.12 val PER: 0.1039
2025-12-16 00:15:39,861: t15.2025.03.14 val PER: 0.2914
2025-12-16 00:15:39,861: t15.2025.03.16 val PER: 0.1780
2025-12-16 00:15:39,861: t15.2025.03.30 val PER: 0.2402
2025-12-16 00:15:39,861: t15.2025.04.13 val PER: 0.2083
2025-12-16 00:15:39,861: New best test PER 0.1303 --> 0.1292
2025-12-16 00:15:39,861: Checkpointing model
2025-12-16 00:15:40,294: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-16 00:15:50,819: Train batch 106200: loss: 0.80 grad norm: 0.79 time: 0.068
2025-12-16 00:16:05,397: Train batch 106400: loss: 0.73 grad norm: 0.81 time: 0.067
2025-12-16 00:16:20,126: Train batch 106600: loss: 0.79 grad norm: 0.77 time: 0.068
2025-12-16 00:16:34,464: Train batch 106800: loss: 0.87 grad norm: 0.09 time: 0.080
2025-12-16 00:16:49,032: Train batch 107000: loss: 0.90 grad norm: 0.26 time: 0.056
2025-12-16 00:17:03,391: Train batch 107200: loss: 0.87 grad norm: 0.12 time: 0.085
2025-12-16 00:17:17,974: Train batch 107400: loss: 0.87 grad norm: 0.02 time: 0.061
2025-12-16 00:17:31,731: Train batch 107600: loss: 0.82 grad norm: 0.16 time: 0.031
2025-12-16 00:17:46,134: Train batch 107800: loss: 0.87 grad norm: 0.80 time: 0.057
2025-12-16 00:18:00,644: Train batch 108000: loss: 0.81 grad norm: 0.14 time: 0.069
2025-12-16 00:18:00,644: Running test after training batch: 108000
2025-12-16 00:18:09,872: Val batch 108000: PER (avg): 0.1294 CTC Loss (avg): 1.1867 time: 9.227
2025-12-16 00:18:09,872: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:18:09,872: t15.2023.08.13 val PER: 0.0915
2025-12-16 00:18:09,872: t15.2023.08.18 val PER: 0.0930
2025-12-16 00:18:09,872: t15.2023.08.20 val PER: 0.0643
2025-12-16 00:18:09,872: t15.2023.08.25 val PER: 0.0843
2025-12-16 00:18:09,872: t15.2023.08.27 val PER: 0.1543
2025-12-16 00:18:09,872: t15.2023.09.01 val PER: 0.0552
2025-12-16 00:18:09,872: t15.2023.09.03 val PER: 0.1330
2025-12-16 00:18:09,873: t15.2023.09.24 val PER: 0.1007
2025-12-16 00:18:09,873: t15.2023.09.29 val PER: 0.1283
2025-12-16 00:18:09,873: t15.2023.10.01 val PER: 0.1579
2025-12-16 00:18:09,873: t15.2023.10.06 val PER: 0.0915
2025-12-16 00:18:09,873: t15.2023.10.08 val PER: 0.1935
2025-12-16 00:18:09,873: t15.2023.10.13 val PER: 0.1823
2025-12-16 00:18:09,873: t15.2023.10.15 val PER: 0.1411
2025-12-16 00:18:09,873: t15.2023.10.20 val PER: 0.1711
2025-12-16 00:18:09,873: t15.2023.10.22 val PER: 0.1069
2025-12-16 00:18:09,873: t15.2023.11.03 val PER: 0.1547
2025-12-16 00:18:09,873: t15.2023.11.04 val PER: 0.0239
2025-12-16 00:18:09,873: t15.2023.11.17 val PER: 0.0420
2025-12-16 00:18:09,873: t15.2023.11.19 val PER: 0.0739
2025-12-16 00:18:09,873: t15.2023.11.26 val PER: 0.1022
2025-12-16 00:18:09,873: t15.2023.12.03 val PER: 0.0756
2025-12-16 00:18:09,873: t15.2023.12.08 val PER: 0.0792
2025-12-16 00:18:09,873: t15.2023.12.10 val PER: 0.0736
2025-12-16 00:18:09,873: t15.2023.12.17 val PER: 0.1518
2025-12-16 00:18:09,873: t15.2023.12.29 val PER: 0.1084
2025-12-16 00:18:09,874: t15.2024.02.25 val PER: 0.0969
2025-12-16 00:18:09,874: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:18:09,874: t15.2024.03.08 val PER: 0.2219
2025-12-16 00:18:09,874: t15.2024.03.15 val PER: 0.1864
2025-12-16 00:18:09,874: t15.2024.03.17 val PER: 0.0962
2025-12-16 00:18:09,874: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:18:09,874: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:18:09,874: t15.2024.05.10 val PER: 0.1426
2025-12-16 00:18:09,874: t15.2024.06.14 val PER: 0.1530
2025-12-16 00:18:09,874: t15.2024.07.19 val PER: 0.1734
2025-12-16 00:18:09,874: t15.2024.07.21 val PER: 0.0848
2025-12-16 00:18:09,874: t15.2024.07.28 val PER: 0.0985
2025-12-16 00:18:09,874: t15.2025.01.10 val PER: 0.2741
2025-12-16 00:18:09,874: t15.2025.01.12 val PER: 0.1070
2025-12-16 00:18:09,874: t15.2025.03.14 val PER: 0.2840
2025-12-16 00:18:09,874: t15.2025.03.16 val PER: 0.1793
2025-12-16 00:18:09,874: t15.2025.03.30 val PER: 0.2506
2025-12-16 00:18:09,874: t15.2025.04.13 val PER: 0.2111
2025-12-16 00:18:24,405: Train batch 108200: loss: 0.73 grad norm: 0.15 time: 0.057
2025-12-16 00:18:38,367: Train batch 108400: loss: 0.93 grad norm: 0.00 time: 0.057
2025-12-16 00:18:52,785: Train batch 108600: loss: 0.84 grad norm: 1.23 time: 0.052
2025-12-16 00:19:06,933: Train batch 108800: loss: 0.78 grad norm: 0.05 time: 0.071
2025-12-16 00:19:20,166: Train batch 109000: loss: 0.86 grad norm: 0.05 time: 0.045
2025-12-16 00:19:28,499: Train batch 109200: loss: 0.80 grad norm: 0.59 time: 0.053
2025-12-16 00:19:42,860: Train batch 109400: loss: 0.74 grad norm: 0.07 time: 0.074
2025-12-16 00:19:57,497: Train batch 109600: loss: 0.73 grad norm: 0.06 time: 0.085
2025-12-16 00:20:11,315: Train batch 109800: loss: 0.79 grad norm: 0.31 time: 0.052
2025-12-16 00:20:25,545: Train batch 110000: loss: 0.85 grad norm: 0.11 time: 0.038
2025-12-16 00:20:25,545: Running test after training batch: 110000
2025-12-16 00:20:34,702: Val batch 110000: PER (avg): 0.1296 CTC Loss (avg): 1.2254 time: 9.157
2025-12-16 00:20:34,702: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:20:34,702: t15.2023.08.13 val PER: 0.0988
2025-12-16 00:20:34,702: t15.2023.08.18 val PER: 0.0956
2025-12-16 00:20:34,702: t15.2023.08.20 val PER: 0.0723
2025-12-16 00:20:34,703: t15.2023.08.25 val PER: 0.0889
2025-12-16 00:20:34,703: t15.2023.08.27 val PER: 0.1672
2025-12-16 00:20:34,703: t15.2023.09.01 val PER: 0.0487
2025-12-16 00:20:34,703: t15.2023.09.03 val PER: 0.1318
2025-12-16 00:20:34,703: t15.2023.09.24 val PER: 0.0995
2025-12-16 00:20:34,703: t15.2023.09.29 val PER: 0.1225
2025-12-16 00:20:34,703: t15.2023.10.01 val PER: 0.1559
2025-12-16 00:20:34,703: t15.2023.10.06 val PER: 0.0829
2025-12-16 00:20:34,703: t15.2023.10.08 val PER: 0.1894
2025-12-16 00:20:34,703: t15.2023.10.13 val PER: 0.1769
2025-12-16 00:20:34,703: t15.2023.10.15 val PER: 0.1325
2025-12-16 00:20:34,703: t15.2023.10.20 val PER: 0.1678
2025-12-16 00:20:34,703: t15.2023.10.22 val PER: 0.1158
2025-12-16 00:20:34,703: t15.2023.11.03 val PER: 0.1574
2025-12-16 00:20:34,703: t15.2023.11.04 val PER: 0.0171
2025-12-16 00:20:34,703: t15.2023.11.17 val PER: 0.0451
2025-12-16 00:20:34,703: t15.2023.11.19 val PER: 0.0778
2025-12-16 00:20:34,703: t15.2023.11.26 val PER: 0.0986
2025-12-16 00:20:34,704: t15.2023.12.03 val PER: 0.0735
2025-12-16 00:20:34,704: t15.2023.12.08 val PER: 0.0846
2025-12-16 00:20:34,704: t15.2023.12.10 val PER: 0.0815
2025-12-16 00:20:34,704: t15.2023.12.17 val PER: 0.1486
2025-12-16 00:20:34,704: t15.2023.12.29 val PER: 0.1071
2025-12-16 00:20:34,704: t15.2024.02.25 val PER: 0.0997
2025-12-16 00:20:34,704: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:20:34,704: t15.2024.03.08 val PER: 0.2219
2025-12-16 00:20:34,704: t15.2024.03.15 val PER: 0.1864
2025-12-16 00:20:34,704: t15.2024.03.17 val PER: 0.1011
2025-12-16 00:20:34,704: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:20:34,704: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:20:34,704: t15.2024.05.10 val PER: 0.1441
2025-12-16 00:20:34,704: t15.2024.06.14 val PER: 0.1609
2025-12-16 00:20:34,704: t15.2024.07.19 val PER: 0.1721
2025-12-16 00:20:34,704: t15.2024.07.21 val PER: 0.0890
2025-12-16 00:20:34,704: t15.2024.07.28 val PER: 0.1007
2025-12-16 00:20:34,704: t15.2025.01.10 val PER: 0.2562
2025-12-16 00:20:34,704: t15.2025.01.12 val PER: 0.1116
2025-12-16 00:20:34,705: t15.2025.03.14 val PER: 0.3003
2025-12-16 00:20:34,705: t15.2025.03.16 val PER: 0.1872
2025-12-16 00:20:34,705: t15.2025.03.30 val PER: 0.2425
2025-12-16 00:20:34,705: t15.2025.04.13 val PER: 0.1954
2025-12-16 00:20:49,284: Train batch 110200: loss: 0.86 grad norm: 0.08 time: 0.056
2025-12-16 00:21:03,248: Train batch 110400: loss: 0.87 grad norm: 0.28 time: 0.038
2025-12-16 00:21:16,306: Train batch 110600: loss: 0.88 grad norm: 0.51 time: 0.064
2025-12-16 00:21:29,783: Train batch 110800: loss: 0.94 grad norm: 0.45 time: 0.051
2025-12-16 00:21:44,822: Train batch 111000: loss: 0.80 grad norm: 0.69 time: 0.061
2025-12-16 00:22:00,152: Train batch 111200: loss: 0.72 grad norm: 0.61 time: 0.060
2025-12-16 00:22:14,481: Train batch 111400: loss: 0.92 grad norm: 0.13 time: 0.070
2025-12-16 00:22:29,258: Train batch 111600: loss: 0.96 grad norm: 0.03 time: 0.087
2025-12-16 00:22:44,047: Train batch 111800: loss: 0.79 grad norm: 0.83 time: 0.071
2025-12-16 00:22:58,892: Train batch 112000: loss: 0.92 grad norm: 0.61 time: 0.068
2025-12-16 00:22:58,892: Running test after training batch: 112000
2025-12-16 00:23:08,087: Val batch 112000: PER (avg): 0.1300 CTC Loss (avg): 1.2239 time: 9.194
2025-12-16 00:23:08,087: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:23:08,087: t15.2023.08.13 val PER: 0.0936
2025-12-16 00:23:08,087: t15.2023.08.18 val PER: 0.0997
2025-12-16 00:23:08,087: t15.2023.08.20 val PER: 0.0715
2025-12-16 00:23:08,087: t15.2023.08.25 val PER: 0.0873
2025-12-16 00:23:08,087: t15.2023.08.27 val PER: 0.1624
2025-12-16 00:23:08,088: t15.2023.09.01 val PER: 0.0560
2025-12-16 00:23:08,088: t15.2023.09.03 val PER: 0.1283
2025-12-16 00:23:08,088: t15.2023.09.24 val PER: 0.1044
2025-12-16 00:23:08,088: t15.2023.09.29 val PER: 0.1244
2025-12-16 00:23:08,088: t15.2023.10.01 val PER: 0.1612
2025-12-16 00:23:08,088: t15.2023.10.06 val PER: 0.0883
2025-12-16 00:23:08,088: t15.2023.10.08 val PER: 0.1962
2025-12-16 00:23:08,088: t15.2023.10.13 val PER: 0.1831
2025-12-16 00:23:08,088: t15.2023.10.15 val PER: 0.1351
2025-12-16 00:23:08,088: t15.2023.10.20 val PER: 0.1711
2025-12-16 00:23:08,088: t15.2023.10.22 val PER: 0.1024
2025-12-16 00:23:08,088: t15.2023.11.03 val PER: 0.1594
2025-12-16 00:23:08,088: t15.2023.11.04 val PER: 0.0273
2025-12-16 00:23:08,088: t15.2023.11.17 val PER: 0.0420
2025-12-16 00:23:08,088: t15.2023.11.19 val PER: 0.0719
2025-12-16 00:23:08,088: t15.2023.11.26 val PER: 0.1145
2025-12-16 00:23:08,088: t15.2023.12.03 val PER: 0.0798
2025-12-16 00:23:08,088: t15.2023.12.08 val PER: 0.0772
2025-12-16 00:23:08,088: t15.2023.12.10 val PER: 0.0710
2025-12-16 00:23:08,088: t15.2023.12.17 val PER: 0.1590
2025-12-16 00:23:08,089: t15.2023.12.29 val PER: 0.1112
2025-12-16 00:23:08,089: t15.2024.02.25 val PER: 0.0969
2025-12-16 00:23:08,089: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:23:08,089: t15.2024.03.08 val PER: 0.2191
2025-12-16 00:23:08,089: t15.2024.03.15 val PER: 0.1857
2025-12-16 00:23:08,089: t15.2024.03.17 val PER: 0.0948
2025-12-16 00:23:08,089: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:23:08,089: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:23:08,089: t15.2024.05.10 val PER: 0.1426
2025-12-16 00:23:08,089: t15.2024.06.14 val PER: 0.1562
2025-12-16 00:23:08,089: t15.2024.07.19 val PER: 0.1747
2025-12-16 00:23:08,089: t15.2024.07.21 val PER: 0.0834
2025-12-16 00:23:08,089: t15.2024.07.28 val PER: 0.1103
2025-12-16 00:23:08,089: t15.2025.01.10 val PER: 0.2631
2025-12-16 00:23:08,089: t15.2025.01.12 val PER: 0.1016
2025-12-16 00:23:08,089: t15.2025.03.14 val PER: 0.2781
2025-12-16 00:23:08,089: t15.2025.03.16 val PER: 0.1715
2025-12-16 00:23:08,089: t15.2025.03.30 val PER: 0.2460
2025-12-16 00:23:08,089: t15.2025.04.13 val PER: 0.1997
2025-12-16 00:23:22,610: Train batch 112200: loss: 0.96 grad norm: 1.26 time: 0.071
2025-12-16 00:23:37,290: Train batch 112400: loss: 0.81 grad norm: 0.18 time: 0.078
2025-12-16 00:23:51,692: Train batch 112600: loss: 0.83 grad norm: 0.79 time: 0.056
2025-12-16 00:24:06,506: Train batch 112800: loss: 0.85 grad norm: 0.16 time: 0.075
2025-12-16 00:24:21,110: Train batch 113000: loss: 0.75 grad norm: 1.27 time: 0.087
2025-12-16 00:24:34,529: Train batch 113200: loss: 0.75 grad norm: 0.98 time: 0.060
2025-12-16 00:24:48,526: Train batch 113400: loss: 0.82 grad norm: 0.25 time: 0.068
2025-12-16 00:25:03,433: Train batch 113600: loss: 0.81 grad norm: 1.03 time: 0.081
2025-12-16 00:25:17,805: Train batch 113800: loss: 0.82 grad norm: 0.70 time: 0.055
2025-12-16 00:25:32,401: Train batch 114000: loss: 0.77 grad norm: 0.73 time: 0.072
2025-12-16 00:25:32,402: Running test after training batch: 114000
2025-12-16 00:25:41,676: Val batch 114000: PER (avg): 0.1284 CTC Loss (avg): 1.1854 time: 9.274
2025-12-16 00:25:41,676: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:25:41,676: t15.2023.08.13 val PER: 0.0915
2025-12-16 00:25:41,676: t15.2023.08.18 val PER: 0.0930
2025-12-16 00:25:41,676: t15.2023.08.20 val PER: 0.0731
2025-12-16 00:25:41,676: t15.2023.08.25 val PER: 0.0783
2025-12-16 00:25:41,676: t15.2023.08.27 val PER: 0.1688
2025-12-16 00:25:41,676: t15.2023.09.01 val PER: 0.0479
2025-12-16 00:25:41,677: t15.2023.09.03 val PER: 0.1306
2025-12-16 00:25:41,677: t15.2023.09.24 val PER: 0.0995
2025-12-16 00:25:41,677: t15.2023.09.29 val PER: 0.1232
2025-12-16 00:25:41,677: t15.2023.10.01 val PER: 0.1546
2025-12-16 00:25:41,677: t15.2023.10.06 val PER: 0.0893
2025-12-16 00:25:41,677: t15.2023.10.08 val PER: 0.1800
2025-12-16 00:25:41,677: t15.2023.10.13 val PER: 0.1823
2025-12-16 00:25:41,677: t15.2023.10.15 val PER: 0.1292
2025-12-16 00:25:41,677: t15.2023.10.20 val PER: 0.1711
2025-12-16 00:25:41,677: t15.2023.10.22 val PER: 0.0969
2025-12-16 00:25:41,677: t15.2023.11.03 val PER: 0.1628
2025-12-16 00:25:41,677: t15.2023.11.04 val PER: 0.0341
2025-12-16 00:25:41,677: t15.2023.11.17 val PER: 0.0389
2025-12-16 00:25:41,678: t15.2023.11.19 val PER: 0.0679
2025-12-16 00:25:41,678: t15.2023.11.26 val PER: 0.1022
2025-12-16 00:25:41,678: t15.2023.12.03 val PER: 0.0714
2025-12-16 00:25:41,678: t15.2023.12.08 val PER: 0.0832
2025-12-16 00:25:41,678: t15.2023.12.10 val PER: 0.0683
2025-12-16 00:25:41,678: t15.2023.12.17 val PER: 0.1538
2025-12-16 00:25:41,678: t15.2023.12.29 val PER: 0.1098
2025-12-16 00:25:41,678: t15.2024.02.25 val PER: 0.0927
2025-12-16 00:25:41,678: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:25:41,678: t15.2024.03.08 val PER: 0.2205
2025-12-16 00:25:41,678: t15.2024.03.15 val PER: 0.1857
2025-12-16 00:25:41,678: t15.2024.03.17 val PER: 0.1025
2025-12-16 00:25:41,678: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:25:41,679: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:25:41,679: t15.2024.05.10 val PER: 0.1456
2025-12-16 00:25:41,679: t15.2024.06.14 val PER: 0.1372
2025-12-16 00:25:41,679: t15.2024.07.19 val PER: 0.1767
2025-12-16 00:25:41,679: t15.2024.07.21 val PER: 0.0800
2025-12-16 00:25:41,679: t15.2024.07.28 val PER: 0.1081
2025-12-16 00:25:41,679: t15.2025.01.10 val PER: 0.2617
2025-12-16 00:25:41,679: t15.2025.01.12 val PER: 0.1001
2025-12-16 00:25:41,679: t15.2025.03.14 val PER: 0.2870
2025-12-16 00:25:41,679: t15.2025.03.16 val PER: 0.1846
2025-12-16 00:25:41,679: t15.2025.03.30 val PER: 0.2494
2025-12-16 00:25:41,679: t15.2025.04.13 val PER: 0.2126
2025-12-16 00:25:41,679: New best test PER 0.1292 --> 0.1284
2025-12-16 00:25:41,679: Checkpointing model
2025-12-16 00:25:42,122: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-16 00:25:56,871: Train batch 114200: loss: 0.97 grad norm: 0.09 time: 0.072
2025-12-16 00:26:11,539: Train batch 114400: loss: 0.99 grad norm: 0.10 time: 0.071
2025-12-16 00:26:26,170: Train batch 114600: loss: 0.78 grad norm: 0.01 time: 0.069
2025-12-16 00:26:40,870: Train batch 114800: loss: 0.79 grad norm: 0.47 time: 0.085
2025-12-16 00:26:54,969: Train batch 115000: loss: 0.90 grad norm: 0.17 time: 0.064
2025-12-16 00:27:09,701: Train batch 115200: loss: 0.74 grad norm: 0.21 time: 0.070
2025-12-16 00:27:24,430: Train batch 115400: loss: 0.75 grad norm: 0.88 time: 0.071
2025-12-16 00:27:38,389: Train batch 115600: loss: 0.77 grad norm: 0.85 time: 0.051
2025-12-16 00:27:53,037: Train batch 115800: loss: 0.81 grad norm: 0.11 time: 0.066
2025-12-16 00:28:07,191: Train batch 116000: loss: 0.82 grad norm: 0.27 time: 0.069
2025-12-16 00:28:07,191: Running test after training batch: 116000
2025-12-16 00:28:16,407: Val batch 116000: PER (avg): 0.1297 CTC Loss (avg): 1.2698 time: 9.216
2025-12-16 00:28:16,407: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:28:16,407: t15.2023.08.13 val PER: 0.1008
2025-12-16 00:28:16,408: t15.2023.08.18 val PER: 0.0905
2025-12-16 00:28:16,408: t15.2023.08.20 val PER: 0.0699
2025-12-16 00:28:16,408: t15.2023.08.25 val PER: 0.0843
2025-12-16 00:28:16,408: t15.2023.08.27 val PER: 0.1559
2025-12-16 00:28:16,408: t15.2023.09.01 val PER: 0.0584
2025-12-16 00:28:16,408: t15.2023.09.03 val PER: 0.1295
2025-12-16 00:28:16,408: t15.2023.09.24 val PER: 0.1117
2025-12-16 00:28:16,408: t15.2023.09.29 val PER: 0.1225
2025-12-16 00:28:16,408: t15.2023.10.01 val PER: 0.1546
2025-12-16 00:28:16,408: t15.2023.10.06 val PER: 0.0840
2025-12-16 00:28:16,408: t15.2023.10.08 val PER: 0.1881
2025-12-16 00:28:16,408: t15.2023.10.13 val PER: 0.1808
2025-12-16 00:28:16,408: t15.2023.10.15 val PER: 0.1285
2025-12-16 00:28:16,408: t15.2023.10.20 val PER: 0.1711
2025-12-16 00:28:16,408: t15.2023.10.22 val PER: 0.1047
2025-12-16 00:28:16,408: t15.2023.11.03 val PER: 0.1574
2025-12-16 00:28:16,408: t15.2023.11.04 val PER: 0.0341
2025-12-16 00:28:16,408: t15.2023.11.17 val PER: 0.0404
2025-12-16 00:28:16,408: t15.2023.11.19 val PER: 0.0679
2025-12-16 00:28:16,409: t15.2023.11.26 val PER: 0.0993
2025-12-16 00:28:16,409: t15.2023.12.03 val PER: 0.0725
2025-12-16 00:28:16,409: t15.2023.12.08 val PER: 0.0852
2025-12-16 00:28:16,409: t15.2023.12.10 val PER: 0.0736
2025-12-16 00:28:16,409: t15.2023.12.17 val PER: 0.1570
2025-12-16 00:28:16,409: t15.2023.12.29 val PER: 0.1023
2025-12-16 00:28:16,409: t15.2024.02.25 val PER: 0.0983
2025-12-16 00:28:16,409: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:28:16,409: t15.2024.03.08 val PER: 0.2361
2025-12-16 00:28:16,409: t15.2024.03.15 val PER: 0.1945
2025-12-16 00:28:16,409: t15.2024.03.17 val PER: 0.1011
2025-12-16 00:28:16,409: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:28:16,409: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:28:16,409: t15.2024.05.10 val PER: 0.1471
2025-12-16 00:28:16,409: t15.2024.06.14 val PER: 0.1514
2025-12-16 00:28:16,409: t15.2024.07.19 val PER: 0.1688
2025-12-16 00:28:16,409: t15.2024.07.21 val PER: 0.0834
2025-12-16 00:28:16,409: t15.2024.07.28 val PER: 0.1096
2025-12-16 00:28:16,409: t15.2025.01.10 val PER: 0.2713
2025-12-16 00:28:16,410: t15.2025.01.12 val PER: 0.0978
2025-12-16 00:28:16,410: t15.2025.03.14 val PER: 0.2870
2025-12-16 00:28:16,410: t15.2025.03.16 val PER: 0.1832
2025-12-16 00:28:16,410: t15.2025.03.30 val PER: 0.2598
2025-12-16 00:28:16,410: t15.2025.04.13 val PER: 0.2083
2025-12-16 00:28:30,771: Train batch 116200: loss: 0.91 grad norm: 0.11 time: 0.094
2025-12-16 00:28:44,771: Train batch 116400: loss: 0.80 grad norm: 0.11 time: 0.055
2025-12-16 00:28:58,310: Train batch 116600: loss: 0.78 grad norm: 1.00 time: 0.054
2025-12-16 00:29:12,017: Train batch 116800: loss: 0.77 grad norm: 0.26 time: 0.069
2025-12-16 00:29:26,743: Train batch 117000: loss: 0.93 grad norm: 2.37 time: 0.072
2025-12-16 00:29:40,924: Train batch 117200: loss: 0.74 grad norm: 0.49 time: 0.088
2025-12-16 00:29:55,315: Train batch 117400: loss: 0.73 grad norm: 0.06 time: 0.055
2025-12-16 00:30:10,044: Train batch 117600: loss: 0.77 grad norm: 0.35 time: 0.087
2025-12-16 00:30:24,625: Train batch 117800: loss: 0.96 grad norm: 0.01 time: 0.079
2025-12-16 00:30:38,769: Train batch 118000: loss: 0.77 grad norm: 0.03 time: 0.066
2025-12-16 00:30:38,770: Running test after training batch: 118000
2025-12-16 00:30:48,118: Val batch 118000: PER (avg): 0.1288 CTC Loss (avg): 1.2281 time: 9.348
2025-12-16 00:30:48,119: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:30:48,119: t15.2023.08.13 val PER: 0.0925
2025-12-16 00:30:48,119: t15.2023.08.18 val PER: 0.0930
2025-12-16 00:30:48,119: t15.2023.08.20 val PER: 0.0659
2025-12-16 00:30:48,119: t15.2023.08.25 val PER: 0.0889
2025-12-16 00:30:48,119: t15.2023.08.27 val PER: 0.1704
2025-12-16 00:30:48,119: t15.2023.09.01 val PER: 0.0479
2025-12-16 00:30:48,119: t15.2023.09.03 val PER: 0.1247
2025-12-16 00:30:48,119: t15.2023.09.24 val PER: 0.0959
2025-12-16 00:30:48,119: t15.2023.09.29 val PER: 0.1213
2025-12-16 00:30:48,119: t15.2023.10.01 val PER: 0.1519
2025-12-16 00:30:48,119: t15.2023.10.06 val PER: 0.0883
2025-12-16 00:30:48,119: t15.2023.10.08 val PER: 0.1922
2025-12-16 00:30:48,119: t15.2023.10.13 val PER: 0.1870
2025-12-16 00:30:48,119: t15.2023.10.15 val PER: 0.1299
2025-12-16 00:30:48,119: t15.2023.10.20 val PER: 0.1678
2025-12-16 00:30:48,119: t15.2023.10.22 val PER: 0.1069
2025-12-16 00:30:48,120: t15.2023.11.03 val PER: 0.1615
2025-12-16 00:30:48,120: t15.2023.11.04 val PER: 0.0239
2025-12-16 00:30:48,120: t15.2023.11.17 val PER: 0.0420
2025-12-16 00:30:48,120: t15.2023.11.19 val PER: 0.0639
2025-12-16 00:30:48,120: t15.2023.11.26 val PER: 0.0920
2025-12-16 00:30:48,120: t15.2023.12.03 val PER: 0.0725
2025-12-16 00:30:48,120: t15.2023.12.08 val PER: 0.0839
2025-12-16 00:30:48,120: t15.2023.12.10 val PER: 0.0802
2025-12-16 00:30:48,120: t15.2023.12.17 val PER: 0.1632
2025-12-16 00:30:48,120: t15.2023.12.29 val PER: 0.1064
2025-12-16 00:30:48,120: t15.2024.02.25 val PER: 0.1011
2025-12-16 00:30:48,120: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:30:48,120: t15.2024.03.08 val PER: 0.2176
2025-12-16 00:30:48,120: t15.2024.03.15 val PER: 0.1864
2025-12-16 00:30:48,120: t15.2024.03.17 val PER: 0.1018
2025-12-16 00:30:48,120: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:30:48,120: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:30:48,120: t15.2024.05.10 val PER: 0.1545
2025-12-16 00:30:48,120: t15.2024.06.14 val PER: 0.1562
2025-12-16 00:30:48,121: t15.2024.07.19 val PER: 0.1628
2025-12-16 00:30:48,121: t15.2024.07.21 val PER: 0.0807
2025-12-16 00:30:48,121: t15.2024.07.28 val PER: 0.1081
2025-12-16 00:30:48,121: t15.2025.01.10 val PER: 0.2590
2025-12-16 00:30:48,121: t15.2025.01.12 val PER: 0.1039
2025-12-16 00:30:48,121: t15.2025.03.14 val PER: 0.2781
2025-12-16 00:30:48,121: t15.2025.03.16 val PER: 0.1950
2025-12-16 00:30:48,121: t15.2025.03.30 val PER: 0.2494
2025-12-16 00:30:48,121: t15.2025.04.13 val PER: 0.2168
2025-12-16 00:31:01,901: Train batch 118200: loss: 0.89 grad norm: 1.57 time: 0.057
2025-12-16 00:31:16,140: Train batch 118400: loss: 0.80 grad norm: 0.83 time: 0.054
2025-12-16 00:31:30,000: Train batch 118600: loss: 0.90 grad norm: 0.23 time: 0.060
2025-12-16 00:31:42,776: Train batch 118800: loss: 0.99 grad norm: 0.36 time: 0.072
2025-12-16 00:31:57,329: Train batch 119000: loss: 0.69 grad norm: 0.12 time: 0.057
2025-12-16 00:32:11,201: Train batch 119200: loss: 0.83 grad norm: 0.02 time: 0.063
2025-12-16 00:32:24,411: Train batch 119400: loss: 0.77 grad norm: 0.72 time: 0.049
2025-12-16 00:32:35,070: Train batch 119600: loss: 0.76 grad norm: 0.72 time: 0.038
2025-12-16 00:32:46,398: Train batch 119800: loss: 0.79 grad norm: 0.73 time: 0.096
2025-12-16 00:32:58,294: Running test after training batch: 119999
2025-12-16 00:33:07,474: Val batch 119999: PER (avg): 0.1268 CTC Loss (avg): 1.2560 time: 9.179
2025-12-16 00:33:07,474: t15.2023.08.11 val PER: 1.0000
2025-12-16 00:33:07,474: t15.2023.08.13 val PER: 0.0946
2025-12-16 00:33:07,474: t15.2023.08.18 val PER: 0.0922
2025-12-16 00:33:07,474: t15.2023.08.20 val PER: 0.0604
2025-12-16 00:33:07,474: t15.2023.08.25 val PER: 0.0843
2025-12-16 00:33:07,474: t15.2023.08.27 val PER: 0.1576
2025-12-16 00:33:07,474: t15.2023.09.01 val PER: 0.0536
2025-12-16 00:33:07,474: t15.2023.09.03 val PER: 0.1211
2025-12-16 00:33:07,474: t15.2023.09.24 val PER: 0.0971
2025-12-16 00:33:07,474: t15.2023.09.29 val PER: 0.1149
2025-12-16 00:33:07,474: t15.2023.10.01 val PER: 0.1658
2025-12-16 00:33:07,474: t15.2023.10.06 val PER: 0.0818
2025-12-16 00:33:07,475: t15.2023.10.08 val PER: 0.1759
2025-12-16 00:33:07,475: t15.2023.10.13 val PER: 0.1815
2025-12-16 00:33:07,475: t15.2023.10.15 val PER: 0.1252
2025-12-16 00:33:07,475: t15.2023.10.20 val PER: 0.1711
2025-12-16 00:33:07,475: t15.2023.10.22 val PER: 0.1080
2025-12-16 00:33:07,475: t15.2023.11.03 val PER: 0.1547
2025-12-16 00:33:07,475: t15.2023.11.04 val PER: 0.0273
2025-12-16 00:33:07,475: t15.2023.11.17 val PER: 0.0404
2025-12-16 00:33:07,475: t15.2023.11.19 val PER: 0.0719
2025-12-16 00:33:07,475: t15.2023.11.26 val PER: 0.0855
2025-12-16 00:33:07,475: t15.2023.12.03 val PER: 0.0735
2025-12-16 00:33:07,475: t15.2023.12.08 val PER: 0.0792
2025-12-16 00:33:07,475: t15.2023.12.10 val PER: 0.0710
2025-12-16 00:33:07,475: t15.2023.12.17 val PER: 0.1486
2025-12-16 00:33:07,475: t15.2023.12.29 val PER: 0.1043
2025-12-16 00:33:07,475: t15.2024.02.25 val PER: 0.0941
2025-12-16 00:33:07,475: t15.2024.03.03 val PER: 1.0000
2025-12-16 00:33:07,475: t15.2024.03.08 val PER: 0.2233
2025-12-16 00:33:07,476: t15.2024.03.15 val PER: 0.1932
2025-12-16 00:33:07,476: t15.2024.03.17 val PER: 0.0983
2025-12-16 00:33:07,476: t15.2024.04.25 val PER: 1.0000
2025-12-16 00:33:07,476: t15.2024.04.28 val PER: 1.0000
2025-12-16 00:33:07,476: t15.2024.05.10 val PER: 0.1471
2025-12-16 00:33:07,476: t15.2024.06.14 val PER: 0.1483
2025-12-16 00:33:07,476: t15.2024.07.19 val PER: 0.1602
2025-12-16 00:33:07,476: t15.2024.07.21 val PER: 0.0772
2025-12-16 00:33:07,476: t15.2024.07.28 val PER: 0.1096
2025-12-16 00:33:07,476: t15.2025.01.10 val PER: 0.2631
2025-12-16 00:33:07,476: t15.2025.01.12 val PER: 0.1039
2025-12-16 00:33:07,476: t15.2025.03.14 val PER: 0.2885
2025-12-16 00:33:07,476: t15.2025.03.16 val PER: 0.1872
2025-12-16 00:33:07,476: t15.2025.03.30 val PER: 0.2517
2025-12-16 00:33:07,476: t15.2025.04.13 val PER: 0.2183
2025-12-16 00:33:07,476: New best test PER 0.1284 --> 0.1268
2025-12-16 00:33:07,476: Checkpointing model
2025-12-16 00:33:07,893: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_220400/checkpoint/best_checkpoint
2025-12-16 00:33:08,081: Best avg val PER achieved: 0.12681
2025-12-16 00:33:08,081: Total training time: 148.98 minutes
