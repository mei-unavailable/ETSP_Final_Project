2025-12-11 16:55:12,048: Using device: cuda:0
2025-12-11 16:55:12,050: Initializing Conformer with U-Net from trained_models/unet_ssl_20251210_065837/unet_mae_epoch_50.pt
2025-12-11 16:55:12,506: Initialized RNN decoding model
2025-12-11 16:55:12,506: UNetEnhancedModel(
  (unet): NeuralUNet(
    (inc): DoubleConv(
      (double_conv): Sequential(
        (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
      )
    )
    (down1): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down2): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down3): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (down4): Down(
      (maxpool_conv): Sequential(
        (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
        (1): DoubleConv(
          (double_conv): Sequential(
            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (2): ReLU(inplace=True)
            (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (5): ReLU(inplace=True)
          )
        )
      )
    )
    (up1): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up2): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up3): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (up4): Up(
      (up): Upsample(scale_factor=2.0, mode='bilinear')
      (conv): DoubleConv(
        (double_conv): Sequential(
          (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (2): ReLU(inplace=True)
          (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (5): ReLU(inplace=True)
        )
      )
    )
    (outc): OutConv(
      (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (decoder): ConformerDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.4, inplace=False)
    (projection): Linear(in_features=7168, out_features=512, bias=True)
    (conformer): Conformer(
      (conformer_layers): ModuleList(
        (0-5): 6 x ConformerLayer(
          (ffn1): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_dropout): Dropout(p=0.4, inplace=False)
          (conv_module): _ConvolutionModule(
            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (sequential): Sequential(
              (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))
              (1): GLU(dim=1)
              (2): Conv1d(512, 512, kernel_size=(31,), stride=(1,), padding=(15,), groups=512)
              (3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (4): SiLU()
              (5): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
              (6): Dropout(p=0.4, inplace=False)
            )
          )
          (ffn2): _FeedForwardModule(
            (sequential): Sequential(
              (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
              (1): Linear(in_features=512, out_features=2048, bias=True)
              (2): SiLU()
              (3): Dropout(p=0.4, inplace=False)
              (4): Linear(in_features=2048, out_features=512, bias=True)
              (5): Dropout(p=0.4, inplace=False)
            )
          )
          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (out): Linear(in_features=512, out_features=41, bias=True)
  )
)
2025-12-11 16:55:12,510: Model has 69,136,170 parameters
2025-12-11 16:55:12,510: Model has 11,819,520 day-specific parameters | 17.10% of total parameters
2025-12-11 16:55:20,969: Successfully initialized datasets
2025-12-11 16:55:22,951: Train batch 0: loss: 640.02 grad norm: 1109.71 time: 1.312
2025-12-11 16:55:22,951: Running test after training batch: 0
2025-12-11 16:55:53,798: Val batch 0: PER (avg): 2.8375 CTC Loss (avg): 652.6244 time: 30.847
2025-12-11 16:55:53,798: t15.2023.08.11 val PER: 1.0000
2025-12-11 16:55:53,798: t15.2023.08.13 val PER: 2.4719
2025-12-11 16:55:53,798: t15.2023.08.18 val PER: 2.6270
2025-12-11 16:55:53,798: t15.2023.08.20 val PER: 2.4456
2025-12-11 16:55:53,798: t15.2023.08.25 val PER: 2.7831
2025-12-11 16:55:53,799: t15.2023.08.27 val PER: 2.3376
2025-12-11 16:55:53,799: t15.2023.09.01 val PER: 2.6826
2025-12-11 16:55:53,799: t15.2023.09.03 val PER: 2.6924
2025-12-11 16:55:53,799: t15.2023.09.24 val PER: 3.0340
2025-12-11 16:55:53,799: t15.2023.09.29 val PER: 3.0223
2025-12-11 16:55:53,799: t15.2023.10.01 val PER: 2.3230
2025-12-11 16:55:53,799: t15.2023.10.06 val PER: 3.1884
2025-12-11 16:55:53,799: t15.2023.10.08 val PER: 2.4614
2025-12-11 16:55:53,799: t15.2023.10.13 val PER: 2.3126
2025-12-11 16:55:53,799: t15.2023.10.15 val PER: 2.6856
2025-12-11 16:55:53,799: t15.2023.10.20 val PER: 2.9195
2025-12-11 16:55:53,799: t15.2023.10.22 val PER: 2.3196
2025-12-11 16:55:53,799: t15.2023.11.03 val PER: 3.4342
2025-12-11 16:55:53,799: t15.2023.11.04 val PER: 3.9556
2025-12-11 16:55:53,799: t15.2023.11.17 val PER: 4.1820
2025-12-11 16:55:53,799: t15.2023.11.19 val PER: 3.7645
2025-12-11 16:55:53,799: t15.2023.11.26 val PER: 3.0804
2025-12-11 16:55:53,799: t15.2023.12.03 val PER: 2.8120
2025-12-11 16:55:53,800: t15.2023.12.08 val PER: 2.9148
2025-12-11 16:55:53,800: t15.2023.12.10 val PER: 3.2602
2025-12-11 16:55:53,800: t15.2023.12.17 val PER: 2.4792
2025-12-11 16:55:53,800: t15.2023.12.29 val PER: 2.9835
2025-12-11 16:55:53,800: t15.2024.02.25 val PER: 2.6166
2025-12-11 16:55:53,800: t15.2024.03.03 val PER: 1.0000
2025-12-11 16:55:53,800: t15.2024.03.08 val PER: 2.8222
2025-12-11 16:55:53,800: t15.2024.03.15 val PER: 2.4465
2025-12-11 16:55:53,800: t15.2024.03.17 val PER: 2.6980
2025-12-11 16:55:53,800: t15.2024.04.25 val PER: 1.0000
2025-12-11 16:55:53,800: t15.2024.04.28 val PER: 1.0000
2025-12-11 16:55:53,800: t15.2024.05.10 val PER: 2.7860
2025-12-11 16:55:53,800: t15.2024.06.14 val PER: 3.0268
2025-12-11 16:55:53,800: t15.2024.07.19 val PER: 2.0244
2025-12-11 16:55:53,800: t15.2024.07.21 val PER: 3.2483
2025-12-11 16:55:53,800: t15.2024.07.28 val PER: 3.3456
2025-12-11 16:55:53,800: t15.2025.01.10 val PER: 2.3251
2025-12-11 16:55:53,800: t15.2025.01.12 val PER: 3.5820
2025-12-11 16:55:53,800: t15.2025.03.14 val PER: 2.0118
2025-12-11 16:55:53,801: t15.2025.03.16 val PER: 3.8887
2025-12-11 16:55:53,801: t15.2025.03.30 val PER: 2.5966
2025-12-11 16:55:53,801: t15.2025.04.13 val PER: 3.4836
2025-12-11 16:55:53,801: New best test PER inf --> 2.8375
2025-12-11 16:55:53,801: Checkpointing model
2025-12-11 16:55:54,275: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251211_165511/checkpoint/best_checkpoint
