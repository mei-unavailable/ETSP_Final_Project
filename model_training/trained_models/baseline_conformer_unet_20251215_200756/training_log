2025-12-15 20:07:57,036: Using device: cuda:0
2025-12-15 20:07:57,101: Initialized RNN decoding model
2025-12-15 20:07:57,101: ConformerDecoder(
  (day_activation): Softsign()
  (day_dropout): Dropout(p=0.2, inplace=False)
  (day_weights): ParameterList(
      (0): Parameter containing: [torch.float32 of size 512x512]
      (1): Parameter containing: [torch.float32 of size 512x512]
      (2): Parameter containing: [torch.float32 of size 512x512]
      (3): Parameter containing: [torch.float32 of size 512x512]
      (4): Parameter containing: [torch.float32 of size 512x512]
      (5): Parameter containing: [torch.float32 of size 512x512]
      (6): Parameter containing: [torch.float32 of size 512x512]
      (7): Parameter containing: [torch.float32 of size 512x512]
      (8): Parameter containing: [torch.float32 of size 512x512]
      (9): Parameter containing: [torch.float32 of size 512x512]
      (10): Parameter containing: [torch.float32 of size 512x512]
      (11): Parameter containing: [torch.float32 of size 512x512]
      (12): Parameter containing: [torch.float32 of size 512x512]
      (13): Parameter containing: [torch.float32 of size 512x512]
      (14): Parameter containing: [torch.float32 of size 512x512]
      (15): Parameter containing: [torch.float32 of size 512x512]
      (16): Parameter containing: [torch.float32 of size 512x512]
      (17): Parameter containing: [torch.float32 of size 512x512]
      (18): Parameter containing: [torch.float32 of size 512x512]
      (19): Parameter containing: [torch.float32 of size 512x512]
      (20): Parameter containing: [torch.float32 of size 512x512]
      (21): Parameter containing: [torch.float32 of size 512x512]
      (22): Parameter containing: [torch.float32 of size 512x512]
      (23): Parameter containing: [torch.float32 of size 512x512]
      (24): Parameter containing: [torch.float32 of size 512x512]
      (25): Parameter containing: [torch.float32 of size 512x512]
      (26): Parameter containing: [torch.float32 of size 512x512]
      (27): Parameter containing: [torch.float32 of size 512x512]
      (28): Parameter containing: [torch.float32 of size 512x512]
      (29): Parameter containing: [torch.float32 of size 512x512]
      (30): Parameter containing: [torch.float32 of size 512x512]
      (31): Parameter containing: [torch.float32 of size 512x512]
      (32): Parameter containing: [torch.float32 of size 512x512]
      (33): Parameter containing: [torch.float32 of size 512x512]
      (34): Parameter containing: [torch.float32 of size 512x512]
      (35): Parameter containing: [torch.float32 of size 512x512]
      (36): Parameter containing: [torch.float32 of size 512x512]
      (37): Parameter containing: [torch.float32 of size 512x512]
      (38): Parameter containing: [torch.float32 of size 512x512]
      (39): Parameter containing: [torch.float32 of size 512x512]
      (40): Parameter containing: [torch.float32 of size 512x512]
      (41): Parameter containing: [torch.float32 of size 512x512]
      (42): Parameter containing: [torch.float32 of size 512x512]
      (43): Parameter containing: [torch.float32 of size 512x512]
      (44): Parameter containing: [torch.float32 of size 512x512]
  )
  (day_biases): ParameterList(
      (0): Parameter containing: [torch.float32 of size 1x512]
      (1): Parameter containing: [torch.float32 of size 1x512]
      (2): Parameter containing: [torch.float32 of size 1x512]
      (3): Parameter containing: [torch.float32 of size 1x512]
      (4): Parameter containing: [torch.float32 of size 1x512]
      (5): Parameter containing: [torch.float32 of size 1x512]
      (6): Parameter containing: [torch.float32 of size 1x512]
      (7): Parameter containing: [torch.float32 of size 1x512]
      (8): Parameter containing: [torch.float32 of size 1x512]
      (9): Parameter containing: [torch.float32 of size 1x512]
      (10): Parameter containing: [torch.float32 of size 1x512]
      (11): Parameter containing: [torch.float32 of size 1x512]
      (12): Parameter containing: [torch.float32 of size 1x512]
      (13): Parameter containing: [torch.float32 of size 1x512]
      (14): Parameter containing: [torch.float32 of size 1x512]
      (15): Parameter containing: [torch.float32 of size 1x512]
      (16): Parameter containing: [torch.float32 of size 1x512]
      (17): Parameter containing: [torch.float32 of size 1x512]
      (18): Parameter containing: [torch.float32 of size 1x512]
      (19): Parameter containing: [torch.float32 of size 1x512]
      (20): Parameter containing: [torch.float32 of size 1x512]
      (21): Parameter containing: [torch.float32 of size 1x512]
      (22): Parameter containing: [torch.float32 of size 1x512]
      (23): Parameter containing: [torch.float32 of size 1x512]
      (24): Parameter containing: [torch.float32 of size 1x512]
      (25): Parameter containing: [torch.float32 of size 1x512]
      (26): Parameter containing: [torch.float32 of size 1x512]
      (27): Parameter containing: [torch.float32 of size 1x512]
      (28): Parameter containing: [torch.float32 of size 1x512]
      (29): Parameter containing: [torch.float32 of size 1x512]
      (30): Parameter containing: [torch.float32 of size 1x512]
      (31): Parameter containing: [torch.float32 of size 1x512]
      (32): Parameter containing: [torch.float32 of size 1x512]
      (33): Parameter containing: [torch.float32 of size 1x512]
      (34): Parameter containing: [torch.float32 of size 1x512]
      (35): Parameter containing: [torch.float32 of size 1x512]
      (36): Parameter containing: [torch.float32 of size 1x512]
      (37): Parameter containing: [torch.float32 of size 1x512]
      (38): Parameter containing: [torch.float32 of size 1x512]
      (39): Parameter containing: [torch.float32 of size 1x512]
      (40): Parameter containing: [torch.float32 of size 1x512]
      (41): Parameter containing: [torch.float32 of size 1x512]
      (42): Parameter containing: [torch.float32 of size 1x512]
      (43): Parameter containing: [torch.float32 of size 1x512]
      (44): Parameter containing: [torch.float32 of size 1x512]
  )
  (input_proj): Sequential(
    (0): Linear(in_features=7168, out_features=256, bias=True)
    (1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    (2): Dropout(p=0.2, inplace=False)
  )
  (pre_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (conformer): Conformer(
    (conformer_layers): ModuleList(
      (0-3): 4 x ConformerLayer(
        (ffn1): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (self_attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
        )
        (self_attn_dropout): Dropout(p=0.2, inplace=False)
        (conv_module): _ConvolutionModule(
          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (sequential): Sequential(
            (0): Conv1d(256, 512, kernel_size=(1,), stride=(1,))
            (1): GLU(dim=1)
            (2): Conv1d(256, 256, kernel_size=(15,), stride=(1,), padding=(7,), groups=256)
            (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (4): SiLU()
            (5): Conv1d(256, 256, kernel_size=(1,), stride=(1,))
            (6): Dropout(p=0.2, inplace=False)
          )
        )
        (ffn2): _FeedForwardModule(
          (sequential): Sequential(
            (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (1): Linear(in_features=256, out_features=512, bias=True)
            (2): SiLU()
            (3): Dropout(p=0.2, inplace=False)
            (4): Linear(in_features=512, out_features=256, bias=True)
            (5): Dropout(p=0.2, inplace=False)
          )
        )
        (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (output_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  (output_dropout): Dropout(p=0.1, inplace=False)
  (out): Linear(in_features=256, out_features=41, bias=True)
)
2025-12-15 20:07:57,103: Model has 17,641,001 parameters
2025-12-15 20:07:57,103: Model has 11,819,520 day-specific parameters | 67.00% of total parameters
2025-12-15 20:08:05,525: Successfully initialized datasets
2025-12-15 20:08:06,979: Train batch 0: loss: 678.50 grad norm: 309.14 time: 1.132
2025-12-15 20:08:06,979: Running test after training batch: 0
2025-12-15 20:08:18,491: Val batch 0: PER (avg): 4.5674 CTC Loss (avg): 711.6542 time: 11.512
2025-12-15 20:08:18,491: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:08:18,491: t15.2023.08.13 val PER: 3.6653
2025-12-15 20:08:18,491: t15.2023.08.18 val PER: 4.1802
2025-12-15 20:08:18,491: t15.2023.08.20 val PER: 3.8674
2025-12-15 20:08:18,492: t15.2023.08.25 val PER: 4.0452
2025-12-15 20:08:18,492: t15.2023.08.27 val PER: 3.6141
2025-12-15 20:08:18,492: t15.2023.09.01 val PER: 4.2127
2025-12-15 20:08:18,492: t15.2023.09.03 val PER: 3.9537
2025-12-15 20:08:18,492: t15.2023.09.24 val PER: 4.8823
2025-12-15 20:08:18,492: t15.2023.09.29 val PER: 4.8922
2025-12-15 20:08:18,492: t15.2023.10.01 val PER: 3.7483
2025-12-15 20:08:18,492: t15.2023.10.06 val PER: 4.7309
2025-12-15 20:08:18,492: t15.2023.10.08 val PER: 3.5737
2025-12-15 20:08:18,492: t15.2023.10.13 val PER: 4.4880
2025-12-15 20:08:18,492: t15.2023.10.15 val PER: 4.6948
2025-12-15 20:08:18,492: t15.2023.10.20 val PER: 4.9362
2025-12-15 20:08:18,492: t15.2023.10.22 val PER: 4.7283
2025-12-15 20:08:18,492: t15.2023.11.03 val PER: 5.3602
2025-12-15 20:08:18,492: t15.2023.11.04 val PER: 6.6416
2025-12-15 20:08:18,492: t15.2023.11.17 val PER: 6.2504
2025-12-15 20:08:18,492: t15.2023.11.19 val PER: 5.1697
2025-12-15 20:08:18,492: t15.2023.11.26 val PER: 4.5826
2025-12-15 20:08:18,492: t15.2023.12.03 val PER: 4.3834
2025-12-15 20:08:18,493: t15.2023.12.08 val PER: 4.7636
2025-12-15 20:08:18,493: t15.2023.12.10 val PER: 5.6071
2025-12-15 20:08:18,493: t15.2023.12.17 val PER: 4.2079
2025-12-15 20:08:18,493: t15.2023.12.29 val PER: 4.3892
2025-12-15 20:08:18,493: t15.2024.02.25 val PER: 4.6419
2025-12-15 20:08:18,493: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:08:18,493: t15.2024.03.08 val PER: 4.1110
2025-12-15 20:08:18,493: t15.2024.03.15 val PER: 4.3221
2025-12-15 20:08:18,493: t15.2024.03.17 val PER: 4.7755
2025-12-15 20:08:18,493: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:08:18,493: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:08:18,493: t15.2024.05.10 val PER: 4.4324
2025-12-15 20:08:18,493: t15.2024.06.14 val PER: 5.2082
2025-12-15 20:08:18,493: t15.2024.07.19 val PER: 3.2808
2025-12-15 20:08:18,493: t15.2024.07.21 val PER: 5.3828
2025-12-15 20:08:18,493: t15.2024.07.28 val PER: 5.4272
2025-12-15 20:08:18,493: t15.2025.01.10 val PER: 3.5220
2025-12-15 20:08:18,493: t15.2025.01.12 val PER: 5.7236
2025-12-15 20:08:18,494: t15.2025.03.14 val PER: 3.2899
2025-12-15 20:08:18,494: t15.2025.03.16 val PER: 5.9215
2025-12-15 20:08:18,494: t15.2025.03.30 val PER: 4.1655
2025-12-15 20:08:18,494: t15.2025.04.13 val PER: 5.1498
2025-12-15 20:08:18,494: New best test PER inf --> 4.5674
2025-12-15 20:08:18,494: Checkpointing model
2025-12-15 20:08:18,614: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:08:35,613: Train batch 200: loss: 549.34 grad norm: 1065.23 time: 0.071
2025-12-15 20:08:48,566: Train batch 400: loss: 304.65 grad norm: 1427.53 time: 0.054
2025-12-15 20:09:03,003: Train batch 600: loss: 114.62 grad norm: 387.65 time: 0.066
2025-12-15 20:09:17,569: Train batch 800: loss: 97.72 grad norm: 19.06 time: 0.054
2025-12-15 20:09:32,308: Train batch 1000: loss: 103.44 grad norm: 18.59 time: 0.066
2025-12-15 20:09:46,353: Train batch 1200: loss: 59.00 grad norm: 19.03 time: 0.065
2025-12-15 20:10:00,929: Train batch 1400: loss: 87.63 grad norm: 20.07 time: 0.068
2025-12-15 20:10:15,303: Train batch 1600: loss: 78.18 grad norm: 34.03 time: 0.054
2025-12-15 20:10:29,818: Train batch 1800: loss: 61.51 grad norm: 35.42 time: 0.082
2025-12-15 20:10:41,096: Train batch 2000: loss: 73.52 grad norm: 42.12 time: 0.037
2025-12-15 20:10:41,096: Running test after training batch: 2000
2025-12-15 20:10:50,046: Val batch 2000: PER (avg): 0.6861 CTC Loss (avg): 96.5548 time: 8.949
2025-12-15 20:10:50,046: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:10:50,046: t15.2023.08.13 val PER: 0.6601
2025-12-15 20:10:50,046: t15.2023.08.18 val PER: 0.6697
2025-12-15 20:10:50,046: t15.2023.08.20 val PER: 0.6553
2025-12-15 20:10:50,046: t15.2023.08.25 val PER: 0.6491
2025-12-15 20:10:50,047: t15.2023.08.27 val PER: 0.7026
2025-12-15 20:10:50,047: t15.2023.09.01 val PER: 0.6526
2025-12-15 20:10:50,047: t15.2023.09.03 val PER: 0.6663
2025-12-15 20:10:50,047: t15.2023.09.24 val PER: 0.6833
2025-12-15 20:10:50,047: t15.2023.09.29 val PER: 0.6937
2025-12-15 20:10:50,047: t15.2023.10.01 val PER: 0.6902
2025-12-15 20:10:50,047: t15.2023.10.06 val PER: 0.6598
2025-12-15 20:10:50,047: t15.2023.10.08 val PER: 0.7064
2025-12-15 20:10:50,047: t15.2023.10.13 val PER: 0.7370
2025-12-15 20:10:50,047: t15.2023.10.15 val PER: 0.6823
2025-12-15 20:10:50,047: t15.2023.10.20 val PER: 0.6678
2025-12-15 20:10:50,047: t15.2023.10.22 val PER: 0.6748
2025-12-15 20:10:50,047: t15.2023.11.03 val PER: 0.6805
2025-12-15 20:10:50,047: t15.2023.11.04 val PER: 0.6177
2025-12-15 20:10:50,047: t15.2023.11.17 val PER: 0.6407
2025-12-15 20:10:50,047: t15.2023.11.19 val PER: 0.6287
2025-12-15 20:10:50,047: t15.2023.11.26 val PER: 0.7203
2025-12-15 20:10:50,047: t15.2023.12.03 val PER: 0.6996
2025-12-15 20:10:50,047: t15.2023.12.08 val PER: 0.6884
2025-12-15 20:10:50,048: t15.2023.12.10 val PER: 0.6899
2025-12-15 20:10:50,048: t15.2023.12.17 val PER: 0.6871
2025-12-15 20:10:50,048: t15.2023.12.29 val PER: 0.6850
2025-12-15 20:10:50,048: t15.2024.02.25 val PER: 0.6756
2025-12-15 20:10:50,048: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:10:50,048: t15.2024.03.08 val PER: 0.6700
2025-12-15 20:10:50,048: t15.2024.03.15 val PER: 0.7136
2025-12-15 20:10:50,048: t15.2024.03.17 val PER: 0.6715
2025-12-15 20:10:50,048: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:10:50,049: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:10:50,049: t15.2024.05.10 val PER: 0.6805
2025-12-15 20:10:50,049: t15.2024.06.14 val PER: 0.6893
2025-12-15 20:10:50,049: t15.2024.07.19 val PER: 0.7304
2025-12-15 20:10:50,049: t15.2024.07.21 val PER: 0.6800
2025-12-15 20:10:50,049: t15.2024.07.28 val PER: 0.6993
2025-12-15 20:10:50,049: t15.2025.01.10 val PER: 0.7300
2025-12-15 20:10:50,050: t15.2025.01.12 val PER: 0.6705
2025-12-15 20:10:50,050: t15.2025.03.14 val PER: 0.7071
2025-12-15 20:10:50,050: t15.2025.03.16 val PER: 0.6741
2025-12-15 20:10:50,050: t15.2025.03.30 val PER: 0.7218
2025-12-15 20:10:50,050: t15.2025.04.13 val PER: 0.6876
2025-12-15 20:10:50,050: New best test PER 4.5674 --> 0.6861
2025-12-15 20:10:50,050: Checkpointing model
2025-12-15 20:10:50,466: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:11:03,225: Train batch 2200: loss: 88.29 grad norm: 48.38 time: 0.071
2025-12-15 20:11:17,420: Train batch 2400: loss: 81.85 grad norm: 56.99 time: 0.067
2025-12-15 20:11:31,352: Train batch 2600: loss: 68.92 grad norm: 40.81 time: 0.071
2025-12-15 20:11:45,655: Train batch 2800: loss: 66.16 grad norm: 59.18 time: 0.078
2025-12-15 20:11:56,432: Train batch 3000: loss: 76.45 grad norm: 74.52 time: 0.033
2025-12-15 20:12:10,824: Train batch 3200: loss: 72.27 grad norm: 63.17 time: 0.058
2025-12-15 20:12:25,272: Train batch 3400: loss: 57.82 grad norm: 59.70 time: 0.055
2025-12-15 20:12:39,638: Train batch 3600: loss: 53.73 grad norm: 49.36 time: 0.078
2025-12-15 20:12:53,857: Train batch 3800: loss: 56.27 grad norm: 57.30 time: 0.068
2025-12-15 20:13:08,272: Train batch 4000: loss: 45.43 grad norm: 50.63 time: 0.069
2025-12-15 20:13:08,273: Running test after training batch: 4000
2025-12-15 20:13:17,314: Val batch 4000: PER (avg): 0.4085 CTC Loss (avg): 53.2078 time: 9.041
2025-12-15 20:13:17,314: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:13:17,314: t15.2023.08.13 val PER: 0.3773
2025-12-15 20:13:17,315: t15.2023.08.18 val PER: 0.3713
2025-12-15 20:13:17,315: t15.2023.08.20 val PER: 0.3598
2025-12-15 20:13:17,315: t15.2023.08.25 val PER: 0.3358
2025-12-15 20:13:17,315: t15.2023.08.27 val PER: 0.4421
2025-12-15 20:13:17,315: t15.2023.09.01 val PER: 0.3271
2025-12-15 20:13:17,315: t15.2023.09.03 val PER: 0.3777
2025-12-15 20:13:17,315: t15.2023.09.24 val PER: 0.3386
2025-12-15 20:13:17,315: t15.2023.09.29 val PER: 0.3791
2025-12-15 20:13:17,315: t15.2023.10.01 val PER: 0.4287
2025-12-15 20:13:17,315: t15.2023.10.06 val PER: 0.3455
2025-12-15 20:13:17,315: t15.2023.10.08 val PER: 0.4601
2025-12-15 20:13:17,315: t15.2023.10.13 val PER: 0.4577
2025-12-15 20:13:17,315: t15.2023.10.15 val PER: 0.3909
2025-12-15 20:13:17,315: t15.2023.10.20 val PER: 0.4027
2025-12-15 20:13:17,315: t15.2023.10.22 val PER: 0.3653
2025-12-15 20:13:17,315: t15.2023.11.03 val PER: 0.4030
2025-12-15 20:13:17,315: t15.2023.11.04 val PER: 0.2355
2025-12-15 20:13:17,315: t15.2023.11.17 val PER: 0.3344
2025-12-15 20:13:17,316: t15.2023.11.19 val PER: 0.3094
2025-12-15 20:13:17,316: t15.2023.11.26 val PER: 0.4486
2025-12-15 20:13:17,316: t15.2023.12.03 val PER: 0.3939
2025-12-15 20:13:17,316: t15.2023.12.08 val PER: 0.4035
2025-12-15 20:13:17,316: t15.2023.12.10 val PER: 0.3719
2025-12-15 20:13:17,316: t15.2023.12.17 val PER: 0.4449
2025-12-15 20:13:17,316: t15.2023.12.29 val PER: 0.4194
2025-12-15 20:13:17,316: t15.2024.02.25 val PER: 0.3497
2025-12-15 20:13:17,316: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:13:17,316: t15.2024.03.08 val PER: 0.4765
2025-12-15 20:13:17,316: t15.2024.03.15 val PER: 0.4534
2025-12-15 20:13:17,316: t15.2024.03.17 val PER: 0.4247
2025-12-15 20:13:17,316: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:13:17,316: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:13:17,316: t15.2024.05.10 val PER: 0.4324
2025-12-15 20:13:17,316: t15.2024.06.14 val PER: 0.4038
2025-12-15 20:13:17,316: t15.2024.07.19 val PER: 0.4904
2025-12-15 20:13:17,316: t15.2024.07.21 val PER: 0.3545
2025-12-15 20:13:17,316: t15.2024.07.28 val PER: 0.3728
2025-12-15 20:13:17,317: t15.2025.01.10 val PER: 0.5275
2025-12-15 20:13:17,317: t15.2025.01.12 val PER: 0.4080
2025-12-15 20:13:17,317: t15.2025.03.14 val PER: 0.5828
2025-12-15 20:13:17,317: t15.2025.03.16 val PER: 0.4555
2025-12-15 20:13:17,317: t15.2025.03.30 val PER: 0.5253
2025-12-15 20:13:17,317: t15.2025.04.13 val PER: 0.4565
2025-12-15 20:13:17,317: New best test PER 0.6861 --> 0.4085
2025-12-15 20:13:17,317: Checkpointing model
2025-12-15 20:13:17,739: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:13:31,999: Train batch 4200: loss: 38.89 grad norm: 46.37 time: 0.052
2025-12-15 20:13:46,402: Train batch 4400: loss: 64.92 grad norm: 72.23 time: 0.072
2025-12-15 20:14:00,552: Train batch 4600: loss: 55.97 grad norm: 60.95 time: 0.074
2025-12-15 20:14:14,696: Train batch 4800: loss: 47.66 grad norm: 54.41 time: 0.065
2025-12-15 20:14:28,962: Train batch 5000: loss: 59.98 grad norm: 79.01 time: 0.083
2025-12-15 20:14:42,966: Train batch 5200: loss: 45.94 grad norm: 58.79 time: 0.066
2025-12-15 20:14:57,377: Train batch 5400: loss: 40.16 grad norm: 56.75 time: 0.057
2025-12-15 20:15:10,996: Train batch 5600: loss: 42.57 grad norm: 67.07 time: 0.077
2025-12-15 20:15:25,342: Train batch 5800: loss: 51.42 grad norm: 69.78 time: 0.088
2025-12-15 20:15:39,736: Train batch 6000: loss: 31.44 grad norm: 49.18 time: 0.060
2025-12-15 20:15:39,737: Running test after training batch: 6000
2025-12-15 20:15:48,790: Val batch 6000: PER (avg): 0.2757 CTC Loss (avg): 31.2025 time: 9.053
2025-12-15 20:15:48,790: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:15:48,790: t15.2023.08.13 val PER: 0.2391
2025-12-15 20:15:48,790: t15.2023.08.18 val PER: 0.2397
2025-12-15 20:15:48,790: t15.2023.08.20 val PER: 0.1994
2025-12-15 20:15:48,790: t15.2023.08.25 val PER: 0.1988
2025-12-15 20:15:48,790: t15.2023.08.27 val PER: 0.3119
2025-12-15 20:15:48,790: t15.2023.09.01 val PER: 0.1989
2025-12-15 20:15:48,790: t15.2023.09.03 val PER: 0.2648
2025-12-15 20:15:48,790: t15.2023.09.24 val PER: 0.2476
2025-12-15 20:15:48,790: t15.2023.09.29 val PER: 0.2597
2025-12-15 20:15:48,791: t15.2023.10.01 val PER: 0.2827
2025-12-15 20:15:48,791: t15.2023.10.06 val PER: 0.2282
2025-12-15 20:15:48,791: t15.2023.10.08 val PER: 0.3207
2025-12-15 20:15:48,791: t15.2023.10.13 val PER: 0.3227
2025-12-15 20:15:48,791: t15.2023.10.15 val PER: 0.2637
2025-12-15 20:15:48,791: t15.2023.10.20 val PER: 0.2852
2025-12-15 20:15:48,791: t15.2023.10.22 val PER: 0.2027
2025-12-15 20:15:48,791: t15.2023.11.03 val PER: 0.2951
2025-12-15 20:15:48,791: t15.2023.11.04 val PER: 0.0785
2025-12-15 20:15:48,791: t15.2023.11.17 val PER: 0.1664
2025-12-15 20:15:48,791: t15.2023.11.19 val PER: 0.1557
2025-12-15 20:15:48,791: t15.2023.11.26 val PER: 0.2906
2025-12-15 20:15:48,791: t15.2023.12.03 val PER: 0.2458
2025-12-15 20:15:48,791: t15.2023.12.08 val PER: 0.2450
2025-12-15 20:15:48,791: t15.2023.12.10 val PER: 0.2339
2025-12-15 20:15:48,791: t15.2023.12.17 val PER: 0.3056
2025-12-15 20:15:48,791: t15.2023.12.29 val PER: 0.2759
2025-12-15 20:15:48,791: t15.2024.02.25 val PER: 0.2261
2025-12-15 20:15:48,792: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:15:48,792: t15.2024.03.08 val PER: 0.3215
2025-12-15 20:15:48,792: t15.2024.03.15 val PER: 0.3221
2025-12-15 20:15:48,792: t15.2024.03.17 val PER: 0.2992
2025-12-15 20:15:48,792: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:15:48,792: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:15:48,792: t15.2024.05.10 val PER: 0.2927
2025-12-15 20:15:48,792: t15.2024.06.14 val PER: 0.3028
2025-12-15 20:15:48,792: t15.2024.07.19 val PER: 0.3540
2025-12-15 20:15:48,792: t15.2024.07.21 val PER: 0.2283
2025-12-15 20:15:48,792: t15.2024.07.28 val PER: 0.2456
2025-12-15 20:15:48,792: t15.2025.01.10 val PER: 0.4174
2025-12-15 20:15:48,792: t15.2025.01.12 val PER: 0.2818
2025-12-15 20:15:48,792: t15.2025.03.14 val PER: 0.4734
2025-12-15 20:15:48,792: t15.2025.03.16 val PER: 0.3455
2025-12-15 20:15:48,792: t15.2025.03.30 val PER: 0.4069
2025-12-15 20:15:48,792: t15.2025.04.13 val PER: 0.3295
2025-12-15 20:15:48,792: New best test PER 0.4085 --> 0.2757
2025-12-15 20:15:48,792: Checkpointing model
2025-12-15 20:15:49,195: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:16:03,595: Train batch 6200: loss: 39.07 grad norm: 66.39 time: 0.087
2025-12-15 20:16:15,653: Train batch 6400: loss: 35.97 grad norm: 49.97 time: 0.056
2025-12-15 20:16:26,575: Train batch 6600: loss: 41.76 grad norm: 64.16 time: 0.090
2025-12-15 20:16:39,963: Train batch 6800: loss: 43.32 grad norm: 67.74 time: 0.069
2025-12-15 20:16:54,554: Train batch 7000: loss: 32.87 grad norm: 47.29 time: 0.049
2025-12-15 20:17:06,482: Train batch 7200: loss: 44.68 grad norm: 63.44 time: 0.054
2025-12-15 20:17:21,006: Train batch 7400: loss: 37.25 grad norm: 51.34 time: 0.071
2025-12-15 20:17:35,550: Train batch 7600: loss: 39.91 grad norm: 59.96 time: 0.053
2025-12-15 20:17:49,434: Train batch 7800: loss: 45.02 grad norm: 72.14 time: 0.083
2025-12-15 20:18:01,553: Train batch 8000: loss: 34.39 grad norm: 62.58 time: 0.072
2025-12-15 20:18:01,554: Running test after training batch: 8000
2025-12-15 20:18:11,057: Val batch 8000: PER (avg): 0.2402 CTC Loss (avg): 27.9102 time: 9.503
2025-12-15 20:18:11,057: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:18:11,057: t15.2023.08.13 val PER: 0.1923
2025-12-15 20:18:11,057: t15.2023.08.18 val PER: 0.1794
2025-12-15 20:18:11,057: t15.2023.08.20 val PER: 0.1843
2025-12-15 20:18:11,057: t15.2023.08.25 val PER: 0.1611
2025-12-15 20:18:11,057: t15.2023.08.27 val PER: 0.2701
2025-12-15 20:18:11,057: t15.2023.09.01 val PER: 0.1526
2025-12-15 20:18:11,057: t15.2023.09.03 val PER: 0.2304
2025-12-15 20:18:11,057: t15.2023.09.24 val PER: 0.1893
2025-12-15 20:18:11,057: t15.2023.09.29 val PER: 0.2246
2025-12-15 20:18:11,057: t15.2023.10.01 val PER: 0.2701
2025-12-15 20:18:11,058: t15.2023.10.06 val PER: 0.1895
2025-12-15 20:18:11,058: t15.2023.10.08 val PER: 0.3004
2025-12-15 20:18:11,058: t15.2023.10.13 val PER: 0.2979
2025-12-15 20:18:11,058: t15.2023.10.15 val PER: 0.2353
2025-12-15 20:18:11,058: t15.2023.10.20 val PER: 0.2886
2025-12-15 20:18:11,058: t15.2023.10.22 val PER: 0.1938
2025-12-15 20:18:11,058: t15.2023.11.03 val PER: 0.2666
2025-12-15 20:18:11,058: t15.2023.11.04 val PER: 0.0819
2025-12-15 20:18:11,058: t15.2023.11.17 val PER: 0.1306
2025-12-15 20:18:11,058: t15.2023.11.19 val PER: 0.1397
2025-12-15 20:18:11,058: t15.2023.11.26 val PER: 0.2471
2025-12-15 20:18:11,058: t15.2023.12.03 val PER: 0.2279
2025-12-15 20:18:11,058: t15.2023.12.08 val PER: 0.2250
2025-12-15 20:18:11,058: t15.2023.12.10 val PER: 0.2076
2025-12-15 20:18:11,058: t15.2023.12.17 val PER: 0.2557
2025-12-15 20:18:11,058: t15.2023.12.29 val PER: 0.2210
2025-12-15 20:18:11,058: t15.2024.02.25 val PER: 0.1826
2025-12-15 20:18:11,058: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:18:11,058: t15.2024.03.08 val PER: 0.3044
2025-12-15 20:18:11,059: t15.2024.03.15 val PER: 0.2852
2025-12-15 20:18:11,059: t15.2024.03.17 val PER: 0.2245
2025-12-15 20:18:11,059: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:18:11,059: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:18:11,059: t15.2024.05.10 val PER: 0.2541
2025-12-15 20:18:11,059: t15.2024.06.14 val PER: 0.2603
2025-12-15 20:18:11,059: t15.2024.07.19 val PER: 0.3092
2025-12-15 20:18:11,059: t15.2024.07.21 val PER: 0.1903
2025-12-15 20:18:11,059: t15.2024.07.28 val PER: 0.2206
2025-12-15 20:18:11,059: t15.2025.01.10 val PER: 0.4008
2025-12-15 20:18:11,059: t15.2025.01.12 val PER: 0.2533
2025-12-15 20:18:11,059: t15.2025.03.14 val PER: 0.4038
2025-12-15 20:18:11,059: t15.2025.03.16 val PER: 0.2814
2025-12-15 20:18:11,059: t15.2025.03.30 val PER: 0.3793
2025-12-15 20:18:11,059: t15.2025.04.13 val PER: 0.2896
2025-12-15 20:18:11,059: New best test PER 0.2757 --> 0.2402
2025-12-15 20:18:11,059: Checkpointing model
2025-12-15 20:18:11,528: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:18:25,033: Train batch 8200: loss: 29.07 grad norm: 48.46 time: 0.051
2025-12-15 20:18:37,206: Train batch 8400: loss: 37.41 grad norm: 61.41 time: 0.042
2025-12-15 20:18:49,849: Train batch 8600: loss: 35.57 grad norm: 65.86 time: 0.055
2025-12-15 20:19:03,336: Train batch 8800: loss: 37.35 grad norm: 74.41 time: 0.075
2025-12-15 20:19:16,469: Train batch 9000: loss: 39.78 grad norm: 57.52 time: 0.052
2025-12-15 20:19:29,513: Train batch 9200: loss: 37.84 grad norm: 48.01 time: 0.062
2025-12-15 20:19:42,875: Train batch 9400: loss: 27.16 grad norm: 42.73 time: 0.076
2025-12-15 20:19:57,136: Train batch 9600: loss: 37.44 grad norm: 67.40 time: 0.064
2025-12-15 20:20:11,611: Train batch 9800: loss: 25.19 grad norm: 31.67 time: 0.048
2025-12-15 20:20:26,202: Train batch 10000: loss: 33.92 grad norm: 54.62 time: 0.063
2025-12-15 20:20:26,203: Running test after training batch: 10000
2025-12-15 20:20:35,663: Val batch 10000: PER (avg): 0.2101 CTC Loss (avg): 25.6422 time: 9.459
2025-12-15 20:20:35,663: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:20:35,663: t15.2023.08.13 val PER: 0.1663
2025-12-15 20:20:35,663: t15.2023.08.18 val PER: 0.1668
2025-12-15 20:20:35,664: t15.2023.08.20 val PER: 0.1541
2025-12-15 20:20:35,664: t15.2023.08.25 val PER: 0.1762
2025-12-15 20:20:35,664: t15.2023.08.27 val PER: 0.2588
2025-12-15 20:20:35,664: t15.2023.09.01 val PER: 0.1307
2025-12-15 20:20:35,664: t15.2023.09.03 val PER: 0.1995
2025-12-15 20:20:35,664: t15.2023.09.24 val PER: 0.1748
2025-12-15 20:20:35,664: t15.2023.09.29 val PER: 0.1978
2025-12-15 20:20:35,664: t15.2023.10.01 val PER: 0.2424
2025-12-15 20:20:35,664: t15.2023.10.06 val PER: 0.1485
2025-12-15 20:20:35,664: t15.2023.10.08 val PER: 0.2612
2025-12-15 20:20:35,664: t15.2023.10.13 val PER: 0.2405
2025-12-15 20:20:35,664: t15.2023.10.15 val PER: 0.2063
2025-12-15 20:20:35,664: t15.2023.10.20 val PER: 0.2483
2025-12-15 20:20:35,664: t15.2023.10.22 val PER: 0.1726
2025-12-15 20:20:35,664: t15.2023.11.03 val PER: 0.2212
2025-12-15 20:20:35,664: t15.2023.11.04 val PER: 0.0478
2025-12-15 20:20:35,664: t15.2023.11.17 val PER: 0.0995
2025-12-15 20:20:35,664: t15.2023.11.19 val PER: 0.1218
2025-12-15 20:20:35,665: t15.2023.11.26 val PER: 0.2080
2025-12-15 20:20:35,665: t15.2023.12.03 val PER: 0.1607
2025-12-15 20:20:35,665: t15.2023.12.08 val PER: 0.1744
2025-12-15 20:20:35,665: t15.2023.12.10 val PER: 0.1472
2025-12-15 20:20:35,665: t15.2023.12.17 val PER: 0.2495
2025-12-15 20:20:35,665: t15.2023.12.29 val PER: 0.2100
2025-12-15 20:20:35,665: t15.2024.02.25 val PER: 0.1545
2025-12-15 20:20:35,665: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:20:35,665: t15.2024.03.08 val PER: 0.2660
2025-12-15 20:20:35,665: t15.2024.03.15 val PER: 0.2677
2025-12-15 20:20:35,665: t15.2024.03.17 val PER: 0.1925
2025-12-15 20:20:35,665: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:20:35,665: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:20:35,665: t15.2024.05.10 val PER: 0.2259
2025-12-15 20:20:35,665: t15.2024.06.14 val PER: 0.2492
2025-12-15 20:20:35,665: t15.2024.07.19 val PER: 0.2795
2025-12-15 20:20:35,665: t15.2024.07.21 val PER: 0.1483
2025-12-15 20:20:35,665: t15.2024.07.28 val PER: 0.1779
2025-12-15 20:20:35,665: t15.2025.01.10 val PER: 0.3788
2025-12-15 20:20:35,666: t15.2025.01.12 val PER: 0.2232
2025-12-15 20:20:35,666: t15.2025.03.14 val PER: 0.3964
2025-12-15 20:20:35,666: t15.2025.03.16 val PER: 0.2644
2025-12-15 20:20:35,666: t15.2025.03.30 val PER: 0.3494
2025-12-15 20:20:35,666: t15.2025.04.13 val PER: 0.2539
2025-12-15 20:20:35,666: New best test PER 0.2402 --> 0.2101
2025-12-15 20:20:35,666: Checkpointing model
2025-12-15 20:20:36,092: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:20:50,217: Train batch 10200: loss: 35.11 grad norm: 56.44 time: 0.062
2025-12-15 20:21:04,500: Train batch 10400: loss: 40.31 grad norm: 54.58 time: 0.063
2025-12-15 20:21:19,012: Train batch 10600: loss: 31.46 grad norm: 34.37 time: 0.034
2025-12-15 20:21:32,552: Train batch 10800: loss: 28.61 grad norm: 36.03 time: 0.056
2025-12-15 20:21:46,967: Train batch 11000: loss: 26.03 grad norm: 40.39 time: 0.056
2025-12-15 20:22:01,409: Train batch 11200: loss: 23.40 grad norm: 40.53 time: 0.067
2025-12-15 20:22:12,942: Train batch 11400: loss: 30.32 grad norm: 54.77 time: 0.080
2025-12-15 20:22:26,853: Train batch 11600: loss: 26.66 grad norm: 40.67 time: 0.056
2025-12-15 20:22:41,376: Train batch 11800: loss: 28.35 grad norm: 58.40 time: 0.076
2025-12-15 20:22:56,262: Train batch 12000: loss: 27.95 grad norm: 35.07 time: 0.073
2025-12-15 20:22:56,262: Running test after training batch: 12000
2025-12-15 20:23:05,672: Val batch 12000: PER (avg): 0.1981 CTC Loss (avg): 24.5870 time: 9.410
2025-12-15 20:23:05,673: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:23:05,673: t15.2023.08.13 val PER: 0.1518
2025-12-15 20:23:05,673: t15.2023.08.18 val PER: 0.1685
2025-12-15 20:23:05,673: t15.2023.08.20 val PER: 0.1191
2025-12-15 20:23:05,673: t15.2023.08.25 val PER: 0.1566
2025-12-15 20:23:05,673: t15.2023.08.27 val PER: 0.2186
2025-12-15 20:23:05,673: t15.2023.09.01 val PER: 0.1136
2025-12-15 20:23:05,673: t15.2023.09.03 val PER: 0.1876
2025-12-15 20:23:05,673: t15.2023.09.24 val PER: 0.1602
2025-12-15 20:23:05,673: t15.2023.09.29 val PER: 0.1774
2025-12-15 20:23:05,673: t15.2023.10.01 val PER: 0.2114
2025-12-15 20:23:05,673: t15.2023.10.06 val PER: 0.1399
2025-12-15 20:23:05,673: t15.2023.10.08 val PER: 0.2558
2025-12-15 20:23:05,673: t15.2023.10.13 val PER: 0.2389
2025-12-15 20:23:05,673: t15.2023.10.15 val PER: 0.1872
2025-12-15 20:23:05,674: t15.2023.10.20 val PER: 0.2383
2025-12-15 20:23:05,674: t15.2023.10.22 val PER: 0.1648
2025-12-15 20:23:05,674: t15.2023.11.03 val PER: 0.2123
2025-12-15 20:23:05,674: t15.2023.11.04 val PER: 0.0512
2025-12-15 20:23:05,674: t15.2023.11.17 val PER: 0.0918
2025-12-15 20:23:05,674: t15.2023.11.19 val PER: 0.1118
2025-12-15 20:23:05,674: t15.2023.11.26 val PER: 0.2087
2025-12-15 20:23:05,674: t15.2023.12.03 val PER: 0.1597
2025-12-15 20:23:05,674: t15.2023.12.08 val PER: 0.1611
2025-12-15 20:23:05,674: t15.2023.12.10 val PER: 0.1472
2025-12-15 20:23:05,674: t15.2023.12.17 val PER: 0.2162
2025-12-15 20:23:05,674: t15.2023.12.29 val PER: 0.1874
2025-12-15 20:23:05,674: t15.2024.02.25 val PER: 0.1404
2025-12-15 20:23:05,674: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:23:05,674: t15.2024.03.08 val PER: 0.2745
2025-12-15 20:23:05,674: t15.2024.03.15 val PER: 0.2695
2025-12-15 20:23:05,674: t15.2024.03.17 val PER: 0.1932
2025-12-15 20:23:05,674: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:23:05,674: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:23:05,675: t15.2024.05.10 val PER: 0.2333
2025-12-15 20:23:05,675: t15.2024.06.14 val PER: 0.2476
2025-12-15 20:23:05,675: t15.2024.07.19 val PER: 0.2610
2025-12-15 20:23:05,675: t15.2024.07.21 val PER: 0.1497
2025-12-15 20:23:05,675: t15.2024.07.28 val PER: 0.1515
2025-12-15 20:23:05,675: t15.2025.01.10 val PER: 0.3512
2025-12-15 20:23:05,675: t15.2025.01.12 val PER: 0.1940
2025-12-15 20:23:05,675: t15.2025.03.14 val PER: 0.3861
2025-12-15 20:23:05,675: t15.2025.03.16 val PER: 0.2539
2025-12-15 20:23:05,675: t15.2025.03.30 val PER: 0.3586
2025-12-15 20:23:05,675: t15.2025.04.13 val PER: 0.2553
2025-12-15 20:23:05,675: New best test PER 0.2101 --> 0.1981
2025-12-15 20:23:05,675: Checkpointing model
2025-12-15 20:23:06,110: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:23:20,187: Train batch 12200: loss: 34.57 grad norm: 46.23 time: 0.055
2025-12-15 20:23:34,657: Train batch 12400: loss: 34.16 grad norm: 58.29 time: 0.070
2025-12-15 20:23:48,969: Train batch 12600: loss: 25.70 grad norm: 35.38 time: 0.060
2025-12-15 20:24:02,596: Train batch 12800: loss: 28.68 grad norm: 59.16 time: 0.045
2025-12-15 20:24:15,610: Train batch 13000: loss: 23.62 grad norm: 31.40 time: 0.052
2025-12-15 20:24:29,937: Train batch 13200: loss: 29.04 grad norm: 32.50 time: 0.055
2025-12-15 20:24:44,139: Train batch 13400: loss: 24.58 grad norm: 45.57 time: 0.069
2025-12-15 20:24:58,082: Train batch 13600: loss: 27.39 grad norm: 29.94 time: 0.061
2025-12-15 20:25:11,192: Train batch 13800: loss: 25.82 grad norm: 35.64 time: 0.072
2025-12-15 20:25:23,632: Train batch 14000: loss: 31.79 grad norm: 49.60 time: 0.081
2025-12-15 20:25:23,632: Running test after training batch: 14000
2025-12-15 20:25:33,318: Val batch 14000: PER (avg): 0.1875 CTC Loss (avg): 24.7381 time: 9.685
2025-12-15 20:25:33,318: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:25:33,318: t15.2023.08.13 val PER: 0.1601
2025-12-15 20:25:33,318: t15.2023.08.18 val PER: 0.1425
2025-12-15 20:25:33,318: t15.2023.08.20 val PER: 0.1247
2025-12-15 20:25:33,318: t15.2023.08.25 val PER: 0.1340
2025-12-15 20:25:33,319: t15.2023.08.27 val PER: 0.2170
2025-12-15 20:25:33,319: t15.2023.09.01 val PER: 0.1039
2025-12-15 20:25:33,319: t15.2023.09.03 val PER: 0.1627
2025-12-15 20:25:33,319: t15.2023.09.24 val PER: 0.1566
2025-12-15 20:25:33,319: t15.2023.09.29 val PER: 0.1723
2025-12-15 20:25:33,319: t15.2023.10.01 val PER: 0.2094
2025-12-15 20:25:33,319: t15.2023.10.06 val PER: 0.1356
2025-12-15 20:25:33,319: t15.2023.10.08 val PER: 0.2300
2025-12-15 20:25:33,319: t15.2023.10.13 val PER: 0.2459
2025-12-15 20:25:33,319: t15.2023.10.15 val PER: 0.1872
2025-12-15 20:25:33,319: t15.2023.10.20 val PER: 0.2215
2025-12-15 20:25:33,319: t15.2023.10.22 val PER: 0.1537
2025-12-15 20:25:33,319: t15.2023.11.03 val PER: 0.2205
2025-12-15 20:25:33,319: t15.2023.11.04 val PER: 0.0478
2025-12-15 20:25:33,319: t15.2023.11.17 val PER: 0.0653
2025-12-15 20:25:33,319: t15.2023.11.19 val PER: 0.1138
2025-12-15 20:25:33,319: t15.2023.11.26 val PER: 0.1775
2025-12-15 20:25:33,320: t15.2023.12.03 val PER: 0.1502
2025-12-15 20:25:33,320: t15.2023.12.08 val PER: 0.1511
2025-12-15 20:25:33,320: t15.2023.12.10 val PER: 0.1577
2025-12-15 20:25:33,320: t15.2023.12.17 val PER: 0.2131
2025-12-15 20:25:33,320: t15.2023.12.29 val PER: 0.1791
2025-12-15 20:25:33,320: t15.2024.02.25 val PER: 0.1306
2025-12-15 20:25:33,320: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:25:33,320: t15.2024.03.08 val PER: 0.2703
2025-12-15 20:25:33,320: t15.2024.03.15 val PER: 0.2402
2025-12-15 20:25:33,320: t15.2024.03.17 val PER: 0.1674
2025-12-15 20:25:33,320: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:25:33,320: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:25:33,320: t15.2024.05.10 val PER: 0.2140
2025-12-15 20:25:33,320: t15.2024.06.14 val PER: 0.2224
2025-12-15 20:25:33,320: t15.2024.07.19 val PER: 0.2683
2025-12-15 20:25:33,320: t15.2024.07.21 val PER: 0.1221
2025-12-15 20:25:33,320: t15.2024.07.28 val PER: 0.1500
2025-12-15 20:25:33,320: t15.2025.01.10 val PER: 0.3333
2025-12-15 20:25:33,320: t15.2025.01.12 val PER: 0.1848
2025-12-15 20:25:33,321: t15.2025.03.14 val PER: 0.3698
2025-12-15 20:25:33,321: t15.2025.03.16 val PER: 0.2251
2025-12-15 20:25:33,321: t15.2025.03.30 val PER: 0.3115
2025-12-15 20:25:33,321: t15.2025.04.13 val PER: 0.2582
2025-12-15 20:25:33,321: New best test PER 0.1981 --> 0.1875
2025-12-15 20:25:33,321: Checkpointing model
2025-12-15 20:25:33,735: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:25:45,874: Train batch 14200: loss: 24.00 grad norm: 46.35 time: 0.077
2025-12-15 20:25:59,481: Train batch 14400: loss: 25.25 grad norm: 39.96 time: 0.055
2025-12-15 20:26:13,752: Train batch 14600: loss: 29.44 grad norm: 48.21 time: 0.073
2025-12-15 20:26:27,000: Train batch 14800: loss: 23.49 grad norm: 45.22 time: 0.056
2025-12-15 20:26:40,880: Train batch 15000: loss: 30.32 grad norm: 39.66 time: 0.056
2025-12-15 20:26:55,233: Train batch 15200: loss: 26.34 grad norm: 42.37 time: 0.052
2025-12-15 20:27:09,305: Train batch 15400: loss: 28.54 grad norm: 46.07 time: 0.089
2025-12-15 20:27:21,802: Train batch 15600: loss: 23.16 grad norm: 34.80 time: 0.053
2025-12-15 20:27:35,380: Train batch 15800: loss: 22.54 grad norm: 22.40 time: 0.073
2025-12-15 20:27:48,650: Train batch 16000: loss: 18.90 grad norm: 24.50 time: 0.075
2025-12-15 20:27:48,651: Running test after training batch: 16000
2025-12-15 20:27:58,179: Val batch 16000: PER (avg): 0.1825 CTC Loss (avg): 24.8555 time: 9.528
2025-12-15 20:27:58,179: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:27:58,179: t15.2023.08.13 val PER: 0.1455
2025-12-15 20:27:58,179: t15.2023.08.18 val PER: 0.1383
2025-12-15 20:27:58,179: t15.2023.08.20 val PER: 0.1160
2025-12-15 20:27:58,179: t15.2023.08.25 val PER: 0.1160
2025-12-15 20:27:58,179: t15.2023.08.27 val PER: 0.1849
2025-12-15 20:27:58,180: t15.2023.09.01 val PER: 0.0942
2025-12-15 20:27:58,180: t15.2023.09.03 val PER: 0.1663
2025-12-15 20:27:58,180: t15.2023.09.24 val PER: 0.1432
2025-12-15 20:27:58,180: t15.2023.09.29 val PER: 0.1793
2025-12-15 20:27:58,180: t15.2023.10.01 val PER: 0.2061
2025-12-15 20:27:58,180: t15.2023.10.06 val PER: 0.1324
2025-12-15 20:27:58,180: t15.2023.10.08 val PER: 0.2219
2025-12-15 20:27:58,180: t15.2023.10.13 val PER: 0.2444
2025-12-15 20:27:58,180: t15.2023.10.15 val PER: 0.1800
2025-12-15 20:27:58,180: t15.2023.10.20 val PER: 0.2081
2025-12-15 20:27:58,180: t15.2023.10.22 val PER: 0.1414
2025-12-15 20:27:58,180: t15.2023.11.03 val PER: 0.2198
2025-12-15 20:27:58,180: t15.2023.11.04 val PER: 0.0785
2025-12-15 20:27:58,180: t15.2023.11.17 val PER: 0.0529
2025-12-15 20:27:58,180: t15.2023.11.19 val PER: 0.1138
2025-12-15 20:27:58,180: t15.2023.11.26 val PER: 0.1732
2025-12-15 20:27:58,180: t15.2023.12.03 val PER: 0.1261
2025-12-15 20:27:58,180: t15.2023.12.08 val PER: 0.1498
2025-12-15 20:27:58,180: t15.2023.12.10 val PER: 0.1196
2025-12-15 20:27:58,180: t15.2023.12.17 val PER: 0.2069
2025-12-15 20:27:58,181: t15.2023.12.29 val PER: 0.1627
2025-12-15 20:27:58,181: t15.2024.02.25 val PER: 0.1110
2025-12-15 20:27:58,181: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:27:58,181: t15.2024.03.08 val PER: 0.2774
2025-12-15 20:27:58,181: t15.2024.03.15 val PER: 0.2527
2025-12-15 20:27:58,181: t15.2024.03.17 val PER: 0.1785
2025-12-15 20:27:58,181: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:27:58,181: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:27:58,181: t15.2024.05.10 val PER: 0.2140
2025-12-15 20:27:58,181: t15.2024.06.14 val PER: 0.2256
2025-12-15 20:27:58,181: t15.2024.07.19 val PER: 0.2426
2025-12-15 20:27:58,181: t15.2024.07.21 val PER: 0.1200
2025-12-15 20:27:58,181: t15.2024.07.28 val PER: 0.1618
2025-12-15 20:27:58,181: t15.2025.01.10 val PER: 0.3430
2025-12-15 20:27:58,181: t15.2025.01.12 val PER: 0.1871
2025-12-15 20:27:58,181: t15.2025.03.14 val PER: 0.3491
2025-12-15 20:27:58,181: t15.2025.03.16 val PER: 0.2225
2025-12-15 20:27:58,181: t15.2025.03.30 val PER: 0.3103
2025-12-15 20:27:58,181: t15.2025.04.13 val PER: 0.2582
2025-12-15 20:27:58,181: New best test PER 0.1875 --> 0.1825
2025-12-15 20:27:58,182: Checkpointing model
2025-12-15 20:27:58,609: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:28:11,536: Train batch 16200: loss: 34.02 grad norm: 39.13 time: 0.070
2025-12-15 20:28:25,970: Train batch 16400: loss: 25.80 grad norm: 36.45 time: 0.060
2025-12-15 20:28:39,367: Train batch 16600: loss: 29.27 grad norm: 44.46 time: 0.061
2025-12-15 20:28:53,744: Train batch 16800: loss: 28.74 grad norm: 33.67 time: 0.069
2025-12-15 20:29:07,292: Train batch 17000: loss: 25.45 grad norm: 64.68 time: 0.060
2025-12-15 20:29:20,812: Train batch 17200: loss: 22.21 grad norm: 21.44 time: 0.043
2025-12-15 20:29:35,131: Train batch 17400: loss: 29.52 grad norm: 58.74 time: 0.055
2025-12-15 20:29:48,431: Train batch 17600: loss: 28.39 grad norm: 41.76 time: 0.044
2025-12-15 20:30:01,763: Train batch 17800: loss: 34.68 grad norm: 62.23 time: 0.079
2025-12-15 20:30:15,969: Train batch 18000: loss: 29.28 grad norm: 46.61 time: 0.052
2025-12-15 20:30:15,970: Running test after training batch: 18000
2025-12-15 20:30:25,179: Val batch 18000: PER (avg): 0.1738 CTC Loss (avg): 24.8884 time: 9.209
2025-12-15 20:30:25,180: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:30:25,180: t15.2023.08.13 val PER: 0.1590
2025-12-15 20:30:25,180: t15.2023.08.18 val PER: 0.1433
2025-12-15 20:30:25,180: t15.2023.08.20 val PER: 0.1056
2025-12-15 20:30:25,180: t15.2023.08.25 val PER: 0.1190
2025-12-15 20:30:25,180: t15.2023.08.27 val PER: 0.2251
2025-12-15 20:30:25,180: t15.2023.09.01 val PER: 0.0836
2025-12-15 20:30:25,180: t15.2023.09.03 val PER: 0.1770
2025-12-15 20:30:25,180: t15.2023.09.24 val PER: 0.1274
2025-12-15 20:30:25,180: t15.2023.09.29 val PER: 0.1506
2025-12-15 20:30:25,180: t15.2023.10.01 val PER: 0.1823
2025-12-15 20:30:25,180: t15.2023.10.06 val PER: 0.1270
2025-12-15 20:30:25,180: t15.2023.10.08 val PER: 0.2314
2025-12-15 20:30:25,180: t15.2023.10.13 val PER: 0.2265
2025-12-15 20:30:25,180: t15.2023.10.15 val PER: 0.1707
2025-12-15 20:30:25,180: t15.2023.10.20 val PER: 0.2148
2025-12-15 20:30:25,180: t15.2023.10.22 val PER: 0.1481
2025-12-15 20:30:25,181: t15.2023.11.03 val PER: 0.2252
2025-12-15 20:30:25,181: t15.2023.11.04 val PER: 0.0512
2025-12-15 20:30:25,181: t15.2023.11.17 val PER: 0.0575
2025-12-15 20:30:25,181: t15.2023.11.19 val PER: 0.0998
2025-12-15 20:30:25,181: t15.2023.11.26 val PER: 0.1616
2025-12-15 20:30:25,181: t15.2023.12.03 val PER: 0.1313
2025-12-15 20:30:25,181: t15.2023.12.08 val PER: 0.1338
2025-12-15 20:30:25,181: t15.2023.12.10 val PER: 0.1143
2025-12-15 20:30:25,181: t15.2023.12.17 val PER: 0.1736
2025-12-15 20:30:25,181: t15.2023.12.29 val PER: 0.1640
2025-12-15 20:30:25,181: t15.2024.02.25 val PER: 0.1138
2025-12-15 20:30:25,181: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:30:25,181: t15.2024.03.08 val PER: 0.2489
2025-12-15 20:30:25,181: t15.2024.03.15 val PER: 0.2308
2025-12-15 20:30:25,181: t15.2024.03.17 val PER: 0.1646
2025-12-15 20:30:25,181: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:30:25,181: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:30:25,181: t15.2024.05.10 val PER: 0.2363
2025-12-15 20:30:25,181: t15.2024.06.14 val PER: 0.2050
2025-12-15 20:30:25,181: t15.2024.07.19 val PER: 0.2162
2025-12-15 20:30:25,182: t15.2024.07.21 val PER: 0.1124
2025-12-15 20:30:25,182: t15.2024.07.28 val PER: 0.1485
2025-12-15 20:30:25,182: t15.2025.01.10 val PER: 0.3375
2025-12-15 20:30:25,182: t15.2025.01.12 val PER: 0.1817
2025-12-15 20:30:25,182: t15.2025.03.14 val PER: 0.3491
2025-12-15 20:30:25,182: t15.2025.03.16 val PER: 0.2186
2025-12-15 20:30:25,182: t15.2025.03.30 val PER: 0.2793
2025-12-15 20:30:25,182: t15.2025.04.13 val PER: 0.2340
2025-12-15 20:30:25,182: New best test PER 0.1825 --> 0.1738
2025-12-15 20:30:25,182: Checkpointing model
2025-12-15 20:30:25,604: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:30:37,834: Train batch 18200: loss: 27.37 grad norm: 34.46 time: 0.033
2025-12-15 20:30:51,452: Train batch 18400: loss: 21.39 grad norm: 54.50 time: 0.076
2025-12-15 20:31:04,531: Train batch 18600: loss: 25.45 grad norm: 53.34 time: 0.055
2025-12-15 20:31:18,231: Train batch 18800: loss: 25.03 grad norm: 37.07 time: 0.067
2025-12-15 20:31:32,335: Train batch 19000: loss: 17.86 grad norm: 30.28 time: 0.068
2025-12-15 20:31:46,562: Train batch 19200: loss: 30.54 grad norm: 55.32 time: 0.055
2025-12-15 20:31:59,260: Train batch 19400: loss: 24.00 grad norm: 33.36 time: 0.070
2025-12-15 20:32:13,107: Train batch 19600: loss: 24.66 grad norm: 23.03 time: 0.092
2025-12-15 20:32:26,637: Train batch 19800: loss: 27.91 grad norm: 32.34 time: 0.077
2025-12-15 20:32:40,801: Train batch 20000: loss: 28.30 grad norm: 47.83 time: 0.079
2025-12-15 20:32:40,802: Running test after training batch: 20000
2025-12-15 20:32:50,011: Val batch 20000: PER (avg): 0.1720 CTC Loss (avg): 25.5549 time: 9.209
2025-12-15 20:32:50,011: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:32:50,011: t15.2023.08.13 val PER: 0.1372
2025-12-15 20:32:50,011: t15.2023.08.18 val PER: 0.1366
2025-12-15 20:32:50,011: t15.2023.08.20 val PER: 0.0985
2025-12-15 20:32:50,012: t15.2023.08.25 val PER: 0.1265
2025-12-15 20:32:50,012: t15.2023.08.27 val PER: 0.1752
2025-12-15 20:32:50,012: t15.2023.09.01 val PER: 0.0893
2025-12-15 20:32:50,012: t15.2023.09.03 val PER: 0.1686
2025-12-15 20:32:50,012: t15.2023.09.24 val PER: 0.1274
2025-12-15 20:32:50,012: t15.2023.09.29 val PER: 0.1436
2025-12-15 20:32:50,012: t15.2023.10.01 val PER: 0.2094
2025-12-15 20:32:50,012: t15.2023.10.06 val PER: 0.1324
2025-12-15 20:32:50,012: t15.2023.10.08 val PER: 0.2192
2025-12-15 20:32:50,012: t15.2023.10.13 val PER: 0.2327
2025-12-15 20:32:50,012: t15.2023.10.15 val PER: 0.1833
2025-12-15 20:32:50,012: t15.2023.10.20 val PER: 0.2081
2025-12-15 20:32:50,012: t15.2023.10.22 val PER: 0.1448
2025-12-15 20:32:50,012: t15.2023.11.03 val PER: 0.2252
2025-12-15 20:32:50,012: t15.2023.11.04 val PER: 0.0614
2025-12-15 20:32:50,012: t15.2023.11.17 val PER: 0.0467
2025-12-15 20:32:50,012: t15.2023.11.19 val PER: 0.0938
2025-12-15 20:32:50,012: t15.2023.11.26 val PER: 0.1333
2025-12-15 20:32:50,013: t15.2023.12.03 val PER: 0.1345
2025-12-15 20:32:50,013: t15.2023.12.08 val PER: 0.1265
2025-12-15 20:32:50,013: t15.2023.12.10 val PER: 0.1288
2025-12-15 20:32:50,013: t15.2023.12.17 val PER: 0.1632
2025-12-15 20:32:50,013: t15.2023.12.29 val PER: 0.1469
2025-12-15 20:32:50,013: t15.2024.02.25 val PER: 0.1053
2025-12-15 20:32:50,013: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:32:50,013: t15.2024.03.08 val PER: 0.2276
2025-12-15 20:32:50,013: t15.2024.03.15 val PER: 0.2295
2025-12-15 20:32:50,013: t15.2024.03.17 val PER: 0.1653
2025-12-15 20:32:50,013: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:32:50,013: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:32:50,013: t15.2024.05.10 val PER: 0.1976
2025-12-15 20:32:50,013: t15.2024.06.14 val PER: 0.2035
2025-12-15 20:32:50,013: t15.2024.07.19 val PER: 0.2334
2025-12-15 20:32:50,013: t15.2024.07.21 val PER: 0.1186
2025-12-15 20:32:50,013: t15.2024.07.28 val PER: 0.1515
2025-12-15 20:32:50,013: t15.2025.01.10 val PER: 0.3444
2025-12-15 20:32:50,013: t15.2025.01.12 val PER: 0.1901
2025-12-15 20:32:50,014: t15.2025.03.14 val PER: 0.3476
2025-12-15 20:32:50,014: t15.2025.03.16 val PER: 0.2186
2025-12-15 20:32:50,014: t15.2025.03.30 val PER: 0.2793
2025-12-15 20:32:50,014: t15.2025.04.13 val PER: 0.2582
2025-12-15 20:32:50,014: New best test PER 0.1738 --> 0.1720
2025-12-15 20:32:50,014: Checkpointing model
2025-12-15 20:32:50,451: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:33:04,402: Train batch 20200: loss: 22.34 grad norm: 43.57 time: 0.076
2025-12-15 20:33:18,874: Train batch 20400: loss: 35.04 grad norm: 51.10 time: 0.056
2025-12-15 20:33:33,449: Train batch 20600: loss: 23.62 grad norm: 17.99 time: 0.086
2025-12-15 20:33:47,214: Train batch 20800: loss: 25.93 grad norm: 83.48 time: 0.036
2025-12-15 20:34:01,447: Train batch 21000: loss: 23.04 grad norm: 38.44 time: 0.071
2025-12-15 20:34:15,453: Train batch 21200: loss: 23.58 grad norm: 32.64 time: 0.089
2025-12-15 20:34:29,713: Train batch 21400: loss: 26.32 grad norm: 26.65 time: 0.073
2025-12-15 20:34:43,064: Train batch 21600: loss: 23.94 grad norm: 30.75 time: 0.057
2025-12-15 20:34:57,273: Train batch 21800: loss: 28.01 grad norm: 75.64 time: 0.074
2025-12-15 20:35:11,378: Train batch 22000: loss: 26.88 grad norm: 47.51 time: 0.067
2025-12-15 20:35:11,378: Running test after training batch: 22000
2025-12-15 20:35:20,551: Val batch 22000: PER (avg): 0.1709 CTC Loss (avg): 25.8551 time: 9.173
2025-12-15 20:35:20,551: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:35:20,551: t15.2023.08.13 val PER: 0.1289
2025-12-15 20:35:20,551: t15.2023.08.18 val PER: 0.1366
2025-12-15 20:35:20,552: t15.2023.08.20 val PER: 0.1033
2025-12-15 20:35:20,552: t15.2023.08.25 val PER: 0.1220
2025-12-15 20:35:20,552: t15.2023.08.27 val PER: 0.1897
2025-12-15 20:35:20,552: t15.2023.09.01 val PER: 0.0990
2025-12-15 20:35:20,552: t15.2023.09.03 val PER: 0.1639
2025-12-15 20:35:20,552: t15.2023.09.24 val PER: 0.1311
2025-12-15 20:35:20,552: t15.2023.09.29 val PER: 0.1519
2025-12-15 20:35:20,552: t15.2023.10.01 val PER: 0.1830
2025-12-15 20:35:20,552: t15.2023.10.06 val PER: 0.1389
2025-12-15 20:35:20,552: t15.2023.10.08 val PER: 0.2422
2025-12-15 20:35:20,552: t15.2023.10.13 val PER: 0.2304
2025-12-15 20:35:20,552: t15.2023.10.15 val PER: 0.1707
2025-12-15 20:35:20,552: t15.2023.10.20 val PER: 0.1913
2025-12-15 20:35:20,552: t15.2023.10.22 val PER: 0.1481
2025-12-15 20:35:20,552: t15.2023.11.03 val PER: 0.2171
2025-12-15 20:35:20,552: t15.2023.11.04 val PER: 0.0478
2025-12-15 20:35:20,552: t15.2023.11.17 val PER: 0.0638
2025-12-15 20:35:20,552: t15.2023.11.19 val PER: 0.1257
2025-12-15 20:35:20,552: t15.2023.11.26 val PER: 0.1442
2025-12-15 20:35:20,553: t15.2023.12.03 val PER: 0.1282
2025-12-15 20:35:20,553: t15.2023.12.08 val PER: 0.1252
2025-12-15 20:35:20,553: t15.2023.12.10 val PER: 0.1156
2025-12-15 20:35:20,553: t15.2023.12.17 val PER: 0.1694
2025-12-15 20:35:20,553: t15.2023.12.29 val PER: 0.1428
2025-12-15 20:35:20,553: t15.2024.02.25 val PER: 0.1096
2025-12-15 20:35:20,553: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:35:20,553: t15.2024.03.08 val PER: 0.2404
2025-12-15 20:35:20,553: t15.2024.03.15 val PER: 0.2301
2025-12-15 20:35:20,553: t15.2024.03.17 val PER: 0.1625
2025-12-15 20:35:20,553: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:35:20,553: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:35:20,553: t15.2024.05.10 val PER: 0.1872
2025-12-15 20:35:20,553: t15.2024.06.14 val PER: 0.2129
2025-12-15 20:35:20,553: t15.2024.07.19 val PER: 0.2287
2025-12-15 20:35:20,553: t15.2024.07.21 val PER: 0.1103
2025-12-15 20:35:20,553: t15.2024.07.28 val PER: 0.1434
2025-12-15 20:35:20,553: t15.2025.01.10 val PER: 0.3375
2025-12-15 20:35:20,553: t15.2025.01.12 val PER: 0.1747
2025-12-15 20:35:20,554: t15.2025.03.14 val PER: 0.3728
2025-12-15 20:35:20,554: t15.2025.03.16 val PER: 0.2238
2025-12-15 20:35:20,554: t15.2025.03.30 val PER: 0.2713
2025-12-15 20:35:20,554: t15.2025.04.13 val PER: 0.2482
2025-12-15 20:35:20,554: New best test PER 0.1720 --> 0.1709
2025-12-15 20:35:20,554: Checkpointing model
2025-12-15 20:35:20,988: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:35:34,901: Train batch 22200: loss: 24.97 grad norm: 25.94 time: 0.054
2025-12-15 20:35:47,736: Train batch 22400: loss: 22.13 grad norm: 20.37 time: 0.064
2025-12-15 20:36:02,195: Train batch 22600: loss: 21.81 grad norm: 44.80 time: 0.073
2025-12-15 20:36:16,627: Train batch 22800: loss: 24.34 grad norm: 27.04 time: 0.065
2025-12-15 20:36:30,764: Train batch 23000: loss: 29.85 grad norm: 39.35 time: 0.093
2025-12-15 20:36:44,472: Train batch 23200: loss: 33.71 grad norm: 43.05 time: 0.078
2025-12-15 20:36:57,181: Train batch 23400: loss: 23.95 grad norm: 32.74 time: 0.077
2025-12-15 20:37:10,252: Train batch 23600: loss: 26.52 grad norm: 29.27 time: 0.081
2025-12-15 20:37:24,655: Train batch 23800: loss: 25.82 grad norm: 24.94 time: 0.052
2025-12-15 20:37:38,940: Train batch 24000: loss: 24.92 grad norm: 25.06 time: 0.057
2025-12-15 20:37:38,941: Running test after training batch: 24000
2025-12-15 20:37:48,222: Val batch 24000: PER (avg): 0.1634 CTC Loss (avg): 25.7489 time: 9.281
2025-12-15 20:37:48,222: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:37:48,222: t15.2023.08.13 val PER: 0.1206
2025-12-15 20:37:48,223: t15.2023.08.18 val PER: 0.1215
2025-12-15 20:37:48,223: t15.2023.08.20 val PER: 0.0890
2025-12-15 20:37:48,223: t15.2023.08.25 val PER: 0.1145
2025-12-15 20:37:48,223: t15.2023.08.27 val PER: 0.1897
2025-12-15 20:37:48,223: t15.2023.09.01 val PER: 0.0860
2025-12-15 20:37:48,223: t15.2023.09.03 val PER: 0.1639
2025-12-15 20:37:48,223: t15.2023.09.24 val PER: 0.1141
2025-12-15 20:37:48,223: t15.2023.09.29 val PER: 0.1493
2025-12-15 20:37:48,223: t15.2023.10.01 val PER: 0.2034
2025-12-15 20:37:48,223: t15.2023.10.06 val PER: 0.1324
2025-12-15 20:37:48,223: t15.2023.10.08 val PER: 0.2341
2025-12-15 20:37:48,223: t15.2023.10.13 val PER: 0.2281
2025-12-15 20:37:48,223: t15.2023.10.15 val PER: 0.1602
2025-12-15 20:37:48,223: t15.2023.10.20 val PER: 0.2013
2025-12-15 20:37:48,223: t15.2023.10.22 val PER: 0.1414
2025-12-15 20:37:48,223: t15.2023.11.03 val PER: 0.1967
2025-12-15 20:37:48,223: t15.2023.11.04 val PER: 0.0512
2025-12-15 20:37:48,223: t15.2023.11.17 val PER: 0.0544
2025-12-15 20:37:48,223: t15.2023.11.19 val PER: 0.0878
2025-12-15 20:37:48,224: t15.2023.11.26 val PER: 0.1304
2025-12-15 20:37:48,224: t15.2023.12.03 val PER: 0.1303
2025-12-15 20:37:48,224: t15.2023.12.08 val PER: 0.1258
2025-12-15 20:37:48,224: t15.2023.12.10 val PER: 0.0999
2025-12-15 20:37:48,224: t15.2023.12.17 val PER: 0.1642
2025-12-15 20:37:48,224: t15.2023.12.29 val PER: 0.1448
2025-12-15 20:37:48,224: t15.2024.02.25 val PER: 0.1194
2025-12-15 20:37:48,224: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:37:48,224: t15.2024.03.08 val PER: 0.2304
2025-12-15 20:37:48,224: t15.2024.03.15 val PER: 0.2145
2025-12-15 20:37:48,224: t15.2024.03.17 val PER: 0.1367
2025-12-15 20:37:48,224: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:37:48,224: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:37:48,224: t15.2024.05.10 val PER: 0.1679
2025-12-15 20:37:48,224: t15.2024.06.14 val PER: 0.1893
2025-12-15 20:37:48,224: t15.2024.07.19 val PER: 0.2373
2025-12-15 20:37:48,224: t15.2024.07.21 val PER: 0.0979
2025-12-15 20:37:48,224: t15.2024.07.28 val PER: 0.1507
2025-12-15 20:37:48,224: t15.2025.01.10 val PER: 0.3209
2025-12-15 20:37:48,225: t15.2025.01.12 val PER: 0.1632
2025-12-15 20:37:48,225: t15.2025.03.14 val PER: 0.3506
2025-12-15 20:37:48,225: t15.2025.03.16 val PER: 0.2003
2025-12-15 20:37:48,225: t15.2025.03.30 val PER: 0.2862
2025-12-15 20:37:48,225: t15.2025.04.13 val PER: 0.2325
2025-12-15 20:37:48,225: New best test PER 0.1709 --> 0.1634
2025-12-15 20:37:48,225: Checkpointing model
2025-12-15 20:37:48,786: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:38:03,451: Train batch 24200: loss: 23.13 grad norm: 37.15 time: 0.070
2025-12-15 20:38:18,119: Train batch 24400: loss: 23.15 grad norm: 32.26 time: 0.069
2025-12-15 20:38:32,912: Train batch 24600: loss: 26.14 grad norm: 29.94 time: 0.082
2025-12-15 20:38:47,451: Train batch 24800: loss: 23.53 grad norm: 67.40 time: 0.054
2025-12-15 20:39:01,709: Train batch 25000: loss: 26.55 grad norm: 18.38 time: 0.067
2025-12-15 20:39:15,787: Train batch 25200: loss: 22.43 grad norm: 23.30 time: 0.080
2025-12-15 20:39:30,184: Train batch 25400: loss: 25.14 grad norm: 27.91 time: 0.067
2025-12-15 20:39:44,741: Train batch 25600: loss: 23.77 grad norm: 27.24 time: 0.078
2025-12-15 20:39:59,036: Train batch 25800: loss: 20.79 grad norm: 18.63 time: 0.069
2025-12-15 20:40:13,027: Train batch 26000: loss: 26.75 grad norm: 22.44 time: 0.065
2025-12-15 20:40:13,028: Running test after training batch: 26000
2025-12-15 20:40:22,248: Val batch 26000: PER (avg): 0.1630 CTC Loss (avg): 26.2442 time: 9.220
2025-12-15 20:40:22,248: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:40:22,248: t15.2023.08.13 val PER: 0.1154
2025-12-15 20:40:22,248: t15.2023.08.18 val PER: 0.1232
2025-12-15 20:40:22,248: t15.2023.08.20 val PER: 0.1001
2025-12-15 20:40:22,248: t15.2023.08.25 val PER: 0.1416
2025-12-15 20:40:22,248: t15.2023.08.27 val PER: 0.1897
2025-12-15 20:40:22,249: t15.2023.09.01 val PER: 0.0779
2025-12-15 20:40:22,249: t15.2023.09.03 val PER: 0.1591
2025-12-15 20:40:22,249: t15.2023.09.24 val PER: 0.1408
2025-12-15 20:40:22,249: t15.2023.09.29 val PER: 0.1646
2025-12-15 20:40:22,249: t15.2023.10.01 val PER: 0.1843
2025-12-15 20:40:22,249: t15.2023.10.06 val PER: 0.1173
2025-12-15 20:40:22,249: t15.2023.10.08 val PER: 0.2233
2025-12-15 20:40:22,249: t15.2023.10.13 val PER: 0.2110
2025-12-15 20:40:22,249: t15.2023.10.15 val PER: 0.1707
2025-12-15 20:40:22,249: t15.2023.10.20 val PER: 0.1779
2025-12-15 20:40:22,249: t15.2023.10.22 val PER: 0.1448
2025-12-15 20:40:22,249: t15.2023.11.03 val PER: 0.2056
2025-12-15 20:40:22,249: t15.2023.11.04 val PER: 0.0512
2025-12-15 20:40:22,249: t15.2023.11.17 val PER: 0.0513
2025-12-15 20:40:22,249: t15.2023.11.19 val PER: 0.1178
2025-12-15 20:40:22,249: t15.2023.11.26 val PER: 0.1355
2025-12-15 20:40:22,249: t15.2023.12.03 val PER: 0.1261
2025-12-15 20:40:22,249: t15.2023.12.08 val PER: 0.1238
2025-12-15 20:40:22,249: t15.2023.12.10 val PER: 0.0854
2025-12-15 20:40:22,250: t15.2023.12.17 val PER: 0.1611
2025-12-15 20:40:22,250: t15.2023.12.29 val PER: 0.1380
2025-12-15 20:40:22,250: t15.2024.02.25 val PER: 0.1166
2025-12-15 20:40:22,250: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:40:22,250: t15.2024.03.08 val PER: 0.2048
2025-12-15 20:40:22,250: t15.2024.03.15 val PER: 0.2020
2025-12-15 20:40:22,250: t15.2024.03.17 val PER: 0.1457
2025-12-15 20:40:22,250: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:40:22,250: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:40:22,250: t15.2024.05.10 val PER: 0.1902
2025-12-15 20:40:22,250: t15.2024.06.14 val PER: 0.1814
2025-12-15 20:40:22,250: t15.2024.07.19 val PER: 0.2406
2025-12-15 20:40:22,250: t15.2024.07.21 val PER: 0.1028
2025-12-15 20:40:22,250: t15.2024.07.28 val PER: 0.1382
2025-12-15 20:40:22,250: t15.2025.01.10 val PER: 0.3361
2025-12-15 20:40:22,250: t15.2025.01.12 val PER: 0.1617
2025-12-15 20:40:22,250: t15.2025.03.14 val PER: 0.3358
2025-12-15 20:40:22,250: t15.2025.03.16 val PER: 0.1963
2025-12-15 20:40:22,250: t15.2025.03.30 val PER: 0.2828
2025-12-15 20:40:22,251: t15.2025.04.13 val PER: 0.2482
2025-12-15 20:40:22,251: New best test PER 0.1634 --> 0.1630
2025-12-15 20:40:22,251: Checkpointing model
2025-12-15 20:40:22,676: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:40:34,226: Train batch 26200: loss: 24.12 grad norm: 22.95 time: 0.065
2025-12-15 20:40:45,065: Train batch 26400: loss: 20.02 grad norm: 101.69 time: 0.042
2025-12-15 20:40:57,011: Train batch 26600: loss: 19.87 grad norm: 22.31 time: 0.077
2025-12-15 20:41:09,878: Train batch 26800: loss: 31.92 grad norm: 33.27 time: 0.055
2025-12-15 20:41:24,289: Train batch 27000: loss: 25.31 grad norm: 11.29 time: 0.077
2025-12-15 20:41:37,529: Train batch 27200: loss: 21.01 grad norm: 24.77 time: 0.060
2025-12-15 20:41:51,833: Train batch 27400: loss: 29.18 grad norm: 45.84 time: 0.068
2025-12-15 20:42:06,355: Train batch 27600: loss: 21.10 grad norm: 12.07 time: 0.075
2025-12-15 20:42:20,860: Train batch 27800: loss: 27.59 grad norm: 28.15 time: 0.069
2025-12-15 20:42:35,179: Train batch 28000: loss: 27.15 grad norm: 23.17 time: 0.088
2025-12-15 20:42:35,180: Running test after training batch: 28000
2025-12-15 20:42:44,402: Val batch 28000: PER (avg): 0.1615 CTC Loss (avg): 26.9889 time: 9.222
2025-12-15 20:42:44,402: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:42:44,402: t15.2023.08.13 val PER: 0.1206
2025-12-15 20:42:44,402: t15.2023.08.18 val PER: 0.1174
2025-12-15 20:42:44,402: t15.2023.08.20 val PER: 0.0961
2025-12-15 20:42:44,402: t15.2023.08.25 val PER: 0.1235
2025-12-15 20:42:44,402: t15.2023.08.27 val PER: 0.1736
2025-12-15 20:42:44,402: t15.2023.09.01 val PER: 0.0804
2025-12-15 20:42:44,403: t15.2023.09.03 val PER: 0.1603
2025-12-15 20:42:44,403: t15.2023.09.24 val PER: 0.1238
2025-12-15 20:42:44,403: t15.2023.09.29 val PER: 0.1583
2025-12-15 20:42:44,403: t15.2023.10.01 val PER: 0.1988
2025-12-15 20:42:44,403: t15.2023.10.06 val PER: 0.1302
2025-12-15 20:42:44,403: t15.2023.10.08 val PER: 0.2355
2025-12-15 20:42:44,403: t15.2023.10.13 val PER: 0.2219
2025-12-15 20:42:44,403: t15.2023.10.15 val PER: 0.1602
2025-12-15 20:42:44,403: t15.2023.10.20 val PER: 0.1711
2025-12-15 20:42:44,403: t15.2023.10.22 val PER: 0.1359
2025-12-15 20:42:44,403: t15.2023.11.03 val PER: 0.2110
2025-12-15 20:42:44,403: t15.2023.11.04 val PER: 0.0546
2025-12-15 20:42:44,403: t15.2023.11.17 val PER: 0.0544
2025-12-15 20:42:44,403: t15.2023.11.19 val PER: 0.0938
2025-12-15 20:42:44,403: t15.2023.11.26 val PER: 0.1377
2025-12-15 20:42:44,403: t15.2023.12.03 val PER: 0.1166
2025-12-15 20:42:44,403: t15.2023.12.08 val PER: 0.1092
2025-12-15 20:42:44,403: t15.2023.12.10 val PER: 0.0841
2025-12-15 20:42:44,403: t15.2023.12.17 val PER: 0.1715
2025-12-15 20:42:44,404: t15.2023.12.29 val PER: 0.1311
2025-12-15 20:42:44,404: t15.2024.02.25 val PER: 0.1250
2025-12-15 20:42:44,404: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:42:44,404: t15.2024.03.08 val PER: 0.2134
2025-12-15 20:42:44,404: t15.2024.03.15 val PER: 0.2051
2025-12-15 20:42:44,404: t15.2024.03.17 val PER: 0.1332
2025-12-15 20:42:44,404: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:42:44,404: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:42:44,404: t15.2024.05.10 val PER: 0.1798
2025-12-15 20:42:44,404: t15.2024.06.14 val PER: 0.1845
2025-12-15 20:42:44,404: t15.2024.07.19 val PER: 0.2426
2025-12-15 20:42:44,404: t15.2024.07.21 val PER: 0.1028
2025-12-15 20:42:44,404: t15.2024.07.28 val PER: 0.1390
2025-12-15 20:42:44,404: t15.2025.01.10 val PER: 0.2934
2025-12-15 20:42:44,404: t15.2025.01.12 val PER: 0.1632
2025-12-15 20:42:44,404: t15.2025.03.14 val PER: 0.3639
2025-12-15 20:42:44,404: t15.2025.03.16 val PER: 0.2029
2025-12-15 20:42:44,404: t15.2025.03.30 val PER: 0.2851
2025-12-15 20:42:44,404: t15.2025.04.13 val PER: 0.2340
2025-12-15 20:42:44,404: New best test PER 0.1630 --> 0.1615
2025-12-15 20:42:44,404: Checkpointing model
2025-12-15 20:42:44,822: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:42:59,465: Train batch 28200: loss: 22.24 grad norm: 28.06 time: 0.050
2025-12-15 20:43:14,212: Train batch 28400: loss: 21.46 grad norm: 19.32 time: 0.056
2025-12-15 20:43:28,666: Train batch 28600: loss: 24.31 grad norm: 34.54 time: 0.055
2025-12-15 20:43:42,490: Train batch 28800: loss: 25.47 grad norm: 59.10 time: 0.056
2025-12-15 20:43:56,020: Train batch 29000: loss: 24.01 grad norm: 16.67 time: 0.040
2025-12-15 20:44:10,240: Train batch 29200: loss: 21.79 grad norm: 25.26 time: 0.056
2025-12-15 20:44:24,869: Train batch 29400: loss: 22.90 grad norm: 16.35 time: 0.086
2025-12-15 20:44:39,459: Train batch 29600: loss: 21.03 grad norm: 27.70 time: 0.054
2025-12-15 20:44:54,091: Train batch 29800: loss: 23.45 grad norm: 15.47 time: 0.068
2025-12-15 20:45:08,494: Train batch 30000: loss: 20.28 grad norm: 27.98 time: 0.076
2025-12-15 20:45:08,495: Running test after training batch: 30000
2025-12-15 20:45:17,730: Val batch 30000: PER (avg): 0.1539 CTC Loss (avg): 25.5743 time: 9.236
2025-12-15 20:45:17,731: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:45:17,731: t15.2023.08.13 val PER: 0.1040
2025-12-15 20:45:17,731: t15.2023.08.18 val PER: 0.1174
2025-12-15 20:45:17,731: t15.2023.08.20 val PER: 0.0969
2025-12-15 20:45:17,731: t15.2023.08.25 val PER: 0.1160
2025-12-15 20:45:17,731: t15.2023.08.27 val PER: 0.1736
2025-12-15 20:45:17,731: t15.2023.09.01 val PER: 0.0836
2025-12-15 20:45:17,731: t15.2023.09.03 val PER: 0.1259
2025-12-15 20:45:17,731: t15.2023.09.24 val PER: 0.1238
2025-12-15 20:45:17,731: t15.2023.09.29 val PER: 0.1512
2025-12-15 20:45:17,731: t15.2023.10.01 val PER: 0.1770
2025-12-15 20:45:17,731: t15.2023.10.06 val PER: 0.1087
2025-12-15 20:45:17,731: t15.2023.10.08 val PER: 0.2206
2025-12-15 20:45:17,731: t15.2023.10.13 val PER: 0.2079
2025-12-15 20:45:17,731: t15.2023.10.15 val PER: 0.1543
2025-12-15 20:45:17,731: t15.2023.10.20 val PER: 0.1913
2025-12-15 20:45:17,732: t15.2023.10.22 val PER: 0.1281
2025-12-15 20:45:17,732: t15.2023.11.03 val PER: 0.1995
2025-12-15 20:45:17,732: t15.2023.11.04 val PER: 0.0410
2025-12-15 20:45:17,732: t15.2023.11.17 val PER: 0.0700
2025-12-15 20:45:17,732: t15.2023.11.19 val PER: 0.0599
2025-12-15 20:45:17,732: t15.2023.11.26 val PER: 0.1188
2025-12-15 20:45:17,732: t15.2023.12.03 val PER: 0.1071
2025-12-15 20:45:17,732: t15.2023.12.08 val PER: 0.1158
2025-12-15 20:45:17,732: t15.2023.12.10 val PER: 0.0802
2025-12-15 20:45:17,732: t15.2023.12.17 val PER: 0.1289
2025-12-15 20:45:17,732: t15.2023.12.29 val PER: 0.1311
2025-12-15 20:45:17,732: t15.2024.02.25 val PER: 0.1278
2025-12-15 20:45:17,732: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:45:17,732: t15.2024.03.08 val PER: 0.2248
2025-12-15 20:45:17,732: t15.2024.03.15 val PER: 0.2064
2025-12-15 20:45:17,732: t15.2024.03.17 val PER: 0.1325
2025-12-15 20:45:17,732: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:45:17,732: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:45:17,732: t15.2024.05.10 val PER: 0.1976
2025-12-15 20:45:17,733: t15.2024.06.14 val PER: 0.1767
2025-12-15 20:45:17,733: t15.2024.07.19 val PER: 0.2307
2025-12-15 20:45:17,733: t15.2024.07.21 val PER: 0.0966
2025-12-15 20:45:17,733: t15.2024.07.28 val PER: 0.1360
2025-12-15 20:45:17,733: t15.2025.01.10 val PER: 0.2961
2025-12-15 20:45:17,733: t15.2025.01.12 val PER: 0.1578
2025-12-15 20:45:17,733: t15.2025.03.14 val PER: 0.3062
2025-12-15 20:45:17,733: t15.2025.03.16 val PER: 0.2016
2025-12-15 20:45:17,733: t15.2025.03.30 val PER: 0.2839
2025-12-15 20:45:17,733: t15.2025.04.13 val PER: 0.2225
2025-12-15 20:45:17,733: New best test PER 0.1615 --> 0.1539
2025-12-15 20:45:17,733: Checkpointing model
2025-12-15 20:45:18,099: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:45:32,717: Train batch 30200: loss: 28.64 grad norm: 43.05 time: 0.056
2025-12-15 20:45:47,309: Train batch 30400: loss: 25.11 grad norm: 25.40 time: 0.068
2025-12-15 20:46:02,013: Train batch 30600: loss: 22.44 grad norm: 21.50 time: 0.071
2025-12-15 20:46:15,143: Train batch 30800: loss: 23.52 grad norm: 22.71 time: 0.049
2025-12-15 20:46:28,806: Train batch 31000: loss: 21.61 grad norm: 18.09 time: 0.073
2025-12-15 20:46:43,201: Train batch 31200: loss: 22.73 grad norm: 23.30 time: 0.053
2025-12-15 20:46:57,297: Train batch 31400: loss: 24.73 grad norm: 25.10 time: 0.055
2025-12-15 20:47:11,221: Train batch 31600: loss: 24.41 grad norm: 16.04 time: 0.056
2025-12-15 20:47:25,229: Train batch 31800: loss: 25.82 grad norm: 35.99 time: 0.090
2025-12-15 20:47:39,817: Train batch 32000: loss: 18.31 grad norm: 31.21 time: 0.065
2025-12-15 20:47:39,817: Running test after training batch: 32000
2025-12-15 20:47:49,482: Val batch 32000: PER (avg): 0.1571 CTC Loss (avg): 27.2770 time: 9.664
2025-12-15 20:47:49,482: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:47:49,483: t15.2023.08.13 val PER: 0.1164
2025-12-15 20:47:49,483: t15.2023.08.18 val PER: 0.1165
2025-12-15 20:47:49,483: t15.2023.08.20 val PER: 0.0905
2025-12-15 20:47:49,483: t15.2023.08.25 val PER: 0.1099
2025-12-15 20:47:49,483: t15.2023.08.27 val PER: 0.1592
2025-12-15 20:47:49,483: t15.2023.09.01 val PER: 0.0877
2025-12-15 20:47:49,483: t15.2023.09.03 val PER: 0.1580
2025-12-15 20:47:49,483: t15.2023.09.24 val PER: 0.1177
2025-12-15 20:47:49,483: t15.2023.09.29 val PER: 0.1544
2025-12-15 20:47:49,483: t15.2023.10.01 val PER: 0.1717
2025-12-15 20:47:49,483: t15.2023.10.06 val PER: 0.1259
2025-12-15 20:47:49,483: t15.2023.10.08 val PER: 0.2206
2025-12-15 20:47:49,483: t15.2023.10.13 val PER: 0.2242
2025-12-15 20:47:49,483: t15.2023.10.15 val PER: 0.1668
2025-12-15 20:47:49,483: t15.2023.10.20 val PER: 0.1812
2025-12-15 20:47:49,483: t15.2023.10.22 val PER: 0.1459
2025-12-15 20:47:49,483: t15.2023.11.03 val PER: 0.2008
2025-12-15 20:47:49,484: t15.2023.11.04 val PER: 0.0239
2025-12-15 20:47:49,484: t15.2023.11.17 val PER: 0.0560
2025-12-15 20:47:49,484: t15.2023.11.19 val PER: 0.0719
2025-12-15 20:47:49,484: t15.2023.11.26 val PER: 0.1319
2025-12-15 20:47:49,484: t15.2023.12.03 val PER: 0.1113
2025-12-15 20:47:49,484: t15.2023.12.08 val PER: 0.1092
2025-12-15 20:47:49,484: t15.2023.12.10 val PER: 0.0946
2025-12-15 20:47:49,484: t15.2023.12.17 val PER: 0.1455
2025-12-15 20:47:49,484: t15.2023.12.29 val PER: 0.1455
2025-12-15 20:47:49,484: t15.2024.02.25 val PER: 0.1124
2025-12-15 20:47:49,484: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:47:49,484: t15.2024.03.08 val PER: 0.2276
2025-12-15 20:47:49,484: t15.2024.03.15 val PER: 0.2126
2025-12-15 20:47:49,484: t15.2024.03.17 val PER: 0.1332
2025-12-15 20:47:49,484: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:47:49,484: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:47:49,484: t15.2024.05.10 val PER: 0.1768
2025-12-15 20:47:49,484: t15.2024.06.14 val PER: 0.1640
2025-12-15 20:47:49,484: t15.2024.07.19 val PER: 0.2241
2025-12-15 20:47:49,484: t15.2024.07.21 val PER: 0.0979
2025-12-15 20:47:49,485: t15.2024.07.28 val PER: 0.1353
2025-12-15 20:47:49,485: t15.2025.01.10 val PER: 0.3306
2025-12-15 20:47:49,485: t15.2025.01.12 val PER: 0.1486
2025-12-15 20:47:49,485: t15.2025.03.14 val PER: 0.3388
2025-12-15 20:47:49,485: t15.2025.03.16 val PER: 0.1990
2025-12-15 20:47:49,485: t15.2025.03.30 val PER: 0.2678
2025-12-15 20:47:49,485: t15.2025.04.13 val PER: 0.2311
2025-12-15 20:48:03,463: Train batch 32200: loss: 23.59 grad norm: 18.84 time: 0.047
2025-12-15 20:48:17,282: Train batch 32400: loss: 28.67 grad norm: 54.65 time: 0.066
2025-12-15 20:48:30,843: Train batch 32600: loss: 23.63 grad norm: 28.00 time: 0.072
2025-12-15 20:48:45,338: Train batch 32800: loss: 26.06 grad norm: 47.92 time: 0.076
2025-12-15 20:48:59,185: Train batch 33000: loss: 19.02 grad norm: 15.76 time: 0.088
2025-12-15 20:49:11,636: Train batch 33200: loss: 27.46 grad norm: 19.47 time: 0.047
2025-12-15 20:49:26,177: Train batch 33400: loss: 23.40 grad norm: 18.71 time: 0.041
2025-12-15 20:49:39,855: Train batch 33600: loss: 23.93 grad norm: 26.16 time: 0.033
2025-12-15 20:49:52,474: Train batch 33800: loss: 26.64 grad norm: 39.57 time: 0.069
2025-12-15 20:50:06,741: Train batch 34000: loss: 26.09 grad norm: 35.99 time: 0.070
2025-12-15 20:50:06,742: Running test after training batch: 34000
2025-12-15 20:50:16,140: Val batch 34000: PER (avg): 0.1526 CTC Loss (avg): 27.8356 time: 9.398
2025-12-15 20:50:16,140: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:50:16,141: t15.2023.08.13 val PER: 0.1216
2025-12-15 20:50:16,141: t15.2023.08.18 val PER: 0.1174
2025-12-15 20:50:16,141: t15.2023.08.20 val PER: 0.0858
2025-12-15 20:50:16,141: t15.2023.08.25 val PER: 0.1099
2025-12-15 20:50:16,141: t15.2023.08.27 val PER: 0.1624
2025-12-15 20:50:16,141: t15.2023.09.01 val PER: 0.0836
2025-12-15 20:50:16,141: t15.2023.09.03 val PER: 0.1663
2025-12-15 20:50:16,141: t15.2023.09.24 val PER: 0.1141
2025-12-15 20:50:16,141: t15.2023.09.29 val PER: 0.1315
2025-12-15 20:50:16,141: t15.2023.10.01 val PER: 0.1770
2025-12-15 20:50:16,141: t15.2023.10.06 val PER: 0.1206
2025-12-15 20:50:16,141: t15.2023.10.08 val PER: 0.2124
2025-12-15 20:50:16,141: t15.2023.10.13 val PER: 0.2265
2025-12-15 20:50:16,141: t15.2023.10.15 val PER: 0.1516
2025-12-15 20:50:16,141: t15.2023.10.20 val PER: 0.1980
2025-12-15 20:50:16,141: t15.2023.10.22 val PER: 0.1437
2025-12-15 20:50:16,141: t15.2023.11.03 val PER: 0.2001
2025-12-15 20:50:16,141: t15.2023.11.04 val PER: 0.0375
2025-12-15 20:50:16,141: t15.2023.11.17 val PER: 0.0544
2025-12-15 20:50:16,142: t15.2023.11.19 val PER: 0.0639
2025-12-15 20:50:16,142: t15.2023.11.26 val PER: 0.1232
2025-12-15 20:50:16,142: t15.2023.12.03 val PER: 0.1166
2025-12-15 20:50:16,142: t15.2023.12.08 val PER: 0.1105
2025-12-15 20:50:16,142: t15.2023.12.10 val PER: 0.0854
2025-12-15 20:50:16,142: t15.2023.12.17 val PER: 0.1362
2025-12-15 20:50:16,142: t15.2023.12.29 val PER: 0.1194
2025-12-15 20:50:16,142: t15.2024.02.25 val PER: 0.1025
2025-12-15 20:50:16,142: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:50:16,142: t15.2024.03.08 val PER: 0.2319
2025-12-15 20:50:16,142: t15.2024.03.15 val PER: 0.2058
2025-12-15 20:50:16,142: t15.2024.03.17 val PER: 0.1185
2025-12-15 20:50:16,142: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:50:16,142: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:50:16,142: t15.2024.05.10 val PER: 0.1828
2025-12-15 20:50:16,142: t15.2024.06.14 val PER: 0.1751
2025-12-15 20:50:16,142: t15.2024.07.19 val PER: 0.2353
2025-12-15 20:50:16,142: t15.2024.07.21 val PER: 0.1034
2025-12-15 20:50:16,142: t15.2024.07.28 val PER: 0.1257
2025-12-15 20:50:16,143: t15.2025.01.10 val PER: 0.3196
2025-12-15 20:50:16,143: t15.2025.01.12 val PER: 0.1563
2025-12-15 20:50:16,143: t15.2025.03.14 val PER: 0.3107
2025-12-15 20:50:16,143: t15.2025.03.16 val PER: 0.1885
2025-12-15 20:50:16,143: t15.2025.03.30 val PER: 0.2506
2025-12-15 20:50:16,143: t15.2025.04.13 val PER: 0.2068
2025-12-15 20:50:16,143: New best test PER 0.1539 --> 0.1526
2025-12-15 20:50:16,143: Checkpointing model
2025-12-15 20:50:16,571: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:50:30,750: Train batch 34200: loss: 23.14 grad norm: 18.15 time: 0.069
2025-12-15 20:50:44,754: Train batch 34400: loss: 28.89 grad norm: 27.71 time: 0.057
2025-12-15 20:50:59,211: Train batch 34600: loss: 24.17 grad norm: 33.25 time: 0.057
2025-12-15 20:51:12,834: Train batch 34800: loss: 21.77 grad norm: 14.92 time: 0.055
2025-12-15 20:51:26,535: Train batch 35000: loss: 26.33 grad norm: 28.11 time: 0.059
2025-12-15 20:51:40,561: Train batch 35200: loss: 28.03 grad norm: 22.38 time: 0.056
2025-12-15 20:51:55,077: Train batch 35400: loss: 26.26 grad norm: 27.22 time: 0.058
2025-12-15 20:52:09,326: Train batch 35600: loss: 28.67 grad norm: 19.96 time: 0.074
2025-12-15 20:52:22,734: Train batch 35800: loss: 24.21 grad norm: 22.15 time: 0.058
2025-12-15 20:52:37,526: Train batch 36000: loss: 25.00 grad norm: 28.86 time: 0.069
2025-12-15 20:52:37,527: Running test after training batch: 36000
2025-12-15 20:52:46,876: Val batch 36000: PER (avg): 0.1488 CTC Loss (avg): 27.3897 time: 9.349
2025-12-15 20:52:46,876: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:52:46,876: t15.2023.08.13 val PER: 0.1216
2025-12-15 20:52:46,876: t15.2023.08.18 val PER: 0.1140
2025-12-15 20:52:46,876: t15.2023.08.20 val PER: 0.0929
2025-12-15 20:52:46,876: t15.2023.08.25 val PER: 0.1099
2025-12-15 20:52:46,876: t15.2023.08.27 val PER: 0.1592
2025-12-15 20:52:46,877: t15.2023.09.01 val PER: 0.0795
2025-12-15 20:52:46,877: t15.2023.09.03 val PER: 0.1770
2025-12-15 20:52:46,877: t15.2023.09.24 val PER: 0.1165
2025-12-15 20:52:46,877: t15.2023.09.29 val PER: 0.1493
2025-12-15 20:52:46,877: t15.2023.10.01 val PER: 0.1816
2025-12-15 20:52:46,877: t15.2023.10.06 val PER: 0.1076
2025-12-15 20:52:46,877: t15.2023.10.08 val PER: 0.2260
2025-12-15 20:52:46,877: t15.2023.10.13 val PER: 0.2071
2025-12-15 20:52:46,877: t15.2023.10.15 val PER: 0.1608
2025-12-15 20:52:46,877: t15.2023.10.20 val PER: 0.2013
2025-12-15 20:52:46,877: t15.2023.10.22 val PER: 0.1269
2025-12-15 20:52:46,877: t15.2023.11.03 val PER: 0.1947
2025-12-15 20:52:46,877: t15.2023.11.04 val PER: 0.0171
2025-12-15 20:52:46,877: t15.2023.11.17 val PER: 0.0575
2025-12-15 20:52:46,877: t15.2023.11.19 val PER: 0.0579
2025-12-15 20:52:46,877: t15.2023.11.26 val PER: 0.1145
2025-12-15 20:52:46,877: t15.2023.12.03 val PER: 0.1145
2025-12-15 20:52:46,877: t15.2023.12.08 val PER: 0.0965
2025-12-15 20:52:46,877: t15.2023.12.10 val PER: 0.0749
2025-12-15 20:52:46,878: t15.2023.12.17 val PER: 0.1351
2025-12-15 20:52:46,878: t15.2023.12.29 val PER: 0.1160
2025-12-15 20:52:46,878: t15.2024.02.25 val PER: 0.0969
2025-12-15 20:52:46,878: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:52:46,878: t15.2024.03.08 val PER: 0.2134
2025-12-15 20:52:46,878: t15.2024.03.15 val PER: 0.2051
2025-12-15 20:52:46,878: t15.2024.03.17 val PER: 0.1179
2025-12-15 20:52:46,878: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:52:46,878: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:52:46,878: t15.2024.05.10 val PER: 0.1768
2025-12-15 20:52:46,878: t15.2024.06.14 val PER: 0.1435
2025-12-15 20:52:46,878: t15.2024.07.19 val PER: 0.2044
2025-12-15 20:52:46,878: t15.2024.07.21 val PER: 0.0938
2025-12-15 20:52:46,878: t15.2024.07.28 val PER: 0.1154
2025-12-15 20:52:46,878: t15.2025.01.10 val PER: 0.3113
2025-12-15 20:52:46,878: t15.2025.01.12 val PER: 0.1540
2025-12-15 20:52:46,878: t15.2025.03.14 val PER: 0.3018
2025-12-15 20:52:46,878: t15.2025.03.16 val PER: 0.1872
2025-12-15 20:52:46,878: t15.2025.03.30 val PER: 0.2598
2025-12-15 20:52:46,878: t15.2025.04.13 val PER: 0.2297
2025-12-15 20:52:46,879: New best test PER 0.1526 --> 0.1488
2025-12-15 20:52:46,879: Checkpointing model
2025-12-15 20:52:47,320: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:53:01,552: Train batch 36200: loss: 24.53 grad norm: 15.62 time: 0.081
2025-12-15 20:53:16,134: Train batch 36400: loss: 26.97 grad norm: 18.93 time: 0.055
2025-12-15 20:53:30,503: Train batch 36600: loss: 24.97 grad norm: 23.30 time: 0.055
2025-12-15 20:53:44,967: Train batch 36800: loss: 24.50 grad norm: 25.12 time: 0.066
2025-12-15 20:53:58,675: Train batch 37000: loss: 24.52 grad norm: 18.59 time: 0.057
2025-12-15 20:54:12,550: Train batch 37200: loss: 25.63 grad norm: 31.50 time: 0.061
2025-12-15 20:54:26,883: Train batch 37400: loss: 19.66 grad norm: 18.09 time: 0.083
2025-12-15 20:54:41,339: Train batch 37600: loss: 25.19 grad norm: 47.98 time: 0.074
2025-12-15 20:54:54,979: Train batch 37800: loss: 27.46 grad norm: 27.18 time: 0.053
2025-12-15 20:55:09,171: Train batch 38000: loss: 23.80 grad norm: 24.69 time: 0.073
2025-12-15 20:55:09,172: Running test after training batch: 38000
2025-12-15 20:55:18,436: Val batch 38000: PER (avg): 0.1493 CTC Loss (avg): 28.1639 time: 9.263
2025-12-15 20:55:18,436: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:55:18,436: t15.2023.08.13 val PER: 0.1091
2025-12-15 20:55:18,436: t15.2023.08.18 val PER: 0.1056
2025-12-15 20:55:18,436: t15.2023.08.20 val PER: 0.0850
2025-12-15 20:55:18,436: t15.2023.08.25 val PER: 0.1145
2025-12-15 20:55:18,436: t15.2023.08.27 val PER: 0.1720
2025-12-15 20:55:18,436: t15.2023.09.01 val PER: 0.0739
2025-12-15 20:55:18,436: t15.2023.09.03 val PER: 0.1544
2025-12-15 20:55:18,436: t15.2023.09.24 val PER: 0.1177
2025-12-15 20:55:18,436: t15.2023.09.29 val PER: 0.1385
2025-12-15 20:55:18,436: t15.2023.10.01 val PER: 0.1744
2025-12-15 20:55:18,436: t15.2023.10.06 val PER: 0.1141
2025-12-15 20:55:18,437: t15.2023.10.08 val PER: 0.1908
2025-12-15 20:55:18,437: t15.2023.10.13 val PER: 0.2149
2025-12-15 20:55:18,437: t15.2023.10.15 val PER: 0.1496
2025-12-15 20:55:18,437: t15.2023.10.20 val PER: 0.1980
2025-12-15 20:55:18,437: t15.2023.10.22 val PER: 0.1403
2025-12-15 20:55:18,437: t15.2023.11.03 val PER: 0.1805
2025-12-15 20:55:18,437: t15.2023.11.04 val PER: 0.0171
2025-12-15 20:55:18,437: t15.2023.11.17 val PER: 0.0467
2025-12-15 20:55:18,437: t15.2023.11.19 val PER: 0.0758
2025-12-15 20:55:18,437: t15.2023.11.26 val PER: 0.1123
2025-12-15 20:55:18,437: t15.2023.12.03 val PER: 0.1134
2025-12-15 20:55:18,437: t15.2023.12.08 val PER: 0.1105
2025-12-15 20:55:18,437: t15.2023.12.10 val PER: 0.0907
2025-12-15 20:55:18,437: t15.2023.12.17 val PER: 0.1258
2025-12-15 20:55:18,437: t15.2023.12.29 val PER: 0.1167
2025-12-15 20:55:18,437: t15.2024.02.25 val PER: 0.1081
2025-12-15 20:55:18,437: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:55:18,437: t15.2024.03.08 val PER: 0.2119
2025-12-15 20:55:18,437: t15.2024.03.15 val PER: 0.1957
2025-12-15 20:55:18,438: t15.2024.03.17 val PER: 0.1227
2025-12-15 20:55:18,438: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:55:18,438: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:55:18,438: t15.2024.05.10 val PER: 0.1679
2025-12-15 20:55:18,438: t15.2024.06.14 val PER: 0.1845
2025-12-15 20:55:18,438: t15.2024.07.19 val PER: 0.2261
2025-12-15 20:55:18,438: t15.2024.07.21 val PER: 0.1062
2025-12-15 20:55:18,438: t15.2024.07.28 val PER: 0.1279
2025-12-15 20:55:18,438: t15.2025.01.10 val PER: 0.3402
2025-12-15 20:55:18,438: t15.2025.01.12 val PER: 0.1524
2025-12-15 20:55:18,438: t15.2025.03.14 val PER: 0.3136
2025-12-15 20:55:18,438: t15.2025.03.16 val PER: 0.1898
2025-12-15 20:55:18,438: t15.2025.03.30 val PER: 0.2724
2025-12-15 20:55:18,438: t15.2025.04.13 val PER: 0.2026
2025-12-15 20:55:32,530: Train batch 38200: loss: 22.25 grad norm: 18.66 time: 0.057
2025-12-15 20:55:47,136: Train batch 38400: loss: 24.00 grad norm: 15.71 time: 0.081
2025-12-15 20:56:01,506: Train batch 38600: loss: 14.69 grad norm: 8.90 time: 0.070
2025-12-15 20:56:15,931: Train batch 38800: loss: 24.54 grad norm: 28.41 time: 0.062
2025-12-15 20:56:30,402: Train batch 39000: loss: 24.20 grad norm: 30.79 time: 0.054
2025-12-15 20:56:45,050: Train batch 39200: loss: 23.33 grad norm: 23.71 time: 0.056
2025-12-15 20:56:58,849: Train batch 39400: loss: 19.87 grad norm: 16.57 time: 0.064
2025-12-15 20:57:13,398: Train batch 39600: loss: 25.67 grad norm: 19.69 time: 0.070
2025-12-15 20:57:27,799: Train batch 39800: loss: 22.10 grad norm: 19.37 time: 0.055
2025-12-15 20:57:41,983: Train batch 40000: loss: 26.60 grad norm: 24.42 time: 0.074
2025-12-15 20:57:41,983: Running test after training batch: 40000
2025-12-15 20:57:51,026: Val batch 40000: PER (avg): 0.1480 CTC Loss (avg): 28.9208 time: 9.043
2025-12-15 20:57:51,026: t15.2023.08.11 val PER: 1.0000
2025-12-15 20:57:51,027: t15.2023.08.13 val PER: 0.1102
2025-12-15 20:57:51,027: t15.2023.08.18 val PER: 0.1056
2025-12-15 20:57:51,027: t15.2023.08.20 val PER: 0.0905
2025-12-15 20:57:51,027: t15.2023.08.25 val PER: 0.1039
2025-12-15 20:57:51,027: t15.2023.08.27 val PER: 0.1849
2025-12-15 20:57:51,027: t15.2023.09.01 val PER: 0.0698
2025-12-15 20:57:51,027: t15.2023.09.03 val PER: 0.1496
2025-12-15 20:57:51,027: t15.2023.09.24 val PER: 0.1129
2025-12-15 20:57:51,027: t15.2023.09.29 val PER: 0.1347
2025-12-15 20:57:51,027: t15.2023.10.01 val PER: 0.1869
2025-12-15 20:57:51,027: t15.2023.10.06 val PER: 0.1184
2025-12-15 20:57:51,027: t15.2023.10.08 val PER: 0.2084
2025-12-15 20:57:51,027: t15.2023.10.13 val PER: 0.2064
2025-12-15 20:57:51,027: t15.2023.10.15 val PER: 0.1477
2025-12-15 20:57:51,027: t15.2023.10.20 val PER: 0.2215
2025-12-15 20:57:51,027: t15.2023.10.22 val PER: 0.1347
2025-12-15 20:57:51,027: t15.2023.11.03 val PER: 0.1866
2025-12-15 20:57:51,027: t15.2023.11.04 val PER: 0.0341
2025-12-15 20:57:51,028: t15.2023.11.17 val PER: 0.0482
2025-12-15 20:57:51,028: t15.2023.11.19 val PER: 0.0519
2025-12-15 20:57:51,028: t15.2023.11.26 val PER: 0.1275
2025-12-15 20:57:51,028: t15.2023.12.03 val PER: 0.1019
2025-12-15 20:57:51,028: t15.2023.12.08 val PER: 0.1079
2025-12-15 20:57:51,028: t15.2023.12.10 val PER: 0.0841
2025-12-15 20:57:51,028: t15.2023.12.17 val PER: 0.1268
2025-12-15 20:57:51,028: t15.2023.12.29 val PER: 0.1167
2025-12-15 20:57:51,028: t15.2024.02.25 val PER: 0.0983
2025-12-15 20:57:51,028: t15.2024.03.03 val PER: 1.0000
2025-12-15 20:57:51,028: t15.2024.03.08 val PER: 0.2091
2025-12-15 20:57:51,028: t15.2024.03.15 val PER: 0.2045
2025-12-15 20:57:51,028: t15.2024.03.17 val PER: 0.1339
2025-12-15 20:57:51,028: t15.2024.04.25 val PER: 1.0000
2025-12-15 20:57:51,028: t15.2024.04.28 val PER: 1.0000
2025-12-15 20:57:51,028: t15.2024.05.10 val PER: 0.1753
2025-12-15 20:57:51,028: t15.2024.06.14 val PER: 0.1672
2025-12-15 20:57:51,028: t15.2024.07.19 val PER: 0.1951
2025-12-15 20:57:51,028: t15.2024.07.21 val PER: 0.0959
2025-12-15 20:57:51,029: t15.2024.07.28 val PER: 0.1140
2025-12-15 20:57:51,029: t15.2025.01.10 val PER: 0.3264
2025-12-15 20:57:51,029: t15.2025.01.12 val PER: 0.1332
2025-12-15 20:57:51,029: t15.2025.03.14 val PER: 0.3314
2025-12-15 20:57:51,029: t15.2025.03.16 val PER: 0.1885
2025-12-15 20:57:51,029: t15.2025.03.30 val PER: 0.2747
2025-12-15 20:57:51,029: t15.2025.04.13 val PER: 0.2225
2025-12-15 20:57:51,029: New best test PER 0.1488 --> 0.1480
2025-12-15 20:57:51,029: Checkpointing model
2025-12-15 20:57:51,433: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 20:58:02,403: Train batch 40200: loss: 28.59 grad norm: 10.91 time: 0.044
2025-12-15 20:58:15,226: Train batch 40400: loss: 20.68 grad norm: 6.55 time: 0.051
2025-12-15 20:58:29,346: Train batch 40600: loss: 26.16 grad norm: 40.69 time: 0.059
2025-12-15 20:58:43,672: Train batch 40800: loss: 24.19 grad norm: 21.69 time: 0.069
2025-12-15 20:58:57,301: Train batch 41000: loss: 25.99 grad norm: 8.99 time: 0.073
2025-12-15 20:59:12,020: Train batch 41200: loss: 21.51 grad norm: 16.76 time: 0.085
2025-12-15 20:59:26,266: Train batch 41400: loss: 22.31 grad norm: 21.82 time: 0.067
2025-12-15 20:59:39,752: Train batch 41600: loss: 22.70 grad norm: 15.59 time: 0.052
2025-12-15 20:59:54,189: Train batch 41800: loss: 21.12 grad norm: 38.89 time: 0.069
2025-12-15 21:00:08,818: Train batch 42000: loss: 24.90 grad norm: 38.50 time: 0.056
2025-12-15 21:00:08,819: Running test after training batch: 42000
2025-12-15 21:00:17,957: Val batch 42000: PER (avg): 0.1454 CTC Loss (avg): 28.6263 time: 9.137
2025-12-15 21:00:17,957: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:00:17,957: t15.2023.08.13 val PER: 0.1008
2025-12-15 21:00:17,957: t15.2023.08.18 val PER: 0.1048
2025-12-15 21:00:17,957: t15.2023.08.20 val PER: 0.0778
2025-12-15 21:00:17,957: t15.2023.08.25 val PER: 0.1054
2025-12-15 21:00:17,957: t15.2023.08.27 val PER: 0.1913
2025-12-15 21:00:17,957: t15.2023.09.01 val PER: 0.0617
2025-12-15 21:00:17,958: t15.2023.09.03 val PER: 0.1580
2025-12-15 21:00:17,958: t15.2023.09.24 val PER: 0.1189
2025-12-15 21:00:17,958: t15.2023.09.29 val PER: 0.1372
2025-12-15 21:00:17,958: t15.2023.10.01 val PER: 0.1724
2025-12-15 21:00:17,958: t15.2023.10.06 val PER: 0.1066
2025-12-15 21:00:17,958: t15.2023.10.08 val PER: 0.2030
2025-12-15 21:00:17,958: t15.2023.10.13 val PER: 0.2071
2025-12-15 21:00:17,958: t15.2023.10.15 val PER: 0.1424
2025-12-15 21:00:17,958: t15.2023.10.20 val PER: 0.1913
2025-12-15 21:00:17,958: t15.2023.10.22 val PER: 0.1414
2025-12-15 21:00:17,958: t15.2023.11.03 val PER: 0.1839
2025-12-15 21:00:17,958: t15.2023.11.04 val PER: 0.0239
2025-12-15 21:00:17,958: t15.2023.11.17 val PER: 0.0591
2025-12-15 21:00:17,958: t15.2023.11.19 val PER: 0.0679
2025-12-15 21:00:17,958: t15.2023.11.26 val PER: 0.1101
2025-12-15 21:00:17,958: t15.2023.12.03 val PER: 0.0998
2025-12-15 21:00:17,958: t15.2023.12.08 val PER: 0.1092
2025-12-15 21:00:17,958: t15.2023.12.10 val PER: 0.0972
2025-12-15 21:00:17,958: t15.2023.12.17 val PER: 0.1206
2025-12-15 21:00:17,958: t15.2023.12.29 val PER: 0.1235
2025-12-15 21:00:17,959: t15.2024.02.25 val PER: 0.0969
2025-12-15 21:00:17,959: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:00:17,959: t15.2024.03.08 val PER: 0.2191
2025-12-15 21:00:17,959: t15.2024.03.15 val PER: 0.2076
2025-12-15 21:00:17,959: t15.2024.03.17 val PER: 0.1032
2025-12-15 21:00:17,959: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:00:17,959: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:00:17,959: t15.2024.05.10 val PER: 0.1620
2025-12-15 21:00:17,959: t15.2024.06.14 val PER: 0.1735
2025-12-15 21:00:17,959: t15.2024.07.19 val PER: 0.1945
2025-12-15 21:00:17,959: t15.2024.07.21 val PER: 0.0897
2025-12-15 21:00:17,959: t15.2024.07.28 val PER: 0.1140
2025-12-15 21:00:17,959: t15.2025.01.10 val PER: 0.3278
2025-12-15 21:00:17,959: t15.2025.01.12 val PER: 0.1393
2025-12-15 21:00:17,959: t15.2025.03.14 val PER: 0.3210
2025-12-15 21:00:17,959: t15.2025.03.16 val PER: 0.1872
2025-12-15 21:00:17,959: t15.2025.03.30 val PER: 0.2701
2025-12-15 21:00:17,959: t15.2025.04.13 val PER: 0.2311
2025-12-15 21:00:17,959: New best test PER 0.1480 --> 0.1454
2025-12-15 21:00:17,959: Checkpointing model
2025-12-15 21:00:18,367: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:00:31,794: Train batch 42200: loss: 23.56 grad norm: 40.71 time: 0.068
2025-12-15 21:00:44,759: Train batch 42400: loss: 26.41 grad norm: 23.42 time: 0.067
2025-12-15 21:00:59,105: Train batch 42600: loss: 21.86 grad norm: 26.63 time: 0.051
2025-12-15 21:01:13,260: Train batch 42800: loss: 24.66 grad norm: 31.34 time: 0.077
2025-12-15 21:01:27,782: Train batch 43000: loss: 24.89 grad norm: 10.03 time: 0.067
2025-12-15 21:01:41,982: Train batch 43200: loss: 32.49 grad norm: 17.91 time: 0.081
2025-12-15 21:01:56,349: Train batch 43400: loss: 21.32 grad norm: 19.86 time: 0.082
2025-12-15 21:02:10,859: Train batch 43600: loss: 21.89 grad norm: 13.76 time: 0.055
2025-12-15 21:02:25,118: Train batch 43800: loss: 22.63 grad norm: 15.96 time: 0.067
2025-12-15 21:02:39,828: Train batch 44000: loss: 22.78 grad norm: 14.13 time: 0.069
2025-12-15 21:02:39,829: Running test after training batch: 44000
2025-12-15 21:02:48,907: Val batch 44000: PER (avg): 0.1465 CTC Loss (avg): 30.9809 time: 9.078
2025-12-15 21:02:48,907: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:02:48,907: t15.2023.08.13 val PER: 0.1185
2025-12-15 21:02:48,907: t15.2023.08.18 val PER: 0.1023
2025-12-15 21:02:48,907: t15.2023.08.20 val PER: 0.0818
2025-12-15 21:02:48,907: t15.2023.08.25 val PER: 0.1054
2025-12-15 21:02:48,907: t15.2023.08.27 val PER: 0.1688
2025-12-15 21:02:48,907: t15.2023.09.01 val PER: 0.0657
2025-12-15 21:02:48,907: t15.2023.09.03 val PER: 0.1425
2025-12-15 21:02:48,907: t15.2023.09.24 val PER: 0.1201
2025-12-15 21:02:48,907: t15.2023.09.29 val PER: 0.1429
2025-12-15 21:02:48,907: t15.2023.10.01 val PER: 0.1651
2025-12-15 21:02:48,908: t15.2023.10.06 val PER: 0.1098
2025-12-15 21:02:48,908: t15.2023.10.08 val PER: 0.2124
2025-12-15 21:02:48,908: t15.2023.10.13 val PER: 0.2056
2025-12-15 21:02:48,908: t15.2023.10.15 val PER: 0.1404
2025-12-15 21:02:48,908: t15.2023.10.20 val PER: 0.2148
2025-12-15 21:02:48,908: t15.2023.10.22 val PER: 0.1336
2025-12-15 21:02:48,908: t15.2023.11.03 val PER: 0.1920
2025-12-15 21:02:48,908: t15.2023.11.04 val PER: 0.0341
2025-12-15 21:02:48,908: t15.2023.11.17 val PER: 0.0622
2025-12-15 21:02:48,908: t15.2023.11.19 val PER: 0.0858
2025-12-15 21:02:48,908: t15.2023.11.26 val PER: 0.1167
2025-12-15 21:02:48,908: t15.2023.12.03 val PER: 0.1103
2025-12-15 21:02:48,908: t15.2023.12.08 val PER: 0.0999
2025-12-15 21:02:48,908: t15.2023.12.10 val PER: 0.0631
2025-12-15 21:02:48,908: t15.2023.12.17 val PER: 0.1237
2025-12-15 21:02:48,908: t15.2023.12.29 val PER: 0.1187
2025-12-15 21:02:48,908: t15.2024.02.25 val PER: 0.0997
2025-12-15 21:02:48,908: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:02:48,908: t15.2024.03.08 val PER: 0.2361
2025-12-15 21:02:48,909: t15.2024.03.15 val PER: 0.2001
2025-12-15 21:02:48,909: t15.2024.03.17 val PER: 0.1144
2025-12-15 21:02:48,909: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:02:48,909: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:02:48,909: t15.2024.05.10 val PER: 0.1768
2025-12-15 21:02:48,909: t15.2024.06.14 val PER: 0.1956
2025-12-15 21:02:48,909: t15.2024.07.19 val PER: 0.2011
2025-12-15 21:02:48,909: t15.2024.07.21 val PER: 0.0959
2025-12-15 21:02:48,909: t15.2024.07.28 val PER: 0.1000
2025-12-15 21:02:48,909: t15.2025.01.10 val PER: 0.3044
2025-12-15 21:02:48,909: t15.2025.01.12 val PER: 0.1301
2025-12-15 21:02:48,909: t15.2025.03.14 val PER: 0.3417
2025-12-15 21:02:48,909: t15.2025.03.16 val PER: 0.1937
2025-12-15 21:02:48,909: t15.2025.03.30 val PER: 0.2690
2025-12-15 21:02:48,909: t15.2025.04.13 val PER: 0.2482
2025-12-15 21:03:02,570: Train batch 44200: loss: 24.81 grad norm: 24.13 time: 0.074
2025-12-15 21:03:17,296: Train batch 44400: loss: 23.84 grad norm: 19.15 time: 0.074
2025-12-15 21:03:31,871: Train batch 44600: loss: 23.12 grad norm: 27.77 time: 0.057
2025-12-15 21:03:46,200: Train batch 44800: loss: 21.96 grad norm: 9.17 time: 0.073
2025-12-15 21:03:59,623: Train batch 45000: loss: 22.26 grad norm: 24.67 time: 0.066
2025-12-15 21:04:14,222: Train batch 45200: loss: 23.64 grad norm: 6.15 time: 0.055
2025-12-15 21:04:28,787: Train batch 45400: loss: 26.63 grad norm: 27.47 time: 0.053
2025-12-15 21:04:42,940: Train batch 45600: loss: 22.74 grad norm: 7.04 time: 0.077
2025-12-15 21:04:57,184: Train batch 45800: loss: 21.01 grad norm: 16.06 time: 0.064
2025-12-15 21:05:11,659: Train batch 46000: loss: 24.78 grad norm: 6.31 time: 0.070
2025-12-15 21:05:11,660: Running test after training batch: 46000
2025-12-15 21:05:20,860: Val batch 46000: PER (avg): 0.1452 CTC Loss (avg): 30.4101 time: 9.200
2025-12-15 21:05:20,860: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:05:20,860: t15.2023.08.13 val PER: 0.1372
2025-12-15 21:05:20,860: t15.2023.08.18 val PER: 0.1048
2025-12-15 21:05:20,860: t15.2023.08.20 val PER: 0.0802
2025-12-15 21:05:20,860: t15.2023.08.25 val PER: 0.0889
2025-12-15 21:05:20,860: t15.2023.08.27 val PER: 0.1785
2025-12-15 21:05:20,860: t15.2023.09.01 val PER: 0.0755
2025-12-15 21:05:20,860: t15.2023.09.03 val PER: 0.1473
2025-12-15 21:05:20,860: t15.2023.09.24 val PER: 0.1177
2025-12-15 21:05:20,860: t15.2023.09.29 val PER: 0.1436
2025-12-15 21:05:20,861: t15.2023.10.01 val PER: 0.1691
2025-12-15 21:05:20,861: t15.2023.10.06 val PER: 0.1141
2025-12-15 21:05:20,861: t15.2023.10.08 val PER: 0.2219
2025-12-15 21:05:20,861: t15.2023.10.13 val PER: 0.1955
2025-12-15 21:05:20,861: t15.2023.10.15 val PER: 0.1450
2025-12-15 21:05:20,861: t15.2023.10.20 val PER: 0.2047
2025-12-15 21:05:20,861: t15.2023.10.22 val PER: 0.1370
2025-12-15 21:05:20,861: t15.2023.11.03 val PER: 0.1974
2025-12-15 21:05:20,861: t15.2023.11.04 val PER: 0.0307
2025-12-15 21:05:20,861: t15.2023.11.17 val PER: 0.0513
2025-12-15 21:05:20,861: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:05:20,861: t15.2023.11.26 val PER: 0.1116
2025-12-15 21:05:20,861: t15.2023.12.03 val PER: 0.0893
2025-12-15 21:05:20,861: t15.2023.12.08 val PER: 0.1019
2025-12-15 21:05:20,861: t15.2023.12.10 val PER: 0.0775
2025-12-15 21:05:20,861: t15.2023.12.17 val PER: 0.1247
2025-12-15 21:05:20,861: t15.2023.12.29 val PER: 0.1132
2025-12-15 21:05:20,861: t15.2024.02.25 val PER: 0.1096
2025-12-15 21:05:20,861: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:05:20,862: t15.2024.03.08 val PER: 0.2048
2025-12-15 21:05:20,862: t15.2024.03.15 val PER: 0.1920
2025-12-15 21:05:20,862: t15.2024.03.17 val PER: 0.1248
2025-12-15 21:05:20,862: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:05:20,862: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:05:20,862: t15.2024.05.10 val PER: 0.1679
2025-12-15 21:05:20,862: t15.2024.06.14 val PER: 0.1656
2025-12-15 21:05:20,862: t15.2024.07.19 val PER: 0.1971
2025-12-15 21:05:20,862: t15.2024.07.21 val PER: 0.0897
2025-12-15 21:05:20,862: t15.2024.07.28 val PER: 0.1184
2025-12-15 21:05:20,862: t15.2025.01.10 val PER: 0.3072
2025-12-15 21:05:20,862: t15.2025.01.12 val PER: 0.1347
2025-12-15 21:05:20,862: t15.2025.03.14 val PER: 0.3121
2025-12-15 21:05:20,862: t15.2025.03.16 val PER: 0.2042
2025-12-15 21:05:20,862: t15.2025.03.30 val PER: 0.2506
2025-12-15 21:05:20,862: t15.2025.04.13 val PER: 0.2354
2025-12-15 21:05:20,862: New best test PER 0.1454 --> 0.1452
2025-12-15 21:05:20,862: Checkpointing model
2025-12-15 21:05:21,293: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:05:35,309: Train batch 46200: loss: 21.49 grad norm: 29.04 time: 0.067
2025-12-15 21:05:49,803: Train batch 46400: loss: 20.77 grad norm: 36.88 time: 0.055
2025-12-15 21:06:03,567: Train batch 46600: loss: 22.33 grad norm: 12.08 time: 0.073
2025-12-15 21:06:16,353: Train batch 46800: loss: 26.80 grad norm: 9.27 time: 0.027
2025-12-15 21:06:28,237: Train batch 47000: loss: 24.05 grad norm: 13.51 time: 0.057
2025-12-15 21:06:42,729: Train batch 47200: loss: 26.21 grad norm: 24.07 time: 0.066
2025-12-15 21:06:57,262: Train batch 47400: loss: 21.56 grad norm: 9.40 time: 0.071
2025-12-15 21:07:11,776: Train batch 47600: loss: 23.51 grad norm: 17.38 time: 0.058
2025-12-15 21:07:25,794: Train batch 47800: loss: 29.79 grad norm: 38.40 time: 0.084
2025-12-15 21:07:39,774: Train batch 48000: loss: 25.83 grad norm: 16.30 time: 0.085
2025-12-15 21:07:39,775: Running test after training batch: 48000
2025-12-15 21:07:48,902: Val batch 48000: PER (avg): 0.1433 CTC Loss (avg): 29.7796 time: 9.127
2025-12-15 21:07:48,903: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:07:48,903: t15.2023.08.13 val PER: 0.1071
2025-12-15 21:07:48,903: t15.2023.08.18 val PER: 0.1106
2025-12-15 21:07:48,903: t15.2023.08.20 val PER: 0.0818
2025-12-15 21:07:48,903: t15.2023.08.25 val PER: 0.0904
2025-12-15 21:07:48,903: t15.2023.08.27 val PER: 0.1559
2025-12-15 21:07:48,903: t15.2023.09.01 val PER: 0.0739
2025-12-15 21:07:48,903: t15.2023.09.03 val PER: 0.1378
2025-12-15 21:07:48,903: t15.2023.09.24 val PER: 0.0959
2025-12-15 21:07:48,903: t15.2023.09.29 val PER: 0.1308
2025-12-15 21:07:48,903: t15.2023.10.01 val PER: 0.1697
2025-12-15 21:07:48,903: t15.2023.10.06 val PER: 0.1098
2025-12-15 21:07:48,903: t15.2023.10.08 val PER: 0.1827
2025-12-15 21:07:48,903: t15.2023.10.13 val PER: 0.2110
2025-12-15 21:07:48,903: t15.2023.10.15 val PER: 0.1391
2025-12-15 21:07:48,903: t15.2023.10.20 val PER: 0.2282
2025-12-15 21:07:48,903: t15.2023.10.22 val PER: 0.1281
2025-12-15 21:07:48,904: t15.2023.11.03 val PER: 0.2015
2025-12-15 21:07:48,904: t15.2023.11.04 val PER: 0.0273
2025-12-15 21:07:48,904: t15.2023.11.17 val PER: 0.0622
2025-12-15 21:07:48,904: t15.2023.11.19 val PER: 0.0918
2025-12-15 21:07:48,904: t15.2023.11.26 val PER: 0.1109
2025-12-15 21:07:48,904: t15.2023.12.03 val PER: 0.1008
2025-12-15 21:07:48,904: t15.2023.12.08 val PER: 0.0965
2025-12-15 21:07:48,904: t15.2023.12.10 val PER: 0.0802
2025-12-15 21:07:48,904: t15.2023.12.17 val PER: 0.1320
2025-12-15 21:07:48,904: t15.2023.12.29 val PER: 0.1153
2025-12-15 21:07:48,904: t15.2024.02.25 val PER: 0.0744
2025-12-15 21:07:48,904: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:07:48,904: t15.2024.03.08 val PER: 0.2034
2025-12-15 21:07:48,904: t15.2024.03.15 val PER: 0.2076
2025-12-15 21:07:48,904: t15.2024.03.17 val PER: 0.1151
2025-12-15 21:07:48,904: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:07:48,904: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:07:48,904: t15.2024.05.10 val PER: 0.1709
2025-12-15 21:07:48,904: t15.2024.06.14 val PER: 0.1562
2025-12-15 21:07:48,904: t15.2024.07.19 val PER: 0.2024
2025-12-15 21:07:48,905: t15.2024.07.21 val PER: 0.0897
2025-12-15 21:07:48,905: t15.2024.07.28 val PER: 0.1103
2025-12-15 21:07:48,905: t15.2025.01.10 val PER: 0.2989
2025-12-15 21:07:48,905: t15.2025.01.12 val PER: 0.1440
2025-12-15 21:07:48,905: t15.2025.03.14 val PER: 0.2988
2025-12-15 21:07:48,905: t15.2025.03.16 val PER: 0.2016
2025-12-15 21:07:48,905: t15.2025.03.30 val PER: 0.2632
2025-12-15 21:07:48,905: t15.2025.04.13 val PER: 0.2311
2025-12-15 21:07:48,905: New best test PER 0.1452 --> 0.1433
2025-12-15 21:07:48,905: Checkpointing model
2025-12-15 21:07:49,341: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:08:03,908: Train batch 48200: loss: 20.71 grad norm: 23.72 time: 0.062
2025-12-15 21:08:18,249: Train batch 48400: loss: 18.39 grad norm: 20.37 time: 0.086
2025-12-15 21:08:32,910: Train batch 48600: loss: 21.68 grad norm: 32.18 time: 0.070
2025-12-15 21:08:47,113: Train batch 48800: loss: 20.91 grad norm: 18.56 time: 0.064
2025-12-15 21:09:01,549: Train batch 49000: loss: 21.60 grad norm: 28.07 time: 0.072
2025-12-15 21:09:16,132: Train batch 49200: loss: 22.61 grad norm: 9.99 time: 0.056
2025-12-15 21:09:30,773: Train batch 49400: loss: 19.76 grad norm: 11.71 time: 0.060
2025-12-15 21:09:45,333: Train batch 49600: loss: 25.78 grad norm: 6.73 time: 0.071
2025-12-15 21:09:59,482: Train batch 49800: loss: 21.16 grad norm: 29.69 time: 0.067
2025-12-15 21:10:13,871: Train batch 50000: loss: 22.33 grad norm: 31.90 time: 0.048
2025-12-15 21:10:13,872: Running test after training batch: 50000
2025-12-15 21:10:22,894: Val batch 50000: PER (avg): 0.1405 CTC Loss (avg): 30.8083 time: 9.022
2025-12-15 21:10:22,895: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:10:22,895: t15.2023.08.13 val PER: 0.1154
2025-12-15 21:10:22,895: t15.2023.08.18 val PER: 0.0905
2025-12-15 21:10:22,895: t15.2023.08.20 val PER: 0.0858
2025-12-15 21:10:22,895: t15.2023.08.25 val PER: 0.1009
2025-12-15 21:10:22,895: t15.2023.08.27 val PER: 0.1688
2025-12-15 21:10:22,895: t15.2023.09.01 val PER: 0.0731
2025-12-15 21:10:22,895: t15.2023.09.03 val PER: 0.1211
2025-12-15 21:10:22,895: t15.2023.09.24 val PER: 0.1104
2025-12-15 21:10:22,895: t15.2023.09.29 val PER: 0.1391
2025-12-15 21:10:22,895: t15.2023.10.01 val PER: 0.1737
2025-12-15 21:10:22,895: t15.2023.10.06 val PER: 0.1044
2025-12-15 21:10:22,895: t15.2023.10.08 val PER: 0.1949
2025-12-15 21:10:22,895: t15.2023.10.13 val PER: 0.1862
2025-12-15 21:10:22,895: t15.2023.10.15 val PER: 0.1655
2025-12-15 21:10:22,895: t15.2023.10.20 val PER: 0.2081
2025-12-15 21:10:22,895: t15.2023.10.22 val PER: 0.1314
2025-12-15 21:10:22,896: t15.2023.11.03 val PER: 0.1872
2025-12-15 21:10:22,896: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:10:22,896: t15.2023.11.17 val PER: 0.0498
2025-12-15 21:10:22,896: t15.2023.11.19 val PER: 0.0798
2025-12-15 21:10:22,896: t15.2023.11.26 val PER: 0.0942
2025-12-15 21:10:22,896: t15.2023.12.03 val PER: 0.0935
2025-12-15 21:10:22,896: t15.2023.12.08 val PER: 0.0939
2025-12-15 21:10:22,896: t15.2023.12.10 val PER: 0.0867
2025-12-15 21:10:22,896: t15.2023.12.17 val PER: 0.1372
2025-12-15 21:10:22,896: t15.2023.12.29 val PER: 0.1098
2025-12-15 21:10:22,896: t15.2024.02.25 val PER: 0.0857
2025-12-15 21:10:22,896: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:10:22,896: t15.2024.03.08 val PER: 0.2233
2025-12-15 21:10:22,896: t15.2024.03.15 val PER: 0.2039
2025-12-15 21:10:22,896: t15.2024.03.17 val PER: 0.1206
2025-12-15 21:10:22,896: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:10:22,896: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:10:22,896: t15.2024.05.10 val PER: 0.1605
2025-12-15 21:10:22,896: t15.2024.06.14 val PER: 0.1798
2025-12-15 21:10:22,897: t15.2024.07.19 val PER: 0.1846
2025-12-15 21:10:22,897: t15.2024.07.21 val PER: 0.0807
2025-12-15 21:10:22,897: t15.2024.07.28 val PER: 0.1037
2025-12-15 21:10:22,897: t15.2025.01.10 val PER: 0.2893
2025-12-15 21:10:22,897: t15.2025.01.12 val PER: 0.1209
2025-12-15 21:10:22,897: t15.2025.03.14 val PER: 0.3166
2025-12-15 21:10:22,897: t15.2025.03.16 val PER: 0.1976
2025-12-15 21:10:22,897: t15.2025.03.30 val PER: 0.2368
2025-12-15 21:10:22,897: t15.2025.04.13 val PER: 0.2183
2025-12-15 21:10:22,897: New best test PER 0.1433 --> 0.1405
2025-12-15 21:10:22,897: Checkpointing model
2025-12-15 21:10:23,334: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:10:37,914: Train batch 50200: loss: 24.99 grad norm: 15.53 time: 0.068
2025-12-15 21:10:52,660: Train batch 50400: loss: 23.35 grad norm: 28.18 time: 0.056
2025-12-15 21:11:07,397: Train batch 50600: loss: 26.45 grad norm: 37.27 time: 0.066
2025-12-15 21:11:21,928: Train batch 50800: loss: 22.95 grad norm: 24.76 time: 0.082
2025-12-15 21:11:36,219: Train batch 51000: loss: 28.31 grad norm: 16.77 time: 0.072
2025-12-15 21:11:50,628: Train batch 51200: loss: 24.92 grad norm: 20.88 time: 0.054
2025-12-15 21:12:05,113: Train batch 51400: loss: 24.14 grad norm: 12.76 time: 0.056
2025-12-15 21:12:19,614: Train batch 51600: loss: 30.19 grad norm: 22.58 time: 0.069
2025-12-15 21:12:33,581: Train batch 51800: loss: 21.99 grad norm: 27.29 time: 0.059
2025-12-15 21:12:48,167: Train batch 52000: loss: 25.56 grad norm: 34.27 time: 0.064
2025-12-15 21:12:48,167: Running test after training batch: 52000
2025-12-15 21:12:57,401: Val batch 52000: PER (avg): 0.1386 CTC Loss (avg): 31.3261 time: 9.233
2025-12-15 21:12:57,401: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:12:57,401: t15.2023.08.13 val PER: 0.1133
2025-12-15 21:12:57,401: t15.2023.08.18 val PER: 0.0964
2025-12-15 21:12:57,401: t15.2023.08.20 val PER: 0.0747
2025-12-15 21:12:57,401: t15.2023.08.25 val PER: 0.0994
2025-12-15 21:12:57,401: t15.2023.08.27 val PER: 0.1559
2025-12-15 21:12:57,401: t15.2023.09.01 val PER: 0.0617
2025-12-15 21:12:57,402: t15.2023.09.03 val PER: 0.1223
2025-12-15 21:12:57,402: t15.2023.09.24 val PER: 0.0971
2025-12-15 21:12:57,402: t15.2023.09.29 val PER: 0.1391
2025-12-15 21:12:57,402: t15.2023.10.01 val PER: 0.1697
2025-12-15 21:12:57,402: t15.2023.10.06 val PER: 0.0969
2025-12-15 21:12:57,402: t15.2023.10.08 val PER: 0.1908
2025-12-15 21:12:57,402: t15.2023.10.13 val PER: 0.1939
2025-12-15 21:12:57,402: t15.2023.10.15 val PER: 0.1549
2025-12-15 21:12:57,402: t15.2023.10.20 val PER: 0.2114
2025-12-15 21:12:57,402: t15.2023.10.22 val PER: 0.1169
2025-12-15 21:12:57,402: t15.2023.11.03 val PER: 0.1872
2025-12-15 21:12:57,402: t15.2023.11.04 val PER: 0.0273
2025-12-15 21:12:57,402: t15.2023.11.17 val PER: 0.0482
2025-12-15 21:12:57,402: t15.2023.11.19 val PER: 0.0719
2025-12-15 21:12:57,402: t15.2023.11.26 val PER: 0.1145
2025-12-15 21:12:57,402: t15.2023.12.03 val PER: 0.0872
2025-12-15 21:12:57,402: t15.2023.12.08 val PER: 0.0866
2025-12-15 21:12:57,402: t15.2023.12.10 val PER: 0.0657
2025-12-15 21:12:57,402: t15.2023.12.17 val PER: 0.1143
2025-12-15 21:12:57,403: t15.2023.12.29 val PER: 0.1187
2025-12-15 21:12:57,403: t15.2024.02.25 val PER: 0.0941
2025-12-15 21:12:57,403: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:12:57,403: t15.2024.03.08 val PER: 0.1892
2025-12-15 21:12:57,403: t15.2024.03.15 val PER: 0.1932
2025-12-15 21:12:57,403: t15.2024.03.17 val PER: 0.1067
2025-12-15 21:12:57,403: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:12:57,403: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:12:57,403: t15.2024.05.10 val PER: 0.1709
2025-12-15 21:12:57,403: t15.2024.06.14 val PER: 0.1546
2025-12-15 21:12:57,403: t15.2024.07.19 val PER: 0.2024
2025-12-15 21:12:57,403: t15.2024.07.21 val PER: 0.0938
2025-12-15 21:12:57,403: t15.2024.07.28 val PER: 0.1176
2025-12-15 21:12:57,403: t15.2025.01.10 val PER: 0.2961
2025-12-15 21:12:57,403: t15.2025.01.12 val PER: 0.1316
2025-12-15 21:12:57,403: t15.2025.03.14 val PER: 0.3107
2025-12-15 21:12:57,403: t15.2025.03.16 val PER: 0.1898
2025-12-15 21:12:57,403: t15.2025.03.30 val PER: 0.2460
2025-12-15 21:12:57,403: t15.2025.04.13 val PER: 0.2154
2025-12-15 21:12:57,403: New best test PER 0.1405 --> 0.1386
2025-12-15 21:12:57,404: Checkpointing model
2025-12-15 21:12:57,834: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:13:11,575: Train batch 52200: loss: 22.73 grad norm: 3.64 time: 0.079
2025-12-15 21:13:25,822: Train batch 52400: loss: 25.24 grad norm: 8.17 time: 0.081
2025-12-15 21:13:40,464: Train batch 52600: loss: 20.92 grad norm: 12.69 time: 0.063
2025-12-15 21:13:55,074: Train batch 52800: loss: 19.91 grad norm: 20.18 time: 0.055
2025-12-15 21:14:09,202: Train batch 53000: loss: 23.76 grad norm: 15.89 time: 0.056
2025-12-15 21:14:23,814: Train batch 53200: loss: 18.89 grad norm: 25.32 time: 0.074
2025-12-15 21:14:38,312: Train batch 53400: loss: 22.12 grad norm: 10.92 time: 0.057
2025-12-15 21:14:52,713: Train batch 53600: loss: 21.28 grad norm: 11.66 time: 0.056
2025-12-15 21:15:06,981: Train batch 53800: loss: 25.22 grad norm: 16.89 time: 0.064
2025-12-15 21:15:21,497: Train batch 54000: loss: 22.09 grad norm: 17.09 time: 0.056
2025-12-15 21:15:21,498: Running test after training batch: 54000
2025-12-15 21:15:30,667: Val batch 54000: PER (avg): 0.1375 CTC Loss (avg): 33.0698 time: 9.170
2025-12-15 21:15:30,668: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:15:30,668: t15.2023.08.13 val PER: 0.1102
2025-12-15 21:15:30,668: t15.2023.08.18 val PER: 0.0956
2025-12-15 21:15:30,668: t15.2023.08.20 val PER: 0.0755
2025-12-15 21:15:30,668: t15.2023.08.25 val PER: 0.1024
2025-12-15 21:15:30,668: t15.2023.08.27 val PER: 0.1431
2025-12-15 21:15:30,668: t15.2023.09.01 val PER: 0.0568
2025-12-15 21:15:30,668: t15.2023.09.03 val PER: 0.1318
2025-12-15 21:15:30,668: t15.2023.09.24 val PER: 0.1032
2025-12-15 21:15:30,668: t15.2023.09.29 val PER: 0.1404
2025-12-15 21:15:30,668: t15.2023.10.01 val PER: 0.1664
2025-12-15 21:15:30,668: t15.2023.10.06 val PER: 0.1098
2025-12-15 21:15:30,668: t15.2023.10.08 val PER: 0.1962
2025-12-15 21:15:30,668: t15.2023.10.13 val PER: 0.1784
2025-12-15 21:15:30,668: t15.2023.10.15 val PER: 0.1417
2025-12-15 21:15:30,668: t15.2023.10.20 val PER: 0.2248
2025-12-15 21:15:30,668: t15.2023.10.22 val PER: 0.1058
2025-12-15 21:15:30,669: t15.2023.11.03 val PER: 0.1777
2025-12-15 21:15:30,669: t15.2023.11.04 val PER: 0.0273
2025-12-15 21:15:30,669: t15.2023.11.17 val PER: 0.0451
2025-12-15 21:15:30,669: t15.2023.11.19 val PER: 0.0559
2025-12-15 21:15:30,669: t15.2023.11.26 val PER: 0.1297
2025-12-15 21:15:30,669: t15.2023.12.03 val PER: 0.1113
2025-12-15 21:15:30,669: t15.2023.12.08 val PER: 0.0839
2025-12-15 21:15:30,669: t15.2023.12.10 val PER: 0.0723
2025-12-15 21:15:30,669: t15.2023.12.17 val PER: 0.1102
2025-12-15 21:15:30,669: t15.2023.12.29 val PER: 0.1187
2025-12-15 21:15:30,669: t15.2024.02.25 val PER: 0.0885
2025-12-15 21:15:30,669: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:15:30,669: t15.2024.03.08 val PER: 0.1949
2025-12-15 21:15:30,669: t15.2024.03.15 val PER: 0.1957
2025-12-15 21:15:30,669: t15.2024.03.17 val PER: 0.1185
2025-12-15 21:15:30,669: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:15:30,669: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:15:30,669: t15.2024.05.10 val PER: 0.1709
2025-12-15 21:15:30,669: t15.2024.06.14 val PER: 0.1656
2025-12-15 21:15:30,670: t15.2024.07.19 val PER: 0.1991
2025-12-15 21:15:30,670: t15.2024.07.21 val PER: 0.0903
2025-12-15 21:15:30,670: t15.2024.07.28 val PER: 0.1066
2025-12-15 21:15:30,670: t15.2025.01.10 val PER: 0.2961
2025-12-15 21:15:30,670: t15.2025.01.12 val PER: 0.1401
2025-12-15 21:15:30,670: t15.2025.03.14 val PER: 0.2899
2025-12-15 21:15:30,670: t15.2025.03.16 val PER: 0.1662
2025-12-15 21:15:30,670: t15.2025.03.30 val PER: 0.2414
2025-12-15 21:15:30,670: t15.2025.04.13 val PER: 0.2054
2025-12-15 21:15:30,670: New best test PER 0.1386 --> 0.1375
2025-12-15 21:15:30,670: Checkpointing model
2025-12-15 21:15:31,106: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:15:42,963: Train batch 54200: loss: 22.17 grad norm: 14.88 time: 0.074
2025-12-15 21:15:57,603: Train batch 54400: loss: 23.59 grad norm: 12.09 time: 0.082
2025-12-15 21:16:11,999: Train batch 54600: loss: 19.44 grad norm: 4.69 time: 0.068
2025-12-15 21:16:25,394: Train batch 54800: loss: 21.77 grad norm: 7.32 time: 0.035
2025-12-15 21:16:39,942: Train batch 55000: loss: 25.93 grad norm: 4.55 time: 0.064
2025-12-15 21:16:53,223: Train batch 55200: loss: 21.97 grad norm: 29.86 time: 0.062
2025-12-15 21:17:07,227: Train batch 55400: loss: 21.30 grad norm: 17.67 time: 0.056
2025-12-15 21:17:21,516: Train batch 55600: loss: 22.62 grad norm: 17.22 time: 0.056
2025-12-15 21:17:36,075: Train batch 55800: loss: 24.20 grad norm: 15.72 time: 0.067
2025-12-15 21:17:49,931: Train batch 56000: loss: 22.26 grad norm: 11.58 time: 0.067
2025-12-15 21:17:49,931: Running test after training batch: 56000
2025-12-15 21:17:59,138: Val batch 56000: PER (avg): 0.1395 CTC Loss (avg): 33.4238 time: 9.206
2025-12-15 21:17:59,138: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:17:59,138: t15.2023.08.13 val PER: 0.1060
2025-12-15 21:17:59,138: t15.2023.08.18 val PER: 0.0855
2025-12-15 21:17:59,138: t15.2023.08.20 val PER: 0.0731
2025-12-15 21:17:59,138: t15.2023.08.25 val PER: 0.1114
2025-12-15 21:17:59,138: t15.2023.08.27 val PER: 0.1479
2025-12-15 21:17:59,138: t15.2023.09.01 val PER: 0.0657
2025-12-15 21:17:59,139: t15.2023.09.03 val PER: 0.1425
2025-12-15 21:17:59,139: t15.2023.09.24 val PER: 0.1117
2025-12-15 21:17:59,139: t15.2023.09.29 val PER: 0.1353
2025-12-15 21:17:59,139: t15.2023.10.01 val PER: 0.1697
2025-12-15 21:17:59,139: t15.2023.10.06 val PER: 0.1152
2025-12-15 21:17:59,139: t15.2023.10.08 val PER: 0.1922
2025-12-15 21:17:59,139: t15.2023.10.13 val PER: 0.1994
2025-12-15 21:17:59,139: t15.2023.10.15 val PER: 0.1510
2025-12-15 21:17:59,139: t15.2023.10.20 val PER: 0.2248
2025-12-15 21:17:59,139: t15.2023.10.22 val PER: 0.1247
2025-12-15 21:17:59,139: t15.2023.11.03 val PER: 0.1825
2025-12-15 21:17:59,139: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:17:59,139: t15.2023.11.17 val PER: 0.0498
2025-12-15 21:17:59,139: t15.2023.11.19 val PER: 0.0798
2025-12-15 21:17:59,139: t15.2023.11.26 val PER: 0.1217
2025-12-15 21:17:59,139: t15.2023.12.03 val PER: 0.1061
2025-12-15 21:17:59,139: t15.2023.12.08 val PER: 0.0766
2025-12-15 21:17:59,139: t15.2023.12.10 val PER: 0.0618
2025-12-15 21:17:59,139: t15.2023.12.17 val PER: 0.1237
2025-12-15 21:17:59,140: t15.2023.12.29 val PER: 0.1078
2025-12-15 21:17:59,140: t15.2024.02.25 val PER: 0.0758
2025-12-15 21:17:59,140: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:17:59,140: t15.2024.03.08 val PER: 0.1920
2025-12-15 21:17:59,140: t15.2024.03.15 val PER: 0.1901
2025-12-15 21:17:59,140: t15.2024.03.17 val PER: 0.1123
2025-12-15 21:17:59,140: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:17:59,140: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:17:59,140: t15.2024.05.10 val PER: 0.1783
2025-12-15 21:17:59,140: t15.2024.06.14 val PER: 0.1420
2025-12-15 21:17:59,140: t15.2024.07.19 val PER: 0.1951
2025-12-15 21:17:59,140: t15.2024.07.21 val PER: 0.0931
2025-12-15 21:17:59,140: t15.2024.07.28 val PER: 0.1206
2025-12-15 21:17:59,140: t15.2025.01.10 val PER: 0.2948
2025-12-15 21:17:59,140: t15.2025.01.12 val PER: 0.1347
2025-12-15 21:17:59,140: t15.2025.03.14 val PER: 0.3077
2025-12-15 21:17:59,140: t15.2025.03.16 val PER: 0.1832
2025-12-15 21:17:59,140: t15.2025.03.30 val PER: 0.2563
2025-12-15 21:17:59,140: t15.2025.04.13 val PER: 0.2325
2025-12-15 21:18:13,692: Train batch 56200: loss: 24.52 grad norm: 25.18 time: 0.056
2025-12-15 21:18:27,881: Train batch 56400: loss: 24.00 grad norm: 3.22 time: 0.068
2025-12-15 21:18:42,534: Train batch 56600: loss: 18.75 grad norm: 23.92 time: 0.055
2025-12-15 21:18:56,626: Train batch 56800: loss: 22.98 grad norm: 12.00 time: 0.065
2025-12-15 21:19:11,052: Train batch 57000: loss: 22.47 grad norm: 15.66 time: 0.062
2025-12-15 21:19:23,855: Train batch 57200: loss: 21.90 grad norm: 12.61 time: 0.055
2025-12-15 21:19:38,541: Train batch 57400: loss: 20.61 grad norm: 16.83 time: 0.073
2025-12-15 21:19:52,983: Train batch 57600: loss: 21.01 grad norm: 2.89 time: 0.056
2025-12-15 21:20:07,588: Train batch 57800: loss: 24.73 grad norm: 67.41 time: 0.081
2025-12-15 21:20:22,176: Train batch 58000: loss: 24.81 grad norm: 25.56 time: 0.079
2025-12-15 21:20:22,176: Running test after training batch: 58000
2025-12-15 21:20:31,339: Val batch 58000: PER (avg): 0.1391 CTC Loss (avg): 34.4089 time: 9.163
2025-12-15 21:20:31,340: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:20:31,340: t15.2023.08.13 val PER: 0.1112
2025-12-15 21:20:31,340: t15.2023.08.18 val PER: 0.0914
2025-12-15 21:20:31,340: t15.2023.08.20 val PER: 0.0842
2025-12-15 21:20:31,340: t15.2023.08.25 val PER: 0.1145
2025-12-15 21:20:31,340: t15.2023.08.27 val PER: 0.1576
2025-12-15 21:20:31,340: t15.2023.09.01 val PER: 0.0739
2025-12-15 21:20:31,340: t15.2023.09.03 val PER: 0.1342
2025-12-15 21:20:31,340: t15.2023.09.24 val PER: 0.1068
2025-12-15 21:20:31,340: t15.2023.09.29 val PER: 0.1308
2025-12-15 21:20:31,340: t15.2023.10.01 val PER: 0.1711
2025-12-15 21:20:31,340: t15.2023.10.06 val PER: 0.1023
2025-12-15 21:20:31,340: t15.2023.10.08 val PER: 0.1935
2025-12-15 21:20:31,340: t15.2023.10.13 val PER: 0.2048
2025-12-15 21:20:31,340: t15.2023.10.15 val PER: 0.1404
2025-12-15 21:20:31,340: t15.2023.10.20 val PER: 0.2215
2025-12-15 21:20:31,340: t15.2023.10.22 val PER: 0.1258
2025-12-15 21:20:31,341: t15.2023.11.03 val PER: 0.1744
2025-12-15 21:20:31,341: t15.2023.11.04 val PER: 0.0307
2025-12-15 21:20:31,341: t15.2023.11.17 val PER: 0.0684
2025-12-15 21:20:31,341: t15.2023.11.19 val PER: 0.0699
2025-12-15 21:20:31,341: t15.2023.11.26 val PER: 0.1051
2025-12-15 21:20:31,341: t15.2023.12.03 val PER: 0.1008
2025-12-15 21:20:31,341: t15.2023.12.08 val PER: 0.0905
2025-12-15 21:20:31,341: t15.2023.12.10 val PER: 0.0631
2025-12-15 21:20:31,341: t15.2023.12.17 val PER: 0.1237
2025-12-15 21:20:31,341: t15.2023.12.29 val PER: 0.1119
2025-12-15 21:20:31,341: t15.2024.02.25 val PER: 0.0913
2025-12-15 21:20:31,341: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:20:31,341: t15.2024.03.08 val PER: 0.2077
2025-12-15 21:20:31,341: t15.2024.03.15 val PER: 0.1914
2025-12-15 21:20:31,341: t15.2024.03.17 val PER: 0.1102
2025-12-15 21:20:31,341: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:20:31,341: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:20:31,341: t15.2024.05.10 val PER: 0.1709
2025-12-15 21:20:31,341: t15.2024.06.14 val PER: 0.1514
2025-12-15 21:20:31,342: t15.2024.07.19 val PER: 0.1951
2025-12-15 21:20:31,342: t15.2024.07.21 val PER: 0.0883
2025-12-15 21:20:31,342: t15.2024.07.28 val PER: 0.1096
2025-12-15 21:20:31,342: t15.2025.01.10 val PER: 0.2893
2025-12-15 21:20:31,342: t15.2025.01.12 val PER: 0.1301
2025-12-15 21:20:31,342: t15.2025.03.14 val PER: 0.3018
2025-12-15 21:20:31,342: t15.2025.03.16 val PER: 0.1911
2025-12-15 21:20:31,342: t15.2025.03.30 val PER: 0.2483
2025-12-15 21:20:31,342: t15.2025.04.13 val PER: 0.2225
2025-12-15 21:20:45,847: Train batch 58200: loss: 23.52 grad norm: 21.58 time: 0.066
2025-12-15 21:21:00,255: Train batch 58400: loss: 20.68 grad norm: 88.52 time: 0.057
2025-12-15 21:21:12,861: Train batch 58600: loss: 21.53 grad norm: 20.77 time: 0.063
2025-12-15 21:21:27,028: Train batch 58800: loss: 23.61 grad norm: 19.87 time: 0.043
2025-12-15 21:21:41,599: Train batch 59000: loss: 23.23 grad norm: 7.15 time: 0.069
2025-12-15 21:21:55,630: Train batch 59200: loss: 26.02 grad norm: 30.42 time: 0.061
2025-12-15 21:22:08,720: Train batch 59400: loss: 25.75 grad norm: 22.97 time: 0.076
2025-12-15 21:22:23,643: Train batch 59600: loss: 25.36 grad norm: 15.02 time: 0.054
2025-12-15 21:22:36,860: Train batch 59800: loss: 23.03 grad norm: 16.31 time: 0.058
2025-12-15 21:22:49,979: Train batch 60000: loss: 21.72 grad norm: 0.35 time: 0.053
2025-12-15 21:22:49,979: Running test after training batch: 60000
2025-12-15 21:22:59,066: Val batch 60000: PER (avg): 0.1357 CTC Loss (avg): 32.9557 time: 9.087
2025-12-15 21:22:59,066: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:22:59,066: t15.2023.08.13 val PER: 0.1050
2025-12-15 21:22:59,066: t15.2023.08.18 val PER: 0.0880
2025-12-15 21:22:59,067: t15.2023.08.20 val PER: 0.0778
2025-12-15 21:22:59,067: t15.2023.08.25 val PER: 0.1190
2025-12-15 21:22:59,067: t15.2023.08.27 val PER: 0.1479
2025-12-15 21:22:59,067: t15.2023.09.01 val PER: 0.0714
2025-12-15 21:22:59,067: t15.2023.09.03 val PER: 0.1295
2025-12-15 21:22:59,067: t15.2023.09.24 val PER: 0.1056
2025-12-15 21:22:59,067: t15.2023.09.29 val PER: 0.1187
2025-12-15 21:22:59,067: t15.2023.10.01 val PER: 0.1651
2025-12-15 21:22:59,067: t15.2023.10.06 val PER: 0.1098
2025-12-15 21:22:59,067: t15.2023.10.08 val PER: 0.1827
2025-12-15 21:22:59,067: t15.2023.10.13 val PER: 0.1901
2025-12-15 21:22:59,067: t15.2023.10.15 val PER: 0.1404
2025-12-15 21:22:59,067: t15.2023.10.20 val PER: 0.2215
2025-12-15 21:22:59,067: t15.2023.10.22 val PER: 0.1169
2025-12-15 21:22:59,067: t15.2023.11.03 val PER: 0.1825
2025-12-15 21:22:59,067: t15.2023.11.04 val PER: 0.0239
2025-12-15 21:22:59,067: t15.2023.11.17 val PER: 0.0451
2025-12-15 21:22:59,067: t15.2023.11.19 val PER: 0.0679
2025-12-15 21:22:59,067: t15.2023.11.26 val PER: 0.1014
2025-12-15 21:22:59,068: t15.2023.12.03 val PER: 0.1061
2025-12-15 21:22:59,068: t15.2023.12.08 val PER: 0.0799
2025-12-15 21:22:59,068: t15.2023.12.10 val PER: 0.0604
2025-12-15 21:22:59,068: t15.2023.12.17 val PER: 0.1227
2025-12-15 21:22:59,068: t15.2023.12.29 val PER: 0.1023
2025-12-15 21:22:59,068: t15.2024.02.25 val PER: 0.0885
2025-12-15 21:22:59,068: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:22:59,068: t15.2024.03.08 val PER: 0.2162
2025-12-15 21:22:59,068: t15.2024.03.15 val PER: 0.1870
2025-12-15 21:22:59,068: t15.2024.03.17 val PER: 0.1116
2025-12-15 21:22:59,068: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:22:59,068: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:22:59,068: t15.2024.05.10 val PER: 0.1694
2025-12-15 21:22:59,068: t15.2024.06.14 val PER: 0.1388
2025-12-15 21:22:59,068: t15.2024.07.19 val PER: 0.1925
2025-12-15 21:22:59,068: t15.2024.07.21 val PER: 0.0876
2025-12-15 21:22:59,068: t15.2024.07.28 val PER: 0.1140
2025-12-15 21:22:59,068: t15.2025.01.10 val PER: 0.2920
2025-12-15 21:22:59,068: t15.2025.01.12 val PER: 0.1386
2025-12-15 21:22:59,069: t15.2025.03.14 val PER: 0.2973
2025-12-15 21:22:59,069: t15.2025.03.16 val PER: 0.1702
2025-12-15 21:22:59,069: t15.2025.03.30 val PER: 0.2494
2025-12-15 21:22:59,069: t15.2025.04.13 val PER: 0.2168
2025-12-15 21:22:59,069: New best test PER 0.1375 --> 0.1357
2025-12-15 21:22:59,069: Checkpointing model
2025-12-15 21:22:59,500: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:23:11,677: Train batch 60200: loss: 24.11 grad norm: 11.10 time: 0.063
2025-12-15 21:23:26,344: Train batch 60400: loss: 20.17 grad norm: 10.85 time: 0.066
2025-12-15 21:23:40,966: Train batch 60600: loss: 23.02 grad norm: 6.43 time: 0.075
2025-12-15 21:23:54,018: Train batch 60800: loss: 23.50 grad norm: 8.35 time: 0.037
2025-12-15 21:24:06,286: Train batch 61000: loss: 21.27 grad norm: 9.71 time: 0.071
2025-12-15 21:24:20,512: Train batch 61200: loss: 26.13 grad norm: 15.86 time: 0.037
2025-12-15 21:24:34,782: Train batch 61400: loss: 22.35 grad norm: 29.48 time: 0.078
2025-12-15 21:24:49,442: Train batch 61600: loss: 25.37 grad norm: 19.59 time: 0.070
2025-12-15 21:25:03,568: Train batch 61800: loss: 17.44 grad norm: 3.67 time: 0.061
2025-12-15 21:25:17,551: Train batch 62000: loss: 22.68 grad norm: 21.58 time: 0.068
2025-12-15 21:25:17,551: Running test after training batch: 62000
2025-12-15 21:25:26,626: Val batch 62000: PER (avg): 0.1344 CTC Loss (avg): 35.0780 time: 9.075
2025-12-15 21:25:26,627: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:25:26,627: t15.2023.08.13 val PER: 0.0967
2025-12-15 21:25:26,627: t15.2023.08.18 val PER: 0.0914
2025-12-15 21:25:26,627: t15.2023.08.20 val PER: 0.0731
2025-12-15 21:25:26,627: t15.2023.08.25 val PER: 0.1069
2025-12-15 21:25:26,627: t15.2023.08.27 val PER: 0.1656
2025-12-15 21:25:26,627: t15.2023.09.01 val PER: 0.0576
2025-12-15 21:25:26,627: t15.2023.09.03 val PER: 0.1342
2025-12-15 21:25:26,627: t15.2023.09.24 val PER: 0.0947
2025-12-15 21:25:26,627: t15.2023.09.29 val PER: 0.1257
2025-12-15 21:25:26,627: t15.2023.10.01 val PER: 0.1605
2025-12-15 21:25:26,627: t15.2023.10.06 val PER: 0.1033
2025-12-15 21:25:26,627: t15.2023.10.08 val PER: 0.1935
2025-12-15 21:25:26,627: t15.2023.10.13 val PER: 0.1955
2025-12-15 21:25:26,627: t15.2023.10.15 val PER: 0.1457
2025-12-15 21:25:26,628: t15.2023.10.20 val PER: 0.2181
2025-12-15 21:25:26,628: t15.2023.10.22 val PER: 0.1214
2025-12-15 21:25:26,628: t15.2023.11.03 val PER: 0.1798
2025-12-15 21:25:26,628: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:25:26,628: t15.2023.11.17 val PER: 0.0529
2025-12-15 21:25:26,628: t15.2023.11.19 val PER: 0.0519
2025-12-15 21:25:26,628: t15.2023.11.26 val PER: 0.1130
2025-12-15 21:25:26,628: t15.2023.12.03 val PER: 0.1061
2025-12-15 21:25:26,628: t15.2023.12.08 val PER: 0.0779
2025-12-15 21:25:26,628: t15.2023.12.10 val PER: 0.0631
2025-12-15 21:25:26,628: t15.2023.12.17 val PER: 0.1310
2025-12-15 21:25:26,628: t15.2023.12.29 val PER: 0.1078
2025-12-15 21:25:26,628: t15.2024.02.25 val PER: 0.0969
2025-12-15 21:25:26,628: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:25:26,628: t15.2024.03.08 val PER: 0.2006
2025-12-15 21:25:26,628: t15.2024.03.15 val PER: 0.1901
2025-12-15 21:25:26,628: t15.2024.03.17 val PER: 0.1095
2025-12-15 21:25:26,628: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:25:26,628: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:25:26,628: t15.2024.05.10 val PER: 0.1605
2025-12-15 21:25:26,629: t15.2024.06.14 val PER: 0.1593
2025-12-15 21:25:26,629: t15.2024.07.19 val PER: 0.1826
2025-12-15 21:25:26,629: t15.2024.07.21 val PER: 0.0869
2025-12-15 21:25:26,629: t15.2024.07.28 val PER: 0.1118
2025-12-15 21:25:26,629: t15.2025.01.10 val PER: 0.3085
2025-12-15 21:25:26,629: t15.2025.01.12 val PER: 0.1085
2025-12-15 21:25:26,629: t15.2025.03.14 val PER: 0.2988
2025-12-15 21:25:26,629: t15.2025.03.16 val PER: 0.1754
2025-12-15 21:25:26,629: t15.2025.03.30 val PER: 0.2103
2025-12-15 21:25:26,629: t15.2025.04.13 val PER: 0.2168
2025-12-15 21:25:26,629: New best test PER 0.1357 --> 0.1344
2025-12-15 21:25:26,629: Checkpointing model
2025-12-15 21:25:27,049: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:25:40,159: Train batch 62200: loss: 17.91 grad norm: 13.96 time: 0.059
2025-12-15 21:25:54,040: Train batch 62400: loss: 20.95 grad norm: 1.21 time: 0.058
2025-12-15 21:26:08,515: Train batch 62600: loss: 21.47 grad norm: 17.12 time: 0.073
2025-12-15 21:26:22,725: Train batch 62800: loss: 29.29 grad norm: 17.80 time: 0.054
2025-12-15 21:26:37,418: Train batch 63000: loss: 18.59 grad norm: 22.85 time: 0.064
2025-12-15 21:26:51,018: Train batch 63200: loss: 21.51 grad norm: 7.80 time: 0.039
2025-12-15 21:27:04,863: Train batch 63400: loss: 23.89 grad norm: 12.98 time: 0.054
2025-12-15 21:27:18,572: Train batch 63600: loss: 19.54 grad norm: 25.40 time: 0.048
2025-12-15 21:27:32,809: Train batch 63800: loss: 24.26 grad norm: 26.02 time: 0.069
2025-12-15 21:27:47,104: Train batch 64000: loss: 22.79 grad norm: 2.44 time: 0.068
2025-12-15 21:27:47,105: Running test after training batch: 64000
2025-12-15 21:27:56,242: Val batch 64000: PER (avg): 0.1340 CTC Loss (avg): 35.3437 time: 9.136
2025-12-15 21:27:56,242: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:27:56,242: t15.2023.08.13 val PER: 0.0967
2025-12-15 21:27:56,242: t15.2023.08.18 val PER: 0.0972
2025-12-15 21:27:56,242: t15.2023.08.20 val PER: 0.0770
2025-12-15 21:27:56,242: t15.2023.08.25 val PER: 0.0889
2025-12-15 21:27:56,242: t15.2023.08.27 val PER: 0.1527
2025-12-15 21:27:56,243: t15.2023.09.01 val PER: 0.0649
2025-12-15 21:27:56,243: t15.2023.09.03 val PER: 0.1200
2025-12-15 21:27:56,243: t15.2023.09.24 val PER: 0.1056
2025-12-15 21:27:56,243: t15.2023.09.29 val PER: 0.1219
2025-12-15 21:27:56,243: t15.2023.10.01 val PER: 0.1625
2025-12-15 21:27:56,243: t15.2023.10.06 val PER: 0.1184
2025-12-15 21:27:56,243: t15.2023.10.08 val PER: 0.1773
2025-12-15 21:27:56,243: t15.2023.10.13 val PER: 0.1808
2025-12-15 21:27:56,243: t15.2023.10.15 val PER: 0.1470
2025-12-15 21:27:56,243: t15.2023.10.20 val PER: 0.2013
2025-12-15 21:27:56,243: t15.2023.10.22 val PER: 0.1281
2025-12-15 21:27:56,243: t15.2023.11.03 val PER: 0.1879
2025-12-15 21:27:56,243: t15.2023.11.04 val PER: 0.0341
2025-12-15 21:27:56,243: t15.2023.11.17 val PER: 0.0482
2025-12-15 21:27:56,243: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:27:56,243: t15.2023.11.26 val PER: 0.1080
2025-12-15 21:27:56,243: t15.2023.12.03 val PER: 0.0830
2025-12-15 21:27:56,243: t15.2023.12.08 val PER: 0.0799
2025-12-15 21:27:56,243: t15.2023.12.10 val PER: 0.0604
2025-12-15 21:27:56,244: t15.2023.12.17 val PER: 0.1216
2025-12-15 21:27:56,244: t15.2023.12.29 val PER: 0.0940
2025-12-15 21:27:56,244: t15.2024.02.25 val PER: 0.0927
2025-12-15 21:27:56,244: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:27:56,244: t15.2024.03.08 val PER: 0.1835
2025-12-15 21:27:56,244: t15.2024.03.15 val PER: 0.1939
2025-12-15 21:27:56,244: t15.2024.03.17 val PER: 0.1039
2025-12-15 21:27:56,244: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:27:56,244: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:27:56,244: t15.2024.05.10 val PER: 0.1679
2025-12-15 21:27:56,244: t15.2024.06.14 val PER: 0.1640
2025-12-15 21:27:56,244: t15.2024.07.19 val PER: 0.1839
2025-12-15 21:27:56,244: t15.2024.07.21 val PER: 0.0821
2025-12-15 21:27:56,244: t15.2024.07.28 val PER: 0.1213
2025-12-15 21:27:56,244: t15.2025.01.10 val PER: 0.2975
2025-12-15 21:27:56,244: t15.2025.01.12 val PER: 0.1155
2025-12-15 21:27:56,244: t15.2025.03.14 val PER: 0.3151
2025-12-15 21:27:56,244: t15.2025.03.16 val PER: 0.1832
2025-12-15 21:27:56,244: t15.2025.03.30 val PER: 0.2356
2025-12-15 21:27:56,245: t15.2025.04.13 val PER: 0.2240
2025-12-15 21:27:56,245: New best test PER 0.1344 --> 0.1340
2025-12-15 21:27:56,245: Checkpointing model
2025-12-15 21:27:56,746: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:28:11,297: Train batch 64200: loss: 26.71 grad norm: 21.14 time: 0.085
2025-12-15 21:28:25,547: Train batch 64400: loss: 24.69 grad norm: 15.77 time: 0.066
2025-12-15 21:28:39,413: Train batch 64600: loss: 25.98 grad norm: 2.75 time: 0.056
2025-12-15 21:28:52,262: Train batch 64800: loss: 19.52 grad norm: 17.79 time: 0.068
2025-12-15 21:29:03,588: Train batch 65000: loss: 22.59 grad norm: 13.39 time: 0.040
2025-12-15 21:29:16,063: Train batch 65200: loss: 20.79 grad norm: 17.30 time: 0.061
2025-12-15 21:29:28,902: Train batch 65400: loss: 21.11 grad norm: 24.28 time: 0.068
2025-12-15 21:29:43,072: Train batch 65600: loss: 20.96 grad norm: 5.81 time: 0.061
2025-12-15 21:29:56,955: Train batch 65800: loss: 19.92 grad norm: 2.12 time: 0.066
2025-12-15 21:30:09,633: Train batch 66000: loss: 24.29 grad norm: 11.12 time: 0.062
2025-12-15 21:30:09,633: Running test after training batch: 66000
2025-12-15 21:30:19,387: Val batch 66000: PER (avg): 0.1329 CTC Loss (avg): 35.6974 time: 9.753
2025-12-15 21:30:19,387: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:30:19,387: t15.2023.08.13 val PER: 0.1019
2025-12-15 21:30:19,387: t15.2023.08.18 val PER: 0.0872
2025-12-15 21:30:19,388: t15.2023.08.20 val PER: 0.0723
2025-12-15 21:30:19,388: t15.2023.08.25 val PER: 0.1084
2025-12-15 21:30:19,388: t15.2023.08.27 val PER: 0.1415
2025-12-15 21:30:19,388: t15.2023.09.01 val PER: 0.0649
2025-12-15 21:30:19,388: t15.2023.09.03 val PER: 0.1235
2025-12-15 21:30:19,388: t15.2023.09.24 val PER: 0.0959
2025-12-15 21:30:19,388: t15.2023.09.29 val PER: 0.1276
2025-12-15 21:30:19,388: t15.2023.10.01 val PER: 0.1645
2025-12-15 21:30:19,388: t15.2023.10.06 val PER: 0.1012
2025-12-15 21:30:19,388: t15.2023.10.08 val PER: 0.1786
2025-12-15 21:30:19,388: t15.2023.10.13 val PER: 0.1924
2025-12-15 21:30:19,388: t15.2023.10.15 val PER: 0.1404
2025-12-15 21:30:19,388: t15.2023.10.20 val PER: 0.2148
2025-12-15 21:30:19,388: t15.2023.10.22 val PER: 0.1180
2025-12-15 21:30:19,388: t15.2023.11.03 val PER: 0.1839
2025-12-15 21:30:19,388: t15.2023.11.04 val PER: 0.0341
2025-12-15 21:30:19,388: t15.2023.11.17 val PER: 0.0467
2025-12-15 21:30:19,388: t15.2023.11.19 val PER: 0.0639
2025-12-15 21:30:19,389: t15.2023.11.26 val PER: 0.1000
2025-12-15 21:30:19,389: t15.2023.12.03 val PER: 0.0903
2025-12-15 21:30:19,389: t15.2023.12.08 val PER: 0.0792
2025-12-15 21:30:19,389: t15.2023.12.10 val PER: 0.0591
2025-12-15 21:30:19,389: t15.2023.12.17 val PER: 0.1247
2025-12-15 21:30:19,389: t15.2023.12.29 val PER: 0.1112
2025-12-15 21:30:19,389: t15.2024.02.25 val PER: 0.0772
2025-12-15 21:30:19,389: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:30:19,389: t15.2024.03.08 val PER: 0.1778
2025-12-15 21:30:19,389: t15.2024.03.15 val PER: 0.1839
2025-12-15 21:30:19,389: t15.2024.03.17 val PER: 0.0990
2025-12-15 21:30:19,389: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:30:19,389: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:30:19,389: t15.2024.05.10 val PER: 0.1634
2025-12-15 21:30:19,389: t15.2024.06.14 val PER: 0.1546
2025-12-15 21:30:19,389: t15.2024.07.19 val PER: 0.1859
2025-12-15 21:30:19,389: t15.2024.07.21 val PER: 0.0793
2025-12-15 21:30:19,389: t15.2024.07.28 val PER: 0.1243
2025-12-15 21:30:19,389: t15.2025.01.10 val PER: 0.2975
2025-12-15 21:30:19,390: t15.2025.01.12 val PER: 0.1201
2025-12-15 21:30:19,390: t15.2025.03.14 val PER: 0.3077
2025-12-15 21:30:19,390: t15.2025.03.16 val PER: 0.1780
2025-12-15 21:30:19,390: t15.2025.03.30 val PER: 0.2402
2025-12-15 21:30:19,390: t15.2025.04.13 val PER: 0.2140
2025-12-15 21:30:19,390: New best test PER 0.1340 --> 0.1329
2025-12-15 21:30:19,390: Checkpointing model
2025-12-15 21:30:19,837: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:30:34,317: Train batch 66200: loss: 17.04 grad norm: 0.33 time: 0.060
2025-12-15 21:30:48,305: Train batch 66400: loss: 17.74 grad norm: 7.35 time: 0.071
2025-12-15 21:31:02,203: Train batch 66600: loss: 19.97 grad norm: 8.21 time: 0.054
2025-12-15 21:31:16,749: Train batch 66800: loss: 23.36 grad norm: 9.14 time: 0.057
2025-12-15 21:31:30,410: Train batch 67000: loss: 23.74 grad norm: 16.10 time: 0.057
2025-12-15 21:31:45,489: Train batch 67200: loss: 19.63 grad norm: 9.01 time: 0.069
2025-12-15 21:31:58,599: Train batch 67400: loss: 22.94 grad norm: 13.09 time: 0.043
2025-12-15 21:32:09,706: Train batch 67600: loss: 24.23 grad norm: 3.20 time: 0.055
2025-12-15 21:32:21,827: Train batch 67800: loss: 21.92 grad norm: 0.84 time: 0.054
2025-12-15 21:32:36,374: Train batch 68000: loss: 20.46 grad norm: 2.79 time: 0.074
2025-12-15 21:32:36,375: Running test after training batch: 68000
2025-12-15 21:32:45,619: Val batch 68000: PER (avg): 0.1332 CTC Loss (avg): 36.9335 time: 9.245
2025-12-15 21:32:45,620: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:32:45,620: t15.2023.08.13 val PER: 0.1071
2025-12-15 21:32:45,620: t15.2023.08.18 val PER: 0.0905
2025-12-15 21:32:45,620: t15.2023.08.20 val PER: 0.0747
2025-12-15 21:32:45,620: t15.2023.08.25 val PER: 0.0934
2025-12-15 21:32:45,620: t15.2023.08.27 val PER: 0.1447
2025-12-15 21:32:45,620: t15.2023.09.01 val PER: 0.0625
2025-12-15 21:32:45,620: t15.2023.09.03 val PER: 0.1247
2025-12-15 21:32:45,620: t15.2023.09.24 val PER: 0.1092
2025-12-15 21:32:45,620: t15.2023.09.29 val PER: 0.1181
2025-12-15 21:32:45,620: t15.2023.10.01 val PER: 0.1697
2025-12-15 21:32:45,620: t15.2023.10.06 val PER: 0.0969
2025-12-15 21:32:45,620: t15.2023.10.08 val PER: 0.1691
2025-12-15 21:32:45,620: t15.2023.10.13 val PER: 0.1932
2025-12-15 21:32:45,620: t15.2023.10.15 val PER: 0.1503
2025-12-15 21:32:45,620: t15.2023.10.20 val PER: 0.2114
2025-12-15 21:32:45,620: t15.2023.10.22 val PER: 0.1281
2025-12-15 21:32:45,621: t15.2023.11.03 val PER: 0.1839
2025-12-15 21:32:45,621: t15.2023.11.04 val PER: 0.0410
2025-12-15 21:32:45,621: t15.2023.11.17 val PER: 0.0451
2025-12-15 21:32:45,621: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:32:45,621: t15.2023.11.26 val PER: 0.1007
2025-12-15 21:32:45,621: t15.2023.12.03 val PER: 0.0882
2025-12-15 21:32:45,621: t15.2023.12.08 val PER: 0.0812
2025-12-15 21:32:45,621: t15.2023.12.10 val PER: 0.0736
2025-12-15 21:32:45,621: t15.2023.12.17 val PER: 0.1289
2025-12-15 21:32:45,621: t15.2023.12.29 val PER: 0.1078
2025-12-15 21:32:45,621: t15.2024.02.25 val PER: 0.0744
2025-12-15 21:32:45,621: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:32:45,621: t15.2024.03.08 val PER: 0.1892
2025-12-15 21:32:45,621: t15.2024.03.15 val PER: 0.1839
2025-12-15 21:32:45,621: t15.2024.03.17 val PER: 0.0914
2025-12-15 21:32:45,621: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:32:45,621: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:32:45,621: t15.2024.05.10 val PER: 0.1679
2025-12-15 21:32:45,621: t15.2024.06.14 val PER: 0.1625
2025-12-15 21:32:45,622: t15.2024.07.19 val PER: 0.1918
2025-12-15 21:32:45,622: t15.2024.07.21 val PER: 0.0807
2025-12-15 21:32:45,622: t15.2024.07.28 val PER: 0.0985
2025-12-15 21:32:45,622: t15.2025.01.10 val PER: 0.3113
2025-12-15 21:32:45,622: t15.2025.01.12 val PER: 0.1309
2025-12-15 21:32:45,622: t15.2025.03.14 val PER: 0.2944
2025-12-15 21:32:45,622: t15.2025.03.16 val PER: 0.1885
2025-12-15 21:32:45,622: t15.2025.03.30 val PER: 0.2253
2025-12-15 21:32:45,622: t15.2025.04.13 val PER: 0.2083
2025-12-15 21:32:58,157: Train batch 68200: loss: 21.14 grad norm: 5.03 time: 0.068
2025-12-15 21:33:12,383: Train batch 68400: loss: 20.38 grad norm: 58.44 time: 0.065
2025-12-15 21:33:26,516: Train batch 68600: loss: 21.55 grad norm: 14.92 time: 0.057
2025-12-15 21:33:40,837: Train batch 68800: loss: 27.11 grad norm: 12.99 time: 0.071
2025-12-15 21:33:55,294: Train batch 69000: loss: 16.66 grad norm: 13.19 time: 0.067
2025-12-15 21:34:10,230: Train batch 69200: loss: 26.19 grad norm: 23.35 time: 0.060
2025-12-15 21:34:24,576: Train batch 69400: loss: 19.01 grad norm: 13.96 time: 0.069
2025-12-15 21:34:39,611: Train batch 69600: loss: 25.36 grad norm: 25.02 time: 0.054
2025-12-15 21:34:54,730: Train batch 69800: loss: 18.91 grad norm: 25.47 time: 0.084
2025-12-15 21:35:09,606: Train batch 70000: loss: 25.82 grad norm: 16.78 time: 0.035
2025-12-15 21:35:09,606: Running test after training batch: 70000
2025-12-15 21:35:18,925: Val batch 70000: PER (avg): 0.1309 CTC Loss (avg): 37.3268 time: 9.319
2025-12-15 21:35:18,926: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:35:18,926: t15.2023.08.13 val PER: 0.0967
2025-12-15 21:35:18,926: t15.2023.08.18 val PER: 0.0930
2025-12-15 21:35:18,926: t15.2023.08.20 val PER: 0.0905
2025-12-15 21:35:18,926: t15.2023.08.25 val PER: 0.1024
2025-12-15 21:35:18,926: t15.2023.08.27 val PER: 0.1511
2025-12-15 21:35:18,926: t15.2023.09.01 val PER: 0.0544
2025-12-15 21:35:18,926: t15.2023.09.03 val PER: 0.1223
2025-12-15 21:35:18,926: t15.2023.09.24 val PER: 0.0934
2025-12-15 21:35:18,926: t15.2023.09.29 val PER: 0.1187
2025-12-15 21:35:18,926: t15.2023.10.01 val PER: 0.1480
2025-12-15 21:35:18,926: t15.2023.10.06 val PER: 0.1023
2025-12-15 21:35:18,927: t15.2023.10.08 val PER: 0.1840
2025-12-15 21:35:18,927: t15.2023.10.13 val PER: 0.1839
2025-12-15 21:35:18,927: t15.2023.10.15 val PER: 0.1510
2025-12-15 21:35:18,927: t15.2023.10.20 val PER: 0.1812
2025-12-15 21:35:18,927: t15.2023.10.22 val PER: 0.1225
2025-12-15 21:35:18,927: t15.2023.11.03 val PER: 0.1852
2025-12-15 21:35:18,927: t15.2023.11.04 val PER: 0.0171
2025-12-15 21:35:18,927: t15.2023.11.17 val PER: 0.0420
2025-12-15 21:35:18,927: t15.2023.11.19 val PER: 0.0739
2025-12-15 21:35:18,927: t15.2023.11.26 val PER: 0.0978
2025-12-15 21:35:18,927: t15.2023.12.03 val PER: 0.0998
2025-12-15 21:35:18,927: t15.2023.12.08 val PER: 0.0699
2025-12-15 21:35:18,927: t15.2023.12.10 val PER: 0.0604
2025-12-15 21:35:18,927: t15.2023.12.17 val PER: 0.1112
2025-12-15 21:35:18,927: t15.2023.12.29 val PER: 0.1009
2025-12-15 21:35:18,927: t15.2024.02.25 val PER: 0.0857
2025-12-15 21:35:18,927: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:35:18,927: t15.2024.03.08 val PER: 0.1750
2025-12-15 21:35:18,927: t15.2024.03.15 val PER: 0.1845
2025-12-15 21:35:18,928: t15.2024.03.17 val PER: 0.0872
2025-12-15 21:35:18,928: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:35:18,928: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:35:18,928: t15.2024.05.10 val PER: 0.1545
2025-12-15 21:35:18,928: t15.2024.06.14 val PER: 0.1514
2025-12-15 21:35:18,928: t15.2024.07.19 val PER: 0.1760
2025-12-15 21:35:18,928: t15.2024.07.21 val PER: 0.0821
2025-12-15 21:35:18,928: t15.2024.07.28 val PER: 0.1162
2025-12-15 21:35:18,928: t15.2025.01.10 val PER: 0.3085
2025-12-15 21:35:18,928: t15.2025.01.12 val PER: 0.1339
2025-12-15 21:35:18,928: t15.2025.03.14 val PER: 0.2870
2025-12-15 21:35:18,928: t15.2025.03.16 val PER: 0.1780
2025-12-15 21:35:18,928: t15.2025.03.30 val PER: 0.2299
2025-12-15 21:35:18,928: t15.2025.04.13 val PER: 0.2397
2025-12-15 21:35:18,928: New best test PER 0.1329 --> 0.1309
2025-12-15 21:35:18,928: Checkpointing model
2025-12-15 21:35:19,369: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:35:33,735: Train batch 70200: loss: 18.99 grad norm: 9.05 time: 0.070
2025-12-15 21:35:47,599: Train batch 70400: loss: 25.18 grad norm: 23.42 time: 0.076
2025-12-15 21:36:02,185: Train batch 70600: loss: 27.80 grad norm: 7.11 time: 0.063
2025-12-15 21:36:16,185: Train batch 70800: loss: 26.47 grad norm: 4.98 time: 0.070
2025-12-15 21:36:30,491: Train batch 71000: loss: 26.06 grad norm: 23.74 time: 0.049
2025-12-15 21:36:44,978: Train batch 71200: loss: 27.25 grad norm: 17.39 time: 0.038
2025-12-15 21:36:56,089: Train batch 71400: loss: 22.75 grad norm: 0.53 time: 0.036
2025-12-15 21:37:09,164: Train batch 71600: loss: 22.63 grad norm: 0.31 time: 0.075
2025-12-15 21:37:23,657: Train batch 71800: loss: 22.34 grad norm: 18.31 time: 0.067
2025-12-15 21:37:38,002: Train batch 72000: loss: 18.08 grad norm: 20.89 time: 0.052
2025-12-15 21:37:38,002: Running test after training batch: 72000
2025-12-15 21:37:47,701: Val batch 72000: PER (avg): 0.1289 CTC Loss (avg): 36.6659 time: 9.699
2025-12-15 21:37:47,702: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:37:47,702: t15.2023.08.13 val PER: 0.0998
2025-12-15 21:37:47,702: t15.2023.08.18 val PER: 0.0788
2025-12-15 21:37:47,702: t15.2023.08.20 val PER: 0.0802
2025-12-15 21:37:47,702: t15.2023.08.25 val PER: 0.0964
2025-12-15 21:37:47,702: t15.2023.08.27 val PER: 0.1431
2025-12-15 21:37:47,702: t15.2023.09.01 val PER: 0.0625
2025-12-15 21:37:47,702: t15.2023.09.03 val PER: 0.1140
2025-12-15 21:37:47,702: t15.2023.09.24 val PER: 0.1007
2025-12-15 21:37:47,702: t15.2023.09.29 val PER: 0.1161
2025-12-15 21:37:47,702: t15.2023.10.01 val PER: 0.1532
2025-12-15 21:37:47,702: t15.2023.10.06 val PER: 0.1012
2025-12-15 21:37:47,702: t15.2023.10.08 val PER: 0.1894
2025-12-15 21:37:47,702: t15.2023.10.13 val PER: 0.1730
2025-12-15 21:37:47,702: t15.2023.10.15 val PER: 0.1365
2025-12-15 21:37:47,703: t15.2023.10.20 val PER: 0.2013
2025-12-15 21:37:47,703: t15.2023.10.22 val PER: 0.1281
2025-12-15 21:37:47,703: t15.2023.11.03 val PER: 0.1777
2025-12-15 21:37:47,703: t15.2023.11.04 val PER: 0.0171
2025-12-15 21:37:47,703: t15.2023.11.17 val PER: 0.0389
2025-12-15 21:37:47,703: t15.2023.11.19 val PER: 0.0559
2025-12-15 21:37:47,703: t15.2023.11.26 val PER: 0.0855
2025-12-15 21:37:47,703: t15.2023.12.03 val PER: 0.0851
2025-12-15 21:37:47,703: t15.2023.12.08 val PER: 0.0772
2025-12-15 21:37:47,703: t15.2023.12.10 val PER: 0.0447
2025-12-15 21:37:47,703: t15.2023.12.17 val PER: 0.1143
2025-12-15 21:37:47,703: t15.2023.12.29 val PER: 0.1009
2025-12-15 21:37:47,703: t15.2024.02.25 val PER: 0.0787
2025-12-15 21:37:47,703: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:37:47,703: t15.2024.03.08 val PER: 0.1878
2025-12-15 21:37:47,703: t15.2024.03.15 val PER: 0.1707
2025-12-15 21:37:47,703: t15.2024.03.17 val PER: 0.0962
2025-12-15 21:37:47,703: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:37:47,704: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:37:47,704: t15.2024.05.10 val PER: 0.1501
2025-12-15 21:37:47,704: t15.2024.06.14 val PER: 0.1640
2025-12-15 21:37:47,704: t15.2024.07.19 val PER: 0.1866
2025-12-15 21:37:47,704: t15.2024.07.21 val PER: 0.0841
2025-12-15 21:37:47,704: t15.2024.07.28 val PER: 0.1029
2025-12-15 21:37:47,704: t15.2025.01.10 val PER: 0.2989
2025-12-15 21:37:47,704: t15.2025.01.12 val PER: 0.1339
2025-12-15 21:37:47,704: t15.2025.03.14 val PER: 0.2973
2025-12-15 21:37:47,704: t15.2025.03.16 val PER: 0.1832
2025-12-15 21:37:47,704: t15.2025.03.30 val PER: 0.2345
2025-12-15 21:37:47,704: t15.2025.04.13 val PER: 0.2468
2025-12-15 21:37:47,704: New best test PER 0.1309 --> 0.1289
2025-12-15 21:37:47,704: Checkpointing model
2025-12-15 21:37:48,142: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:38:02,708: Train batch 72200: loss: 27.19 grad norm: 3.44 time: 0.056
2025-12-15 21:38:16,278: Train batch 72400: loss: 19.73 grad norm: 27.60 time: 0.043
2025-12-15 21:38:29,596: Train batch 72600: loss: 20.50 grad norm: 20.77 time: 0.044
2025-12-15 21:38:40,121: Train batch 72800: loss: 18.95 grad norm: 8.48 time: 0.053
2025-12-15 21:38:50,765: Train batch 73000: loss: 22.91 grad norm: 15.74 time: 0.054
2025-12-15 21:39:00,684: Train batch 73200: loss: 22.95 grad norm: 2.45 time: 0.063
2025-12-15 21:39:14,116: Train batch 73400: loss: 25.48 grad norm: 12.88 time: 0.089
2025-12-15 21:39:27,349: Train batch 73600: loss: 26.21 grad norm: 8.18 time: 0.072
2025-12-15 21:39:41,272: Train batch 73800: loss: 21.22 grad norm: 12.36 time: 0.043
2025-12-15 21:39:55,230: Train batch 74000: loss: 21.86 grad norm: 1.13 time: 0.056
2025-12-15 21:39:55,231: Running test after training batch: 74000
2025-12-15 21:40:04,478: Val batch 74000: PER (avg): 0.1309 CTC Loss (avg): 38.8131 time: 9.247
2025-12-15 21:40:04,478: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:40:04,478: t15.2023.08.13 val PER: 0.0988
2025-12-15 21:40:04,478: t15.2023.08.18 val PER: 0.0905
2025-12-15 21:40:04,478: t15.2023.08.20 val PER: 0.0818
2025-12-15 21:40:04,478: t15.2023.08.25 val PER: 0.0858
2025-12-15 21:40:04,478: t15.2023.08.27 val PER: 0.1431
2025-12-15 21:40:04,479: t15.2023.09.01 val PER: 0.0633
2025-12-15 21:40:04,479: t15.2023.09.03 val PER: 0.1211
2025-12-15 21:40:04,479: t15.2023.09.24 val PER: 0.0971
2025-12-15 21:40:04,479: t15.2023.09.29 val PER: 0.1232
2025-12-15 21:40:04,479: t15.2023.10.01 val PER: 0.1579
2025-12-15 21:40:04,479: t15.2023.10.06 val PER: 0.0947
2025-12-15 21:40:04,479: t15.2023.10.08 val PER: 0.1813
2025-12-15 21:40:04,479: t15.2023.10.13 val PER: 0.1916
2025-12-15 21:40:04,479: t15.2023.10.15 val PER: 0.1285
2025-12-15 21:40:04,479: t15.2023.10.20 val PER: 0.1946
2025-12-15 21:40:04,479: t15.2023.10.22 val PER: 0.1180
2025-12-15 21:40:04,479: t15.2023.11.03 val PER: 0.1744
2025-12-15 21:40:04,479: t15.2023.11.04 val PER: 0.0137
2025-12-15 21:40:04,479: t15.2023.11.17 val PER: 0.0404
2025-12-15 21:40:04,479: t15.2023.11.19 val PER: 0.0679
2025-12-15 21:40:04,479: t15.2023.11.26 val PER: 0.0819
2025-12-15 21:40:04,479: t15.2023.12.03 val PER: 0.0893
2025-12-15 21:40:04,479: t15.2023.12.08 val PER: 0.0732
2025-12-15 21:40:04,480: t15.2023.12.10 val PER: 0.0631
2025-12-15 21:40:04,480: t15.2023.12.17 val PER: 0.1216
2025-12-15 21:40:04,480: t15.2023.12.29 val PER: 0.0988
2025-12-15 21:40:04,480: t15.2024.02.25 val PER: 0.0744
2025-12-15 21:40:04,480: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:40:04,480: t15.2024.03.08 val PER: 0.1935
2025-12-15 21:40:04,480: t15.2024.03.15 val PER: 0.1801
2025-12-15 21:40:04,480: t15.2024.03.17 val PER: 0.1067
2025-12-15 21:40:04,480: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:40:04,480: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:40:04,480: t15.2024.05.10 val PER: 0.1605
2025-12-15 21:40:04,480: t15.2024.06.14 val PER: 0.1483
2025-12-15 21:40:04,480: t15.2024.07.19 val PER: 0.1912
2025-12-15 21:40:04,480: t15.2024.07.21 val PER: 0.0793
2025-12-15 21:40:04,480: t15.2024.07.28 val PER: 0.1162
2025-12-15 21:40:04,480: t15.2025.01.10 val PER: 0.3072
2025-12-15 21:40:04,480: t15.2025.01.12 val PER: 0.1370
2025-12-15 21:40:04,480: t15.2025.03.14 val PER: 0.3062
2025-12-15 21:40:04,480: t15.2025.03.16 val PER: 0.1832
2025-12-15 21:40:04,480: t15.2025.03.30 val PER: 0.2402
2025-12-15 21:40:04,481: t15.2025.04.13 val PER: 0.2254
2025-12-15 21:40:18,788: Train batch 74200: loss: 22.62 grad norm: 21.67 time: 0.083
2025-12-15 21:40:32,763: Train batch 74400: loss: 21.49 grad norm: 2.59 time: 0.058
2025-12-15 21:40:46,884: Train batch 74600: loss: 20.39 grad norm: 11.30 time: 0.090
2025-12-15 21:41:01,267: Train batch 74800: loss: 20.60 grad norm: 9.51 time: 0.058
2025-12-15 21:41:15,461: Train batch 75000: loss: 18.86 grad norm: 8.56 time: 0.063
2025-12-15 21:41:29,979: Train batch 75200: loss: 20.14 grad norm: 13.55 time: 0.066
2025-12-15 21:41:44,151: Train batch 75400: loss: 24.60 grad norm: 15.46 time: 0.059
2025-12-15 21:41:58,365: Train batch 75600: loss: 18.52 grad norm: 5.25 time: 0.066
2025-12-15 21:42:11,970: Train batch 75800: loss: 25.61 grad norm: 17.65 time: 0.058
2025-12-15 21:42:25,988: Train batch 76000: loss: 21.41 grad norm: 6.74 time: 0.064
2025-12-15 21:42:25,989: Running test after training batch: 76000
2025-12-15 21:42:35,169: Val batch 76000: PER (avg): 0.1287 CTC Loss (avg): 38.7133 time: 9.180
2025-12-15 21:42:35,169: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:42:35,169: t15.2023.08.13 val PER: 0.0988
2025-12-15 21:42:35,169: t15.2023.08.18 val PER: 0.0914
2025-12-15 21:42:35,169: t15.2023.08.20 val PER: 0.0770
2025-12-15 21:42:35,169: t15.2023.08.25 val PER: 0.0904
2025-12-15 21:42:35,169: t15.2023.08.27 val PER: 0.1543
2025-12-15 21:42:35,169: t15.2023.09.01 val PER: 0.0576
2025-12-15 21:42:35,169: t15.2023.09.03 val PER: 0.1211
2025-12-15 21:42:35,169: t15.2023.09.24 val PER: 0.0874
2025-12-15 21:42:35,169: t15.2023.09.29 val PER: 0.1270
2025-12-15 21:42:35,169: t15.2023.10.01 val PER: 0.1565
2025-12-15 21:42:35,169: t15.2023.10.06 val PER: 0.1023
2025-12-15 21:42:35,169: t15.2023.10.08 val PER: 0.1922
2025-12-15 21:42:35,169: t15.2023.10.13 val PER: 0.1784
2025-12-15 21:42:35,170: t15.2023.10.15 val PER: 0.1371
2025-12-15 21:42:35,170: t15.2023.10.20 val PER: 0.1812
2025-12-15 21:42:35,170: t15.2023.10.22 val PER: 0.1147
2025-12-15 21:42:35,170: t15.2023.11.03 val PER: 0.1750
2025-12-15 21:42:35,170: t15.2023.11.04 val PER: 0.0239
2025-12-15 21:42:35,170: t15.2023.11.17 val PER: 0.0342
2025-12-15 21:42:35,170: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:42:35,170: t15.2023.11.26 val PER: 0.0942
2025-12-15 21:42:35,170: t15.2023.12.03 val PER: 0.0861
2025-12-15 21:42:35,170: t15.2023.12.08 val PER: 0.0732
2025-12-15 21:42:35,170: t15.2023.12.10 val PER: 0.0552
2025-12-15 21:42:35,170: t15.2023.12.17 val PER: 0.1050
2025-12-15 21:42:35,170: t15.2023.12.29 val PER: 0.1009
2025-12-15 21:42:35,170: t15.2024.02.25 val PER: 0.0702
2025-12-15 21:42:35,170: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:42:35,170: t15.2024.03.08 val PER: 0.1906
2025-12-15 21:42:35,170: t15.2024.03.15 val PER: 0.1789
2025-12-15 21:42:35,170: t15.2024.03.17 val PER: 0.0921
2025-12-15 21:42:35,170: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:42:35,171: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:42:35,171: t15.2024.05.10 val PER: 0.1649
2025-12-15 21:42:35,171: t15.2024.06.14 val PER: 0.1593
2025-12-15 21:42:35,171: t15.2024.07.19 val PER: 0.1898
2025-12-15 21:42:35,171: t15.2024.07.21 val PER: 0.0793
2025-12-15 21:42:35,171: t15.2024.07.28 val PER: 0.1000
2025-12-15 21:42:35,171: t15.2025.01.10 val PER: 0.3085
2025-12-15 21:42:35,171: t15.2025.01.12 val PER: 0.1201
2025-12-15 21:42:35,171: t15.2025.03.14 val PER: 0.2796
2025-12-15 21:42:35,171: t15.2025.03.16 val PER: 0.1846
2025-12-15 21:42:35,171: t15.2025.03.30 val PER: 0.2402
2025-12-15 21:42:35,171: t15.2025.04.13 val PER: 0.2225
2025-12-15 21:42:35,171: New best test PER 0.1289 --> 0.1287
2025-12-15 21:42:35,171: Checkpointing model
2025-12-15 21:42:35,606: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:42:48,762: Train batch 76200: loss: 23.15 grad norm: 5.50 time: 0.062
2025-12-15 21:43:02,951: Train batch 76400: loss: 21.49 grad norm: 20.13 time: 0.045
2025-12-15 21:43:16,228: Train batch 76600: loss: 20.61 grad norm: 8.60 time: 0.061
2025-12-15 21:43:30,466: Train batch 76800: loss: 24.68 grad norm: 21.25 time: 0.086
2025-12-15 21:43:43,931: Train batch 77000: loss: 19.96 grad norm: 0.62 time: 0.072
2025-12-15 21:43:56,908: Train batch 77200: loss: 20.34 grad norm: 30.47 time: 0.072
2025-12-15 21:44:11,011: Train batch 77400: loss: 19.05 grad norm: 3.13 time: 0.057
2025-12-15 21:44:25,658: Train batch 77600: loss: 17.89 grad norm: 12.82 time: 0.070
2025-12-15 21:44:40,166: Train batch 77800: loss: 24.08 grad norm: 6.14 time: 0.055
2025-12-15 21:44:54,325: Train batch 78000: loss: 20.32 grad norm: 1.98 time: 0.037
2025-12-15 21:44:54,326: Running test after training batch: 78000
2025-12-15 21:45:04,073: Val batch 78000: PER (avg): 0.1263 CTC Loss (avg): 39.3776 time: 9.747
2025-12-15 21:45:04,073: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:45:04,073: t15.2023.08.13 val PER: 0.0988
2025-12-15 21:45:04,073: t15.2023.08.18 val PER: 0.0830
2025-12-15 21:45:04,073: t15.2023.08.20 val PER: 0.0747
2025-12-15 21:45:04,073: t15.2023.08.25 val PER: 0.0919
2025-12-15 21:45:04,073: t15.2023.08.27 val PER: 0.1511
2025-12-15 21:45:04,073: t15.2023.09.01 val PER: 0.0576
2025-12-15 21:45:04,074: t15.2023.09.03 val PER: 0.1093
2025-12-15 21:45:04,074: t15.2023.09.24 val PER: 0.0959
2025-12-15 21:45:04,074: t15.2023.09.29 val PER: 0.1219
2025-12-15 21:45:04,074: t15.2023.10.01 val PER: 0.1387
2025-12-15 21:45:04,074: t15.2023.10.06 val PER: 0.0915
2025-12-15 21:45:04,074: t15.2023.10.08 val PER: 0.1827
2025-12-15 21:45:04,074: t15.2023.10.13 val PER: 0.1893
2025-12-15 21:45:04,074: t15.2023.10.15 val PER: 0.1417
2025-12-15 21:45:04,074: t15.2023.10.20 val PER: 0.2081
2025-12-15 21:45:04,074: t15.2023.10.22 val PER: 0.1058
2025-12-15 21:45:04,074: t15.2023.11.03 val PER: 0.1839
2025-12-15 21:45:04,074: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:45:04,074: t15.2023.11.17 val PER: 0.0404
2025-12-15 21:45:04,074: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:45:04,074: t15.2023.11.26 val PER: 0.0870
2025-12-15 21:45:04,074: t15.2023.12.03 val PER: 0.0851
2025-12-15 21:45:04,074: t15.2023.12.08 val PER: 0.0639
2025-12-15 21:45:04,074: t15.2023.12.10 val PER: 0.0604
2025-12-15 21:45:04,075: t15.2023.12.17 val PER: 0.1133
2025-12-15 21:45:04,075: t15.2023.12.29 val PER: 0.0947
2025-12-15 21:45:04,075: t15.2024.02.25 val PER: 0.0815
2025-12-15 21:45:04,075: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:45:04,075: t15.2024.03.08 val PER: 0.1750
2025-12-15 21:45:04,075: t15.2024.03.15 val PER: 0.1745
2025-12-15 21:45:04,075: t15.2024.03.17 val PER: 0.0983
2025-12-15 21:45:04,075: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:45:04,075: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:45:04,075: t15.2024.05.10 val PER: 0.1456
2025-12-15 21:45:04,075: t15.2024.06.14 val PER: 0.1341
2025-12-15 21:45:04,075: t15.2024.07.19 val PER: 0.1819
2025-12-15 21:45:04,075: t15.2024.07.21 val PER: 0.0828
2025-12-15 21:45:04,075: t15.2024.07.28 val PER: 0.1044
2025-12-15 21:45:04,075: t15.2025.01.10 val PER: 0.3085
2025-12-15 21:45:04,075: t15.2025.01.12 val PER: 0.1239
2025-12-15 21:45:04,075: t15.2025.03.14 val PER: 0.2766
2025-12-15 21:45:04,075: t15.2025.03.16 val PER: 0.1675
2025-12-15 21:45:04,075: t15.2025.03.30 val PER: 0.2368
2025-12-15 21:45:04,076: t15.2025.04.13 val PER: 0.2211
2025-12-15 21:45:04,076: New best test PER 0.1287 --> 0.1263
2025-12-15 21:45:04,076: Checkpointing model
2025-12-15 21:45:04,734: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:45:19,126: Train batch 78200: loss: 21.96 grad norm: 6.41 time: 0.065
2025-12-15 21:45:33,498: Train batch 78400: loss: 22.81 grad norm: 1.27 time: 0.058
2025-12-15 21:45:47,788: Train batch 78600: loss: 19.74 grad norm: 3.03 time: 0.073
2025-12-15 21:46:01,419: Train batch 78800: loss: 20.80 grad norm: 3.95 time: 0.086
2025-12-15 21:46:13,855: Train batch 79000: loss: 24.03 grad norm: 3.96 time: 0.087
2025-12-15 21:46:28,456: Train batch 79200: loss: 21.04 grad norm: 25.14 time: 0.071
2025-12-15 21:46:43,021: Train batch 79400: loss: 26.33 grad norm: 4.50 time: 0.054
2025-12-15 21:46:56,424: Train batch 79600: loss: 21.50 grad norm: 0.54 time: 0.068
2025-12-15 21:47:10,264: Train batch 79800: loss: 20.48 grad norm: 22.13 time: 0.056
2025-12-15 21:47:23,674: Train batch 80000: loss: 21.57 grad norm: 0.50 time: 0.071
2025-12-15 21:47:23,675: Running test after training batch: 80000
2025-12-15 21:47:33,118: Val batch 80000: PER (avg): 0.1258 CTC Loss (avg): 39.6272 time: 9.443
2025-12-15 21:47:33,119: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:47:33,119: t15.2023.08.13 val PER: 0.0904
2025-12-15 21:47:33,119: t15.2023.08.18 val PER: 0.0754
2025-12-15 21:47:33,119: t15.2023.08.20 val PER: 0.0715
2025-12-15 21:47:33,119: t15.2023.08.25 val PER: 0.0843
2025-12-15 21:47:33,119: t15.2023.08.27 val PER: 0.1431
2025-12-15 21:47:33,119: t15.2023.09.01 val PER: 0.0593
2025-12-15 21:47:33,119: t15.2023.09.03 val PER: 0.1200
2025-12-15 21:47:33,119: t15.2023.09.24 val PER: 0.0922
2025-12-15 21:47:33,119: t15.2023.09.29 val PER: 0.1117
2025-12-15 21:47:33,119: t15.2023.10.01 val PER: 0.1480
2025-12-15 21:47:33,119: t15.2023.10.06 val PER: 0.0926
2025-12-15 21:47:33,119: t15.2023.10.08 val PER: 0.1922
2025-12-15 21:47:33,119: t15.2023.10.13 val PER: 0.1831
2025-12-15 21:47:33,119: t15.2023.10.15 val PER: 0.1371
2025-12-15 21:47:33,119: t15.2023.10.20 val PER: 0.2047
2025-12-15 21:47:33,119: t15.2023.10.22 val PER: 0.1169
2025-12-15 21:47:33,120: t15.2023.11.03 val PER: 0.1818
2025-12-15 21:47:33,120: t15.2023.11.04 val PER: 0.0239
2025-12-15 21:47:33,120: t15.2023.11.17 val PER: 0.0435
2025-12-15 21:47:33,120: t15.2023.11.19 val PER: 0.0639
2025-12-15 21:47:33,120: t15.2023.11.26 val PER: 0.0891
2025-12-15 21:47:33,120: t15.2023.12.03 val PER: 0.0872
2025-12-15 21:47:33,120: t15.2023.12.08 val PER: 0.0639
2025-12-15 21:47:33,120: t15.2023.12.10 val PER: 0.0591
2025-12-15 21:47:33,120: t15.2023.12.17 val PER: 0.1029
2025-12-15 21:47:33,120: t15.2023.12.29 val PER: 0.0892
2025-12-15 21:47:33,120: t15.2024.02.25 val PER: 0.0660
2025-12-15 21:47:33,120: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:47:33,120: t15.2024.03.08 val PER: 0.1863
2025-12-15 21:47:33,120: t15.2024.03.15 val PER: 0.1782
2025-12-15 21:47:33,120: t15.2024.03.17 val PER: 0.0886
2025-12-15 21:47:33,120: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:47:33,120: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:47:33,120: t15.2024.05.10 val PER: 0.1516
2025-12-15 21:47:33,120: t15.2024.06.14 val PER: 0.1514
2025-12-15 21:47:33,121: t15.2024.07.19 val PER: 0.1813
2025-12-15 21:47:33,121: t15.2024.07.21 val PER: 0.0862
2025-12-15 21:47:33,121: t15.2024.07.28 val PER: 0.1074
2025-12-15 21:47:33,121: t15.2025.01.10 val PER: 0.3003
2025-12-15 21:47:33,121: t15.2025.01.12 val PER: 0.1209
2025-12-15 21:47:33,121: t15.2025.03.14 val PER: 0.2929
2025-12-15 21:47:33,121: t15.2025.03.16 val PER: 0.1832
2025-12-15 21:47:33,121: t15.2025.03.30 val PER: 0.2310
2025-12-15 21:47:33,121: t15.2025.04.13 val PER: 0.2111
2025-12-15 21:47:33,121: New best test PER 0.1263 --> 0.1258
2025-12-15 21:47:33,121: Checkpointing model
2025-12-15 21:47:33,589: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:47:47,836: Train batch 80200: loss: 22.69 grad norm: 4.23 time: 0.065
2025-12-15 21:48:01,369: Train batch 80400: loss: 19.43 grad norm: 33.14 time: 0.070
2025-12-15 21:48:15,658: Train batch 80600: loss: 21.32 grad norm: 9.96 time: 0.057
2025-12-15 21:48:27,766: Train batch 80800: loss: 21.58 grad norm: 20.04 time: 0.047
2025-12-15 21:48:41,381: Train batch 81000: loss: 16.78 grad norm: 14.30 time: 0.049
2025-12-15 21:48:55,348: Train batch 81200: loss: 18.04 grad norm: 18.35 time: 0.069
2025-12-15 21:49:09,208: Train batch 81400: loss: 19.84 grad norm: 3.42 time: 0.088
2025-12-15 21:49:23,576: Train batch 81600: loss: 25.27 grad norm: 2.88 time: 0.068
2025-12-15 21:49:38,129: Train batch 81800: loss: 21.33 grad norm: 20.43 time: 0.076
2025-12-15 21:49:51,114: Train batch 82000: loss: 22.81 grad norm: 7.00 time: 0.067
2025-12-15 21:49:51,115: Running test after training batch: 82000
2025-12-15 21:50:00,553: Val batch 82000: PER (avg): 0.1248 CTC Loss (avg): 39.5285 time: 9.438
2025-12-15 21:50:00,553: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:50:00,553: t15.2023.08.13 val PER: 0.0936
2025-12-15 21:50:00,553: t15.2023.08.18 val PER: 0.0838
2025-12-15 21:50:00,553: t15.2023.08.20 val PER: 0.0715
2025-12-15 21:50:00,553: t15.2023.08.25 val PER: 0.0843
2025-12-15 21:50:00,553: t15.2023.08.27 val PER: 0.1431
2025-12-15 21:50:00,553: t15.2023.09.01 val PER: 0.0601
2025-12-15 21:50:00,553: t15.2023.09.03 val PER: 0.1295
2025-12-15 21:50:00,553: t15.2023.09.24 val PER: 0.0922
2025-12-15 21:50:00,553: t15.2023.09.29 val PER: 0.1193
2025-12-15 21:50:00,553: t15.2023.10.01 val PER: 0.1420
2025-12-15 21:50:00,553: t15.2023.10.06 val PER: 0.0926
2025-12-15 21:50:00,554: t15.2023.10.08 val PER: 0.1867
2025-12-15 21:50:00,554: t15.2023.10.13 val PER: 0.1792
2025-12-15 21:50:00,554: t15.2023.10.15 val PER: 0.1430
2025-12-15 21:50:00,554: t15.2023.10.20 val PER: 0.1980
2025-12-15 21:50:00,554: t15.2023.10.22 val PER: 0.0935
2025-12-15 21:50:00,554: t15.2023.11.03 val PER: 0.1832
2025-12-15 21:50:00,554: t15.2023.11.04 val PER: 0.0171
2025-12-15 21:50:00,554: t15.2023.11.17 val PER: 0.0295
2025-12-15 21:50:00,554: t15.2023.11.19 val PER: 0.0619
2025-12-15 21:50:00,554: t15.2023.11.26 val PER: 0.0862
2025-12-15 21:50:00,554: t15.2023.12.03 val PER: 0.0840
2025-12-15 21:50:00,554: t15.2023.12.08 val PER: 0.0539
2025-12-15 21:50:00,554: t15.2023.12.10 val PER: 0.0604
2025-12-15 21:50:00,554: t15.2023.12.17 val PER: 0.1195
2025-12-15 21:50:00,554: t15.2023.12.29 val PER: 0.0927
2025-12-15 21:50:00,554: t15.2024.02.25 val PER: 0.0702
2025-12-15 21:50:00,554: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:50:00,554: t15.2024.03.08 val PER: 0.1792
2025-12-15 21:50:00,554: t15.2024.03.15 val PER: 0.1795
2025-12-15 21:50:00,555: t15.2024.03.17 val PER: 0.0969
2025-12-15 21:50:00,555: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:50:00,555: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:50:00,555: t15.2024.05.10 val PER: 0.1679
2025-12-15 21:50:00,555: t15.2024.06.14 val PER: 0.1609
2025-12-15 21:50:00,555: t15.2024.07.19 val PER: 0.1872
2025-12-15 21:50:00,555: t15.2024.07.21 val PER: 0.0821
2025-12-15 21:50:00,555: t15.2024.07.28 val PER: 0.0971
2025-12-15 21:50:00,555: t15.2025.01.10 val PER: 0.2920
2025-12-15 21:50:00,555: t15.2025.01.12 val PER: 0.1201
2025-12-15 21:50:00,555: t15.2025.03.14 val PER: 0.2663
2025-12-15 21:50:00,555: t15.2025.03.16 val PER: 0.1597
2025-12-15 21:50:00,555: t15.2025.03.30 val PER: 0.2241
2025-12-15 21:50:00,555: t15.2025.04.13 val PER: 0.2068
2025-12-15 21:50:00,555: New best test PER 0.1258 --> 0.1248
2025-12-15 21:50:00,555: Checkpointing model
2025-12-15 21:50:00,980: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:50:14,633: Train batch 82200: loss: 21.28 grad norm: 22.03 time: 0.058
2025-12-15 21:50:28,745: Train batch 82400: loss: 22.81 grad norm: 0.26 time: 0.079
2025-12-15 21:50:42,893: Train batch 82600: loss: 21.08 grad norm: 9.19 time: 0.067
2025-12-15 21:50:56,843: Train batch 82800: loss: 24.54 grad norm: 18.60 time: 0.073
2025-12-15 21:51:10,918: Train batch 83000: loss: 24.04 grad norm: 28.39 time: 0.087
2025-12-15 21:51:24,424: Train batch 83200: loss: 17.21 grad norm: 1.46 time: 0.060
2025-12-15 21:51:37,694: Train batch 83400: loss: 23.24 grad norm: 6.49 time: 0.074
2025-12-15 21:51:51,566: Train batch 83600: loss: 30.87 grad norm: 16.56 time: 0.060
2025-12-15 21:52:05,432: Train batch 83800: loss: 23.99 grad norm: 10.15 time: 0.075
2025-12-15 21:52:18,588: Train batch 84000: loss: 25.68 grad norm: 4.86 time: 0.074
2025-12-15 21:52:18,588: Running test after training batch: 84000
2025-12-15 21:52:27,998: Val batch 84000: PER (avg): 0.1241 CTC Loss (avg): 41.0606 time: 9.410
2025-12-15 21:52:27,999: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:52:27,999: t15.2023.08.13 val PER: 0.0915
2025-12-15 21:52:27,999: t15.2023.08.18 val PER: 0.0847
2025-12-15 21:52:27,999: t15.2023.08.20 val PER: 0.0659
2025-12-15 21:52:27,999: t15.2023.08.25 val PER: 0.0919
2025-12-15 21:52:27,999: t15.2023.08.27 val PER: 0.1447
2025-12-15 21:52:27,999: t15.2023.09.01 val PER: 0.0552
2025-12-15 21:52:27,999: t15.2023.09.03 val PER: 0.1259
2025-12-15 21:52:27,999: t15.2023.09.24 val PER: 0.0971
2025-12-15 21:52:27,999: t15.2023.09.29 val PER: 0.1136
2025-12-15 21:52:27,999: t15.2023.10.01 val PER: 0.1532
2025-12-15 21:52:27,999: t15.2023.10.06 val PER: 0.0969
2025-12-15 21:52:27,999: t15.2023.10.08 val PER: 0.2016
2025-12-15 21:52:27,999: t15.2023.10.13 val PER: 0.1885
2025-12-15 21:52:27,999: t15.2023.10.15 val PER: 0.1430
2025-12-15 21:52:27,999: t15.2023.10.20 val PER: 0.1980
2025-12-15 21:52:28,000: t15.2023.10.22 val PER: 0.1058
2025-12-15 21:52:28,000: t15.2023.11.03 val PER: 0.1764
2025-12-15 21:52:28,000: t15.2023.11.04 val PER: 0.0171
2025-12-15 21:52:28,000: t15.2023.11.17 val PER: 0.0311
2025-12-15 21:52:28,000: t15.2023.11.19 val PER: 0.0679
2025-12-15 21:52:28,000: t15.2023.11.26 val PER: 0.0906
2025-12-15 21:52:28,000: t15.2023.12.03 val PER: 0.0893
2025-12-15 21:52:28,000: t15.2023.12.08 val PER: 0.0546
2025-12-15 21:52:28,000: t15.2023.12.10 val PER: 0.0631
2025-12-15 21:52:28,000: t15.2023.12.17 val PER: 0.1050
2025-12-15 21:52:28,000: t15.2023.12.29 val PER: 0.0933
2025-12-15 21:52:28,000: t15.2024.02.25 val PER: 0.0744
2025-12-15 21:52:28,000: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:52:28,000: t15.2024.03.08 val PER: 0.1679
2025-12-15 21:52:28,000: t15.2024.03.15 val PER: 0.1682
2025-12-15 21:52:28,000: t15.2024.03.17 val PER: 0.0872
2025-12-15 21:52:28,000: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:52:28,000: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:52:28,000: t15.2024.05.10 val PER: 0.1590
2025-12-15 21:52:28,001: t15.2024.06.14 val PER: 0.1498
2025-12-15 21:52:28,001: t15.2024.07.19 val PER: 0.1839
2025-12-15 21:52:28,001: t15.2024.07.21 val PER: 0.0738
2025-12-15 21:52:28,001: t15.2024.07.28 val PER: 0.1007
2025-12-15 21:52:28,001: t15.2025.01.10 val PER: 0.2727
2025-12-15 21:52:28,001: t15.2025.01.12 val PER: 0.1232
2025-12-15 21:52:28,001: t15.2025.03.14 val PER: 0.2663
2025-12-15 21:52:28,001: t15.2025.03.16 val PER: 0.1649
2025-12-15 21:52:28,001: t15.2025.03.30 val PER: 0.2276
2025-12-15 21:52:28,001: t15.2025.04.13 val PER: 0.2126
2025-12-15 21:52:28,001: New best test PER 0.1248 --> 0.1241
2025-12-15 21:52:28,001: Checkpointing model
2025-12-15 21:52:28,411: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:52:40,838: Train batch 84200: loss: 23.54 grad norm: 0.53 time: 0.056
2025-12-15 21:52:52,388: Train batch 84400: loss: 17.39 grad norm: 19.37 time: 0.057
2025-12-15 21:53:06,816: Train batch 84600: loss: 20.52 grad norm: 8.66 time: 0.064
2025-12-15 21:53:20,418: Train batch 84800: loss: 21.86 grad norm: 9.82 time: 0.095
2025-12-15 21:53:35,034: Train batch 85000: loss: 22.27 grad norm: 3.29 time: 0.058
2025-12-15 21:53:49,185: Train batch 85200: loss: 22.49 grad norm: 20.17 time: 0.080
2025-12-15 21:54:02,985: Train batch 85400: loss: 23.53 grad norm: 0.97 time: 0.055
2025-12-15 21:54:17,273: Train batch 85600: loss: 22.64 grad norm: 0.25 time: 0.059
2025-12-15 21:54:31,556: Train batch 85800: loss: 27.03 grad norm: 0.81 time: 0.066
2025-12-15 21:54:43,410: Train batch 86000: loss: 24.46 grad norm: 15.74 time: 0.049
2025-12-15 21:54:43,411: Running test after training batch: 86000
2025-12-15 21:54:52,707: Val batch 86000: PER (avg): 0.1239 CTC Loss (avg): 40.2138 time: 9.296
2025-12-15 21:54:52,707: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:54:52,707: t15.2023.08.13 val PER: 0.0873
2025-12-15 21:54:52,707: t15.2023.08.18 val PER: 0.0771
2025-12-15 21:54:52,707: t15.2023.08.20 val PER: 0.0707
2025-12-15 21:54:52,707: t15.2023.08.25 val PER: 0.0828
2025-12-15 21:54:52,707: t15.2023.08.27 val PER: 0.1511
2025-12-15 21:54:52,708: t15.2023.09.01 val PER: 0.0593
2025-12-15 21:54:52,708: t15.2023.09.03 val PER: 0.1235
2025-12-15 21:54:52,708: t15.2023.09.24 val PER: 0.1056
2025-12-15 21:54:52,708: t15.2023.09.29 val PER: 0.1200
2025-12-15 21:54:52,708: t15.2023.10.01 val PER: 0.1506
2025-12-15 21:54:52,708: t15.2023.10.06 val PER: 0.0915
2025-12-15 21:54:52,708: t15.2023.10.08 val PER: 0.1827
2025-12-15 21:54:52,708: t15.2023.10.13 val PER: 0.1815
2025-12-15 21:54:52,708: t15.2023.10.15 val PER: 0.1391
2025-12-15 21:54:52,708: t15.2023.10.20 val PER: 0.2013
2025-12-15 21:54:52,708: t15.2023.10.22 val PER: 0.1069
2025-12-15 21:54:52,708: t15.2023.11.03 val PER: 0.1737
2025-12-15 21:54:52,708: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:54:52,708: t15.2023.11.17 val PER: 0.0467
2025-12-15 21:54:52,708: t15.2023.11.19 val PER: 0.0579
2025-12-15 21:54:52,708: t15.2023.11.26 val PER: 0.0870
2025-12-15 21:54:52,708: t15.2023.12.03 val PER: 0.0788
2025-12-15 21:54:52,708: t15.2023.12.08 val PER: 0.0526
2025-12-15 21:54:52,708: t15.2023.12.10 val PER: 0.0578
2025-12-15 21:54:52,709: t15.2023.12.17 val PER: 0.1195
2025-12-15 21:54:52,709: t15.2023.12.29 val PER: 0.1002
2025-12-15 21:54:52,709: t15.2024.02.25 val PER: 0.0801
2025-12-15 21:54:52,709: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:54:52,709: t15.2024.03.08 val PER: 0.1735
2025-12-15 21:54:52,709: t15.2024.03.15 val PER: 0.1751
2025-12-15 21:54:52,709: t15.2024.03.17 val PER: 0.0865
2025-12-15 21:54:52,709: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:54:52,709: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:54:52,709: t15.2024.05.10 val PER: 0.1545
2025-12-15 21:54:52,709: t15.2024.06.14 val PER: 0.1703
2025-12-15 21:54:52,709: t15.2024.07.19 val PER: 0.1892
2025-12-15 21:54:52,709: t15.2024.07.21 val PER: 0.0731
2025-12-15 21:54:52,709: t15.2024.07.28 val PER: 0.0971
2025-12-15 21:54:52,709: t15.2025.01.10 val PER: 0.2645
2025-12-15 21:54:52,709: t15.2025.01.12 val PER: 0.1109
2025-12-15 21:54:52,709: t15.2025.03.14 val PER: 0.2751
2025-12-15 21:54:52,709: t15.2025.03.16 val PER: 0.1885
2025-12-15 21:54:52,709: t15.2025.03.30 val PER: 0.2069
2025-12-15 21:54:52,710: t15.2025.04.13 val PER: 0.2097
2025-12-15 21:54:52,710: New best test PER 0.1241 --> 0.1239
2025-12-15 21:54:52,710: Checkpointing model
2025-12-15 21:54:53,156: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:55:03,544: Train batch 86200: loss: 28.46 grad norm: 16.80 time: 0.068
2025-12-15 21:55:15,058: Train batch 86400: loss: 19.14 grad norm: 0.26 time: 0.055
2025-12-15 21:55:26,951: Train batch 86600: loss: 21.24 grad norm: 1.96 time: 0.056
2025-12-15 21:55:41,254: Train batch 86800: loss: 20.66 grad norm: 0.86 time: 0.038
2025-12-15 21:55:54,648: Train batch 87000: loss: 29.96 grad norm: 14.98 time: 0.059
2025-12-15 21:56:08,943: Train batch 87200: loss: 19.76 grad norm: 0.75 time: 0.060
2025-12-15 21:56:23,060: Train batch 87400: loss: 22.70 grad norm: 13.27 time: 0.072
2025-12-15 21:56:37,174: Train batch 87600: loss: 20.71 grad norm: 10.57 time: 0.071
2025-12-15 21:56:50,902: Train batch 87800: loss: 23.74 grad norm: 4.17 time: 0.058
2025-12-15 21:57:05,277: Train batch 88000: loss: 22.34 grad norm: 21.05 time: 0.058
2025-12-15 21:57:05,278: Running test after training batch: 88000
2025-12-15 21:57:14,557: Val batch 88000: PER (avg): 0.1235 CTC Loss (avg): 42.0361 time: 9.279
2025-12-15 21:57:14,557: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:57:14,557: t15.2023.08.13 val PER: 0.0884
2025-12-15 21:57:14,557: t15.2023.08.18 val PER: 0.0830
2025-12-15 21:57:14,557: t15.2023.08.20 val PER: 0.0755
2025-12-15 21:57:14,557: t15.2023.08.25 val PER: 0.0843
2025-12-15 21:57:14,557: t15.2023.08.27 val PER: 0.1495
2025-12-15 21:57:14,557: t15.2023.09.01 val PER: 0.0576
2025-12-15 21:57:14,557: t15.2023.09.03 val PER: 0.1128
2025-12-15 21:57:14,557: t15.2023.09.24 val PER: 0.0983
2025-12-15 21:57:14,558: t15.2023.09.29 val PER: 0.1200
2025-12-15 21:57:14,558: t15.2023.10.01 val PER: 0.1546
2025-12-15 21:57:14,558: t15.2023.10.06 val PER: 0.0947
2025-12-15 21:57:14,558: t15.2023.10.08 val PER: 0.1908
2025-12-15 21:57:14,558: t15.2023.10.13 val PER: 0.1924
2025-12-15 21:57:14,558: t15.2023.10.15 val PER: 0.1358
2025-12-15 21:57:14,558: t15.2023.10.20 val PER: 0.1980
2025-12-15 21:57:14,558: t15.2023.10.22 val PER: 0.1024
2025-12-15 21:57:14,558: t15.2023.11.03 val PER: 0.1832
2025-12-15 21:57:14,558: t15.2023.11.04 val PER: 0.0205
2025-12-15 21:57:14,558: t15.2023.11.17 val PER: 0.0327
2025-12-15 21:57:14,558: t15.2023.11.19 val PER: 0.0639
2025-12-15 21:57:14,558: t15.2023.11.26 val PER: 0.0833
2025-12-15 21:57:14,558: t15.2023.12.03 val PER: 0.0830
2025-12-15 21:57:14,558: t15.2023.12.08 val PER: 0.0553
2025-12-15 21:57:14,558: t15.2023.12.10 val PER: 0.0578
2025-12-15 21:57:14,558: t15.2023.12.17 val PER: 0.1123
2025-12-15 21:57:14,558: t15.2023.12.29 val PER: 0.1002
2025-12-15 21:57:14,559: t15.2024.02.25 val PER: 0.0730
2025-12-15 21:57:14,559: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:57:14,559: t15.2024.03.08 val PER: 0.1593
2025-12-15 21:57:14,559: t15.2024.03.15 val PER: 0.1726
2025-12-15 21:57:14,559: t15.2024.03.17 val PER: 0.0823
2025-12-15 21:57:14,559: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:57:14,559: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:57:14,559: t15.2024.05.10 val PER: 0.1501
2025-12-15 21:57:14,559: t15.2024.06.14 val PER: 0.1593
2025-12-15 21:57:14,559: t15.2024.07.19 val PER: 0.1813
2025-12-15 21:57:14,559: t15.2024.07.21 val PER: 0.0772
2025-12-15 21:57:14,559: t15.2024.07.28 val PER: 0.0978
2025-12-15 21:57:14,559: t15.2025.01.10 val PER: 0.2782
2025-12-15 21:57:14,559: t15.2025.01.12 val PER: 0.1070
2025-12-15 21:57:14,559: t15.2025.03.14 val PER: 0.2663
2025-12-15 21:57:14,559: t15.2025.03.16 val PER: 0.1728
2025-12-15 21:57:14,559: t15.2025.03.30 val PER: 0.2253
2025-12-15 21:57:14,559: t15.2025.04.13 val PER: 0.2111
2025-12-15 21:57:14,559: New best test PER 0.1239 --> 0.1235
2025-12-15 21:57:14,559: Checkpointing model
2025-12-15 21:57:14,968: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:57:27,138: Train batch 88200: loss: 20.52 grad norm: 0.23 time: 0.056
2025-12-15 21:57:41,741: Train batch 88400: loss: 23.32 grad norm: 13.61 time: 0.073
2025-12-15 21:57:55,290: Train batch 88600: loss: 24.52 grad norm: 1.96 time: 0.057
2025-12-15 21:58:09,089: Train batch 88800: loss: 24.18 grad norm: 2.02 time: 0.054
2025-12-15 21:58:22,510: Train batch 89000: loss: 21.15 grad norm: 3.52 time: 0.054
2025-12-15 21:58:36,879: Train batch 89200: loss: 25.90 grad norm: 27.57 time: 0.083
2025-12-15 21:58:49,474: Train batch 89400: loss: 23.01 grad norm: 2.21 time: 0.062
2025-12-15 21:59:03,484: Train batch 89600: loss: 22.00 grad norm: 4.89 time: 0.082
2025-12-15 21:59:16,263: Train batch 89800: loss: 21.22 grad norm: 7.94 time: 0.058
2025-12-15 21:59:30,356: Train batch 90000: loss: 20.54 grad norm: 2.07 time: 0.058
2025-12-15 21:59:30,357: Running test after training batch: 90000
2025-12-15 21:59:39,559: Val batch 90000: PER (avg): 0.1234 CTC Loss (avg): 43.2847 time: 9.202
2025-12-15 21:59:39,560: t15.2023.08.11 val PER: 1.0000
2025-12-15 21:59:39,560: t15.2023.08.13 val PER: 0.0925
2025-12-15 21:59:39,560: t15.2023.08.18 val PER: 0.0830
2025-12-15 21:59:39,560: t15.2023.08.20 val PER: 0.0755
2025-12-15 21:59:39,560: t15.2023.08.25 val PER: 0.0843
2025-12-15 21:59:39,560: t15.2023.08.27 val PER: 0.1463
2025-12-15 21:59:39,560: t15.2023.09.01 val PER: 0.0576
2025-12-15 21:59:39,560: t15.2023.09.03 val PER: 0.1211
2025-12-15 21:59:39,560: t15.2023.09.24 val PER: 0.0959
2025-12-15 21:59:39,560: t15.2023.09.29 val PER: 0.1117
2025-12-15 21:59:39,560: t15.2023.10.01 val PER: 0.1466
2025-12-15 21:59:39,560: t15.2023.10.06 val PER: 0.1001
2025-12-15 21:59:39,560: t15.2023.10.08 val PER: 0.1786
2025-12-15 21:59:39,560: t15.2023.10.13 val PER: 0.1862
2025-12-15 21:59:39,560: t15.2023.10.15 val PER: 0.1444
2025-12-15 21:59:39,560: t15.2023.10.20 val PER: 0.2047
2025-12-15 21:59:39,560: t15.2023.10.22 val PER: 0.1158
2025-12-15 21:59:39,560: t15.2023.11.03 val PER: 0.1716
2025-12-15 21:59:39,560: t15.2023.11.04 val PER: 0.0239
2025-12-15 21:59:39,561: t15.2023.11.17 val PER: 0.0311
2025-12-15 21:59:39,561: t15.2023.11.19 val PER: 0.0559
2025-12-15 21:59:39,561: t15.2023.11.26 val PER: 0.0819
2025-12-15 21:59:39,561: t15.2023.12.03 val PER: 0.0809
2025-12-15 21:59:39,561: t15.2023.12.08 val PER: 0.0586
2025-12-15 21:59:39,561: t15.2023.12.10 val PER: 0.0591
2025-12-15 21:59:39,561: t15.2023.12.17 val PER: 0.1164
2025-12-15 21:59:39,561: t15.2023.12.29 val PER: 0.1002
2025-12-15 21:59:39,561: t15.2024.02.25 val PER: 0.0702
2025-12-15 21:59:39,561: t15.2024.03.03 val PER: 1.0000
2025-12-15 21:59:39,561: t15.2024.03.08 val PER: 0.1792
2025-12-15 21:59:39,561: t15.2024.03.15 val PER: 0.1695
2025-12-15 21:59:39,561: t15.2024.03.17 val PER: 0.0774
2025-12-15 21:59:39,561: t15.2024.04.25 val PER: 1.0000
2025-12-15 21:59:39,561: t15.2024.04.28 val PER: 1.0000
2025-12-15 21:59:39,561: t15.2024.05.10 val PER: 0.1620
2025-12-15 21:59:39,561: t15.2024.06.14 val PER: 0.1562
2025-12-15 21:59:39,561: t15.2024.07.19 val PER: 0.1813
2025-12-15 21:59:39,561: t15.2024.07.21 val PER: 0.0724
2025-12-15 21:59:39,562: t15.2024.07.28 val PER: 0.1022
2025-12-15 21:59:39,562: t15.2025.01.10 val PER: 0.2769
2025-12-15 21:59:39,562: t15.2025.01.12 val PER: 0.1047
2025-12-15 21:59:39,562: t15.2025.03.14 val PER: 0.2825
2025-12-15 21:59:39,562: t15.2025.03.16 val PER: 0.1715
2025-12-15 21:59:39,562: t15.2025.03.30 val PER: 0.2218
2025-12-15 21:59:39,562: t15.2025.04.13 val PER: 0.2183
2025-12-15 21:59:39,562: New best test PER 0.1235 --> 0.1234
2025-12-15 21:59:39,562: Checkpointing model
2025-12-15 21:59:39,969: Saved model to checkpoint: trained_models/baseline_conformer_unet_20251215_200756/checkpoint/best_checkpoint
2025-12-15 21:59:52,816: Train batch 90200: loss: 20.03 grad norm: 9.64 time: 0.057
2025-12-15 22:00:07,142: Train batch 90400: loss: 22.42 grad norm: 0.10 time: 0.058
2025-12-15 22:00:21,456: Train batch 90600: loss: 23.85 grad norm: 1.21 time: 0.072
2025-12-15 22:00:35,923: Train batch 90800: loss: 20.77 grad norm: 9.84 time: 0.068
2025-12-15 22:00:49,545: Train batch 91000: loss: 20.86 grad norm: 1.84 time: 0.067
2025-12-15 22:01:02,971: Train batch 91200: loss: 22.32 grad norm: 12.43 time: 0.060
2025-12-15 22:01:16,307: Train batch 91400: loss: 23.20 grad norm: 1.53 time: 0.069
2025-12-15 22:01:29,320: Train batch 91600: loss: 22.34 grad norm: 36.24 time: 0.086
