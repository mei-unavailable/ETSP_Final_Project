2025-12-08 05:08:56,307: Using device: cuda:0
2025-12-08 05:08:56,770: Using torch.compile
2025-12-08 05:08:57,452: Initialized RNN decoding model
2025-12-08 05:08:57,452: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (day_layer_activation): Softsign()
    (day_weights): ParameterList(
        (0): Parameter containing: [torch.float32 of size 512x512]
        (1): Parameter containing: [torch.float32 of size 512x512]
        (2): Parameter containing: [torch.float32 of size 512x512]
        (3): Parameter containing: [torch.float32 of size 512x512]
        (4): Parameter containing: [torch.float32 of size 512x512]
        (5): Parameter containing: [torch.float32 of size 512x512]
        (6): Parameter containing: [torch.float32 of size 512x512]
        (7): Parameter containing: [torch.float32 of size 512x512]
        (8): Parameter containing: [torch.float32 of size 512x512]
        (9): Parameter containing: [torch.float32 of size 512x512]
        (10): Parameter containing: [torch.float32 of size 512x512]
        (11): Parameter containing: [torch.float32 of size 512x512]
        (12): Parameter containing: [torch.float32 of size 512x512]
        (13): Parameter containing: [torch.float32 of size 512x512]
        (14): Parameter containing: [torch.float32 of size 512x512]
        (15): Parameter containing: [torch.float32 of size 512x512]
        (16): Parameter containing: [torch.float32 of size 512x512]
        (17): Parameter containing: [torch.float32 of size 512x512]
        (18): Parameter containing: [torch.float32 of size 512x512]
        (19): Parameter containing: [torch.float32 of size 512x512]
        (20): Parameter containing: [torch.float32 of size 512x512]
        (21): Parameter containing: [torch.float32 of size 512x512]
        (22): Parameter containing: [torch.float32 of size 512x512]
        (23): Parameter containing: [torch.float32 of size 512x512]
        (24): Parameter containing: [torch.float32 of size 512x512]
        (25): Parameter containing: [torch.float32 of size 512x512]
        (26): Parameter containing: [torch.float32 of size 512x512]
        (27): Parameter containing: [torch.float32 of size 512x512]
        (28): Parameter containing: [torch.float32 of size 512x512]
        (29): Parameter containing: [torch.float32 of size 512x512]
        (30): Parameter containing: [torch.float32 of size 512x512]
        (31): Parameter containing: [torch.float32 of size 512x512]
        (32): Parameter containing: [torch.float32 of size 512x512]
        (33): Parameter containing: [torch.float32 of size 512x512]
        (34): Parameter containing: [torch.float32 of size 512x512]
        (35): Parameter containing: [torch.float32 of size 512x512]
        (36): Parameter containing: [torch.float32 of size 512x512]
        (37): Parameter containing: [torch.float32 of size 512x512]
        (38): Parameter containing: [torch.float32 of size 512x512]
        (39): Parameter containing: [torch.float32 of size 512x512]
        (40): Parameter containing: [torch.float32 of size 512x512]
        (41): Parameter containing: [torch.float32 of size 512x512]
        (42): Parameter containing: [torch.float32 of size 512x512]
        (43): Parameter containing: [torch.float32 of size 512x512]
        (44): Parameter containing: [torch.float32 of size 512x512]
    )
    (day_biases): ParameterList(
        (0): Parameter containing: [torch.float32 of size 1x512]
        (1): Parameter containing: [torch.float32 of size 1x512]
        (2): Parameter containing: [torch.float32 of size 1x512]
        (3): Parameter containing: [torch.float32 of size 1x512]
        (4): Parameter containing: [torch.float32 of size 1x512]
        (5): Parameter containing: [torch.float32 of size 1x512]
        (6): Parameter containing: [torch.float32 of size 1x512]
        (7): Parameter containing: [torch.float32 of size 1x512]
        (8): Parameter containing: [torch.float32 of size 1x512]
        (9): Parameter containing: [torch.float32 of size 1x512]
        (10): Parameter containing: [torch.float32 of size 1x512]
        (11): Parameter containing: [torch.float32 of size 1x512]
        (12): Parameter containing: [torch.float32 of size 1x512]
        (13): Parameter containing: [torch.float32 of size 1x512]
        (14): Parameter containing: [torch.float32 of size 1x512]
        (15): Parameter containing: [torch.float32 of size 1x512]
        (16): Parameter containing: [torch.float32 of size 1x512]
        (17): Parameter containing: [torch.float32 of size 1x512]
        (18): Parameter containing: [torch.float32 of size 1x512]
        (19): Parameter containing: [torch.float32 of size 1x512]
        (20): Parameter containing: [torch.float32 of size 1x512]
        (21): Parameter containing: [torch.float32 of size 1x512]
        (22): Parameter containing: [torch.float32 of size 1x512]
        (23): Parameter containing: [torch.float32 of size 1x512]
        (24): Parameter containing: [torch.float32 of size 1x512]
        (25): Parameter containing: [torch.float32 of size 1x512]
        (26): Parameter containing: [torch.float32 of size 1x512]
        (27): Parameter containing: [torch.float32 of size 1x512]
        (28): Parameter containing: [torch.float32 of size 1x512]
        (29): Parameter containing: [torch.float32 of size 1x512]
        (30): Parameter containing: [torch.float32 of size 1x512]
        (31): Parameter containing: [torch.float32 of size 1x512]
        (32): Parameter containing: [torch.float32 of size 1x512]
        (33): Parameter containing: [torch.float32 of size 1x512]
        (34): Parameter containing: [torch.float32 of size 1x512]
        (35): Parameter containing: [torch.float32 of size 1x512]
        (36): Parameter containing: [torch.float32 of size 1x512]
        (37): Parameter containing: [torch.float32 of size 1x512]
        (38): Parameter containing: [torch.float32 of size 1x512]
        (39): Parameter containing: [torch.float32 of size 1x512]
        (40): Parameter containing: [torch.float32 of size 1x512]
        (41): Parameter containing: [torch.float32 of size 1x512]
        (42): Parameter containing: [torch.float32 of size 1x512]
        (43): Parameter containing: [torch.float32 of size 1x512]
        (44): Parameter containing: [torch.float32 of size 1x512]
    )
    (day_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7168, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-12-08 05:08:57,453: Model has 44,315,177 parameters
2025-12-08 05:08:57,453: Model has 11,819,520 day-specific parameters | 26.67% of total parameters
2025-12-08 05:09:06,040: Successfully initialized datasets
2025-12-08 05:09:21,345: Train batch 0: loss: 757.39 grad norm: 383.78 time: 13.551
2025-12-08 05:09:21,345: Running test after training batch: 0
2025-12-08 05:10:06,898: Val batch 0: PER (avg): 1.2633 CTC Loss (avg): 706.8855 time: 45.553
2025-12-08 05:10:06,898: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:10:06,898: t15.2023.08.13 val PER: 1.0956
2025-12-08 05:10:06,898: t15.2023.08.18 val PER: 1.2347
2025-12-08 05:10:06,898: t15.2023.08.20 val PER: 1.1485
2025-12-08 05:10:06,898: t15.2023.08.25 val PER: 1.2229
2025-12-08 05:10:06,899: t15.2023.08.27 val PER: 1.1897
2025-12-08 05:10:06,899: t15.2023.09.01 val PER: 1.2419
2025-12-08 05:10:06,899: t15.2023.09.03 val PER: 1.2399
2025-12-08 05:10:06,899: t15.2023.09.24 val PER: 1.2985
2025-12-08 05:10:06,899: t15.2023.09.29 val PER: 1.3197
2025-12-08 05:10:06,899: t15.2023.10.01 val PER: 1.0614
2025-12-08 05:10:06,899: t15.2023.10.06 val PER: 1.3089
2025-12-08 05:10:06,899: t15.2023.10.08 val PER: 1.0325
2025-12-08 05:10:06,899: t15.2023.10.13 val PER: 1.2234
2025-12-08 05:10:06,899: t15.2023.10.15 val PER: 1.2861
2025-12-08 05:10:06,899: t15.2023.10.20 val PER: 1.2383
2025-12-08 05:10:06,899: t15.2023.10.22 val PER: 1.3029
2025-12-08 05:10:06,899: t15.2023.11.03 val PER: 1.3467
2025-12-08 05:10:06,899: t15.2023.11.04 val PER: 1.6655
2025-12-08 05:10:06,899: t15.2023.11.17 val PER: 1.7263
2025-12-08 05:10:06,899: t15.2023.11.19 val PER: 1.5050
2025-12-08 05:10:06,899: t15.2023.11.26 val PER: 1.2572
2025-12-08 05:10:06,900: t15.2023.12.03 val PER: 1.2626
2025-12-08 05:10:06,900: t15.2023.12.08 val PER: 1.2923
2025-12-08 05:10:06,900: t15.2023.12.10 val PER: 1.3088
2025-12-08 05:10:06,900: t15.2023.12.17 val PER: 1.1746
2025-12-08 05:10:06,900: t15.2023.12.29 val PER: 1.2045
2025-12-08 05:10:06,900: t15.2024.02.25 val PER: 1.2556
2025-12-08 05:10:06,900: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:10:06,900: t15.2024.03.08 val PER: 1.2504
2025-12-08 05:10:06,900: t15.2024.03.15 val PER: 1.1851
2025-12-08 05:10:06,900: t15.2024.03.17 val PER: 1.2950
2025-12-08 05:10:06,900: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:10:06,900: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:10:06,900: t15.2024.05.10 val PER: 1.1605
2025-12-08 05:10:06,900: t15.2024.06.14 val PER: 1.4069
2025-12-08 05:10:06,900: t15.2024.07.19 val PER: 1.0224
2025-12-08 05:10:06,900: t15.2024.07.21 val PER: 1.4338
2025-12-08 05:10:06,900: t15.2024.07.28 val PER: 1.4169
2025-12-08 05:10:06,900: t15.2025.01.10 val PER: 0.9972
2025-12-08 05:10:06,900: t15.2025.01.12 val PER: 1.5712
2025-12-08 05:10:06,901: t15.2025.03.14 val PER: 1.0192
2025-12-08 05:10:06,901: t15.2025.03.16 val PER: 1.4660
2025-12-08 05:10:06,901: t15.2025.03.30 val PER: 1.2057
2025-12-08 05:10:06,901: t15.2025.04.13 val PER: 1.3210
2025-12-08 05:10:06,901: New best test PER inf --> 1.2633
2025-12-08 05:10:06,901: Checkpointing model
2025-12-08 05:10:07,353: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:10:40,984: Train batch 200: loss: 98.63 grad norm: 41.86 time: 0.158
2025-12-08 05:11:09,663: Train batch 400: loss: 74.73 grad norm: 35.05 time: 0.197
2025-12-08 05:11:38,638: Train batch 600: loss: 71.11 grad norm: 61.15 time: 0.128
2025-12-08 05:12:07,591: Train batch 800: loss: 37.56 grad norm: 38.84 time: 0.117
2025-12-08 05:12:36,317: Train batch 1000: loss: 32.77 grad norm: 40.42 time: 0.149
2025-12-08 05:13:04,836: Train batch 1200: loss: 32.27 grad norm: 35.80 time: 0.157
2025-12-08 05:13:33,524: Train batch 1400: loss: 30.17 grad norm: 42.77 time: 0.111
2025-12-08 05:14:02,817: Train batch 1600: loss: 23.03 grad norm: 38.92 time: 0.150
2025-12-08 05:14:31,143: Train batch 1800: loss: 25.72 grad norm: 39.90 time: 0.141
2025-12-08 05:14:59,776: Train batch 2000: loss: 18.79 grad norm: 34.53 time: 0.109
2025-12-08 05:14:59,776: Running test after training batch: 2000
2025-12-08 05:15:10,440: Val batch 2000: PER (avg): 0.2255 CTC Loss (avg): 22.6229 time: 10.664
2025-12-08 05:15:10,440: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:15:10,440: t15.2023.08.13 val PER: 0.1757
2025-12-08 05:15:10,440: t15.2023.08.18 val PER: 0.1936
2025-12-08 05:15:10,441: t15.2023.08.20 val PER: 0.1549
2025-12-08 05:15:10,441: t15.2023.08.25 val PER: 0.1581
2025-12-08 05:15:10,441: t15.2023.08.27 val PER: 0.2749
2025-12-08 05:15:10,441: t15.2023.09.01 val PER: 0.1380
2025-12-08 05:15:10,441: t15.2023.09.03 val PER: 0.2090
2025-12-08 05:15:10,441: t15.2023.09.24 val PER: 0.1978
2025-12-08 05:15:10,441: t15.2023.09.29 val PER: 0.1934
2025-12-08 05:15:10,441: t15.2023.10.01 val PER: 0.2292
2025-12-08 05:15:10,441: t15.2023.10.06 val PER: 0.1561
2025-12-08 05:15:10,441: t15.2023.10.08 val PER: 0.3085
2025-12-08 05:15:10,441: t15.2023.10.13 val PER: 0.2925
2025-12-08 05:15:10,441: t15.2023.10.15 val PER: 0.2070
2025-12-08 05:15:10,441: t15.2023.10.20 val PER: 0.2282
2025-12-08 05:15:10,441: t15.2023.10.22 val PER: 0.2038
2025-12-08 05:15:10,441: t15.2023.11.03 val PER: 0.2361
2025-12-08 05:15:10,441: t15.2023.11.04 val PER: 0.0546
2025-12-08 05:15:10,441: t15.2023.11.17 val PER: 0.0949
2025-12-08 05:15:10,441: t15.2023.11.19 val PER: 0.1118
2025-12-08 05:15:10,441: t15.2023.11.26 val PER: 0.2522
2025-12-08 05:15:10,442: t15.2023.12.03 val PER: 0.1870
2025-12-08 05:15:10,442: t15.2023.12.08 val PER: 0.2064
2025-12-08 05:15:10,442: t15.2023.12.10 val PER: 0.1761
2025-12-08 05:15:10,442: t15.2023.12.17 val PER: 0.2339
2025-12-08 05:15:10,442: t15.2023.12.29 val PER: 0.2210
2025-12-08 05:15:10,442: t15.2024.02.25 val PER: 0.1952
2025-12-08 05:15:10,442: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:15:10,442: t15.2024.03.08 val PER: 0.2802
2025-12-08 05:15:10,442: t15.2024.03.15 val PER: 0.2977
2025-12-08 05:15:10,442: t15.2024.03.17 val PER: 0.2329
2025-12-08 05:15:10,442: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:15:10,442: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:15:10,442: t15.2024.05.10 val PER: 0.2422
2025-12-08 05:15:10,442: t15.2024.06.14 val PER: 0.2303
2025-12-08 05:15:10,442: t15.2024.07.19 val PER: 0.2986
2025-12-08 05:15:10,442: t15.2024.07.21 val PER: 0.1607
2025-12-08 05:15:10,442: t15.2024.07.28 val PER: 0.2338
2025-12-08 05:15:10,442: t15.2025.01.10 val PER: 0.3678
2025-12-08 05:15:10,443: t15.2025.01.12 val PER: 0.2209
2025-12-08 05:15:10,443: t15.2025.03.14 val PER: 0.3950
2025-12-08 05:15:10,443: t15.2025.03.16 val PER: 0.2605
2025-12-08 05:15:10,443: t15.2025.03.30 val PER: 0.3517
2025-12-08 05:15:10,443: t15.2025.04.13 val PER: 0.2953
2025-12-08 05:15:10,443: New best test PER 1.2633 --> 0.2255
2025-12-08 05:15:10,443: Checkpointing model
2025-12-08 05:15:11,508: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:15:39,393: Train batch 2200: loss: 19.82 grad norm: 45.25 time: 0.110
2025-12-08 05:16:07,573: Train batch 2400: loss: 17.08 grad norm: 37.06 time: 0.165
2025-12-08 05:16:35,700: Train batch 2600: loss: 9.94 grad norm: 28.47 time: 0.142
2025-12-08 05:17:04,856: Train batch 2800: loss: 8.09 grad norm: 23.57 time: 0.150
2025-12-08 05:17:33,556: Train batch 3000: loss: 13.38 grad norm: 34.02 time: 0.109
2025-12-08 05:18:01,533: Train batch 3200: loss: 10.94 grad norm: 34.33 time: 0.084
2025-12-08 05:18:30,561: Train batch 3400: loss: 14.07 grad norm: 41.60 time: 0.119
2025-12-08 05:18:58,728: Train batch 3600: loss: 8.66 grad norm: 32.46 time: 0.108
2025-12-08 05:19:27,994: Train batch 3800: loss: 7.41 grad norm: 27.94 time: 0.215
2025-12-08 05:19:56,386: Train batch 4000: loss: 5.81 grad norm: 22.39 time: 0.117
2025-12-08 05:19:56,387: Running test after training batch: 4000
2025-12-08 05:20:07,219: Val batch 4000: PER (avg): 0.1570 CTC Loss (avg): 17.5510 time: 10.833
2025-12-08 05:20:07,220: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:20:07,220: t15.2023.08.13 val PER: 0.1102
2025-12-08 05:20:07,220: t15.2023.08.18 val PER: 0.1215
2025-12-08 05:20:07,220: t15.2023.08.20 val PER: 0.0977
2025-12-08 05:20:07,220: t15.2023.08.25 val PER: 0.1145
2025-12-08 05:20:07,220: t15.2023.08.27 val PER: 0.1929
2025-12-08 05:20:07,220: t15.2023.09.01 val PER: 0.0698
2025-12-08 05:20:07,220: t15.2023.09.03 val PER: 0.1603
2025-12-08 05:20:07,220: t15.2023.09.24 val PER: 0.1299
2025-12-08 05:20:07,220: t15.2023.09.29 val PER: 0.1347
2025-12-08 05:20:07,220: t15.2023.10.01 val PER: 0.1704
2025-12-08 05:20:07,220: t15.2023.10.06 val PER: 0.1076
2025-12-08 05:20:07,220: t15.2023.10.08 val PER: 0.2476
2025-12-08 05:20:07,220: t15.2023.10.13 val PER: 0.2242
2025-12-08 05:20:07,221: t15.2023.10.15 val PER: 0.1503
2025-12-08 05:20:07,221: t15.2023.10.20 val PER: 0.1980
2025-12-08 05:20:07,221: t15.2023.10.22 val PER: 0.1347
2025-12-08 05:20:07,221: t15.2023.11.03 val PER: 0.1764
2025-12-08 05:20:07,221: t15.2023.11.04 val PER: 0.0341
2025-12-08 05:20:07,221: t15.2023.11.17 val PER: 0.0467
2025-12-08 05:20:07,221: t15.2023.11.19 val PER: 0.0659
2025-12-08 05:20:07,221: t15.2023.11.26 val PER: 0.1341
2025-12-08 05:20:07,221: t15.2023.12.03 val PER: 0.1187
2025-12-08 05:20:07,221: t15.2023.12.08 val PER: 0.1125
2025-12-08 05:20:07,221: t15.2023.12.10 val PER: 0.0894
2025-12-08 05:20:07,221: t15.2023.12.17 val PER: 0.1642
2025-12-08 05:20:07,221: t15.2023.12.29 val PER: 0.1373
2025-12-08 05:20:07,221: t15.2024.02.25 val PER: 0.1320
2025-12-08 05:20:07,221: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:20:07,221: t15.2024.03.08 val PER: 0.2176
2025-12-08 05:20:07,221: t15.2024.03.15 val PER: 0.2220
2025-12-08 05:20:07,222: t15.2024.03.17 val PER: 0.1374
2025-12-08 05:20:07,222: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:20:07,222: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:20:07,222: t15.2024.05.10 val PER: 0.1738
2025-12-08 05:20:07,222: t15.2024.06.14 val PER: 0.1798
2025-12-08 05:20:07,222: t15.2024.07.19 val PER: 0.2162
2025-12-08 05:20:07,222: t15.2024.07.21 val PER: 0.0938
2025-12-08 05:20:07,222: t15.2024.07.28 val PER: 0.1478
2025-12-08 05:20:07,222: t15.2025.01.10 val PER: 0.2920
2025-12-08 05:20:07,222: t15.2025.01.12 val PER: 0.1570
2025-12-08 05:20:07,222: t15.2025.03.14 val PER: 0.3713
2025-12-08 05:20:07,222: t15.2025.03.16 val PER: 0.1846
2025-12-08 05:20:07,222: t15.2025.03.30 val PER: 0.2920
2025-12-08 05:20:07,222: t15.2025.04.13 val PER: 0.2411
2025-12-08 05:20:07,222: New best test PER 0.2255 --> 0.1570
2025-12-08 05:20:07,222: Checkpointing model
2025-12-08 05:20:08,401: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:20:36,463: Train batch 4200: loss: 5.82 grad norm: 30.32 time: 0.123
2025-12-08 05:21:04,765: Train batch 4400: loss: 4.97 grad norm: 20.74 time: 0.176
2025-12-08 05:21:33,203: Train batch 4600: loss: 3.33 grad norm: 20.01 time: 0.140
2025-12-08 05:22:01,490: Train batch 4800: loss: 7.49 grad norm: 41.49 time: 0.144
2025-12-08 05:22:30,139: Train batch 5000: loss: 4.98 grad norm: 30.34 time: 0.134
2025-12-08 05:22:58,150: Train batch 5200: loss: 7.00 grad norm: 33.45 time: 0.149
2025-12-08 05:23:26,346: Train batch 5400: loss: 5.20 grad norm: 26.97 time: 0.110
2025-12-08 05:23:54,824: Train batch 5600: loss: 2.37 grad norm: 19.00 time: 0.094
2025-12-08 05:24:23,011: Train batch 5800: loss: 3.59 grad norm: 26.47 time: 0.192
2025-12-08 05:24:51,254: Train batch 6000: loss: 4.43 grad norm: 26.95 time: 0.169
2025-12-08 05:24:51,255: Running test after training batch: 6000
2025-12-08 05:25:02,144: Val batch 6000: PER (avg): 0.1415 CTC Loss (avg): 17.7005 time: 10.889
2025-12-08 05:25:02,144: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:25:02,144: t15.2023.08.13 val PER: 0.0988
2025-12-08 05:25:02,144: t15.2023.08.18 val PER: 0.0956
2025-12-08 05:25:02,144: t15.2023.08.20 val PER: 0.0842
2025-12-08 05:25:02,145: t15.2023.08.25 val PER: 0.1024
2025-12-08 05:25:02,145: t15.2023.08.27 val PER: 0.1640
2025-12-08 05:25:02,145: t15.2023.09.01 val PER: 0.0698
2025-12-08 05:25:02,145: t15.2023.09.03 val PER: 0.1425
2025-12-08 05:25:02,145: t15.2023.09.24 val PER: 0.1117
2025-12-08 05:25:02,145: t15.2023.09.29 val PER: 0.1257
2025-12-08 05:25:02,145: t15.2023.10.01 val PER: 0.1618
2025-12-08 05:25:02,145: t15.2023.10.06 val PER: 0.0915
2025-12-08 05:25:02,145: t15.2023.10.08 val PER: 0.2084
2025-12-08 05:25:02,145: t15.2023.10.13 val PER: 0.1986
2025-12-08 05:25:02,145: t15.2023.10.15 val PER: 0.1615
2025-12-08 05:25:02,145: t15.2023.10.20 val PER: 0.2047
2025-12-08 05:25:02,145: t15.2023.10.22 val PER: 0.1236
2025-12-08 05:25:02,145: t15.2023.11.03 val PER: 0.1744
2025-12-08 05:25:02,145: t15.2023.11.04 val PER: 0.0273
2025-12-08 05:25:02,145: t15.2023.11.17 val PER: 0.0327
2025-12-08 05:25:02,145: t15.2023.11.19 val PER: 0.0479
2025-12-08 05:25:02,145: t15.2023.11.26 val PER: 0.1072
2025-12-08 05:25:02,146: t15.2023.12.03 val PER: 0.1008
2025-12-08 05:25:02,146: t15.2023.12.08 val PER: 0.0985
2025-12-08 05:25:02,146: t15.2023.12.10 val PER: 0.0670
2025-12-08 05:25:02,146: t15.2023.12.17 val PER: 0.1268
2025-12-08 05:25:02,146: t15.2023.12.29 val PER: 0.1167
2025-12-08 05:25:02,146: t15.2024.02.25 val PER: 0.1124
2025-12-08 05:25:02,146: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:25:02,146: t15.2024.03.08 val PER: 0.2063
2025-12-08 05:25:02,146: t15.2024.03.15 val PER: 0.2001
2025-12-08 05:25:02,146: t15.2024.03.17 val PER: 0.1220
2025-12-08 05:25:02,146: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:25:02,146: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:25:02,146: t15.2024.05.10 val PER: 0.1471
2025-12-08 05:25:02,146: t15.2024.06.14 val PER: 0.1577
2025-12-08 05:25:02,146: t15.2024.07.19 val PER: 0.2030
2025-12-08 05:25:02,146: t15.2024.07.21 val PER: 0.0903
2025-12-08 05:25:02,146: t15.2024.07.28 val PER: 0.1206
2025-12-08 05:25:02,146: t15.2025.01.10 val PER: 0.2879
2025-12-08 05:25:02,146: t15.2025.01.12 val PER: 0.1378
2025-12-08 05:25:02,147: t15.2025.03.14 val PER: 0.3314
2025-12-08 05:25:02,147: t15.2025.03.16 val PER: 0.1885
2025-12-08 05:25:02,147: t15.2025.03.30 val PER: 0.2770
2025-12-08 05:25:02,147: t15.2025.04.13 val PER: 0.2197
2025-12-08 05:25:02,147: New best test PER 0.1570 --> 0.1415
2025-12-08 05:25:02,147: Checkpointing model
2025-12-08 05:25:03,193: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:25:31,363: Train batch 6200: loss: 3.74 grad norm: 28.85 time: 0.127
2025-12-08 05:25:59,014: Train batch 6400: loss: 3.63 grad norm: 24.39 time: 0.105
2025-12-08 05:26:27,681: Train batch 6600: loss: 3.58 grad norm: 21.99 time: 0.120
2025-12-08 05:26:56,827: Train batch 6800: loss: 3.46 grad norm: 23.34 time: 0.119
2025-12-08 05:27:25,990: Train batch 7000: loss: 2.48 grad norm: 17.00 time: 0.114
2025-12-08 05:27:54,237: Train batch 7200: loss: 2.13 grad norm: 17.49 time: 0.099
2025-12-08 05:28:22,828: Train batch 7400: loss: 2.60 grad norm: 32.41 time: 0.121
2025-12-08 05:28:51,486: Train batch 7600: loss: 6.00 grad norm: 32.86 time: 0.125
2025-12-08 05:29:20,352: Train batch 7800: loss: 4.15 grad norm: 23.90 time: 0.107
2025-12-08 05:29:49,162: Train batch 8000: loss: 1.98 grad norm: 18.39 time: 0.098
2025-12-08 05:29:49,162: Running test after training batch: 8000
2025-12-08 05:29:59,703: Val batch 8000: PER (avg): 0.1323 CTC Loss (avg): 17.6642 time: 10.541
2025-12-08 05:29:59,703: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:29:59,703: t15.2023.08.13 val PER: 0.0873
2025-12-08 05:29:59,703: t15.2023.08.18 val PER: 0.1006
2025-12-08 05:29:59,703: t15.2023.08.20 val PER: 0.0786
2025-12-08 05:29:59,703: t15.2023.08.25 val PER: 0.0813
2025-12-08 05:29:59,704: t15.2023.08.27 val PER: 0.1559
2025-12-08 05:29:59,704: t15.2023.09.01 val PER: 0.0682
2025-12-08 05:29:59,704: t15.2023.09.03 val PER: 0.1330
2025-12-08 05:29:59,704: t15.2023.09.24 val PER: 0.1141
2025-12-08 05:29:59,704: t15.2023.09.29 val PER: 0.1219
2025-12-08 05:29:59,704: t15.2023.10.01 val PER: 0.1605
2025-12-08 05:29:59,704: t15.2023.10.06 val PER: 0.0872
2025-12-08 05:29:59,704: t15.2023.10.08 val PER: 0.2084
2025-12-08 05:29:59,704: t15.2023.10.13 val PER: 0.1715
2025-12-08 05:29:59,704: t15.2023.10.15 val PER: 0.1365
2025-12-08 05:29:59,704: t15.2023.10.20 val PER: 0.1745
2025-12-08 05:29:59,704: t15.2023.10.22 val PER: 0.1214
2025-12-08 05:29:59,704: t15.2023.11.03 val PER: 0.1635
2025-12-08 05:29:59,704: t15.2023.11.04 val PER: 0.0273
2025-12-08 05:29:59,704: t15.2023.11.17 val PER: 0.0311
2025-12-08 05:29:59,704: t15.2023.11.19 val PER: 0.0299
2025-12-08 05:29:59,705: t15.2023.11.26 val PER: 0.0899
2025-12-08 05:29:59,705: t15.2023.12.03 val PER: 0.0809
2025-12-08 05:29:59,705: t15.2023.12.08 val PER: 0.0779
2025-12-08 05:29:59,705: t15.2023.12.10 val PER: 0.0802
2025-12-08 05:29:59,705: t15.2023.12.17 val PER: 0.1216
2025-12-08 05:29:59,705: t15.2023.12.29 val PER: 0.1057
2025-12-08 05:29:59,705: t15.2024.02.25 val PER: 0.0899
2025-12-08 05:29:59,705: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:29:59,705: t15.2024.03.08 val PER: 0.1963
2025-12-08 05:29:59,705: t15.2024.03.15 val PER: 0.1932
2025-12-08 05:29:59,705: t15.2024.03.17 val PER: 0.1165
2025-12-08 05:29:59,705: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:29:59,705: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:29:59,705: t15.2024.05.10 val PER: 0.1441
2025-12-08 05:29:59,705: t15.2024.06.14 val PER: 0.1498
2025-12-08 05:29:59,705: t15.2024.07.19 val PER: 0.1997
2025-12-08 05:29:59,706: t15.2024.07.21 val PER: 0.0855
2025-12-08 05:29:59,706: t15.2024.07.28 val PER: 0.1154
2025-12-08 05:29:59,706: t15.2025.01.10 val PER: 0.2548
2025-12-08 05:29:59,706: t15.2025.01.12 val PER: 0.1239
2025-12-08 05:29:59,706: t15.2025.03.14 val PER: 0.3373
2025-12-08 05:29:59,706: t15.2025.03.16 val PER: 0.1872
2025-12-08 05:29:59,706: t15.2025.03.30 val PER: 0.2494
2025-12-08 05:29:59,706: t15.2025.04.13 val PER: 0.2197
2025-12-08 05:29:59,706: New best test PER 0.1415 --> 0.1323
2025-12-08 05:29:59,706: Checkpointing model
2025-12-08 05:30:00,837: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:30:28,751: Train batch 8200: loss: 2.51 grad norm: 19.75 time: 0.163
2025-12-08 05:30:57,142: Train batch 8400: loss: 3.12 grad norm: 22.54 time: 0.093
2025-12-08 05:31:25,642: Train batch 8600: loss: 2.67 grad norm: 24.84 time: 0.151
2025-12-08 05:31:54,741: Train batch 8800: loss: 1.88 grad norm: 16.04 time: 0.105
2025-12-08 05:32:23,270: Train batch 9000: loss: 3.77 grad norm: 27.63 time: 0.121
2025-12-08 05:32:52,122: Train batch 9200: loss: 3.55 grad norm: 26.47 time: 0.138
2025-12-08 05:33:20,647: Train batch 9400: loss: 2.68 grad norm: 23.34 time: 0.151
2025-12-08 05:33:48,802: Train batch 9600: loss: 2.39 grad norm: 33.26 time: 0.134
2025-12-08 05:34:17,340: Train batch 9800: loss: 0.64 grad norm: 9.56 time: 0.132
2025-12-08 05:34:45,992: Train batch 10000: loss: 0.83 grad norm: 13.13 time: 0.113
2025-12-08 05:34:45,993: Running test after training batch: 10000
2025-12-08 05:34:56,494: Val batch 10000: PER (avg): 0.1294 CTC Loss (avg): 19.0395 time: 10.501
2025-12-08 05:34:56,494: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:34:56,494: t15.2023.08.13 val PER: 0.0842
2025-12-08 05:34:56,494: t15.2023.08.18 val PER: 0.0855
2025-12-08 05:34:56,494: t15.2023.08.20 val PER: 0.0707
2025-12-08 05:34:56,494: t15.2023.08.25 val PER: 0.0843
2025-12-08 05:34:56,494: t15.2023.08.27 val PER: 0.1367
2025-12-08 05:34:56,494: t15.2023.09.01 val PER: 0.0593
2025-12-08 05:34:56,495: t15.2023.09.03 val PER: 0.1544
2025-12-08 05:34:56,495: t15.2023.09.24 val PER: 0.1080
2025-12-08 05:34:56,495: t15.2023.09.29 val PER: 0.1225
2025-12-08 05:34:56,495: t15.2023.10.01 val PER: 0.1605
2025-12-08 05:34:56,495: t15.2023.10.06 val PER: 0.0829
2025-12-08 05:34:56,495: t15.2023.10.08 val PER: 0.2043
2025-12-08 05:34:56,495: t15.2023.10.13 val PER: 0.1792
2025-12-08 05:34:56,495: t15.2023.10.15 val PER: 0.1391
2025-12-08 05:34:56,495: t15.2023.10.20 val PER: 0.1711
2025-12-08 05:34:56,495: t15.2023.10.22 val PER: 0.0969
2025-12-08 05:34:56,495: t15.2023.11.03 val PER: 0.1547
2025-12-08 05:34:56,495: t15.2023.11.04 val PER: 0.0239
2025-12-08 05:34:56,495: t15.2023.11.17 val PER: 0.0264
2025-12-08 05:34:56,495: t15.2023.11.19 val PER: 0.0240
2025-12-08 05:34:56,495: t15.2023.11.26 val PER: 0.0899
2025-12-08 05:34:56,495: t15.2023.12.03 val PER: 0.0756
2025-12-08 05:34:56,495: t15.2023.12.08 val PER: 0.0759
2025-12-08 05:34:56,495: t15.2023.12.10 val PER: 0.0723
2025-12-08 05:34:56,496: t15.2023.12.17 val PER: 0.1227
2025-12-08 05:34:56,496: t15.2023.12.29 val PER: 0.1030
2025-12-08 05:34:56,496: t15.2024.02.25 val PER: 0.0941
2025-12-08 05:34:56,496: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:34:56,496: t15.2024.03.08 val PER: 0.1991
2025-12-08 05:34:56,496: t15.2024.03.15 val PER: 0.2026
2025-12-08 05:34:56,496: t15.2024.03.17 val PER: 0.1046
2025-12-08 05:34:56,496: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:34:56,496: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:34:56,496: t15.2024.05.10 val PER: 0.1293
2025-12-08 05:34:56,496: t15.2024.06.14 val PER: 0.1325
2025-12-08 05:34:56,496: t15.2024.07.19 val PER: 0.1859
2025-12-08 05:34:56,496: t15.2024.07.21 val PER: 0.0710
2025-12-08 05:34:56,496: t15.2024.07.28 val PER: 0.1118
2025-12-08 05:34:56,496: t15.2025.01.10 val PER: 0.2617
2025-12-08 05:34:56,496: t15.2025.01.12 val PER: 0.1286
2025-12-08 05:34:56,496: t15.2025.03.14 val PER: 0.3521
2025-12-08 05:34:56,496: t15.2025.03.16 val PER: 0.1819
2025-12-08 05:34:56,496: t15.2025.03.30 val PER: 0.2632
2025-12-08 05:34:56,497: t15.2025.04.13 val PER: 0.2282
2025-12-08 05:34:56,497: New best test PER 0.1323 --> 0.1294
2025-12-08 05:34:56,497: Checkpointing model
2025-12-08 05:34:57,611: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:35:25,958: Train batch 10200: loss: 2.38 grad norm: 22.43 time: 0.132
2025-12-08 05:35:54,732: Train batch 10400: loss: 0.89 grad norm: 9.66 time: 0.177
2025-12-08 05:36:23,542: Train batch 10600: loss: 0.69 grad norm: 12.54 time: 0.095
2025-12-08 05:36:52,245: Train batch 10800: loss: 1.45 grad norm: 16.76 time: 0.134
2025-12-08 05:37:20,419: Train batch 11000: loss: 1.45 grad norm: 22.50 time: 0.132
2025-12-08 05:37:48,965: Train batch 11200: loss: 1.77 grad norm: 23.77 time: 0.234
2025-12-08 05:38:16,413: Train batch 11400: loss: 0.83 grad norm: 11.15 time: 0.112
2025-12-08 05:38:45,301: Train batch 11600: loss: 0.55 grad norm: 14.39 time: 0.116
2025-12-08 05:39:14,485: Train batch 11800: loss: 1.70 grad norm: 32.04 time: 0.113
2025-12-08 05:39:42,864: Train batch 12000: loss: 1.66 grad norm: 15.88 time: 0.105
2025-12-08 05:39:42,864: Running test after training batch: 12000
2025-12-08 05:39:53,603: Val batch 12000: PER (avg): 0.1258 CTC Loss (avg): 19.4076 time: 10.738
2025-12-08 05:39:53,603: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:39:53,603: t15.2023.08.13 val PER: 0.0842
2025-12-08 05:39:53,603: t15.2023.08.18 val PER: 0.0889
2025-12-08 05:39:53,603: t15.2023.08.20 val PER: 0.0731
2025-12-08 05:39:53,603: t15.2023.08.25 val PER: 0.0768
2025-12-08 05:39:53,603: t15.2023.08.27 val PER: 0.1318
2025-12-08 05:39:53,603: t15.2023.09.01 val PER: 0.0584
2025-12-08 05:39:53,603: t15.2023.09.03 val PER: 0.1401
2025-12-08 05:39:53,603: t15.2023.09.24 val PER: 0.1007
2025-12-08 05:39:53,603: t15.2023.09.29 val PER: 0.1161
2025-12-08 05:39:53,604: t15.2023.10.01 val PER: 0.1532
2025-12-08 05:39:53,604: t15.2023.10.06 val PER: 0.0710
2025-12-08 05:39:53,604: t15.2023.10.08 val PER: 0.2016
2025-12-08 05:39:53,604: t15.2023.10.13 val PER: 0.1761
2025-12-08 05:39:53,604: t15.2023.10.15 val PER: 0.1371
2025-12-08 05:39:53,604: t15.2023.10.20 val PER: 0.1711
2025-12-08 05:39:53,604: t15.2023.10.22 val PER: 0.1136
2025-12-08 05:39:53,604: t15.2023.11.03 val PER: 0.1493
2025-12-08 05:39:53,604: t15.2023.11.04 val PER: 0.0239
2025-12-08 05:39:53,604: t15.2023.11.17 val PER: 0.0187
2025-12-08 05:39:53,604: t15.2023.11.19 val PER: 0.0259
2025-12-08 05:39:53,604: t15.2023.11.26 val PER: 0.0790
2025-12-08 05:39:53,604: t15.2023.12.03 val PER: 0.0725
2025-12-08 05:39:53,604: t15.2023.12.08 val PER: 0.0686
2025-12-08 05:39:53,604: t15.2023.12.10 val PER: 0.0631
2025-12-08 05:39:53,604: t15.2023.12.17 val PER: 0.1091
2025-12-08 05:39:53,604: t15.2023.12.29 val PER: 0.0961
2025-12-08 05:39:53,604: t15.2024.02.25 val PER: 0.0885
2025-12-08 05:39:53,605: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:39:53,605: t15.2024.03.08 val PER: 0.1977
2025-12-08 05:39:53,605: t15.2024.03.15 val PER: 0.2064
2025-12-08 05:39:53,605: t15.2024.03.17 val PER: 0.1081
2025-12-08 05:39:53,605: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:39:53,605: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:39:53,605: t15.2024.05.10 val PER: 0.1352
2025-12-08 05:39:53,605: t15.2024.06.14 val PER: 0.1183
2025-12-08 05:39:53,605: t15.2024.07.19 val PER: 0.1866
2025-12-08 05:39:53,605: t15.2024.07.21 val PER: 0.0731
2025-12-08 05:39:53,605: t15.2024.07.28 val PER: 0.1118
2025-12-08 05:39:53,605: t15.2025.01.10 val PER: 0.2700
2025-12-08 05:39:53,605: t15.2025.01.12 val PER: 0.1401
2025-12-08 05:39:53,605: t15.2025.03.14 val PER: 0.3447
2025-12-08 05:39:53,605: t15.2025.03.16 val PER: 0.1741
2025-12-08 05:39:53,605: t15.2025.03.30 val PER: 0.2345
2025-12-08 05:39:53,605: t15.2025.04.13 val PER: 0.1954
2025-12-08 05:39:53,605: New best test PER 0.1294 --> 0.1258
2025-12-08 05:39:53,605: Checkpointing model
2025-12-08 05:39:55,029: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:40:23,413: Train batch 12200: loss: 0.56 grad norm: 8.66 time: 0.139
2025-12-08 05:40:52,210: Train batch 12400: loss: 0.33 grad norm: 6.04 time: 0.115
2025-12-08 05:41:21,243: Train batch 12600: loss: 0.67 grad norm: 10.36 time: 0.137
2025-12-08 05:41:49,544: Train batch 12800: loss: 0.68 grad norm: 10.55 time: 0.124
2025-12-08 05:42:18,968: Train batch 13000: loss: 0.73 grad norm: 10.95 time: 0.097
2025-12-08 05:42:47,667: Train batch 13200: loss: 0.73 grad norm: 13.86 time: 0.092
2025-12-08 05:43:16,031: Train batch 13400: loss: 1.40 grad norm: 19.46 time: 0.146
2025-12-08 05:43:44,012: Train batch 13600: loss: 0.85 grad norm: 15.12 time: 0.106
2025-12-08 05:44:13,074: Train batch 13800: loss: 0.84 grad norm: 24.46 time: 0.159
2025-12-08 05:44:41,740: Train batch 14000: loss: 1.54 grad norm: 19.59 time: 0.158
2025-12-08 05:44:41,740: Running test after training batch: 14000
2025-12-08 05:44:52,177: Val batch 14000: PER (avg): 0.1242 CTC Loss (avg): 19.5993 time: 10.437
2025-12-08 05:44:52,178: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:44:52,178: t15.2023.08.13 val PER: 0.0873
2025-12-08 05:44:52,178: t15.2023.08.18 val PER: 0.0905
2025-12-08 05:44:52,178: t15.2023.08.20 val PER: 0.0699
2025-12-08 05:44:52,178: t15.2023.08.25 val PER: 0.0858
2025-12-08 05:44:52,178: t15.2023.08.27 val PER: 0.1431
2025-12-08 05:44:52,178: t15.2023.09.01 val PER: 0.0544
2025-12-08 05:44:52,178: t15.2023.09.03 val PER: 0.1449
2025-12-08 05:44:52,178: t15.2023.09.24 val PER: 0.1080
2025-12-08 05:44:52,178: t15.2023.09.29 val PER: 0.1206
2025-12-08 05:44:52,178: t15.2023.10.01 val PER: 0.1612
2025-12-08 05:44:52,178: t15.2023.10.06 val PER: 0.0818
2025-12-08 05:44:52,178: t15.2023.10.08 val PER: 0.1719
2025-12-08 05:44:52,178: t15.2023.10.13 val PER: 0.1715
2025-12-08 05:44:52,178: t15.2023.10.15 val PER: 0.1259
2025-12-08 05:44:52,178: t15.2023.10.20 val PER: 0.1678
2025-12-08 05:44:52,179: t15.2023.10.22 val PER: 0.1036
2025-12-08 05:44:52,179: t15.2023.11.03 val PER: 0.1567
2025-12-08 05:44:52,179: t15.2023.11.04 val PER: 0.0273
2025-12-08 05:44:52,179: t15.2023.11.17 val PER: 0.0202
2025-12-08 05:44:52,179: t15.2023.11.19 val PER: 0.0200
2025-12-08 05:44:52,179: t15.2023.11.26 val PER: 0.0826
2025-12-08 05:44:52,179: t15.2023.12.03 val PER: 0.0756
2025-12-08 05:44:52,179: t15.2023.12.08 val PER: 0.0739
2025-12-08 05:44:52,179: t15.2023.12.10 val PER: 0.0565
2025-12-08 05:44:52,179: t15.2023.12.17 val PER: 0.1029
2025-12-08 05:44:52,179: t15.2023.12.29 val PER: 0.0899
2025-12-08 05:44:52,179: t15.2024.02.25 val PER: 0.0927
2025-12-08 05:44:52,179: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:44:52,179: t15.2024.03.08 val PER: 0.1977
2025-12-08 05:44:52,179: t15.2024.03.15 val PER: 0.1832
2025-12-08 05:44:52,179: t15.2024.03.17 val PER: 0.0962
2025-12-08 05:44:52,179: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:44:52,179: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:44:52,180: t15.2024.05.10 val PER: 0.1233
2025-12-08 05:44:52,180: t15.2024.06.14 val PER: 0.1404
2025-12-08 05:44:52,180: t15.2024.07.19 val PER: 0.1846
2025-12-08 05:44:52,180: t15.2024.07.21 val PER: 0.0710
2025-12-08 05:44:52,180: t15.2024.07.28 val PER: 0.0963
2025-12-08 05:44:52,180: t15.2025.01.10 val PER: 0.2603
2025-12-08 05:44:52,180: t15.2025.01.12 val PER: 0.1170
2025-12-08 05:44:52,180: t15.2025.03.14 val PER: 0.3565
2025-12-08 05:44:52,180: t15.2025.03.16 val PER: 0.2003
2025-12-08 05:44:52,180: t15.2025.03.30 val PER: 0.2368
2025-12-08 05:44:52,180: t15.2025.04.13 val PER: 0.2140
2025-12-08 05:44:52,180: New best test PER 0.1258 --> 0.1242
2025-12-08 05:44:52,180: Checkpointing model
2025-12-08 05:44:53,280: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:45:22,529: Train batch 14200: loss: 1.10 grad norm: 17.11 time: 0.189
2025-12-08 05:45:50,597: Train batch 14400: loss: 1.22 grad norm: 21.67 time: 0.179
2025-12-08 05:46:19,357: Train batch 14600: loss: 1.19 grad norm: 21.67 time: 0.166
2025-12-08 05:46:47,267: Train batch 14800: loss: 0.21 grad norm: 9.44 time: 0.103
2025-12-08 05:47:16,821: Train batch 15000: loss: 1.03 grad norm: 16.53 time: 0.199
2025-12-08 05:47:46,663: Train batch 15200: loss: 1.03 grad norm: 22.11 time: 0.140
2025-12-08 05:48:16,535: Train batch 15400: loss: 0.88 grad norm: 14.96 time: 0.122
2025-12-08 05:48:45,482: Train batch 15600: loss: 1.08 grad norm: 14.29 time: 0.190
2025-12-08 05:49:15,581: Train batch 15800: loss: 0.57 grad norm: 11.18 time: 0.105
2025-12-08 05:49:46,462: Train batch 16000: loss: 0.77 grad norm: 14.27 time: 0.103
2025-12-08 05:49:46,462: Running test after training batch: 16000
2025-12-08 05:49:56,890: Val batch 16000: PER (avg): 0.1221 CTC Loss (avg): 20.7196 time: 10.427
2025-12-08 05:49:56,890: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:49:56,890: t15.2023.08.13 val PER: 0.0852
2025-12-08 05:49:56,890: t15.2023.08.18 val PER: 0.0738
2025-12-08 05:49:56,890: t15.2023.08.20 val PER: 0.0667
2025-12-08 05:49:56,890: t15.2023.08.25 val PER: 0.0828
2025-12-08 05:49:56,890: t15.2023.08.27 val PER: 0.1543
2025-12-08 05:49:56,890: t15.2023.09.01 val PER: 0.0471
2025-12-08 05:49:56,890: t15.2023.09.03 val PER: 0.1176
2025-12-08 05:49:56,890: t15.2023.09.24 val PER: 0.1056
2025-12-08 05:49:56,890: t15.2023.09.29 val PER: 0.1149
2025-12-08 05:49:56,891: t15.2023.10.01 val PER: 0.1532
2025-12-08 05:49:56,891: t15.2023.10.06 val PER: 0.0786
2025-12-08 05:49:56,891: t15.2023.10.08 val PER: 0.1786
2025-12-08 05:49:56,891: t15.2023.10.13 val PER: 0.1746
2025-12-08 05:49:56,891: t15.2023.10.15 val PER: 0.1272
2025-12-08 05:49:56,891: t15.2023.10.20 val PER: 0.1644
2025-12-08 05:49:56,891: t15.2023.10.22 val PER: 0.0947
2025-12-08 05:49:56,891: t15.2023.11.03 val PER: 0.1533
2025-12-08 05:49:56,891: t15.2023.11.04 val PER: 0.0171
2025-12-08 05:49:56,891: t15.2023.11.17 val PER: 0.0187
2025-12-08 05:49:56,891: t15.2023.11.19 val PER: 0.0279
2025-12-08 05:49:56,891: t15.2023.11.26 val PER: 0.0797
2025-12-08 05:49:56,891: t15.2023.12.03 val PER: 0.0641
2025-12-08 05:49:56,891: t15.2023.12.08 val PER: 0.0726
2025-12-08 05:49:56,891: t15.2023.12.10 val PER: 0.0644
2025-12-08 05:49:56,891: t15.2023.12.17 val PER: 0.0936
2025-12-08 05:49:56,891: t15.2023.12.29 val PER: 0.0975
2025-12-08 05:49:56,891: t15.2024.02.25 val PER: 0.0801
2025-12-08 05:49:56,892: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:49:56,892: t15.2024.03.08 val PER: 0.2091
2025-12-08 05:49:56,892: t15.2024.03.15 val PER: 0.1914
2025-12-08 05:49:56,892: t15.2024.03.17 val PER: 0.1151
2025-12-08 05:49:56,892: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:49:56,892: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:49:56,892: t15.2024.05.10 val PER: 0.1367
2025-12-08 05:49:56,892: t15.2024.06.14 val PER: 0.1451
2025-12-08 05:49:56,892: t15.2024.07.19 val PER: 0.1786
2025-12-08 05:49:56,892: t15.2024.07.21 val PER: 0.0766
2025-12-08 05:49:56,892: t15.2024.07.28 val PER: 0.0971
2025-12-08 05:49:56,892: t15.2025.01.10 val PER: 0.2438
2025-12-08 05:49:56,892: t15.2025.01.12 val PER: 0.1255
2025-12-08 05:49:56,892: t15.2025.03.14 val PER: 0.3388
2025-12-08 05:49:56,892: t15.2025.03.16 val PER: 0.1584
2025-12-08 05:49:56,892: t15.2025.03.30 val PER: 0.2448
2025-12-08 05:49:56,892: t15.2025.04.13 val PER: 0.2011
2025-12-08 05:49:56,892: New best test PER 0.1242 --> 0.1221
2025-12-08 05:49:56,892: Checkpointing model
2025-12-08 05:49:57,944: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:50:27,642: Train batch 16200: loss: 0.94 grad norm: 11.39 time: 0.108
2025-12-08 05:50:57,391: Train batch 16400: loss: 1.12 grad norm: 21.54 time: 0.172
2025-12-08 05:51:26,124: Train batch 16600: loss: 0.53 grad norm: 11.82 time: 0.110
2025-12-08 05:51:56,187: Train batch 16800: loss: 0.96 grad norm: 19.30 time: 0.132
2025-12-08 05:52:26,768: Train batch 17000: loss: 0.67 grad norm: 10.99 time: 0.187
2025-12-08 05:52:57,000: Train batch 17200: loss: 0.38 grad norm: 9.91 time: 0.145
2025-12-08 05:53:26,936: Train batch 17400: loss: 1.08 grad norm: 17.81 time: 0.198
2025-12-08 05:53:56,185: Train batch 17600: loss: 0.32 grad norm: 6.92 time: 0.148
2025-12-08 05:54:26,115: Train batch 17800: loss: 0.51 grad norm: 8.79 time: 0.110
2025-12-08 05:54:56,527: Train batch 18000: loss: 0.86 grad norm: 15.08 time: 0.193
2025-12-08 05:54:56,528: Running test after training batch: 18000
2025-12-08 05:55:07,021: Val batch 18000: PER (avg): 0.1214 CTC Loss (avg): 20.9839 time: 10.493
2025-12-08 05:55:07,021: t15.2023.08.11 val PER: 1.0000
2025-12-08 05:55:07,021: t15.2023.08.13 val PER: 0.0936
2025-12-08 05:55:07,022: t15.2023.08.18 val PER: 0.0872
2025-12-08 05:55:07,022: t15.2023.08.20 val PER: 0.0715
2025-12-08 05:55:07,022: t15.2023.08.25 val PER: 0.0858
2025-12-08 05:55:07,022: t15.2023.08.27 val PER: 0.1495
2025-12-08 05:55:07,022: t15.2023.09.01 val PER: 0.0487
2025-12-08 05:55:07,022: t15.2023.09.03 val PER: 0.1306
2025-12-08 05:55:07,022: t15.2023.09.24 val PER: 0.1032
2025-12-08 05:55:07,022: t15.2023.09.29 val PER: 0.1168
2025-12-08 05:55:07,022: t15.2023.10.01 val PER: 0.1473
2025-12-08 05:55:07,022: t15.2023.10.06 val PER: 0.0743
2025-12-08 05:55:07,022: t15.2023.10.08 val PER: 0.1908
2025-12-08 05:55:07,022: t15.2023.10.13 val PER: 0.1552
2025-12-08 05:55:07,022: t15.2023.10.15 val PER: 0.1325
2025-12-08 05:55:07,022: t15.2023.10.20 val PER: 0.1577
2025-12-08 05:55:07,022: t15.2023.10.22 val PER: 0.1024
2025-12-08 05:55:07,022: t15.2023.11.03 val PER: 0.1608
2025-12-08 05:55:07,022: t15.2023.11.04 val PER: 0.0171
2025-12-08 05:55:07,022: t15.2023.11.17 val PER: 0.0280
2025-12-08 05:55:07,023: t15.2023.11.19 val PER: 0.0319
2025-12-08 05:55:07,023: t15.2023.11.26 val PER: 0.0717
2025-12-08 05:55:07,023: t15.2023.12.03 val PER: 0.0735
2025-12-08 05:55:07,023: t15.2023.12.08 val PER: 0.0593
2025-12-08 05:55:07,023: t15.2023.12.10 val PER: 0.0526
2025-12-08 05:55:07,023: t15.2023.12.17 val PER: 0.0915
2025-12-08 05:55:07,023: t15.2023.12.29 val PER: 0.0872
2025-12-08 05:55:07,023: t15.2024.02.25 val PER: 0.0702
2025-12-08 05:55:07,023: t15.2024.03.03 val PER: 1.0000
2025-12-08 05:55:07,023: t15.2024.03.08 val PER: 0.2020
2025-12-08 05:55:07,023: t15.2024.03.15 val PER: 0.1845
2025-12-08 05:55:07,023: t15.2024.03.17 val PER: 0.0921
2025-12-08 05:55:07,023: t15.2024.04.25 val PER: 1.0000
2025-12-08 05:55:07,023: t15.2024.04.28 val PER: 1.0000
2025-12-08 05:55:07,023: t15.2024.05.10 val PER: 0.1412
2025-12-08 05:55:07,023: t15.2024.06.14 val PER: 0.1372
2025-12-08 05:55:07,023: t15.2024.07.19 val PER: 0.1635
2025-12-08 05:55:07,023: t15.2024.07.21 val PER: 0.0634
2025-12-08 05:55:07,023: t15.2024.07.28 val PER: 0.1206
2025-12-08 05:55:07,024: t15.2025.01.10 val PER: 0.2755
2025-12-08 05:55:07,024: t15.2025.01.12 val PER: 0.1255
2025-12-08 05:55:07,024: t15.2025.03.14 val PER: 0.3432
2025-12-08 05:55:07,024: t15.2025.03.16 val PER: 0.1754
2025-12-08 05:55:07,024: t15.2025.03.30 val PER: 0.2506
2025-12-08 05:55:07,024: t15.2025.04.13 val PER: 0.1954
2025-12-08 05:55:07,024: New best test PER 0.1221 --> 0.1214
2025-12-08 05:55:07,024: Checkpointing model
2025-12-08 05:55:08,071: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 05:55:38,131: Train batch 18200: loss: 0.32 grad norm: 9.04 time: 0.118
2025-12-08 05:56:07,934: Train batch 18400: loss: 0.34 grad norm: 7.25 time: 0.156
2025-12-08 05:56:38,015: Train batch 18600: loss: 0.76 grad norm: 13.30 time: 0.197
2025-12-08 05:57:07,474: Train batch 18800: loss: 0.79 grad norm: 14.48 time: 0.132
2025-12-08 05:57:37,756: Train batch 19000: loss: 0.75 grad norm: 15.62 time: 0.153
2025-12-08 05:58:07,759: Train batch 19200: loss: 0.88 grad norm: 23.14 time: 0.162
2025-12-08 05:58:38,154: Train batch 19400: loss: 0.73 grad norm: 11.41 time: 0.111
2025-12-08 05:59:07,662: Train batch 19600: loss: 0.60 grad norm: 9.36 time: 0.106
2025-12-08 05:59:37,032: Train batch 19800: loss: 0.16 grad norm: 7.82 time: 0.094
2025-12-08 06:00:06,558: Train batch 20000: loss: 0.29 grad norm: 42.16 time: 0.113
2025-12-08 06:00:06,559: Running test after training batch: 20000
2025-12-08 06:00:17,132: Val batch 20000: PER (avg): 0.1191 CTC Loss (avg): 21.0910 time: 10.572
2025-12-08 06:00:17,132: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:00:17,132: t15.2023.08.13 val PER: 0.0863
2025-12-08 06:00:17,132: t15.2023.08.18 val PER: 0.0855
2025-12-08 06:00:17,132: t15.2023.08.20 val PER: 0.0739
2025-12-08 06:00:17,132: t15.2023.08.25 val PER: 0.0828
2025-12-08 06:00:17,132: t15.2023.08.27 val PER: 0.1592
2025-12-08 06:00:17,132: t15.2023.09.01 val PER: 0.0463
2025-12-08 06:00:17,132: t15.2023.09.03 val PER: 0.1354
2025-12-08 06:00:17,132: t15.2023.09.24 val PER: 0.0995
2025-12-08 06:00:17,132: t15.2023.09.29 val PER: 0.1149
2025-12-08 06:00:17,132: t15.2023.10.01 val PER: 0.1427
2025-12-08 06:00:17,133: t15.2023.10.06 val PER: 0.0775
2025-12-08 06:00:17,133: t15.2023.10.08 val PER: 0.1746
2025-12-08 06:00:17,133: t15.2023.10.13 val PER: 0.1583
2025-12-08 06:00:17,133: t15.2023.10.15 val PER: 0.1206
2025-12-08 06:00:17,133: t15.2023.10.20 val PER: 0.1644
2025-12-08 06:00:17,133: t15.2023.10.22 val PER: 0.0913
2025-12-08 06:00:17,133: t15.2023.11.03 val PER: 0.1459
2025-12-08 06:00:17,133: t15.2023.11.04 val PER: 0.0205
2025-12-08 06:00:17,133: t15.2023.11.17 val PER: 0.0140
2025-12-08 06:00:17,133: t15.2023.11.19 val PER: 0.0319
2025-12-08 06:00:17,133: t15.2023.11.26 val PER: 0.0754
2025-12-08 06:00:17,133: t15.2023.12.03 val PER: 0.0641
2025-12-08 06:00:17,133: t15.2023.12.08 val PER: 0.0559
2025-12-08 06:00:17,133: t15.2023.12.10 val PER: 0.0499
2025-12-08 06:00:17,133: t15.2023.12.17 val PER: 0.1019
2025-12-08 06:00:17,133: t15.2023.12.29 val PER: 0.0837
2025-12-08 06:00:17,133: t15.2024.02.25 val PER: 0.0787
2025-12-08 06:00:17,133: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:00:17,133: t15.2024.03.08 val PER: 0.1778
2025-12-08 06:00:17,134: t15.2024.03.15 val PER: 0.1907
2025-12-08 06:00:17,134: t15.2024.03.17 val PER: 0.1060
2025-12-08 06:00:17,134: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:00:17,134: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:00:17,134: t15.2024.05.10 val PER: 0.1218
2025-12-08 06:00:17,134: t15.2024.06.14 val PER: 0.1404
2025-12-08 06:00:17,134: t15.2024.07.19 val PER: 0.1760
2025-12-08 06:00:17,134: t15.2024.07.21 val PER: 0.0683
2025-12-08 06:00:17,134: t15.2024.07.28 val PER: 0.0912
2025-12-08 06:00:17,134: t15.2025.01.10 val PER: 0.2617
2025-12-08 06:00:17,134: t15.2025.01.12 val PER: 0.1239
2025-12-08 06:00:17,134: t15.2025.03.14 val PER: 0.3550
2025-12-08 06:00:17,134: t15.2025.03.16 val PER: 0.1636
2025-12-08 06:00:17,134: t15.2025.03.30 val PER: 0.2460
2025-12-08 06:00:17,134: t15.2025.04.13 val PER: 0.2111
2025-12-08 06:00:17,134: New best test PER 0.1214 --> 0.1191
2025-12-08 06:00:17,134: Checkpointing model
2025-12-08 06:00:18,187: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:00:48,036: Train batch 20200: loss: 0.60 grad norm: 10.22 time: 0.110
2025-12-08 06:01:18,103: Train batch 20400: loss: 0.54 grad norm: 14.73 time: 0.128
2025-12-08 06:01:47,856: Train batch 20600: loss: 0.68 grad norm: 20.52 time: 0.122
2025-12-08 06:02:18,250: Train batch 20800: loss: 0.40 grad norm: 12.34 time: 0.134
2025-12-08 06:02:49,238: Train batch 21000: loss: 0.34 grad norm: 11.34 time: 0.115
2025-12-08 06:03:18,914: Train batch 21200: loss: 0.54 grad norm: 12.84 time: 0.122
2025-12-08 06:03:49,124: Train batch 21400: loss: 0.62 grad norm: 17.41 time: 0.134
2025-12-08 06:04:18,706: Train batch 21600: loss: 0.72 grad norm: 12.52 time: 0.126
2025-12-08 06:04:48,345: Train batch 21800: loss: 0.41 grad norm: 11.90 time: 0.162
2025-12-08 06:05:18,273: Train batch 22000: loss: 0.28 grad norm: 8.34 time: 0.098
2025-12-08 06:05:18,273: Running test after training batch: 22000
2025-12-08 06:05:28,875: Val batch 22000: PER (avg): 0.1198 CTC Loss (avg): 21.9125 time: 10.601
2025-12-08 06:05:28,875: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:05:28,875: t15.2023.08.13 val PER: 0.0842
2025-12-08 06:05:28,875: t15.2023.08.18 val PER: 0.0754
2025-12-08 06:05:28,875: t15.2023.08.20 val PER: 0.0635
2025-12-08 06:05:28,875: t15.2023.08.25 val PER: 0.0994
2025-12-08 06:05:28,875: t15.2023.08.27 val PER: 0.1431
2025-12-08 06:05:28,875: t15.2023.09.01 val PER: 0.0503
2025-12-08 06:05:28,875: t15.2023.09.03 val PER: 0.1390
2025-12-08 06:05:28,876: t15.2023.09.24 val PER: 0.1019
2025-12-08 06:05:28,876: t15.2023.09.29 val PER: 0.1155
2025-12-08 06:05:28,876: t15.2023.10.01 val PER: 0.1612
2025-12-08 06:05:28,876: t15.2023.10.06 val PER: 0.0743
2025-12-08 06:05:28,876: t15.2023.10.08 val PER: 0.1719
2025-12-08 06:05:28,876: t15.2023.10.13 val PER: 0.1606
2025-12-08 06:05:28,876: t15.2023.10.15 val PER: 0.1239
2025-12-08 06:05:28,876: t15.2023.10.20 val PER: 0.1812
2025-12-08 06:05:28,876: t15.2023.10.22 val PER: 0.1024
2025-12-08 06:05:28,876: t15.2023.11.03 val PER: 0.1486
2025-12-08 06:05:28,876: t15.2023.11.04 val PER: 0.0205
2025-12-08 06:05:28,876: t15.2023.11.17 val PER: 0.0218
2025-12-08 06:05:28,876: t15.2023.11.19 val PER: 0.0240
2025-12-08 06:05:28,876: t15.2023.11.26 val PER: 0.0681
2025-12-08 06:05:28,876: t15.2023.12.03 val PER: 0.0588
2025-12-08 06:05:28,876: t15.2023.12.08 val PER: 0.0672
2025-12-08 06:05:28,877: t15.2023.12.10 val PER: 0.0486
2025-12-08 06:05:28,877: t15.2023.12.17 val PER: 0.0904
2025-12-08 06:05:28,877: t15.2023.12.29 val PER: 0.0837
2025-12-08 06:05:28,877: t15.2024.02.25 val PER: 0.0716
2025-12-08 06:05:28,877: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:05:28,877: t15.2024.03.08 val PER: 0.1963
2025-12-08 06:05:28,877: t15.2024.03.15 val PER: 0.1882
2025-12-08 06:05:28,877: t15.2024.03.17 val PER: 0.1011
2025-12-08 06:05:28,877: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:05:28,877: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:05:28,877: t15.2024.05.10 val PER: 0.1189
2025-12-08 06:05:28,877: t15.2024.06.14 val PER: 0.1483
2025-12-08 06:05:28,877: t15.2024.07.19 val PER: 0.1833
2025-12-08 06:05:28,877: t15.2024.07.21 val PER: 0.0766
2025-12-08 06:05:28,877: t15.2024.07.28 val PER: 0.1096
2025-12-08 06:05:28,877: t15.2025.01.10 val PER: 0.2424
2025-12-08 06:05:28,877: t15.2025.01.12 val PER: 0.1193
2025-12-08 06:05:28,878: t15.2025.03.14 val PER: 0.3343
2025-12-08 06:05:28,878: t15.2025.03.16 val PER: 0.1754
2025-12-08 06:05:28,878: t15.2025.03.30 val PER: 0.2437
2025-12-08 06:05:28,878: t15.2025.04.13 val PER: 0.1840
2025-12-08 06:05:57,581: Train batch 22200: loss: 0.90 grad norm: 14.23 time: 0.134
2025-12-08 06:06:28,145: Train batch 22400: loss: 1.44 grad norm: 8.30 time: 0.183
2025-12-08 06:06:58,129: Train batch 22600: loss: 0.70 grad norm: 14.68 time: 0.127
2025-12-08 06:07:28,076: Train batch 22800: loss: 0.87 grad norm: 13.76 time: 0.106
2025-12-08 06:07:57,780: Train batch 23000: loss: 0.61 grad norm: 13.18 time: 0.091
2025-12-08 06:08:28,766: Train batch 23200: loss: 0.88 grad norm: 17.70 time: 0.148
2025-12-08 06:08:58,989: Train batch 23400: loss: 0.01 grad norm: 1.97 time: 0.104
2025-12-08 06:09:28,977: Train batch 23600: loss: 0.79 grad norm: 18.13 time: 0.151
2025-12-08 06:09:58,902: Train batch 23800: loss: 0.34 grad norm: 13.33 time: 0.120
2025-12-08 06:10:28,838: Train batch 24000: loss: 0.41 grad norm: 10.42 time: 0.218
2025-12-08 06:10:28,838: Running test after training batch: 24000
2025-12-08 06:10:39,089: Val batch 24000: PER (avg): 0.1191 CTC Loss (avg): 22.4258 time: 10.251
2025-12-08 06:10:39,089: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:10:39,090: t15.2023.08.13 val PER: 0.0707
2025-12-08 06:10:39,090: t15.2023.08.18 val PER: 0.0721
2025-12-08 06:10:39,090: t15.2023.08.20 val PER: 0.0572
2025-12-08 06:10:39,090: t15.2023.08.25 val PER: 0.0738
2025-12-08 06:10:39,090: t15.2023.08.27 val PER: 0.1334
2025-12-08 06:10:39,090: t15.2023.09.01 val PER: 0.0463
2025-12-08 06:10:39,090: t15.2023.09.03 val PER: 0.1366
2025-12-08 06:10:39,090: t15.2023.09.24 val PER: 0.0934
2025-12-08 06:10:39,090: t15.2023.09.29 val PER: 0.1085
2025-12-08 06:10:39,090: t15.2023.10.01 val PER: 0.1453
2025-12-08 06:10:39,090: t15.2023.10.06 val PER: 0.0753
2025-12-08 06:10:39,090: t15.2023.10.08 val PER: 0.1922
2025-12-08 06:10:39,090: t15.2023.10.13 val PER: 0.1676
2025-12-08 06:10:39,090: t15.2023.10.15 val PER: 0.1365
2025-12-08 06:10:39,090: t15.2023.10.20 val PER: 0.1779
2025-12-08 06:10:39,090: t15.2023.10.22 val PER: 0.1147
2025-12-08 06:10:39,090: t15.2023.11.03 val PER: 0.1526
2025-12-08 06:10:39,090: t15.2023.11.04 val PER: 0.0102
2025-12-08 06:10:39,091: t15.2023.11.17 val PER: 0.0218
2025-12-08 06:10:39,091: t15.2023.11.19 val PER: 0.0180
2025-12-08 06:10:39,091: t15.2023.11.26 val PER: 0.0667
2025-12-08 06:10:39,091: t15.2023.12.03 val PER: 0.0777
2025-12-08 06:10:39,091: t15.2023.12.08 val PER: 0.0573
2025-12-08 06:10:39,091: t15.2023.12.10 val PER: 0.0434
2025-12-08 06:10:39,091: t15.2023.12.17 val PER: 0.1112
2025-12-08 06:10:39,091: t15.2023.12.29 val PER: 0.0728
2025-12-08 06:10:39,091: t15.2024.02.25 val PER: 0.0744
2025-12-08 06:10:39,091: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:10:39,091: t15.2024.03.08 val PER: 0.2048
2025-12-08 06:10:39,091: t15.2024.03.15 val PER: 0.1895
2025-12-08 06:10:39,091: t15.2024.03.17 val PER: 0.1025
2025-12-08 06:10:39,091: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:10:39,091: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:10:39,091: t15.2024.05.10 val PER: 0.1322
2025-12-08 06:10:39,091: t15.2024.06.14 val PER: 0.1435
2025-12-08 06:10:39,091: t15.2024.07.19 val PER: 0.1688
2025-12-08 06:10:39,091: t15.2024.07.21 val PER: 0.0717
2025-12-08 06:10:39,092: t15.2024.07.28 val PER: 0.1191
2025-12-08 06:10:39,092: t15.2025.01.10 val PER: 0.2617
2025-12-08 06:10:39,092: t15.2025.01.12 val PER: 0.1239
2025-12-08 06:10:39,092: t15.2025.03.14 val PER: 0.3491
2025-12-08 06:10:39,092: t15.2025.03.16 val PER: 0.1492
2025-12-08 06:10:39,092: t15.2025.03.30 val PER: 0.2287
2025-12-08 06:10:39,092: t15.2025.04.13 val PER: 0.2040
2025-12-08 06:10:39,092: New best test PER 0.1191 --> 0.1191
2025-12-08 06:10:39,092: Checkpointing model
2025-12-08 06:10:40,222: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:11:11,197: Train batch 24200: loss: 0.59 grad norm: 16.67 time: 0.096
2025-12-08 06:11:41,628: Train batch 24400: loss: 0.61 grad norm: 17.20 time: 0.139
2025-12-08 06:12:10,946: Train batch 24600: loss: 0.33 grad norm: 9.88 time: 0.137
2025-12-08 06:12:40,860: Train batch 24800: loss: 0.28 grad norm: 8.65 time: 0.103
2025-12-08 06:13:10,327: Train batch 25000: loss: 0.78 grad norm: 12.88 time: 0.131
2025-12-08 06:13:40,133: Train batch 25200: loss: 0.39 grad norm: 9.10 time: 0.130
2025-12-08 06:14:10,108: Train batch 25400: loss: 0.21 grad norm: 8.54 time: 0.116
2025-12-08 06:14:40,005: Train batch 25600: loss: 0.40 grad norm: 11.75 time: 0.156
2025-12-08 06:15:09,291: Train batch 25800: loss: 0.20 grad norm: 5.33 time: 0.120
2025-12-08 06:15:39,106: Train batch 26000: loss: 0.55 grad norm: 11.09 time: 0.109
2025-12-08 06:15:39,106: Running test after training batch: 26000
2025-12-08 06:15:49,622: Val batch 26000: PER (avg): 0.1187 CTC Loss (avg): 23.3688 time: 10.516
2025-12-08 06:15:49,622: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:15:49,623: t15.2023.08.13 val PER: 0.0717
2025-12-08 06:15:49,623: t15.2023.08.18 val PER: 0.0687
2025-12-08 06:15:49,623: t15.2023.08.20 val PER: 0.0691
2025-12-08 06:15:49,623: t15.2023.08.25 val PER: 0.0768
2025-12-08 06:15:49,623: t15.2023.08.27 val PER: 0.1576
2025-12-08 06:15:49,623: t15.2023.09.01 val PER: 0.0495
2025-12-08 06:15:49,623: t15.2023.09.03 val PER: 0.1306
2025-12-08 06:15:49,623: t15.2023.09.24 val PER: 0.0983
2025-12-08 06:15:49,623: t15.2023.09.29 val PER: 0.1117
2025-12-08 06:15:49,623: t15.2023.10.01 val PER: 0.1453
2025-12-08 06:15:49,623: t15.2023.10.06 val PER: 0.0743
2025-12-08 06:15:49,623: t15.2023.10.08 val PER: 0.1746
2025-12-08 06:15:49,623: t15.2023.10.13 val PER: 0.1629
2025-12-08 06:15:49,623: t15.2023.10.15 val PER: 0.1285
2025-12-08 06:15:49,623: t15.2023.10.20 val PER: 0.1879
2025-12-08 06:15:49,623: t15.2023.10.22 val PER: 0.0969
2025-12-08 06:15:49,623: t15.2023.11.03 val PER: 0.1608
2025-12-08 06:15:49,623: t15.2023.11.04 val PER: 0.0137
2025-12-08 06:15:49,624: t15.2023.11.17 val PER: 0.0140
2025-12-08 06:15:49,624: t15.2023.11.19 val PER: 0.0299
2025-12-08 06:15:49,624: t15.2023.11.26 val PER: 0.0703
2025-12-08 06:15:49,624: t15.2023.12.03 val PER: 0.0672
2025-12-08 06:15:49,624: t15.2023.12.08 val PER: 0.0679
2025-12-08 06:15:49,624: t15.2023.12.10 val PER: 0.0565
2025-12-08 06:15:49,624: t15.2023.12.17 val PER: 0.1008
2025-12-08 06:15:49,624: t15.2023.12.29 val PER: 0.0707
2025-12-08 06:15:49,624: t15.2024.02.25 val PER: 0.0941
2025-12-08 06:15:49,624: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:15:49,624: t15.2024.03.08 val PER: 0.1991
2025-12-08 06:15:49,624: t15.2024.03.15 val PER: 0.1907
2025-12-08 06:15:49,624: t15.2024.03.17 val PER: 0.0921
2025-12-08 06:15:49,624: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:15:49,624: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:15:49,624: t15.2024.05.10 val PER: 0.1233
2025-12-08 06:15:49,624: t15.2024.06.14 val PER: 0.1309
2025-12-08 06:15:49,624: t15.2024.07.19 val PER: 0.1859
2025-12-08 06:15:49,625: t15.2024.07.21 val PER: 0.0648
2025-12-08 06:15:49,625: t15.2024.07.28 val PER: 0.0978
2025-12-08 06:15:49,625: t15.2025.01.10 val PER: 0.2727
2025-12-08 06:15:49,625: t15.2025.01.12 val PER: 0.1132
2025-12-08 06:15:49,625: t15.2025.03.14 val PER: 0.3269
2025-12-08 06:15:49,625: t15.2025.03.16 val PER: 0.1702
2025-12-08 06:15:49,625: t15.2025.03.30 val PER: 0.2437
2025-12-08 06:15:49,625: t15.2025.04.13 val PER: 0.1926
2025-12-08 06:15:49,625: New best test PER 0.1191 --> 0.1187
2025-12-08 06:15:49,625: Checkpointing model
2025-12-08 06:15:50,739: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:16:20,157: Train batch 26200: loss: 0.34 grad norm: 60.01 time: 0.097
2025-12-08 06:16:50,289: Train batch 26400: loss: 1.14 grad norm: 13.61 time: 0.119
2025-12-08 06:17:20,645: Train batch 26600: loss: 0.42 grad norm: 14.74 time: 0.168
2025-12-08 06:17:50,014: Train batch 26800: loss: 0.20 grad norm: 6.24 time: 0.145
2025-12-08 06:18:20,221: Train batch 27000: loss: 0.31 grad norm: 10.16 time: 0.100
2025-12-08 06:18:50,339: Train batch 27200: loss: 0.30 grad norm: 14.79 time: 0.109
2025-12-08 06:19:20,200: Train batch 27400: loss: 0.20 grad norm: 7.24 time: 0.132
2025-12-08 06:19:49,772: Train batch 27600: loss: 0.20 grad norm: 6.63 time: 0.114
2025-12-08 06:20:19,737: Train batch 27800: loss: 0.36 grad norm: 13.39 time: 0.097
2025-12-08 06:20:50,020: Train batch 28000: loss: 0.60 grad norm: 14.23 time: 0.183
2025-12-08 06:20:50,021: Running test after training batch: 28000
2025-12-08 06:21:00,599: Val batch 28000: PER (avg): 0.1159 CTC Loss (avg): 22.4115 time: 10.578
2025-12-08 06:21:00,599: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:21:00,599: t15.2023.08.13 val PER: 0.0790
2025-12-08 06:21:00,599: t15.2023.08.18 val PER: 0.0813
2025-12-08 06:21:00,600: t15.2023.08.20 val PER: 0.0635
2025-12-08 06:21:00,600: t15.2023.08.25 val PER: 0.0798
2025-12-08 06:21:00,600: t15.2023.08.27 val PER: 0.1431
2025-12-08 06:21:00,600: t15.2023.09.01 val PER: 0.0495
2025-12-08 06:21:00,600: t15.2023.09.03 val PER: 0.1235
2025-12-08 06:21:00,600: t15.2023.09.24 val PER: 0.0898
2025-12-08 06:21:00,600: t15.2023.09.29 val PER: 0.1085
2025-12-08 06:21:00,600: t15.2023.10.01 val PER: 0.1460
2025-12-08 06:21:00,600: t15.2023.10.06 val PER: 0.0710
2025-12-08 06:21:00,600: t15.2023.10.08 val PER: 0.1813
2025-12-08 06:21:00,600: t15.2023.10.13 val PER: 0.1660
2025-12-08 06:21:00,600: t15.2023.10.15 val PER: 0.1193
2025-12-08 06:21:00,600: t15.2023.10.20 val PER: 0.1846
2025-12-08 06:21:00,600: t15.2023.10.22 val PER: 0.1058
2025-12-08 06:21:00,600: t15.2023.11.03 val PER: 0.1459
2025-12-08 06:21:00,600: t15.2023.11.04 val PER: 0.0205
2025-12-08 06:21:00,600: t15.2023.11.17 val PER: 0.0124
2025-12-08 06:21:00,600: t15.2023.11.19 val PER: 0.0220
2025-12-08 06:21:00,600: t15.2023.11.26 val PER: 0.0710
2025-12-08 06:21:00,601: t15.2023.12.03 val PER: 0.0662
2025-12-08 06:21:00,601: t15.2023.12.08 val PER: 0.0606
2025-12-08 06:21:00,601: t15.2023.12.10 val PER: 0.0407
2025-12-08 06:21:00,601: t15.2023.12.17 val PER: 0.0811
2025-12-08 06:21:00,601: t15.2023.12.29 val PER: 0.0741
2025-12-08 06:21:00,601: t15.2024.02.25 val PER: 0.0772
2025-12-08 06:21:00,601: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:21:00,601: t15.2024.03.08 val PER: 0.1849
2025-12-08 06:21:00,601: t15.2024.03.15 val PER: 0.1826
2025-12-08 06:21:00,601: t15.2024.03.17 val PER: 0.0962
2025-12-08 06:21:00,601: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:21:00,601: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:21:00,601: t15.2024.05.10 val PER: 0.1174
2025-12-08 06:21:00,601: t15.2024.06.14 val PER: 0.1388
2025-12-08 06:21:00,601: t15.2024.07.19 val PER: 0.1740
2025-12-08 06:21:00,601: t15.2024.07.21 val PER: 0.0648
2025-12-08 06:21:00,601: t15.2024.07.28 val PER: 0.0971
2025-12-08 06:21:00,601: t15.2025.01.10 val PER: 0.2727
2025-12-08 06:21:00,601: t15.2025.01.12 val PER: 0.1055
2025-12-08 06:21:00,602: t15.2025.03.14 val PER: 0.3240
2025-12-08 06:21:00,602: t15.2025.03.16 val PER: 0.1545
2025-12-08 06:21:00,602: t15.2025.03.30 val PER: 0.2379
2025-12-08 06:21:00,602: t15.2025.04.13 val PER: 0.2368
2025-12-08 06:21:00,602: New best test PER 0.1187 --> 0.1159
2025-12-08 06:21:00,602: Checkpointing model
2025-12-08 06:21:01,720: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:21:31,966: Train batch 28200: loss: 0.45 grad norm: 10.72 time: 0.137
2025-12-08 06:22:01,852: Train batch 28400: loss: 0.26 grad norm: 7.41 time: 0.149
2025-12-08 06:22:31,637: Train batch 28600: loss: 0.87 grad norm: 42.69 time: 0.131
2025-12-08 06:23:01,368: Train batch 28800: loss: 0.44 grad norm: 14.17 time: 0.154
2025-12-08 06:23:31,612: Train batch 29000: loss: 0.09 grad norm: 3.52 time: 0.134
2025-12-08 06:24:01,420: Train batch 29200: loss: 0.25 grad norm: 8.20 time: 0.141
2025-12-08 06:24:31,036: Train batch 29400: loss: 0.20 grad norm: 12.41 time: 0.148
2025-12-08 06:25:01,089: Train batch 29600: loss: 0.10 grad norm: 6.63 time: 0.122
2025-12-08 06:25:30,993: Train batch 29800: loss: 0.07 grad norm: 3.74 time: 0.136
2025-12-08 06:26:01,480: Train batch 30000: loss: 0.31 grad norm: 10.32 time: 0.133
2025-12-08 06:26:01,480: Running test after training batch: 30000
2025-12-08 06:26:11,974: Val batch 30000: PER (avg): 0.1154 CTC Loss (avg): 23.3658 time: 10.494
2025-12-08 06:26:11,975: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:26:11,975: t15.2023.08.13 val PER: 0.0769
2025-12-08 06:26:11,975: t15.2023.08.18 val PER: 0.0704
2025-12-08 06:26:11,975: t15.2023.08.20 val PER: 0.0627
2025-12-08 06:26:11,975: t15.2023.08.25 val PER: 0.0828
2025-12-08 06:26:11,975: t15.2023.08.27 val PER: 0.1656
2025-12-08 06:26:11,975: t15.2023.09.01 val PER: 0.0479
2025-12-08 06:26:11,975: t15.2023.09.03 val PER: 0.1164
2025-12-08 06:26:11,975: t15.2023.09.24 val PER: 0.0922
2025-12-08 06:26:11,975: t15.2023.09.29 val PER: 0.1174
2025-12-08 06:26:11,975: t15.2023.10.01 val PER: 0.1420
2025-12-08 06:26:11,975: t15.2023.10.06 val PER: 0.0700
2025-12-08 06:26:11,975: t15.2023.10.08 val PER: 0.1854
2025-12-08 06:26:11,975: t15.2023.10.13 val PER: 0.1645
2025-12-08 06:26:11,975: t15.2023.10.15 val PER: 0.1220
2025-12-08 06:26:11,976: t15.2023.10.20 val PER: 0.1745
2025-12-08 06:26:11,976: t15.2023.10.22 val PER: 0.0980
2025-12-08 06:26:11,976: t15.2023.11.03 val PER: 0.1574
2025-12-08 06:26:11,976: t15.2023.11.04 val PER: 0.0239
2025-12-08 06:26:11,976: t15.2023.11.17 val PER: 0.0156
2025-12-08 06:26:11,976: t15.2023.11.19 val PER: 0.0140
2025-12-08 06:26:11,976: t15.2023.11.26 val PER: 0.0601
2025-12-08 06:26:11,976: t15.2023.12.03 val PER: 0.0620
2025-12-08 06:26:11,976: t15.2023.12.08 val PER: 0.0573
2025-12-08 06:26:11,976: t15.2023.12.10 val PER: 0.0368
2025-12-08 06:26:11,976: t15.2023.12.17 val PER: 0.1050
2025-12-08 06:26:11,976: t15.2023.12.29 val PER: 0.0769
2025-12-08 06:26:11,976: t15.2024.02.25 val PER: 0.0815
2025-12-08 06:26:11,976: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:26:11,976: t15.2024.03.08 val PER: 0.1664
2025-12-08 06:26:11,976: t15.2024.03.15 val PER: 0.1820
2025-12-08 06:26:11,976: t15.2024.03.17 val PER: 0.0872
2025-12-08 06:26:11,976: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:26:11,976: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:26:11,976: t15.2024.05.10 val PER: 0.1263
2025-12-08 06:26:11,977: t15.2024.06.14 val PER: 0.1230
2025-12-08 06:26:11,977: t15.2024.07.19 val PER: 0.1668
2025-12-08 06:26:11,977: t15.2024.07.21 val PER: 0.0634
2025-12-08 06:26:11,977: t15.2024.07.28 val PER: 0.0934
2025-12-08 06:26:11,977: t15.2025.01.10 val PER: 0.2617
2025-12-08 06:26:11,977: t15.2025.01.12 val PER: 0.1078
2025-12-08 06:26:11,977: t15.2025.03.14 val PER: 0.3491
2025-12-08 06:26:11,977: t15.2025.03.16 val PER: 0.1584
2025-12-08 06:26:11,977: t15.2025.03.30 val PER: 0.2322
2025-12-08 06:26:11,977: t15.2025.04.13 val PER: 0.2411
2025-12-08 06:26:11,977: New best test PER 0.1159 --> 0.1154
2025-12-08 06:26:11,977: Checkpointing model
2025-12-08 06:26:13,024: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:26:42,438: Train batch 30200: loss: 0.02 grad norm: 1.91 time: 0.101
2025-12-08 06:27:12,590: Train batch 30400: loss: 0.35 grad norm: 12.44 time: 0.113
2025-12-08 06:27:42,576: Train batch 30600: loss: 0.10 grad norm: 3.37 time: 0.121
2025-12-08 06:28:12,737: Train batch 30800: loss: 0.26 grad norm: 7.31 time: 0.123
2025-12-08 06:28:42,880: Train batch 31000: loss: 0.22 grad norm: 5.80 time: 0.158
2025-12-08 06:29:12,657: Train batch 31200: loss: 0.18 grad norm: 7.59 time: 0.153
2025-12-08 06:29:42,269: Train batch 31400: loss: 0.21 grad norm: 6.46 time: 0.118
2025-12-08 06:30:11,029: Train batch 31600: loss: 0.24 grad norm: 6.58 time: 0.129
2025-12-08 06:30:41,514: Train batch 31800: loss: 0.18 grad norm: 6.62 time: 0.164
2025-12-08 06:31:11,281: Train batch 32000: loss: 0.41 grad norm: 7.41 time: 0.096
2025-12-08 06:31:11,282: Running test after training batch: 32000
2025-12-08 06:31:21,795: Val batch 32000: PER (avg): 0.1114 CTC Loss (avg): 22.5734 time: 10.513
2025-12-08 06:31:21,795: t15.2023.08.11 val PER: 1.0000
2025-12-08 06:31:21,795: t15.2023.08.13 val PER: 0.0686
2025-12-08 06:31:21,795: t15.2023.08.18 val PER: 0.0679
2025-12-08 06:31:21,795: t15.2023.08.20 val PER: 0.0651
2025-12-08 06:31:21,795: t15.2023.08.25 val PER: 0.0723
2025-12-08 06:31:21,795: t15.2023.08.27 val PER: 0.1511
2025-12-08 06:31:21,796: t15.2023.09.01 val PER: 0.0438
2025-12-08 06:31:21,796: t15.2023.09.03 val PER: 0.1188
2025-12-08 06:31:21,796: t15.2023.09.24 val PER: 0.0910
2025-12-08 06:31:21,796: t15.2023.09.29 val PER: 0.1066
2025-12-08 06:31:21,796: t15.2023.10.01 val PER: 0.1480
2025-12-08 06:31:21,796: t15.2023.10.06 val PER: 0.0689
2025-12-08 06:31:21,796: t15.2023.10.08 val PER: 0.1813
2025-12-08 06:31:21,796: t15.2023.10.13 val PER: 0.1583
2025-12-08 06:31:21,796: t15.2023.10.15 val PER: 0.1200
2025-12-08 06:31:21,796: t15.2023.10.20 val PER: 0.1779
2025-12-08 06:31:21,796: t15.2023.10.22 val PER: 0.0902
2025-12-08 06:31:21,796: t15.2023.11.03 val PER: 0.1493
2025-12-08 06:31:21,796: t15.2023.11.04 val PER: 0.0205
2025-12-08 06:31:21,796: t15.2023.11.17 val PER: 0.0187
2025-12-08 06:31:21,796: t15.2023.11.19 val PER: 0.0279
2025-12-08 06:31:21,796: t15.2023.11.26 val PER: 0.0645
2025-12-08 06:31:21,796: t15.2023.12.03 val PER: 0.0483
2025-12-08 06:31:21,796: t15.2023.12.08 val PER: 0.0466
2025-12-08 06:31:21,796: t15.2023.12.10 val PER: 0.0368
2025-12-08 06:31:21,797: t15.2023.12.17 val PER: 0.0863
2025-12-08 06:31:21,797: t15.2023.12.29 val PER: 0.0741
2025-12-08 06:31:21,797: t15.2024.02.25 val PER: 0.0702
2025-12-08 06:31:21,797: t15.2024.03.03 val PER: 1.0000
2025-12-08 06:31:21,797: t15.2024.03.08 val PER: 0.1835
2025-12-08 06:31:21,797: t15.2024.03.15 val PER: 0.1751
2025-12-08 06:31:21,797: t15.2024.03.17 val PER: 0.0858
2025-12-08 06:31:21,797: t15.2024.04.25 val PER: 1.0000
2025-12-08 06:31:21,797: t15.2024.04.28 val PER: 1.0000
2025-12-08 06:31:21,797: t15.2024.05.10 val PER: 0.1218
2025-12-08 06:31:21,797: t15.2024.06.14 val PER: 0.1467
2025-12-08 06:31:21,797: t15.2024.07.19 val PER: 0.1714
2025-12-08 06:31:21,797: t15.2024.07.21 val PER: 0.0559
2025-12-08 06:31:21,797: t15.2024.07.28 val PER: 0.0846
2025-12-08 06:31:21,797: t15.2025.01.10 val PER: 0.2603
2025-12-08 06:31:21,797: t15.2025.01.12 val PER: 0.1078
2025-12-08 06:31:21,797: t15.2025.03.14 val PER: 0.3284
2025-12-08 06:31:21,797: t15.2025.03.16 val PER: 0.1505
2025-12-08 06:31:21,797: t15.2025.03.30 val PER: 0.2299
2025-12-08 06:31:21,797: t15.2025.04.13 val PER: 0.2111
2025-12-08 06:31:21,798: New best test PER 0.1154 --> 0.1114
2025-12-08 06:31:21,798: Checkpointing model
2025-12-08 06:31:22,926: Saved model to checkpoint: trained_models/baseline_rnn_v2_20251208_050855/checkpoint/best_checkpoint
2025-12-08 06:31:52,100: Train batch 32200: loss: 0.23 grad norm: 7.84 time: 0.110
2025-12-08 06:32:22,194: Train batch 32400: loss: 0.39 grad norm: 14.46 time: 0.097
2025-12-08 06:32:52,389: Train batch 32600: loss: 0.29 grad norm: 15.10 time: 0.113
2025-12-08 06:33:21,995: Train batch 32800: loss: 0.22 grad norm: 8.57 time: 0.120
2025-12-08 06:33:51,193: Train batch 33000: loss: 0.11 grad norm: 4.79 time: 0.135
2025-12-08 06:34:20,665: Train batch 33200: loss: 0.79 grad norm: 10.04 time: 0.106
2025-12-08 06:34:50,898: Train batch 33400: loss: 0.20 grad norm: 9.77 time: 0.175
2025-12-08 06:35:19,792: Train batch 33600: loss: 0.16 grad norm: 6.52 time: 0.136
