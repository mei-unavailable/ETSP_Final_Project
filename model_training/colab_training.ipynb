{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290e214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install omegaconf editdistance hydra-core bitarray\n",
    "# Install other requirements if needed (Colab usually has torch, numpy, etc.)\n",
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64336b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Data\n",
    "import os\n",
    "if not os.path.exists('../data/hdf5_data_final'):\n",
    "    print(\"Downloading data...\")\n",
    "    !python ../download_data.py\n",
    "else:\n",
    "    print(\"Data already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024123c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import logging\n",
    "from omegaconf import OmegaConf\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the current directory to path so we can import modules\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Import project modules\n",
    "from rnn_trainer import BrainToTextDecoder_Trainer\n",
    "from dataset import BrainToTextDataset, train_test_split_indicies\n",
    "from unet_model import NeuralUNet\n",
    "from train_ssl import get_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee19731",
   "metadata": {},
   "source": [
    "## 1. Train Supervised Model (RNN or Conformer)\n",
    "\n",
    "Configure the training parameters below. You can switch between `rnn` and `conformer` architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e5780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load default arguments\n",
    "args = OmegaConf.load('rnn_args.yaml')\n",
    "\n",
    "# --- Modify Configuration Here ---\n",
    "args.model.type = 'rnn'  # Options: 'rnn', 'conformer'\n",
    "args.gpu_number = '0'\n",
    "args.num_training_batches = 120000\n",
    "\n",
    "# Increase batch size to utilize more GPU memory\n",
    "# Default is 64. Try 128, 256, or 512 depending on your GPU memory (80GB can likely handle 256+)\n",
    "args.dataset.batch_size = 256 \n",
    "\n",
    "# For Conformer, you might want to adjust these:\n",
    "# args.model.type = 'conformer'\n",
    "# args.lr_max = 0.0005\n",
    "\n",
    "print(f\"Training Model: {args.model.type}\")\n",
    "print(f\"Batch Size: {args.dataset.batch_size}\")\n",
    "print(OmegaConf.to_yaml(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb71853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Training\n",
    "trainer = BrainToTextDecoder_Trainer(args)\n",
    "metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3b9516",
   "metadata": {},
   "source": [
    "## 2. Train Self-Supervised Model (U-Net)\n",
    "\n",
    "Train a Masked Autoencoder (MAE) using a U-Net architecture for feature learning/denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b11d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSL Hyperparameters\n",
    "MASK_RATIO = 0.5\n",
    "# Increase batch size for SSL as well\n",
    "BATCH_SIZE = 64 # Try 64, 128, or higher\n",
    "LR = 1e-4\n",
    "EPOCHS = 50\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SAVE_DIR = f'trained_models/unet_ssl_{timestamp}'\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training U-Net SSL on {DEVICE}\")\n",
    "print(f\"Saving to {SAVE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2609ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Dataset for SSL\n",
    "args = OmegaConf.load('rnn_args.yaml') # Load args to get data paths\n",
    "\n",
    "train_files, test_files = train_test_split_indicies(\n",
    "    os.path.join('../data', 't15_copyTaskData_description.csv'),\n",
    "    test_percentage=0.1\n",
    ")\n",
    "\n",
    "train_dataset = BrainToTextDataset(\n",
    "    train_files,\n",
    "    n_batches=200, # Smaller number for SSL iteration\n",
    "    split='train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    days_per_batch=1\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=None, num_workers=4)\n",
    "\n",
    "# Initialize Model\n",
    "model = NeuralUNet(n_channels=1, n_classes=1).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch in pbar:\n",
    "        # Data shape: (B, T, C)\n",
    "        neural_data = batch['neural_features'].to(DEVICE)\n",
    "        \n",
    "        # Generate mask\n",
    "        mask = get_mask(neural_data, MASK_RATIO, device=DEVICE)\n",
    "        \n",
    "        # Apply mask (simulate missing data)\n",
    "        # Note: In MAE, we usually replace masked patches with a learnable token or 0\n",
    "        # Here we just zero it out for simplicity as per original script logic implication\n",
    "        masked_input = neural_data * (1 - mask.squeeze(1))\n",
    "        \n",
    "        # Add channel dim for U-Net: (B, 1, T, C)\n",
    "        masked_input = masked_input.unsqueeze(1)\n",
    "        target = neural_data.unsqueeze(1)\n",
    "        \n",
    "        # Forward\n",
    "        output = model(masked_input)\n",
    "        \n",
    "        # Compute loss only on masked regions\n",
    "        loss = (criterion(output, target) * mask).sum() / (mask.sum() + 1e-6)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1} Average Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, f'unet_epoch_{epoch+1}.pth'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
